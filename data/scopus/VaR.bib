@article{Cobo.2011, 
year = {2011}, 
title = {{An approach for detecting, quantifying, and visualizing the evolution of a research field: A practical application to the Fuzzy Sets Theory field}}, 
author = {Cobo, M.J. and López-Herrera, A.G. and Herrera-Viedma, E. and Herrera, F.}, 
journal = {Journal of Informetrics}, 
issn = {1751-1577}, 
doi = {10.1016/j.joi.2010.10.002}, 
abstract = {{This paper presents an approach to analyze the thematic evolution of a given research field. This approach combines performance analysis and science mapping for detecting and visualizing conceptual subdomains (particular themes or general thematic areas). It allows us to quantify and visualize the thematic evolution of a given research field. To do this, co-word analysis is used in a longitudinal framework in order to detect the different themes treated by the research field across the given time period. The performance analysis uses different bibliometric measures, including the h-index, with the purpose of measuring the impact of both the detected themes and thematic areas. The presented approach includes a visualization method for showing the thematic evolution of the studied field.Then, as an example, the thematic evolution of the Fuzzy Sets Theory field is analyzed using the two most important journals in the topic: Fuzzy Sets and Systems and IEEE Transactions on Fuzzy Systems.}}, 
pages = {146--166}, 
number = {1}, 
volume = {5}, 
keywords = {}, 
local-url = {file://localhost/Users/barry/Documents/Papers%20Library/Cobo-An%20approach%20for%20detecting,%20quantifying,%20and%20visualizing%20the%20evolution%20of%20a%20research%20field-%20A%20practical%20application%20to%20the%20Fuzzy%20Sets%20Theory%20field-2011-Journal%20of%20Informetrics.pdf}
}
@article{10.1111/mafi.12192, 
year = {2019}, 
title = {{Value-at-Risk bounds with two-sided dependence information}}, 
author = {Lux, Thibaut and Rüschendorf, Ludger}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12192}, 
abstract = {{Value-at-Risk (VaR) bounds for aggregated risks have been derived in the literature in settings where, besides the marginal distributions of the individual risk factors, one-sided bounds for the joint distribution or the copula of the risks are available. In applications, it turns out that these improved standard bounds on VaR tend to be too wide to be relevant for practical applications, especially when the number of risk factors is large or when the dependence restriction is not strong enough. In this paper, we develop a method to compute VaR bounds when besides the marginal distributions of the risk factors, two-sided dependence information in form of an upper and a lower bound on the copula of the risk factors is available. The method is based on a relaxation of the exact dual bounds that we derive by means of the Monge–Kantorovich transportation duality. In several applications, we illustrate that two-sided dependence information typically leads to strongly improved bounds on the VaR of aggregations. © 2018 Wiley Periodicals, Inc.}}, 
pages = {967--1000}, 
number = {3}, 
volume = {29}
}
@article{10.1080/02533839.2021.1884602, 
year = {2021}, 
title = {{Applying value at risk and riskiness models to analyze the flood loss of transportation construction projects in Taiwan}}, 
author = {Chen, Po-Han and Peng, Tsu-Te and Chen, Chieh-Jan}, 
journal = {Journal of the Chinese Institute of Engineers}, 
issn = {02533839}, 
doi = {10.1080/02533839.2021.1884602}, 
abstract = {{Because of Taiwan’s specific geographical location, disastrous weather phenomena such as typhoons, extreme rainfall, flooding, and regional flooding occur frequently, resulting in devastating casualties and property loss. The losses caused by flooding are NT\$5.5 billion per year. This study used insurance claims data on flood damage in Taiwan’s transportation construction projects (from 1996 to 2007) as a sample space. Statistical actuarial model analysis was performed to normalize the loss factors, thereby enabling loss model testing and quantitative analysis of the normalized loss factors. To evaluate the risk in transportation construction, two models were separately applied in the analysis: the value at risk and riskiness models. The results may aid in estimating flooding damage losses in transportation construction projects. © 2021 The Chinese Institute of Engineers.}}, 
pages = {1--6}, 
number = {3}, 
volume = {44}
}
@article{10.1108/afr-06-2013-0027, 
year = {2014}, 
title = {{Are agribusiness stocks an investor safe haven?}}, 
author = {D’Antoni, Jeremy M. and Detre, Joshua Dean}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/afr-06-2013-0027}, 
abstract = {{Purpose – The purpose of this paper is to determine how an index of agribusiness stocks performs relative to the S\&P 500 particularly in times of recession. Design/methodology/approach – Using value-weighted indexes of agribusiness stocks, large cap US stocks, and copula estimation, the paper quantifies the correlation in potential investment portfolios. The information obtained from the copula estimated dependence measures and Value at Risk (VaR) allows to examine the diversification benefits of holding agribusiness stocks in the portfolio relative to the S\&P 500. Findings – The results provide limited evidence that the addition of agribusiness stocks to a portfolio are able to provide significant diversification benefits to a portfolio of domestic equities, as represented by the S\&P 500 index. The VaR analysis also indicates the risk of extreme losses remained relatively stable both across time and portfolio weightings. Research limitations/implications – While this research examines a broad-based agribusiness stock index, there exists a number of sub-assets classes within the analyzed index that should be analyzed to see if the offer benefits to investors. In addition, only stocks traded on US-based stock indexes are included in this analysis; as such, the authors would like to extend the research to have a more global approach. Practical implications – The findings suggest that investors who are looking to a broad-based agribusiness stock index to provide more diversification in their portfolio, may find it unattractive from a both a risk management and profit maximizing perspective. However, that does not mean that the agribusiness stock index might be an affective complement to a portfolio that contains multiple other assets classes. Originality/value – The issue of correlation convergence during financial crises is one of great concern to investors. To the authors’ knowledge, this is the first paper that uses copulas to evaluate the role of agribusiness stocks in an investor’s portfolio. © Emerald Group Publishing Limited.}}, 
pages = {522--538}, 
number = {4}, 
volume = {74}
}
@article{10.1002/asmb.2237, 
year = {2017}, 
title = {{Bayesian tail-risk forecasting using realized GARCH}}, 
author = {Contino, Christian and Gerlach, Richard H.}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2237}, 
abstract = {{A realized generalized autoregressive conditional heteroskedastic (GARCH) model is developed within a Bayesian framework for the purpose of forecasting value at risk and conditional value at risk. Student-t and skewed-t return distributions are combined with Gaussian and student-t distributions in the measurement equation to forecast tail risk in eight international equity index markets over a 4-year period. Three realized measures are considered within this framework. A Bayesian estimator is developed that compares favourably, in simulations, with maximum likelihood, both in estimation and forecasting. The realized GARCH models show a marked improvement compared with ordinary GARCH for both value-at-risk and conditional value-at-risk forecasting. This improvement is consistent across a variety of data and choice of distributions. Realized GARCH models incorporating a skewed student-t distribution for returns are favoured overall, with the choice of measurement equation error distribution and realized measure being of lesser importance. Copyright © 2017 John Wiley \& Sons, Ltd. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {213--236}, 
number = {2}, 
volume = {33}
}
@article{10.1016/j.ijforecast.2011.12.004, 
year = {2012}, 
title = {{Forecasting Value-at-Risk using nonlinear regression quantiles and the intra-day range}}, 
author = {Chen, Cathy W.S. and Gerlach, Richard and Hwang, Bruce B.K. and McAleer, Michael}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2011.12.004}, 
abstract = {{Some novel nonlinear threshold conditional autoregressive VaR (CAViaR) models are proposed that incorporate intra-day price ranges. Model estimation is performed using a Bayesian approach via the link with the Skewed-Laplace distribution. The performances of a range of risk models during the 2008-09 financial crisis are examined, including an evaluation of the way in which the crisis affected the performance of VaR forecasting. An empirical analysis is conducted on five Asia-Pacific Economic Cooperation stock market indices and two exchange rate series. Standard back-testing criteria are used to measure and assess the forecast performances of a variety of risk models. The proposed threshold CAViaR model, incorporating range information, is shown to forecast VaR more effectively and more accurately than other models, across the series considered. © 2012 International Institute of Forecasters.}}, 
pages = {557--574}, 
number = {3}, 
volume = {28}
}
@article{10.1016/j.ijforecast.2017.11.007, 
year = {2018}, 
title = {{An approximate long-memory range-based approach for value at risk estimation}}, 
author = {Meng, Xiaochun and Taylor, James W.}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2017.11.007}, 
abstract = {{This paper proposes new approximate long-memory VaR models that incorporate intra-day price ranges. These models use lagged intra-day range with the feature of considering different range components calculated over different time horizons. We also investigate the impact of the market overnight return on the VaR forecasts, which has not yet been considered with the range in VaR estimation. Model estimation is performed using linear quantile regression. An empirical analysis is conducted on 18 market indices. In spite of the simplicity of the proposed methods, the empirical results show that they successfully capture the main features of the financial returns and are competitive with established benchmark methods. The empirical results also show that several of the proposed range-based VaR models, utilizing both the intra-day range and the overnight returns, are able to outperform GARCH-based methods and CAViaR models. © 2018}}, 
pages = {377--388}, 
number = {3}, 
volume = {34}
}
@article{10.3390/risks4040048, 
year = {2016}, 
title = {{How does reinsurance create value to an insurer? A cost-benefit analysis incorporating default risk}}, 
author = {Lo, Ambrose}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks4040048}, 
abstract = {{Reinsurance is often empirically hailed as a value-adding risk management strategy which an insurer can utilize to achieve various business objectives. In the context of a distortion-risk-measure-based three-party model incorporating a policyholder, insurer and reinsurer, this article formulates explicitly the optimal insurance–reinsurance strategies from the perspective of the insurer. Our analytic solutions are complemented by intuitive but scientifically rigorous explanations on the marginal cost and benefit considerations underlying the optimal insurance–reinsurance decisions. These cost-benefit discussions not only cast light on the economic motivations for an insurer to engage in insurance with the policyholder and in reinsurance with the reinsurer, but also mathematically formalize the value created by reinsurance with respect to stabilizing the loss portfolio and enlarging the underwriting capacity of an insurer. Our model also allows for the reinsurer’s failure to deliver on its promised indemnity when the regulatory capital of the reinsurer is depleted by the reinsured loss. The reduction in the benefits of reinsurance to the insurer as a result of the reinsurer’s default is quantified, and its influence on the optimal insurance–reinsurance policies analyzed. © 2016 by the author; licensee MDPI, Basel, Switzerland.}}, 
pages = {48}, 
number = {4}, 
volume = {4}
}
@article{10.1109/cso.2011.188, 
year = {2011}, 
title = {{Optimal portfolios with power and log utilities}}, 
author = {Xu, Yongjia and Lai, Yongzeng and Xi, Xiaojing}, 
journal = {2011 Fourth International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2011.188}, 
abstract = {{This paper discusses portfolio optimization problems under Gaussian models using Capital-at-Risk and Earning-at-Risk as risk measures with power and log utilities. Explicit expressions of Capital-at-Risk and Earning-at-Risk for both utility functions are obtained, so are conditions satisfied by the corresponding optimal portfolios. © 2011 IEEE.}}, 
pages = {1271--1275}, 
number = {NA}, 
volume = {NA}
}
@article{10.1260/095830503322663375, 
year = {2003}, 
title = {{Including investment risk in large-scale power-market models}}, 
author = {Lemming, Jacob and Meibom, Peter}, 
journal = {Energy \& Environment}, 
issn = {0958305X}, 
doi = {10.1260/095830503322663375}, 
abstract = {{Long-term energy market models can be used to examine investments in production technologies, however, with market liberalisation it is crucial that such models include investment risks and investor behaviour. This paper analyses how the effect of investment risk on production technology selection can be included in large-scale partial equilibrium models of the power market. The analyses are divided into a part about risk measures appropriate for power market investors and a more technical part about the combination of a risk-adjustment model and a partial-equilibrium model. To illustrate the analyses quantitatively, a framework based on an iterative interaction between the equilibrium model and a separate risk-adjustment module was constructed. To illustrate the features of the proposed modelling approach we examined how uncertainty in demand and variable costs affects the optimal choice of production technologies.}}, 
pages = {599--626}, 
number = {5}, 
volume = {14}
}
@article{10.1016/s0378-4266(02)00271-6, 
year = {2002}, 
title = {{Conditional value-at-risk for general loss distributions}}, 
author = {Rockafellar, R.Tyrrell and Uryasev, Stanislav}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00271-6}, 
abstract = {{Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {1443--1471}, 
number = {7}, 
volume = {26}
}
@article{10.1002/for.1251, 
year = {2013}, 
title = {{Using CAViaR models with implied volatility for value-at-risk estimation}}, 
author = {Jeon, Jooyoung and Taylor, James W.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1251}, 
abstract = {{This paper proposes value-at risk (VaR) estimation methods that are a synthesis of conditional autoregressive value at risk (CAViaR) time series models and implied volatility. The appeal of this proposal is that it merges information from the historical time series and the different information supplied by the market's expectation of risk. Forecast-combining methods, with weights estimated using quantile regression, are considered. We also investigate plugging implied volatility into the CAViaR modelsâa procedure that has not been considered in the VaR area so far. Results for daily index returns indicate that the newly proposed methods are comparable or superior to individual methods, such as the standard CAViaR models and quantiles constructed from implied volatility and the empirical distribution of standardised residuals. We find that the implied volatility has more explanatory power as the focus moves further out into the left tail of the conditional distribution of S\&P 500 daily returns. Copyright © 2012 John Wiley \& Sons, Ltd. Copyright © 2012 John Wiley \& Sons, Ltd.}}, 
pages = {62--74}, 
number = {1}, 
volume = {32}
}
@article{10.1109/access.2019.2946260, 
year = {2019}, 
title = {{Bayesian Inference for Optimal Risk Hedging Strategy Using Put Options with Stock Liquidity}}, 
author = {Gao, Rui and Li, Yaqiong and Bai, Yanfei and Hong, Shanlan}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2019.2946260}, 
abstract = {{This paper considers the problem of hedging the risk exposure to imperfectly liquid stock by investing in put options. In an incomplete market, we firstly obtain a closed-form pricing formula of the European put option with liquidity-adjustment by measure transformation. Then, an optimal hedging strategy which minimizes the Value-at-Risk (VaR) of the hedged portfolio is deduced by determining an optimal strike price for the put option. Furthermore, we provide a new perspective to estimate parameters entering the minimal VaR, since the likelihood function is analytically intractable. A Bayesian statistical method is proposed to perform posterior inference on the minimal VaR and the optimal strike price. Empirical results show that the risk hedging strategy with liquidity-adjustment differs from the hedging strategy based on Black-Scholes model. The effect of the stock liquidity on risk hedging strategy is significant. These results can provide more decision information for institutions and investors with different risk preferences to avoid risk. © 2019 IEEE.}}, 
pages = {146046--146056}, 
number = {NA}, 
volume = {7}
}
@article{10.1049/cp.2009.1033, 
year = {2009}, 
title = {{Risk assessment of distribution system: real case application of value at risk metrics}}, 
author = {Schreiner, A and Balzer, G and Precht, A}, 
journal = {IET Conference Publications}, 
issn = {NA}, 
doi = {10.1049/cp.2009.1033}, 
abstract = {{The risk oriented asset management is a new trend for optimization of investments in distribution grids. The approach is based on the derivation of operational risk metrics in accordance with the reliability indices of the system components (or assets) and outage costs. The big challenge of risk assessment methods is to find out the appropriated risk metric which equally reflects the severity and the likelihood of some contingences of considered distribution systems. The wide used metric of risk assessment in the finance and insurance industries as well as nowadays in energy trading is the Value at Risk method (VaR). VaR is the large applied metric for estimation of losses within defined confidence limits and considered time period. The proposed approach is based on the Loss Distribution Approach (LDA) and VaR derivation as measure for risk. Application of LDA and VaR has its roots in a traditional field of insurance risk theory.}}, 
pages = {862--862}, 
number = {550 CP}, 
volume = {NA}
}
@article{10.1017/s1748499519000113, 
year = {2020}, 
title = {{Risk management with Tail Quasi-Linear Means}}, 
author = {Bäuerle, Nicole and Shushi, Tomer}, 
journal = {Annals of Actuarial Science}, 
issn = {17484995}, 
doi = {10.1017/s1748499519000113}, 
eprint = {1902.06941}, 
abstract = {{We generalise Quasi-Linear Means by restricting to the tail of the risk distribution and show that this can be a useful quantity in risk management since it comprises in its general form the Value at Risk, the Conditional Tail Expectation and the Entropic Risk Measure in a unified way. We then investigate the fundamental properties of the proposed measure and show its unique features and implications in the risk measurement process. Furthermore, we derive formulas for truncated elliptical models of losses and provide formulas for selected members of such models. © Institute and Faculty of Actuaries 2019.}}, 
pages = {170--187}, 
number = {1}, 
volume = {14}
}
@article{10.1016/j.jempfin.2017.11.010, 
year = {2018}, 
title = {{A factor-based approach of bond portfolio value-at-risk: The informational roles of macroeconomic and financial stress factors}}, 
author = {Tu, Anthony H. and Chen, Cathy Yi-Hsuan}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2017.11.010}, 
abstract = {{Based on the Nelson–Siegel term structure framework, we develop a new factor-augmented model for the computation of the value-at-risk (VaR) of bond portfolios, and examine whether the inclusion of information contained within macroeconomic variables and financial stress shocks can enhance the accuracy of VaR forecasts. We examine three Citi US bond indices and the empirical results reveal that: (1) based upon the geometric-VaR backtest, proposed by Pelletier and Wei (2016), the new factor-augmented approach provides reasonably accurate VaR forecasts; (2) there is a clear tendency toward better VaR forecasting performance as a result of the inclusion of the macroeconomic variables and financial stress shocks in the Nelson–Siegel factor model; (3) the impact of the inclusion of financial stress shocks appears to be stronger than the impact of the inclusion of the macroeconomic variables. © 2017 Elsevier B.V.}}, 
pages = {243--268}, 
number = {NA}, 
volume = {45}
}
@article{10.1057/s41283-018-0036-1, 
year = {2018}, 
title = {{Risk and return of a trend-chasing application in financial markets: An empirical test}}, 
author = {Ilomäki, Jukka}, 
journal = {Risk Management}, 
issn = {14603799}, 
doi = {10.1057/s41283-018-0036-1}, 
abstract = {{The paper introduces an application of the moving average trend-chasing rule that effectively reduces the risk of portfolios. The results are fairly robust: all our moving average lags produce about 36\% (34\%) less Value-at-Risk and about 31\% (30\%) less expected shortfall without giving up any returns on average after transaction costs compared to the buy-and-hold strategy, calculated in local currencies (in U.S. dollars). In addition, the paper finds that the volatility of returns follows a similar pattern by producing on average 29\% (30\%) less volatility in local currencies (in U.S. dollars). Moreover, the CAPM betas of the trading rule are significantly lower (50\%) than in the buy-and-hold strategy. © 2018 Macmillan Publishers Ltd., part of Springer Nature.}}, 
pages = {258--272}, 
number = {3}, 
volume = {20}
}
@article{10.1515/snde-2012-0021, 
year = {2013}, 
title = {{Stochastic volatility model with regime-switching skewness in heavy-tailed errors for exchange rate returns}}, 
author = {Nakajima, Jouchi}, 
journal = {Studies in Nonlinear Dynamics and Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2012-0021}, 
abstract = {{A Bayesian analysis of the stochastic volatility model with regime-switching skewness in heavy-tailed errors is proposed using a generalized hyperbolic (GH) skew Student's t-distribution. The skewness parameter is allowed to shift according to a first-order Markov switching process. We summarize Bayesian methods for model fitting and discuss analyses of exchange rate return time series. Empirical results show that interpretable regime-switching skewness can improve model fit and Value-at-Risk performance in a comparison against several other SV models with constant skewness or jump diffusions. © 2013 by Walter de Gruyter Berlin Boston.}}, 
pages = {499--520}, 
number = {5}, 
volume = {17}
}
@article{10.1109/icmss.2009.5304560, 
year = {2009}, 
title = {{An optimized model of supply chain network structure based on CVaR constraint}}, 
author = {Changbao, Zhong and Xiaoping, Wei}, 
journal = {2009 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2009.5304560}, 
abstract = {{The past optimized model of SC network structure always choose not the risk, but rather the cost or customer satisfaction merely as the decision goal, which is not correspondent with practical decision. In order to supply scientific optimized model for decision makers preferring different risks, we introduced CVaR(conditonal value-at-risk), a kind of recently developed instrument for measuring financial risks, to study the optimization of risk-avoiding SC network structure and to measure the risk of SC network. We also established a multi-objective optimized model taking respectively the risk and the cost of SC network as its decision goal to make the SC network built on this model more competitive. ©2009 Crown.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.matcom.2012.06.013, 
year = {2013}, 
title = {{Risk management of risk under the Basel Accord: A Bayesian approach to forecasting Value-at-Risk of VIX futures}}, 
author = {Casarin, Roberto and Chang, Chia-Lin and Jimenez-Martin, Juan-Angel and McAleer, Michael and Pérez-Amaral, Teodosio}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2012.06.013}, 
abstract = {{It is well known that the Basel II Accord requires banks and other Authorized Deposit-taking Institutions (ADIs) to communicate their daily risk forecasts to the appropriate monetary authorities at the beginning of each trading day, using one or more risk models, whether individually or as combinations, to measure Value-at-Risk (VaR). The risk estimates of these models are used to determine capital requirements and associated capital costs of ADIs, depending in part on the number of previous violations, whereby realised losses exceed the estimated VaR. Previous papers proposed a new approach to model selection for predicting VaR, consisting of combining alternative risk models, and comparing conservative and aggressive strategies for choosing between VaR models. This paper, using Bayesian and non-Bayesian combinations of models addresses the question of risk management of risk, namely VaR of VIX futures prices, and extends the approaches given in previous papers to examine how different risk management strategies performed during the 2008-2009 global financial crisis (GFC). The use of time-varying weights using Bayesian methods, allows dynamic combinations of the different models to obtain a more accurate VaR forecasts than the estimates and forecasts that might be produced by a single model of risk. One of these dynamic combinations is endogenously determined by the pass performance in terms of daily capital charges of the individual models. This can improve the strategies to minimize daily capital charges, which is a central objective of ADIs. The empirical results suggest that an aggressive strategy of choosing the Supremum of single model forecasts, as compared with Bayesian and non-Bayesian combinations of models, is preferred to other alternatives, and is robust during the GFC. © 2012 IMACS.}}, 
pages = {183--204}, 
number = {NA}, 
volume = {94}
}
@article{10.1016/j.iref.2014.09.002, 
year = {2015}, 
title = {{Measuring sovereign risk contagion in the Eurozone}}, 
author = {Suh, Sangwon}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2014.09.002}, 
abstract = {{This paper proposes new measures of financial contagion, as observed during the recent Eurozone sovereign debt crisis. The new measures, referred to as contagion Value-at-Risk and contagion Expected Shortfall, are based on popular risk exposure measures and therefore can provide useful practical information for investors. For this purpose, we construct a new model that disentangles contagion from interdependence. We find that contagion effects fluctuate dynamically, sometimes greatly deviating from mean levels. In addition, the economic value of contagion proves to be quite large, even in stable economies. © 2014 Elsevier Inc.}}, 
pages = {45--65}, 
number = {NA}, 
volume = {35}
}
@article{10.3846/16111699.2017.1357049, 
year = {2017}, 
title = {{Hull-White’s value at risk model: case study of Baltic equities market}}, 
author = {Radivojević, Nikola and Ćurčić, Nikola V. and Vukajlović, Djurdjica Dj.}, 
journal = {Journal of Business Economics and Management}, 
issn = {16111699}, 
doi = {10.3846/16111699.2017.1357049}, 
abstract = {{Analysis of the applicability of the Hull and White (FHS) model on the Baltic equities market has not been the subject of significant research, especially not in the context of meeting the Basel Committee backtesting rules. The paper discusses the applicability of different variants of this model, in order to answer the question whether any variants (and which of them) of the model can be used in these markets in the context of the Basel II and III standards. The survey results show that 1) there isn't an optimal variant of this model, but that risk managers have to keep in mind stylized facts of financial returns when they specify the FHS model; 2) according to different criteria of the validity of the model (Basel II and III standards) different variants of models are differently ranked, which suggests that selection of a suitable model implies the use of a large number of different criteria, the model validity and loss function, especially those who take care of the size of tail loss and ES. © 2017 Vilnius Gediminas Technical University (VGTU) Press.}}, 
pages = {1023–1041--1023–1041}, 
number = {5}, 
volume = {18}
}
@article{10.1016/j.najef.2013.02.007, 
year = {2013}, 
title = {{Stress testing correlation matrices for risk management}}, 
author = {So, Mike K.P. and Wong, Jerry and Asai, Manabu}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2013.02.007}, 
abstract = {{Evaluating portfolio risk typically requires that correlation estimates of security returns be made. Historical financial events have shown that correlations can rise quickly, causing a huge increase in portfolio risk. Therefore, in stress testing portfolios, it is important to consider the influence of a sudden surge in selected correlations. Standard correlation stress testing mechanisms require us to change the selected correlations to designated values. However, the correlation matrix may become non-positive definite after some of its entries are altered. This paper proposes a blocking method by which an existing correlation matrix can be converted to incorporate change while keeping the matrix positive definite. In comparison with existing methods that usually only achieve semi-positive definiteness, the proposed method outperforms in the revised elements, while the approximation error of the non-revised elements is maintained within acceptable levels. Simulations show that our method is efficient and performs well for dimensions of 100, 500 and 1000. Our method is also shown to be more reliable in stress testing higher dimension correlation matrices. Information on the performance of the blocking method using a high-dimensional real data is also provided. © 2013 Elsevier Inc.}}, 
pages = {310--322}, 
number = {NA}, 
volume = {26}
}
@article{10.1016/j.ejor.2013.04.033, 
year = {2013}, 
title = {{Decision tree analysis for a risk averse decision maker: CVaR Criterion}}, 
author = {Eskandarzadeh, Saman and Eshghi, Kourosh}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2013.04.033}, 
abstract = {{Risk aversion is a prevalent phenomenon when sufficiently large amounts are at risk. In this paper, we introduce a new prescriptive approach for coping with risk in sequential decision problems with discrete scenario space. We use Conditional Value-at-Risk (CVaR) risk measure as optimization criterion and prove that there is an explicit linear representation of the proposed model for the problem. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {131--140}, 
number = {1}, 
volume = {231}
}
@article{10.1093/imaman/dph031, 
year = {2005}, 
title = {{Theory of optimal consumption and portfolio selection under a Capital-at-Risk (CaR) and a Value-at-Risk (VaR) constraint}}, 
author = {Atkinson, C}, 
journal = {IMA Journal of Management Mathematics}, 
issn = {1471678X}, 
doi = {10.1093/imaman/dph031}, 
abstract = {{The solution to the optimal portfolio selection and consumption rule subject to Capital-at-Risk and Value-at-Risk constraints is derived via the use of stochastic dynamic programming. © Institute of Mathematics and its Applications 2005; all rights reserved.}}, 
pages = {37--70}, 
number = {1}, 
volume = {16}
}
@article{10.1016/j.asoc.2016.08.003, 
year = {2016}, 
title = {{Quantile autoregression neural network model with applications to evaluating value at risk}}, 
author = {Xu, Qifa and Liu, Xi and Jiang, Cuixia and Yu, Keming}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2016.08.003}, 
abstract = {{We develop a new quantile autoregression neural network (QARNN) model based on an artificial neural network architecture. The proposed QARNN model is flexible and can be used to explore potential nonlinear relationships among quantiles in time series data. By optimizing an approximate error function and standard gradient based optimization algorithms, QARNN outputs conditional quantile functions recursively. The utility of our new model is illustrated by Monte Carlo simulation studies and empirical analyses of three real stock indices from the Hong Kong Hang Seng Index (HSI), the US S\&P500 Index (S\&P500) and the Financial Times Stock Exchange 100 Index (FTSE100). © 2016 Elsevier B.V.}}, 
pages = {1--12}, 
number = {NA}, 
volume = {49}
}
@article{10.1016/s0167-6687(99)00036-0, 
year = {1999}, 
title = {{A synthesis of risk measures for capital adequacy}}, 
author = {Wirch, Julia Lynn and Hardy, Mary R.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/s0167-6687(99)00036-0}, 
abstract = {{We discuss the concept of the risk measure as an expectation using a probability distortion, and classify the standard risk measures according to their associated distortion functions. Using two examples, we explore the features of the different measures. © 1999 Elsevier Science B.V.}}, 
pages = {337--347}, 
number = {3}, 
volume = {25}
}
@article{10.1093/jjfinec/nbx019, 
year = {2017}, 
title = {{Forecasting Value-at-Risk under temporal and portfolio aggregation}}, 
author = {Kole, Erik and Markwat, Thijs and Opschoor, Anne and Dijk, Dick van}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbx019}, 
abstract = {{We examine the impact of temporal and portfolio aggregation on the quality of Value-at-Risk (VaR) forecasts over a horizon of 10 trading days for a well-diversified portfolio of stocks, bonds and alternative investments. The VaR forecasts are constructed based on daily, weekly, or biweekly returns of all constituent assets separately, gathered into portfolios based on asset class, or into a single portfolio. We compare the impact of aggregation with that of choosing a model for the conditional volatilities and correlations, the distribution for the innovations, and the method of forecast construction. We find that the level of temporal aggregation is most important. Daily returns form the best basis for VaR forecasts. Modeling the portfolio at the asset or asset class level works better than complete portfolio aggregation, but differences are smaller. The differences from the model, distribution, and forecast choices are also smaller compared with temporal aggregation. © The Author, 2017. Published by Oxford University Press. All rights reserved.}}, 
pages = {649--677}, 
number = {4}, 
volume = {15}
}
@article{10.1016/j.spl.2013.04.016, 
year = {2013}, 
title = {{Risk measures for skew normal mixtures}}, 
author = {Bernardi, Mauro}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2013.04.016}, 
abstract = {{In this paper we show that linear combinations of multivariate skew normal mixtures can be represented as finite mixtures of univariate skew normals. Based on this result we provide an analytical formula for some well known risk measures. © 2013 Elsevier B.V.}}, 
pages = {1819--1824}, 
number = {8}, 
volume = {83}
}
@article{10.1111/j.1468-036x.2008.00467.x, 
year = {2009}, 
title = {{A new approach for using lévy processes for determining high-frequency value-at-risk predictions}}, 
author = {Sun, Wei and Rachev, Svetlozar and Fabozzi, Frank J.}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/j.1468-036x.2008.00467.x}, 
abstract = {{A new approach for using Lévy processes to compute value-at-risk (VaR) using high-frequency data is presented in this paper. The approach is a parametric model using an ARMA(1,1)-GARCH(1,1) model where the tail events are modelled using fractional Lévy stable noise and Lévy stable distribution. Using high-frequency data for the German DAX Index, the VaR estimates from this approach are compared to those of a standard nonparametric estimation method that captures the empirical distribution function, and with models where tail events are modelled using Gaussian distribution and fractional Gaussian noise. The results suggest that the proposed parametric approach yields superior predictive performance. © 2008 Blackwell Publishing Ltd.}}, 
pages = {340--361}, 
number = {2}, 
volume = {15}
}
@article{10.1007/s11464-013-0278-x, 
year = {2013}, 
title = {{VaR Criteria for optimal limited change-loss and truncated change-loss reinsurance}}, 
author = {Ma, Xiaojing and Wu, Lan}, 
journal = {Frontiers of Mathematics in China}, 
issn = {16733452}, 
doi = {10.1007/s11464-013-0278-x}, 
abstract = {{Reinsurance can provide an effective way for insurer to manage its risk exposure. In this paper, we further analyze the optimal reinsurance models recently proposed by J. Cai and K. S. Tan [Astin Bulletin, 2007, 37(1): 93-112]. With the criteria of minimizing the value-at-risk (VaR) risk measure of insurer's total loss exposure, we derive the optimal values of sharing proportion a, retention d, and layer l of two reinsurance treaties: the limited change-loss f(x) = a\{(x - d)+ - (x - l)+\} and the truncated change-loss f(x) = a(x-d)+I(x≤l). Both of the reinsurance plans have been considered to be more realistic and practical in the real business. Our solutions have several appealing features: (i) there is only one condition to verify for the existence of optimal limited change-loss reinsurance while there always exists an optimal truncated change-loss reinsurance, (ii) the resulting optimal parameters have simple analytic forms which depend only on assumed loss distribution, reinsurer's safety loading, and insurer's risk tolerance, (iii) the optimal retention d for limited change-loss reinsurance is the same as that by Cai and Tan while the optimal value is smaller for truncated change-loss, (iv) the optimal sharing proportion and layer are always the same for both reinsurance plans, (v) minimized VaR are strictly lower than the values derived by Cai and Tan, (vi) the optimization results reveal possible drawbacks of VaR-based risk management that a heavy tail risk exposure may be expressed by lower VaR. © 2013 Higher Education Press and Springer-Verlag Berlin Heidelberg.}}, 
pages = {583--608}, 
number = {3}, 
volume = {8}
}
@article{10.1287/opre.1120.1072, 
year = {2012}, 
title = {{Conditional value-at-risk and average value-at-risk: Estimation and asymptotics}}, 
author = {Chun, So Yeon and Shapiro, Alexander and Uryasev, Stan}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1120.1072}, 
abstract = {{We discuss linear regression approaches to the estimation of law-invariant conditional risk measures. Two estimation procedures are considered and compared; one is based on residual analysis of the standard least-squares method, and the other is in the spirit of the M-estimation approach used in robust statistics. In particular, value-at-risk and average valueat- risk measures are discussed in detail. Large sample statistical inference of the estimators is derived. Furthermore, finite sample properties of the proposed estimators are investigated and compared with theoretical derivations in an extensive Monte Carlo study. Empirical results on the real data (different financial asset classes) are also provided to illustrate the performance of the estimators. © 2012 INFORMS.}}, 
pages = {739--756}, 
number = {4}, 
volume = {60}
}
@article{10.1016/j.jeconom.2009.05.004, 
year = {2009}, 
title = {{Regression density estimation using smooth adaptive Gaussian mixtures}}, 
author = {Villani, Mattias and Kohn, Robert and Giordani, Paolo}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2009.05.004}, 
abstract = {{We model a regression density flexibly so that at each value of the covariates the density is a mixture of normals with the means, variances and mixture probabilities of the components changing smoothly as a function of the covariates. The model extends the existing models in two important ways. First, the components are allowed to be heteroscedastic regressions as the standard model with homoscedastic regressions can give a poor fit to heteroscedastic data, especially when the number of covariates is large. Furthermore, we typically need fewer components, which makes it easier to interpret the model and speeds up the computation. The second main extension is to introduce a novel variable selection prior into all the components of the model. The variable selection prior acts as a self-adjusting mechanism that prevents overfitting and makes it feasible to fit flexible high-dimensional surfaces. We use Bayesian inference and Markov Chain Monte Carlo methods to estimate the model. Simulated and real examples are used to show that the full generality of our model is required to fit a large class of densities, but also that special cases of the general model are interesting models for economic data. © 2009 Elsevier B.V.}}, 
pages = {155--173}, 
number = {2}, 
volume = {153}
}
@article{10.1177/0972150920927357, 
year = {2020}, 
title = {{Idiosyncrasies of Intraday Risk in Emerging and Developed Markets: Efficacy of the MCS-GARCH Model and Extreme Value Theory}}, 
author = {Banerjee, Aditya and Paul, Samit}, 
journal = {Global Business Review}, 
issn = {09721509}, 
doi = {10.1177/0972150920927357}, 
abstract = {{This article aims to study the effectiveness of the multi-step multiplicative component generalized autoregressive conditional heteroscedasticity (MCS-GARCH) model in forecasting intraday value at risk (VaR) and expected shortfall (ES) for both emerging and developed markets. The intraday VaR- and ES-forecasting performance of the MCS-GARCH model is compared with that of three other models. Five-minute returns data of stock market indices from four different countries are used in this study: India, China, the USA and the UK. The VaR and ES quantiles are estimated with the help of conditional extreme value theory (EVT) framework as well as with simple normal distribution assumption. The backtesting of VaR and ES suggest that the conditional EVT models provide superior forecasts compared to their GARCH counterparts. However, the accuracy of these models differs between emerging and developed markets. The multi-step MCS-GARCH or its EVT counterpart fails to outperform other competing models consistently, especially in emerging markets. The inability of the MCS-GARCH model in forecasting intraday risk accurately suggests that the multiple steps in the estimation of this model may lead to the errors-in-variables problem. The results of this study are unique in their suggestion that multi-step models may not be a better option in the estimation of VaR and ES. © 2020 International Management Institute, New Delhi.}}, 
pages = {097215092092735}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2008.09.005, 
year = {2008}, 
title = {{Nonparametric estimation of conditional VaR and expected shortfall}}, 
author = {Cai, Zongwu and Wang, Xian}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2008.09.005}, 
abstract = {{This paper considers a new nonparametric estimation of conditional value-at-risk and expected shortfall functions. Conditional value-at-risk is estimated by inverting the weighted double kernel local linear estimate of the conditional distribution function. The nonparametric estimator of conditional expected shortfall is constructed by a plugging-in method. Both the asymptotic normality and consistency of the proposed nonparametric estimators are established at both boundary and interior points for time series data. We show that the weighted double kernel local linear conditional distribution estimator has the advantages of always being a distribution, continuous, and differentiable, besides the good properties from both the double kernel local linear and weighted Nadaraya-Watson estimators. Moreover, an ad hoc data-driven fashion bandwidth selection method is proposed, based on the nonparametric version of the Akaike information criterion. Finally, an empirical study is carried out to illustrate the finite sample performance of the proposed estimators.}}, 
pages = {120--130}, 
number = {1}, 
volume = {147}
}
@article{10.1016/j.ejor.2017.09.009, 
year = {2018}, 
title = {{Computing near-optimal Value-at-Risk portfolios using integer programming techniques}}, 
author = {Babat, Onur and Vera, Juan C. and Zuluaga, Luis F.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2017.09.009}, 
eprint = {2107.07339}, 
abstract = {{Value-at-Risk (VaR) is one of the main regulatory tools used for risk management purposes. However, it is difficult to compute optimal VaR portfolios; that is, an optimal risk-reward portfolio allocation using VaR as the risk measure. This is due to VaR being non-convex and of combinatorial nature. In particular, it is well-known that the VaR portfolio problem can be formulated as a mixed-integer linear program (MILP) that is difficult to solve with current MILP solvers for medium to large-scale instances of the problem. Here, we present an algorithm to compute near-optimal VaR portfolios that takes advantage of this MILP formulation and provides a guarantee of the solution's near-optimality. As a byproduct, we obtain an algorithm to compute tight upper bounds on the VaR portfolio problem that outperform related algorithms proposed in the literature for this purpose. The near-optimality guarantee provided by the proposed algorithm is obtained thanks to the relation between minimum risk portfolios satisfying a reward benchmark and the corresponding maximum reward portfolios satisfying a risk benchmark. These alternate formulations of the portfolio allocation problem have been frequently studied in the case of convex risk measures and concave reward functions. Here, this relationship is considered for general risk measures and reward functions. To illustrate the efficiency of the presented algorithm, numerical results are presented using historical asset returns from the US financial market. © 2017 Elsevier B.V.}}, 
pages = {304--315}, 
number = {1}, 
volume = {266}
}
@article{10.1007/s00181-020-01905-4, 
year = {2021}, 
title = {{Backtesting and estimation error: value-at-risk overviolation rate}}, 
author = {Tsafack, Georges and Cataldo, James}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-020-01905-4}, 
abstract = {{Financial institutions and regulators use value-at-risk (VaR) and related measures as a tool for financial risk management. It is therefore critical to appropriately assess the quality of VaR forecasts and reporting. The VaR estimation error creates an additional source of imprecision. We show that even an unbiased estimator of VaR is likely to produce a systematic overviolation. We then propose an adjustment to account for the issue. A Monte Carlo study illustrates the overviolation problem and the effectiveness of the adjustment. An application to Fama–French portfolios returns series highlights the need to further account for tail behavior in the data. Applying the adjustment to the normal distribution performs relatively well for a less prudential level (5\% VaR), but is unable to provide enough buffer to overcome the overviolation for more prudential levels (1\% or 0.5\%VaR). Using the empirical distribution for more prudential levels improves risk forecasts. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {1351--1396}, 
number = {3}, 
volume = {61}
}
@article{10.1016/j.insmatheco.2019.02.007, 
year = {2019}, 
title = {{Analysis of risk bounds in partially specified additive factor models}}, 
author = {Rüschendorf, L.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2019.02.007}, 
abstract = {{The study of worst case scenarios for risk measures (e.g. the Value at Risk) when the underlying risk vector (or portfolio of risks) is not completely specified is a central topic in the literature on robust risk measurement. In this paper we discuss partially specified factor models as introduced in Bernard et al. (2017) in more detail for the class of additive factor models which admit more explicit results. These results allow to describe in more detail the reduction of risk bounds obtainable by this method in dependence on the degree of positive resp. negative dependence induced by the systematic risk factors. The insight may help in applications of this reduction method to get a better qualitative impression on the range of influence of the partially specified factor structure. © 2019 Elsevier B.V.}}, 
pages = {115--121}, 
number = {NA}, 
volume = {86}
}
@article{10.2436/20.8080.02.86, 
year = {2019}, 
title = {{Tail risk measures using flexible parametric distributions}}, 
author = {}, 
issn = {16962281}, 
doi = {10.2436/20.8080.02.86}, 
abstract = {{We propose a new type of risk measure for non-negative random variables that focuses on the tail of the distribution. The measure is inspired in general parametric distributions that are well-known in the statistical analysis of the size of income. We derive simple expressions for the conditional moments of these distributions, and we show that they are suitable for analysis of tail risk. The proposed method can easily be implemented in practice because it provides a simple one-step way to compute value-at-risk and tail value-at-risk. We show an illustration with currency exchange data. The data and implementation are open access for reproducibility. © 2019 Institut d'Estadistica de Catalunya. All rights reserved.}}, 
number = {2}, 
volume = {43}
}
@article{10.1016/j.jclepro.2019.118338, 
year = {2019}, 
title = {{Asymmetric and extreme influence of energy price changes on renewable energy stock performance}}, 
author = {Xia, Tongshui and Ji, Qiang and Zhang, Dayong and Han, Jinhong}, 
journal = {Journal of Cleaner Production}, 
issn = {09596526}, 
doi = {10.1016/j.jclepro.2019.118338}, 
abstract = {{Promoting the development of renewable energy has become the key factor to solve the problems of energy and climate change issues. However, its development is largely constrained by the prices of traditional fossil energy. This paper explores the influence of various fossil energy price changes on renewable energy stock returns using a network approach. Specifically, a positive and negative returns network and value-at-risk (VaR) network are constructed separately for identifying the asymmetric and extreme information spillover. Our findings show the fossil energy–renewable energy network system has a relatively high level of interdependence. The electricity market behaves as the major contributor to the changes of renewable energy returns in the returns connectedness network, while oil and coal contribute most to the changes of renewable energy returns in the VaR connectedness network. The dynamic results show that the contributions of fossil energy price changes to renewable energy returns have strong time-varying pattern with high volatility over time. The total connectedness in the positive returns network is slightly stronger than that in the negative returns network for most of the time during our sample period. © 2019 Elsevier Ltd}}, 
pages = {118338}, 
number = {NA}, 
volume = {241}
}
@article{10.1016/j.cam.2018.04.025, 
year = {2018}, 
title = {{Recovery of ruin probability and Value at Risk from the scaled Laplace transform inversion}}, 
author = {Fadahunsi, A.I. and Mnatsakanov, R.M.}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2018.04.025}, 
abstract = {{In this paper, we propose three modified approximations of the ruin probability and the inverse function of the ruin probability using the inversion of the scaled values of Laplace transform suggested by Mnatsakanov et al. (2015). The problem of evaluating numerically the tail-Value at Risk of an insurance portfolio is also discussed briefly. Performances of the proposed constructions are demonstrated via the graphs and tables using several examples. © 2018 Elsevier B.V.}}, 
pages = {249--262}, 
number = {NA}, 
volume = {342}
}
@article{10.1016/j.jbankfin.2005.05.014, 
year = {2006}, 
title = {{Nonlinear term structure dependence: Copula functions, empirics, and risk implications}}, 
author = {Junker, Markus and Szimayer, Alex and Wagner, Niklas}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.05.014}, 
abstract = {{This paper documents nonlinear cross-sectional dependence in the term structure of US-Treasury yields and points out risk management implications. The analysis is based on a Kalman filter estimation of a two-factor affine model which specifies the yield curve dynamics. We then apply a broad class of copula functions for modeling dependence in factors spanning the yield curve. Our sample of monthly yields in the 1982-2001 period provides evidence of upper tail dependence in yield innovations; i.e., large positive interest rate shocks tend to occur under increased dependence. In contrast, the best-fitting copula model coincides with zero lower tail dependence. This asymmetry has substantial risk management implications. We give an example in estimating bond portfolio loss quantiles and report the biases which result from an application of the normal dependence model. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {1171--1199}, 
number = {4}, 
volume = {30}
}
@article{10.1002/for.1167, 
year = {2010}, 
title = {{A decision rule to minimize daily capital charges in forecasting value-at-risk}}, 
author = {McAleer, Michael and Jimenez‐Martin, Juan‐Angel and Pérez‐Amaral, Teodosio}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1167}, 
abstract = {{Under the Basel II Accord, banks and other authorized deposit-taking institutions (ADIs) have to communicate their daily risk estimates to the monetary authorities at the beginning of the trading day, using a variety of value-at-risk (VaR) models to measure risk. Sometimes the risk estimates communicated using these models are too high, thereby leading to large capital requirements and high capital costs. At other times, the risk estimates are too low, leading to excessive violations, so that realized losses are above the estimated risk. In this paper we analyze the profit-maximizing problem of an ADI subject to capital requirements under the Basel II Accord as ADIs have to choose an optimal VaR reporting strategy that minimizes daily capital charges. Accordingly, we suggest a dynamic communication and forecasting strategy that responds to violations in a discrete and instantaneous manner, while adapting more slowly in periods of no violations. We apply the proposed strategy to Standard \& Poor's 500 Index and show there can be substantial savings in daily capital charges, while restricting the number of violations to within the Basel II penalty limits. Copyright © 2009 John Wiley \& Sons, Ltd.}}, 
pages = {617--634}, 
number = {7}, 
volume = {29}
}
@article{10.1016/j.renene.2021.05.134, 
year = {2021}, 
title = {{Estimating the market risk of clean energy technologies companies using the expected shortfall approach}}, 
author = {Pradhan, Ashis Kumar and Tiwari, Aviral Kumar}, 
journal = {Renewable Energy}, 
issn = {09601481}, 
doi = {10.1016/j.renene.2021.05.134}, 
abstract = {{In this study, we assess and estimate the market risk of firms that use clean energy technologies in their production process by using the expected shortfall (ES) regression-based backtest approach and value at risk (VaR) method. We use the WilderHill Clean energy Index from 2001 to 2018 and jointly assess the tail distribution of the risk model. Our findings show that ES forecast results are not misleading during the financial turmoil or during the full sample period. Therefore, our findings indicate that the ES approach can be an alternative valuable diagnostic tool to VaR for the estimation of market risk for financial institutions and regulators. © 2021 Elsevier Ltd}}, 
pages = {95--100}, 
number = {NA}, 
volume = {177}
}
@article{10.13335/j.1000-3673.pst.2020.1445, 
year = {2021}, 
title = {{Load Restoration Optimization of Wind Power Integrated System Considering Security Region Examination [基于安全域校核的含风电系统负荷恢复优化]}}, 
author = {}, 
issn = {10003673}, 
doi = {10.13335/j.1000-3673.pst.2020.1445}, 
abstract = {{Large-scale wind power access may bring about potential safety risks to the load recovery process under extreme output scenarios. In order to cope with this problem, firstly, the predicted wind power outputs and the extreme output value are obtained by using the Copula theory, the Latin hypercube sampling and the value-at-risk (VAR) method. Secondly, based on the analysis of the active static security region (ASSR) of the load recovery time steps, the safety adjustment factors that characterizes the active power adjustment margins of different load nodes are proposed. Furthermore, by taking the load importance and safety adjustment factor as the weighting factors to measure the restoration values, the load restoration mixed integer non-linear programming (MILP) model with the optimization goal of maximizing the time step restoration value is established and solved with the CPLEX. Having the example of the IEEE-39 node system, the simulation results show that adjusting the restoration plan according to security region check results of the restoration plan in the extreme wind power output scenario, the load restoration effect and the safety of the restoration process are balanced. © 2021, Power System Technology Press. All right reserved.}}, 
number = {8}, 
volume = {45}
}
@article{10.1016/j.jfineco.2004.06.005, 
year = {2005}, 
title = {{Do hedge funds have enough capital? A value-at-risk approach}}, 
author = {Gupta, Anurag and Liang, Bing}, 
journal = {Journal of Financial Economics}, 
issn = {0304405X}, 
doi = {10.1016/j.jfineco.2004.06.005}, 
abstract = {{We examine the risk characteristics and capital adequacy of hedge funds through the Value-at-Risk approach. Using extensive data on nearly 1,500 hedge funds, we find only 3.7\% live and 10.9\% dead funds are undercapitalized as of March 2003. Moreover, the undercapitalized funds are relatively small and constitute a tiny fraction of total fund assets in our sample. Cross-sectionally, the variability in fund capitalization is related to size, investment style, age, and management fee. Hedge fund risk and capitalization also display significant time variation. Traditional risk measures like standard deviation or leverage ratios fail to detect these trends. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {219--253}, 
number = {1}, 
volume = {77}
}
@article{10.21314/jrmv.2020.222, 
year = {2020}, 
title = {{Old-fashioned parametric models are still the best: A comparison of value-at-risk approaches in several volatility states}}, 
author = {Buczyński, Mateusz and Chlebus, Marcin}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2020.222}, 
abstract = {{Numerous advances in the modeling techniques of value-at-risk (VaR) have provided financial institutions with a wide range of market risk approaches. However, which model to use depends on the state of volatility. We present backtesting results for 1\% and 2.5\% VaR of six indexes from emerging and developed countries using several of the best-known VaR models, including generalized autoregressive conditional heteroscedasticity (GARCH), extreme value theory (EVT), conditional autoregressive VaR (CAViaR) and filtered historical simulation (FHS) with multiple sets of parameters. The backtesting procedure is based on the excess ratio, Kupiec and Christoffersen tests for multiple thresholds and cost functions. The main contribution of this paper is that we compared the models in four different scenarios, with different states of volatility in the training and testing samples. The results indicate that the best of the models, ie, the least affected by changes in the volatility, is GARCH.1; 1/ with a standardized Student t distribution. Nonparametric techniques (eg, CAViaR with GARCH setup or FHS with a skewed normal distribution) have very prominent results in testing periods with low volatility, but they are worse in turbulent periods. We also discuss an automatic method to set an extreme distribution threshold for EVT models as well as several ensembling methods for VaR, of which the minimum VaR estimate from the best models-in particular, a minimum of GARCH.1; 1/ with a standardized Student t distribution and either the EVT or the CAViaR model-has been proven to give very good results. © 2020 Infopro Digital Risk (IP) Limited.}}, 
number = {2}, 
volume = {14}
}
@article{10.1016/j.insmatheco.2012.05.005, 
year = {2012}, 
title = {{Optimal reinsurance under variance related premium principles}}, 
author = {Chi, Yichun}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.05.005}, 
abstract = {{In this paper, we investigate the optimal form of reinsurance when the insurer seeks to minimize the value at risk(VaR) or the conditional value at risk(CVaR) of his/her total risk exposure. In order to exclude the moral hazard from a reinsurance treaty, both the ceded and retained loss functions are constrained to be increasing. Under the additional assumption that the reinsurance premium is calculated by a variance related principle, we show that the layer reinsurance is always optimal over both the VaR and CVaR criteria. Finally, the variance and standard deviation premium principles are applied to illustrate how to derive the optimal deductible and the upper limit of layer reinsurance. © 2012 Elsevier B.V.}}, 
pages = {310--321}, 
number = {2}, 
volume = {51}
}
@article{10.4028/www.scientific.net/amm.416-417.1914, 
year = {2013}, 
title = {{A study of evaluation model of human errors at risk for the petroleum operation}}, 
author = {Liu, Bao Fa and Liu, Shang Hai}, 
journal = {Applied Mechanics and Materials}, 
issn = {16609336}, 
doi = {10.4028/www.scientific.net/amm.416-417.1914}, 
abstract = {{Because the petroleum production is larger investment, higher risk, and the traditional risk evaluation tools usually gave an evaluated value only, which being not in accord with the actual, the paper introduces the ideas of "Value at Risk" into the Human Errors (HEs) quantification for the petroleum operation, and develops the the model of "Human Errors at Risk" (HEaR) to quantify the HEs. The model can in detail depict the actual risk statuses of production system under different risk conditions. © (2013) Trans Tech Publications, Switzerland.}}, 
pages = {1914--1919}, 
number = {NA}, 
volume = {416-417}
}
@article{10.1016/j.jempfin.2019.08.004, 
year = {2019}, 
title = {{Range-based DCC models for covariance and value-at-risk forecasting}}, 
author = {Fiszeder, Piotr and Fałdziński, Marcin and Molnár, Peter}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2019.08.004}, 
abstract = {{The dynamic conditional correlation (DCC) model by Engle (2002) is one of the most popular multivariate volatility models. This model is based solely on closing prices. It has been documented in the literature that the high and low prices of a given day can be used to obtain an efficient volatility estimation. We therefore suggest a model that incorporates high and low prices into the DCC framework. We conduct an empirical evaluation of this model on three datasets: currencies, stocks, and commodity exchange traded funds. Regardless of whether we consider in-sample fit, covariance forecasts or value-at-risk forecasts, our model outperforms not only the standard DCC model, but also an alternative range-based DCC model. © 2019 The Authors}}, 
pages = {58--76}, 
number = {NA}, 
volume = {54}
}
@article{10.1109/tpwrs.2017.2678761, 
year = {2017}, 
title = {{Quantifying Risk of Wind Power Ramps in ERCOT}}, 
author = {Zhao, Jie and Abedi, Sajjad and He, Miao and Du, Pengwei and Sharma, Sandip and Blevins, Bill}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2017.2678761}, 
abstract = {{Hourly wind power ramps in ERCOT are studied by applying extreme value theory. Mean excess plot reveals that the tail behavior of large hourly wind power ramps indeed follows a generalized Pareto distribution. The location, shape, and scale parameters of generalized Pareto distribution are then determined by using mean excess plot and the least square technique, from which risk measures including α quantile value at risk and conditional value at risk are calculated. © 2012 IEEE.}}, 
pages = {4970--4971}, 
number = {6}, 
volume = {32}
}
@article{10.1080/14697681003652514, 
year = {2010}, 
title = {{Portfolio selection based on the mean-VaR efficient frontier}}, 
author = {Tsao, Chueh-Yung}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697681003652514}, 
abstract = {{Value-at-Risk (VaR) has become one of the standard measures for assessing risk not only in the financial industry but also for asset allocations of individual investors. The traditional mean-variance framework for portfolio selection should, however, be revised when the investor's concern is the VaR instead of the standard deviation. This is especially true when asset returns are not normal. In this paper, we incorporate VaR in portfolio selection, and we propose a mean-VaR efficient frontier. Due to the two-objective optimization problem that is associated with the mean-VaR framework, an evolutionary multi-objective approach is required to construct the mean-VaR efficient frontier. Specifically, we consider the elitist non-dominated sorting Genetic Algorithm (NSGA-II). From our empirical analysis, we conclude that the risk-averse investor might inefficiently allocate his/her wealth if his/her decision is based on the mean-variance framework. © 2010 Taylor \& Francis.}}, 
pages = {931--945}, 
number = {8}, 
volume = {10}
}
@article{10.3390/e19050226, 
year = {2017}, 
title = {{Information entropy and measures of market risk}}, 
author = {Pele, Daniel Traian and Lazar, Emese and Dufour, Alfonso}, 
journal = {Entropy}, 
issn = {10994300}, 
doi = {10.3390/e19050226}, 
abstract = {{In this paper we investigate the relationship between the information entropy of the distribution of intraday returns and intraday and daily measures of market risk. Using data on the EUR/JPY exchange rate, we find a negative relationship between entropy and intraday Value-at-Risk, and also between entropy and intraday Expected Shortfall. This relationship is then used to forecast daily Value-at-Risk, using the entropy of the distribution of intraday returns as a predictor. © 2017 by the authors.}}, 
pages = {226}, 
number = {5}, 
volume = {19}
}
@article{10.3390/math9121336, 
year = {2021}, 
title = {{A new extended geometric distribution: Properties, regression model, and actuarial applications}}, 
author = {Almazah, Mohammed Mohammed Ahmed and Erbayram, Tenzile and Akdoğan, Yunus and Sobhi, Mashail M. AL and Afify, Ahmed Z.}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math9121336}, 
abstract = {{In this paper, a new modified version of geometric distribution is proposed. The newly introduced model is called transmuted record type geometric (TRTG) distribution. TRTG distribution is a good alternative to the negative binomial, Poisson and geometric distributions in modeling real data encountered in several applied fields. The main statistical properties of the new distribution were obtained. We determined the measures of value at risk and tail value at risk for the TRTG distribution. These measures are important quantities in actuarial sciences for portfolio optimization under uncertainty. The TRTG parameters were estimated via maximum likelihood, moments, proportions, and Bayesian estimation methods, and the simulation results were determined to explore their performance. Furthermore, a new count regression model based on the TRTG distribution was proposed. Four real data applications were adopted to illustrate the applicability of the TRTG distribution and its count regression model. These applications showed empirically that the TRTG distribution outperforms some important discrete models such as the negative binomial, transmuted geometric, discrete Burr, discrete Chen, geometric, and Poisson distributions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {1336}, 
number = {12}, 
volume = {9}
}
@article{10.1239/jap/1409932674, 
year = {2014}, 
title = {{Asymptotic bounds for the distribution of the sum of dependent random variables}}, 
author = {Wang, Ruodu}, 
journal = {Journal of Applied Probability}, 
issn = {00219002}, 
doi = {10.1239/jap/1409932674}, 
abstract = {{Suppose thatX1, . . . , Xn are random variables with the same known marginal distribution F but unknown dependence structure. In this paper we study the smallest possible value of P(X1 +-+ Xn < s) over all possible dependence structures, denoted by mn,F (s). We show that mn,F (ns) → 0 for s no more than the mean of F under weak assumptions. We also derive a limit of mn,F (ns) for any s € R with an error of at most n 1/6 for general continuous distributions. An application of our result to risk management confirms that the worst-case value at risk is asymptotically equivalent to the worst-case expected shortfall for risk aggregation with dependence uncertainty. In the last part of this paper we present a dual presentation of the theory of complete mixability and give dual proofs of theorems in the literature on this concept.. © 2014 Applied Probability Trust.}}, 
pages = {780--798}, 
number = {3}, 
volume = {51}
}
@article{10.1109/fskd.2010.5569276, 
year = {2010}, 
title = {{Risk measure under impact of news}}, 
author = {Ou, Shide and Yi, Danhui}, 
journal = {2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery}, 
issn = {NA}, 
doi = {10.1109/fskd.2010.5569276}, 
abstract = {{In order to find effective methods measuring stock market risk, Shanghai Stock Indices are observed by positive news and negative news. The impact of news on return volatility is researched by heteroskedasticity models. Empirical research shows that under circumstance of positive news, bad news only reduces current return volatility, while the relationship between return and variance is not notable. Under the circumstance of negative news, bad news results in volatility growth, the relationship between return and variance isn't also notable. By comparing, we find out that there is large difference between values at risk under different circumstances. Finally we propose the method estimating by hierarchical level. Simulation results show that it is effective. ©2010 IEEE.}}, 
pages = {2909--2913}, 
number = {NA}, 
volume = {6}
}
@article{10.1016/j.jbankfin.2012.08.021, 
year = {2013}, 
title = {{Market crises and Basel capital requirements: Could Basel III have been different? Evidence from Portugal, Ireland, Greece and Spain (PIGS)}}, 
author = {Rossignolo, Adrián F. and Fethi, Meryem Duygun and Shaban, Mohamed}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.08.021}, 
abstract = {{Basel III represents a crucial step in strengthening the capital rules underlying banking operations, aimed at reducing the probability and severity of a systemic crisis. Alongside two supplementary capital buffers, the Basel Committee of Banking Supervision imposed severe pressure on the Value-at-Risk based Internal Models Approach in order to increase. This is to increase the capital base by adding the stressed Value-at-Risk component in an effort to reduce reliance on internal models while keeping the Standardized Approach avenue open. However, even though those measures might appear theoretically correct, evidence gathered for long and short exposures in Portugal, Italy, Greece and Spain highlights several defects in Basel III. We emphasize that leptokurtic models, primarily those derived from Extreme Value Theory, should be enforced in the regulations given their superior performance in market crises, and that Basel II could have shielded against 2008 mayhem provided that heavy-tailed techniques had been employed. © 2012 Elsevier B.V.}}, 
pages = {1323--1339}, 
number = {5}, 
volume = {37}
}
@article{10.1080/03610918.2018.1520873, 
year = {2020}, 
title = {{On the asymptotics of value-at-risk for portfolio loss under bivariate Eyraud-Farlie-Gumbel-Morgenstern copula and heavy tails}}, 
author = {Xing, Guo-dong and Gan, Xiaoli and Li, Xiaohu and Yang, Shanchao}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2018.1520873}, 
abstract = {{In the setting of bivariate Eyraud-Farlie-Gumbel-Morgenstern copula and heavy tails characterized by the power law of tail decay, we give the asymptotics of value-at-risk for portfolio loss as the confidence level tends to one. It can be seen from the obtained asymptotics that diversification decreases the value-at-risk of portfolio loss for the tail index greater than one and increases the value-at-risk of portfolio loss for the tail index less than one. To illustrate the obtained results, a relevant example is shown. © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--10}, 
number = {9}, 
volume = {49}
}
@article{10.23919/icif.2018.8455843, 
year = {2018}, 
title = {{Estimation of Value-at-Risk Using Mixture Copula Model for Heavy-Tailed Operational Risk Losses in Financial, Insurance Climatological Data}}, 
author = {Guharay, Sabyasachi and Chang, KC and Xu, Jie}, 
journal = {2018 21st International Conference on Information Fusion (FUSION)}, 
issn = {NA}, 
doi = {10.23919/icif.2018.8455843}, 
abstract = {{Data fusion techniques are being regularly used for analysis in Operational Risk Management (ORM). A popular and commonly used risk metric of interest, Value-at-Risk (VaR), has always been difficult to robustly estimate for different data types. The classical Monte Carlo simulation (MCS) approach (denoted henceforth as classical approach) assumes the independence of loss severity and loss frequency. In practice, this assumption may not always hold. To overcome this limitation and handle cases with heavy-tail data and more robustly estimate the corresponding VaR, we adopt a new approach known as Mixture Copula-based Parametric Modeling of Frequency and Severity (MCPFS). The proposed approach is verified via large-scale MCS experiments and validated on four publicly available financial datasets. We compare MCPFS with the classical approach for robust VaR estimation. We observe that the classical approach estimates VaR poorly while the MCPFS methodologies attain better VaR estimates for real-world data. These studies provide real-world evidence that the MCPFS methodologies have merits for its use to accurately estimate VaR. © 2018 ISIF}}, 
pages = {2330--2337}, 
number = {NA}, 
volume = {NA}
}
@article{10.1360/n012019-00079, 
year = {2021}, 
title = {{Semiparametric varying-coefficient expectile model for estimating value at risk on dependent samples [相依样本下半参数变系数期望分位数风险度量模型统计推断]}}, 
author = {Zijian, Wang and Yong, Zhou and Fanping, Zeng}, 
journal = {SCIENTIA SINICA Mathematica}, 
issn = {16747216}, 
doi = {10.1360/n012019-00079}, 
abstract = {{We introduce a semiparametric varying-coefficient model to study the expectile-based value at risk (EVaR) based on the α-mixing assumption. The model considers the risk factors as well as the dynamic structure and the interaction of these factors. Meanwhile, EVaR not only has great advantages such as the subadditivity or easy to calculate, but also is very sensitive to the scale of losses compared with the conventional quantile-based value at risk (QVaR), which makes it as a more mature and efficient way of the risk measure in extreme risk situations. We develop a three-stage method to estimate the parameters of the varying-coefficient part and the constant-coefficient part. We also establish the challenging consistency and the asymptotic normality of all the resultant estimation in three stages under the time series samples. To save computing time, we propose to use a one-step procedure to compute the estimation. Financial time series samples are not independent data. It has more challenge to establish the large sample properties of the estimators. We introduce the dividing methods to prove the limit theory of the α-mixing series, and obtain the asymptotic properties of the parametric constant coefficients and the nonparametric varying-coefficients. In our simulation, three models' results show the accuracy and robustness of our estimation. The real financial data example performs as an application of our model at the end of our study. © 2021, Science Press. All right reserved.}}, 
pages = {1377}, 
number = {9}, 
volume = {51}
}
@article{10.1016/j.najef.2017.02.006, 
year = {2017}, 
title = {{Testing and comparing the performance of dynamic variance and correlation models in value-at-risk estimation}}, 
author = {Li, Leon}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2017.02.006}, 
abstract = {{This study addresses and examines certain advanced approaches for value-at-risk (VaR) estimation. In particular, we employ a multivariate generalized autoregressive conditionally heteroskedastic (MVGARCH) model involving time-varying settings and multivariate Markov switching autoregressive conditionally heteroskedastic (MVSWARCH) model with regime-switching techniques and compare them with a conventional linear regression-based (LRB) model. Our empirical findings are as follows: First, while the LRB VaR model behaves reasonably well in tranquil periods, it significantly underestimates actual risk during unstable periods. Second, in comparison with the LRB VaR model, MVGARCH- and MVSWARCH-based VaR models do better under unusual conditions, whereas better models are needed to estimate VaR. Third, dynamic variance settings improve the accuracy of VaR estimates. However, the effect of dynamic correlation designs on VaR is marginal. This study addresses and examines certain advanced approaches for value-at-risk (VaR) estimation. In particular, we employ a multivariate generalized autoregressive conditionally heteroskedastic (MVGARCH) model involving time-varying settings and multivariate Markov switching autoregressive conditionally heteroskedastic (MVSWARCH) model with regime-switching techniques and compare them with a conventional linear regression-based (LRB) model. Our empirical findings are as follows: First, while the LRB VaR model behaves reasonably well in tranquil periods, it significantly underestimates actual risk during unstable periods. Second, in comparison with the LRB VaR model, MVGARCH- and MVSWARCH-based VaR models do better under unusual conditions, whereas better models are needed to estimate VaR. Third, dynamic variance settings improve the accuracy of VaR estimates. However, the effect of dynamic correlation designs on VaR is marginal. © 2017 Elsevier Inc.}}, 
pages = {116--135}, 
number = {NA}, 
volume = {40}
}
@article{10.1093/jjfinec/nby016, 
year = {2019}, 
title = {{Extreme Conditional Tail Moment Estimation under Serial Dependence}}, 
author = {Hoga, Yannick}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nby016}, 
abstract = {{A wide range of risk measures can be written as functions of conditional tail moments (CTMs) and value-at-risk (VaR), for instance the expected shortfall (ES). In this paper, we derive joint central limit theory for semi-parametric estimates of CTMs, including in particular ES, at arbitrarily small risk levels. We also derive confidence corridors for VaR at different levels far out in the tail, which allows for simultaneous inference. We work under a semi-parametric Pareto-type assumption on the distributional tail of the observations and only require an extremal-near epoch dependence assumption on the serial dependence. In simulations, our semi-parametric ES estimate is often shown to be more accurate in terms of mean absolute deviation than extant non- and semi-parametric estimates. An empirical application to the extreme swings in Volkswagen log-returns during the failed takeover attempt by Porsche illustrates the proposed methods. © 2018 The Author(s) 2018. Published by Oxford University Press. All rights reserved.}}, 
pages = {587--615}, 
number = {4}, 
volume = {17}
}
@article{10.21314/jrmv.2015.146, 
year = {2015}, 
title = {{Commodity value-at-risk modeling: Comparing RiskMetrics, historic simulation and quantile regression}}, 
author = {Steen, Marie and Westgaard, Sjur and Gjølberg, Ole}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2015.146}, 
abstract = {{Commodities constitute a nonhomogeneous asset class. Return distributions differ widely across different commodities, both in terms of tail fatness and skewness. These are features that we need to take into account when modeling risk. In this paper, we outline the return characteristics of nineteen different commodity futures during the period 1992-2013.We then evaluate the performance of two standard risk modeling approaches, ie, RiskMetrics and historical simulation, against a quantile regression (QR) approach. Our findings strongly support the conclusion that QR outperforms these standard approaches in predicting value-at-risk for most commodities. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {49--78}, 
number = {2}, 
volume = {9}
}
@article{10.1016/j.insmatheco.2011.08.002, 
year = {2012}, 
title = {{Translation-invariant and positive-homogeneous risk measures and optimal portfolio management in the presence of a riskless component}}, 
author = {Landsman, Zinoviy and Makov, Udi}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2011.08.002}, 
abstract = {{Risk portfolio optimization, with translation-invariant and positive-homogeneous risk measures, leads to the problem of minimizing a combination of a linear functional and a square root of a quadratic functional for the case of elliptical multivariate underlying distributions.This problem was recently treated by the authors for the case when the portfolio does not contain a riskless component. When it does, however, the initial covariance matrix σ becomes singular and the problem becomes more complicated. In the paper we focus on this case and provide an explicit closed-form solution of the minimization problem, and the condition under which this solution exists. The results are illustrated using data of 10 stocks from the NASDAQ Computer Index. © 2011 Elsevier B.V.}}, 
pages = {94--98}, 
number = {1}, 
volume = {50}
}
@article{10.1016/j.jfs.2005.07.002, 
year = {2006}, 
title = {{A comparative analysis of macro stress-testing methodologies with application to Finland}}, 
author = {Sorge, Marco and Virolainen, Kimmo}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2005.07.002}, 
abstract = {{This paper reviews the state-of-the-art of macro stress-testing methodologies. We assess the progress made both in the econometric analysis of balance sheet indicators and in the simulation of value-at-risk measures to assess system-wide vulnerabilities. To illustrate the main analytical approaches in the literature, we estimate two different models for stress-testing purposes using data for Finland over the time period from 1986 to 2003. The Finnish experience in the early 1990s appears particularly suited for macro stress-testing as it includes a severe recession with significantly higher-than-average default rates and banks' loan losses. We highlight a number of methodological challenges that still remain concerning in particular the correlation of market and credit risks over time and across institutions, the limited time horizon generally used for macro stress-testing and the potential instability of reduced-form parameter estimates because of feedback effects. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {113--151}, 
number = {2}, 
volume = {2}
}
@article{10.1109/msie.2011.5707643, 
year = {2011}, 
title = {{Value at risk calculations:An empirical analysis for stock market in China}}, 
author = {Li, Xiumin and Xia, Cai}, 
journal = {MSIE 2011}, 
issn = {NA}, 
doi = {10.1109/msie.2011.5707643}, 
abstract = {{This paper applies the extreme value theory on Value-at-Risk (VaR) calculation, which focuses on modeling the tails of the return distribution rather than the whole distribution. Extreme value theory has more meaning during the volatile market conditions, under which the distribution of returns almost has a fat tail. Because of its biggest advantage of comprehensive, VaR calculation has become the mainstream method of measuring risk. This paper deals with the behavior of the tails of financial series. More specifically, the focus is the use of extreme value theory to assess tail related risk. The application of extreme value theory is illustrated by an example from the Shanghai and Shenzhen stock market data. The paper concludes that the both distributions are fat tails and the limit distribution for centralized and normalized maxima is a Frechet type. So the extreme value theory is properly applied and it's a useful complement to traditional VaR methods. ©2011 IEEE.}}, 
pages = {1228--1231}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11135-007-9157-4, 
year = {2009}, 
title = {{Model choice and value-at-risk estimation}}, 
author = {Ouyang, Zisheng}, 
journal = {Quality \& Quantity}, 
issn = {00335177}, 
doi = {10.1007/s11135-007-9157-4}, 
abstract = {{Value at risk (VaR) is a commonly used tool to measure market risk. In this paper, we discuss the problems of model choice and VaR performance. The VaRs of daily returns of the Shanghai and Shenzhen indexes are calculated using equally weighted moving average (EQMA), exponentially weighted moving average (EWMA), GARCH(1,1), empirical density estimation method, and the Pareto-type extreme-value distribution methods. Considering the length of the window and the requirement for adequate capital, back testing indicates that the Pareto-type extreme-value distribution method reflects the real market risk more accurately than the other models. © 2008 Springer Science+Business Media B.V.}}, 
pages = {983--991}, 
number = {6}, 
volume = {43}
}
@article{10.1109/cec.2002.1007017, 
year = {2002}, 
title = {{Evaluating strategies for foreign exchange risk reduction}}, 
author = {Everson, Mark P. and Mangin, Christophe G. E. and Stumpo, Cody and Schwartz, JoAnn M. and Ostrowski, David A.}, 
journal = {Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)}, 
issn = {NA}, 
doi = {10.1109/cec.2002.1007017}, 
abstract = {{Companies' risk due to foreign exchange (FX) rates can be reduced using market-based instruments. We have used the value-at-risk methodology to evaluate FX risk, and have developed a genetic algorithm-based optimization framework for evaluating different strategies. The best strategy is to hedge an identical percentage of each FX exposure. © 2002 IEEE.}}, 
pages = {735--740}, 
number = {NA}, 
volume = {1}
}
@article{10.1080/02664763.2020.1789076, 
year = {2020}, 
title = {{Dichotomous unimodal compound models: application to the distribution of insurance losses}}, 
author = {Tomarchio, Salvatore D. and Punzo, Antonio}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664763.2020.1789076}, 
abstract = {{A correct modelization of the insurance losses distribution is crucial in the insurance industry. This distribution is generally highly positively skewed, unimodal hump-shaped, and with a heavy right tail. Compound models are a profitable way to accommodate situations in which some of the probability masses are shifted to the tails of the distribution. Therefore, in this work, a general approach to compound unimodal hump-shaped distributions with a mixing dichotomous distribution is introduced. A 2-parameter unimodal hump-shaped distribution, defined on a positive support, is considered and reparametrized with respect to the mode and to another parameter related to the distribution variability. The compound is performed by scaling the latter parameter by means of a dichotomous mixing distribution that governs the tail behavior of the resulting model. The proposed model can also allow for automatic detection of typical and atypical losses via a simple procedure based on maximum a posteriori probabilities. Unimodal gamma and log-normal are considered as examples of unimodal hump-shaped distributions. The resulting models are firstly evaluated in a sensitivity study and then fitted to two real insurance loss datasets, along with several well-known competitors. Likelihood-based information criteria and risk measures are used to compare the models. © 2020, © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--26}, 
number = {NA}, 
volume = {NA}
}
@article{10.1239/jap/1339878792, 
year = {2012}, 
title = {{Risk measures and multivariate extensions of Breiman's theorem}}, 
author = {Fougeres, Anne-Laure and Mercadier, Cecile}, 
journal = {Journal of Applied Probability}, 
issn = {00219002}, 
doi = {10.1239/jap/1339878792}, 
abstract = {{The modeling of insurance risks has received an increasing amount of attention because of solvency capital requirements. The ruin probability has become a standard risk measure to assess regulatory capital. In this paper we focus on discrete-time models for the finite time horizon. Several results are available in the literature to calibrate the ruin probability by means of the sum of the tail probabilities of individual claim amounts. The aim of this work is to obtain asymptotics for such probabilities under multivariate regular variation and, more precisely, to derive them from extensions of Breiman's theorem. We thus present new situations where the ruin probability admits computable equivalents. We also derive asymptotics for the value at risk. © Applied Probability Trust 2012.}}, 
pages = {364--384}, 
number = {2}, 
volume = {49}
}
@article{10.1007/s10436-006-0053-z, 
year = {2007}, 
title = {{Proprietary trading losses in banks: Do banks invest sufficiently in control?}}, 
author = {Instefjord, Norvald and Sasaki, Kouji}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-006-0053-z}, 
abstract = {{This paper analyzes management and control issues linked to the employment of traders who engage in proprietary trading activity for their employer (a bank). The bank can invest in control and monitoring of these traders, and the paper evaluates the profitability of such investments. We find that the investment in control is distorted due to interfering market microstructure effects. The bank is inclined to underinvest in control of its traders because traders who are not too closely monitored generate extra liquidity in the market. Bank supervision might be needed, therefore, to correct for such effects. We evaluate the effectiveness of the value-at-risk capital adequacy requirement proposed by the Bank for International Settlements, and find that this approach correctly targets the banks that are the most vulnerable, i.e. those that are the most at risk of underinvesting in its control and monitoring systems. © Springer-Verlag 2006.}}, 
pages = {329--350}, 
number = {3}, 
volume = {3}
}
@article{10.21003/ea.v170-10, 
year = {2018}, 
title = {{Optimal portfolios vis-à-vis corporate governance ratings: Some UK evidence}}, 
author = {University, Victoria and Nor, Safwan Mohd and Zawawi, Nur Haiza Muhammad and Terengganu, University of Malaysia}, 
journal = {Economic Annals-ХХI}, 
issn = {17286220}, 
doi = {10.21003/ea.v170-10}, 
abstract = {{Socially responsible investments may offer investors higher returns because of the perceived lower risk and thus associated cost (monitoring, litigation, etc.), although it might also be less profitable as posited by proponents of the Efficient Market Hypothesis where higher risk is compensated with higher returns. Corporate governance (CG) - one of the key components in socially responsible investing - has been extensively studied for evaluating its relationship with firm performance. In this paper, we extend prior literature by exploring the investment performances of two distinct portfolios built using strong versus weak corporate governance firms. We contribute by investigating the value of corporate governance (or lack thereof) in formulating portfolios. Using London Stock Exchange data for the period January 2012 through June 2018 and both ends of the quartile spectrum from 2017 Good Governance Report, we optimize each portfolio based on their Sharpe criterion. Our findings offer some practical and theoretical implications. Investors who are conscious about CG and attempt to maximize Sharpe measure by investing in strong governance firms may face lower portfolio risk by foregoing higher returns. Whereas reduction in value-at-risk midway onwards appears to suggest investment in companies with strong CG would less likely to fail in the long run. Volatility and downside volatility results tell similar story. Indeed, from the agency theoretical perspective, companies with strong CG would lead to lower agency cost (and risk) and better firm performance. We find profitable outcomes for both portfolios, although out-of-sample, weak governance portfolio dominates in terms of several key performance metrics. © Institute of Society Transformation, 2018.}}, 
pages = {57--63}, 
number = {3-4}, 
volume = {170}
}
@article{10.1109/wsc.2009.5429323, 
year = {2009}, 
title = {{Introduction to financial risk assessment using Monte Carlo simulation}}, 
author = {Strong, Robert A. and Steiger, Natalie M. and Wilson, James R.}, 
journal = {Proceedings of the 2009 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2009.5429323}, 
abstract = {{The fundamental principles of financial risk assessment are discussed, with primary emphasis on using simulation to evaluate and compare alternative investments. First we introduce the key measures of performance for such investments, including net present value, internal rate of return, and modified internal rate of return. Next we discuss types of risk and the key measures of risk, including expected present value; the mean, standard deviation, and coefficient of variation of the rate of return; and the risk premium. Finally we detail the following applications: (i) stand-alone risk assessment for a capital-budgeting problem; (ii) comparison of risk-free and risky investment strategies designed merely to keep up with the cost of living; (iii) value-at-risk (VAR) analysis for a single-stock investment; (iv) VAR analyses for two-asset portfolios consisting of stock and either call or put options; and (v) VAR analyses for two-asset portfolios consisting of both puts and calls. ©2009 IEEE.}}, 
pages = {99--118}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ocecoaman.2021.105872, 
year = {2021}, 
title = {{Measuring left-tail risk of fish species}}, 
author = {Lopetegui, Itsaso and Valle, Ikerne del}, 
journal = {Ocean \& Coastal Management}, 
issn = {09645691}, 
doi = {10.1016/j.ocecoaman.2021.105872}, 
abstract = {{The main objective of this paper is to perform a risk analysis for the key commercial fish species in the FAO area 27 by means of a bundle of financial left-tail risk indicators, including Value-at-Risk (VaR), Expected Shortfall (ES) and Expectiles (EX), and panel data of catches (Qit) to measure the left-tail risk of catches (LTRi); an empirical and probabilistic measure of the worst-case reduction of catches resulting from huge negative shocks. LTRi can be useful, not only to classify the fish species according to their risk level, but also, using the appropriate weights, to infer the risk to any other aggregation level such as fleet, fishing community or fishing country. In this paper, we are employing our species level (LTRi) estimations to calculate the left-tail risk of catches of the EU fishing countries (LTRj), a country level proxy variable for the risk inherent to the fishing activity itself. © 2021 The Authors}}, 
pages = {105872}, 
number = {NA}, 
volume = {213}
}
@article{10.1016/j.jeconom.2014.06.019, 
year = {2015}, 
title = {{Risk-parameter estimation in volatility models}}, 
author = {Francq, Christian and Zakoïan, Jean-Michel}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2014.06.019}, 
abstract = {{This paper introduces the concept of risk parameter in conditional volatility models of the form t=σt(θ0)ηt and develops statistical procedures to estimate this parameter. For a given risk measure r, the risk parameter is expressed as a function of the volatility coefficients θ0 and the risk, r(ηt), of the innovation process. A two-step method is proposed to successively estimate these quantities. An alternative one-step approach, relying on a reparameterization of the model and the use of a non Gaussian QML, is proposed. Asymptotic results are established for smooth risk measures, as well as for the Value-at-Risk (VaR). Asymptotic comparisons of the two approaches for VaR estimation suggest a superiority of the one-step method when the innovations are heavy-tailed. For standard GARCH models, the comparison only depends on characteristics of the innovations distribution, not on the volatility parameters. Monte-Carlo experiments and an empirical study illustrate the superiority of the one-step approach for financial series. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {158--173}, 
number = {1}, 
volume = {184}
}
@article{10.1007/s10614-021-10123-8, 
year = {2021}, 
title = {{Inaccurate Value at Risk Estimations: Bad Modeling or Inappropriate Data?}}, 
author = {Vasileiou, Evangelos}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-021-10123-8}, 
pmid = {34177119}, 
abstract = {{Forecasting accurate Value-at-Risk (VaR) estimations is a crucial task in applied financial risk management. Even though there have been significant advances in the field of financial econometrics, many crises have been documented throughout the world in the last decades. An explanation for this discrepancy is that many contemporary models are too complex and cannot be easily understood and implemented in the financial industry (Fama in Financ Anal J 51:75–80, 1995; Ross in AIMR conference proceedings, vol. 1993, no. 6, pp. 11–15, Association for Investment Management and Research, 1993). In order to bridge this theory–practice gap, we present a computational method based on the leverage effect. This method allows us to focus on financial theory and remove complexity. Examining the US stock market (2000–2020), we provide empirical evidence that our newly suggested approach, which uses only the most appropriate observation period, significantly increases the accuracy of the Conventional Delta Normal VaR model and generates VaR estimations which are as accurate as those of advanced econometric models, such as GARCH(1,1). © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1--17}, 
number = {NA}, 
volume = {NA}
}
@article{10.1515/snde-2017-0119, 
year = {2019}, 
title = {{Foster-Hart optimization for currency portfolios}}, 
author = {Kurosaki, Tetsuo and Kim, Young Shin}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2017-0119}, 
abstract = {{We examine the effectiveness of Foster-Hart optimization for currency portfolios. Compared to stock trading, short selling is quite common in currency trading. Combining long and short positions leads to maintaining positive expected portfolio returns. Foster-Hart optimization is more applicable to currency portfolios than to stock portfolios because the Foster-Hart risk measure is not defined for the gamble whose expected returns are negative. Our sample portfolio consists of ten European currencies. For time series analysis, we employ a generalized autoregressive conditional heteroscedasticity (GARCH) model with multivariate normal tempered stable (MNTS) distributed residuals in order to capture fat-tailedness, skewness, and asymmetric interdependence of exchange rate dynamics. Statistical tests indicate that the model is recommendable among the candidate models. We establish that Foster-Hart optimization is more profitable than standard techniques in this context. © 2019 Walter de Gruyter GmbH, Berlin/Boston 2019.}}, 
pages = {20170119}, 
number = {2}, 
volume = {23}
}
@article{10.1016/j.jbankfin.2014.03.019, 
year = {2014}, 
title = {{Risk models-at-risk}}, 
author = {Boucher, Christophe M. and Daníelsson, Jón and Kouontchou, Patrick S. and Maillet, Bertrand B.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2014.03.019}, 
abstract = {{The experience from the global financial crisis has raised serious concerns about the accuracy of standard risk measures as tools for the quantification of extreme downward risks. A key reason for this is that risk measures are subject to a model risk due, e.g. to specification and estimation uncertainty. While regulators have proposed that financial institutions assess the model risk, there is no accepted approach for computing such a risk. We propose a remedy for this by a general framework for the computation of risk measures robust to model risk by empirically adjusting the imperfect risk forecasts by outcomes from backtesting frameworks, considering the desirable quality of VaR models such as the frequency, independence and magnitude of violations. We also provide a fair comparison between the main risk models using the same metric that corresponds to model risk required corrections. © 2014 Elsevier B.V.}}, 
pages = {72--92}, 
number = {1}, 
volume = {44}
}
@article{10.1109/3ca.2010.5533637, 
year = {2010}, 
title = {{Measuring of value at risk (VAR) on emerging stock markets by neural networks method}}, 
author = {Chen, Cheng-Te and Hsieh, Chin-Shan}, 
journal = {2010 International Symposium on Computer, Communication, Control and Automation (3CA)}, 
issn = {NA}, 
doi = {10.1109/3ca.2010.5533637}, 
abstract = {{This study using neural network method for estimating VAR in emerging stock markets include Chinese and Hong Kong stock markets. Empirical results showed that the neural network method has outperformed conventional methods (historical simulation (HS), variance/covariance and the Monte Carlo simulation) in estimating VAR. © 2010 IEEE.}}, 
pages = {137--140}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.jimonfin.2014.01.006, 
year = {2014}, 
title = {{Downside risk and portfolio diversification in the euro-zone equity markets with special consideration of the crisis period}}, 
author = {Liu, Tengdong and Hammoudeh, Shawkat and Santos, Paulo Araújo}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2014.01.006}, 
abstract = {{This study examines the Value-at-Risk for ten euro-zone equity markets individually and also divided into two groups: PIIGS (Portugal, Italy, Ireland, Greece and Spain) and the Core (Austria, Finland, France, Germany and the Netherlands), employing four VaR estimation and evaluation methods considered over the full period and the pre- and post-global crisis subperiods 1 and 2. The backtesting results are also evaluated according to the Basel capital requirements. The results demonstrate that the CEVT methods meetall the statistical criteria the best for most individual equity indices over the full period, but these results over the two subperiods for those two methods are mixed, compared to those the DPOT methods. Moreover, the two optimal group portfolios of the PIIGS and the Core as well as the grand portfolio that combines the ten indices do not show much diversification benefits. The PIIGS portfolio selects Spain's IBEX only, while that of the Core opts for Austria's ATX only in the full period and subperiod 1. However, Germany's DAX overwhelmingly dominates both the Core and the Grand portfolios in subperiod 2. © 2014 Elsevier Ltd.}}, 
pages = {47--68}, 
number = {NA}, 
volume = {44}
}
@article{10.1080/23322039.2021.1920150, 
year = {2021}, 
title = {{Measurement of extreme market risk: Insights from a comprehensive literature review}}, 
author = {Chakraborty, Gourab and Chandrashekhar, G. R. and Balasubramanian, G.}, 
journal = {Cogent Economics \& Finance}, 
issn = {23322039}, 
doi = {10.1080/23322039.2021.1920150}, 
abstract = {{The experience of past financial market turmoil suggests that in addition to eroding investor wealth, the severe consequences of rare extreme market events can spillover and impair the broader real economies. In this context, this paper is an evaluation of the methodological and empirical advances in the measurement of the extreme market risk. This paper argues that a major reason for the origin of such risks post 1980s has been the unintended consequence of asymmetric monetary policy to sustain the rise of financial markets. Thereafter, this review identified the value at risk (VaR) and VaR-based alternative expected shortfall (ES) as the principal measures of extreme market risk. The deficiencies in the standard modelling approaches for VaR-ES measures have led to several advanced estimation methodologies. However, the lack of identification of optimal methodology, in the internal models approach (IMA) regime where financial institutions (FI’s) can choose suitable VaR-ES modelling technique incentivizes regulatory arbitrage and other inconsistencies. Therefore, this paper investigates the theoretical and empirical research literature on VaR and ES estimation for financial asset market prices. This paper finds that the extreme value theory (EVT) followed closely by the filtered historical simulation (FHS) are highly accurate methodologies. In addition, Mixture distributions, asymmetric and non-linear versions of the conditional quantile (CQ) approach, (volatility) asymmetry and long memory conditional volatility models, especially those assuming skewed and leptokurtic distributions offer accurate VaR and ES measures. © 2021 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.}}, 
pages = {1920150}, 
number = {1}, 
volume = {9}
}
@article{10.1080/20430795.2016.1188538, 
year = {2016}, 
title = {{The construction of an investment portfolio using stochastic programming}}, 
author = {Kabašinskas, Audrius and Kadikinaitė, Lina}, 
journal = {Journal of Sustainable Finance \& Investment}, 
issn = {20430795}, 
doi = {10.1080/20430795.2016.1188538}, 
abstract = {{The aim of this paper is to construct a portfolio of eight different stocks from New York Stock Exchange market (AIR, ABM, TSCO, HLX, KO, DIS, AMZN, and VZ) using stochastic programming. The next stage (period) prices are generated using a stochastic difference equation in order to introduce uncertainty. For the portfolio selection, we use three different risk measures–min–max decision rule, value-at-risk, and conditional value-at-risk. After constructing three different portfolios, they are compared using well-known efficiency ratios–Sharpe, Sortino, and Rachev ratios. © 2016, © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--10}, 
number = {3}, 
volume = {6}
}
@article{10.1080/24754269.2020.1758500, 
year = {2020}, 
title = {{Optimal reinsurance designs based on risk measures: a review}}, 
author = {Cai, Jun and Chi, Yichun}, 
journal = {Statistical Theory and Related Fields}, 
issn = {24754269}, 
doi = {10.1080/24754269.2020.1758500}, 
abstract = {{Reinsurance is an effective way for an insurance company to control its risk. How to design an optimal reinsurance contract is not only a key topic in actuarial science, but also an interesting research question in mathematics and statistics. Optimal reinsurance design problems can be proposed from different perspectives. Risk measures as tools of quantitative risk management have been extensively used in insurance and finance. Optimal reinsurance designs based on risk measures have been widely studied in the literature of insurance and become an active research topic. Different research approaches have been developed and many interesting results have been obtained in this area. These approaches and results have potential applications in future research. In this article, we review the recent advances in optimal reinsurance designs based on risk measures in static models and discuss some interesting problems on this topic for future research. © 2020, © East China Normal University 2020.}}, 
pages = {1--13}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jimonfin.2011.08.002, 
year = {2011}, 
title = {{Emerging markets and portfolio foreign exchange risk: An empirical investigation using a value-at-risk decomposition technique}}, 
author = {Sirr, Gordon and Garvey, John and Gallagher, Liam}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2011.08.002}, 
abstract = {{The correlation between a portfolio's equity and foreign exchange components plays a role in reducing foreign exchange exposure. Investors must account for this correlation when determining the extent of foreign exchange risk in emerging market equity portfolio investments. This study employs a VaR risk factor mapping technique, under the variance-covariance VaR approach, to decompose portfolio risk in Argentina, Brazil, China, India, Mexico and Russia. For comparison purposes, the same technique is used to decompose portfolio risk in the US. The study is conducted from the perspective of a European equity investor with a portfolio of equities in each country. By employing the VaR decomposition technique, the correlation between a portfolio's equity and foreign exchange components is taken into account and portfolio foreign exchange risk is extracted from portfolio systematic risk. Our results uniquely demonstrate significant variation in foreign exchange risk in emerging markets. © 2011 Elsevier Ltd.}}, 
pages = {1749--1772}, 
number = {8}, 
volume = {30}
}
@article{10.1007/978-3-642-17254-0_12, 
year = {2012}, 
title = {{Value at risk estimation}}, 
author = {Chen, Ying and Lu, Jun}, 
issn = {NA}, 
doi = {10.1007/978-3-642-17254-0\_12}, 
abstract = {{This chapter reviews the recent developments of Value at Risk (VaR) estimation. In this survey, the most available univariate and multivariatemethods are presented. The robustness and accuracy of these estimation methods are investigated based on the simulated and real data. In the backtesting procedure, the conditional coverage test (Christoffersen, Int. Econ. Rev. 39:841-862, 1998), the dynamic quantile test (Engle and Manganelli, J. Bus. Econ. Stat. 22(4):367-381, 2004) and Ljung-Box test (Berkowitz and O’Brien, J. Finance 57(3):1093-1111, 2002) are used to justify the performance of the methods. © Springer-Verlag Berlin Heidelberg 2012.}}, 
pages = {307--333}, 
number = {NA}, 
volume = {NA}
}
@article{10.2753/ree1540-496x430601, 
year = {2007}, 
title = {{Revenue volatility and fiscal risks: An application of value-at-risk techniques to Hong Kong's fiscal policy}}, 
author = {Porter, Nathan}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.2753/ree1540-496x430601}, 
abstract = {{Revenue volatility poses challenges for fiscal policy makers. It can create risks to service provision, require borrowing, or entail sudden tax changes. This paper investigates the use of value-at-risk techniques to measure the fiscal risks caused by volatility as well as the sensitivity of measured risks to policies that may limit volatility. The revenue of Hong Kong's Special Administrative Region (SAR) is among the most volatile in Asia, and thus is a natural case for applying these techniques. Reflecting its revenue volatility, Hong Kong's SAR has traditionally held high fiscal savings (reserves), and the value of the self-insurance these savings provide is also discussed. © 2007 M.E. Sharpe, Inc. All rights reserved.}}, 
pages = {6--24}, 
number = {6}, 
volume = {43}
}
@article{10.1007/s00184-013-0469-1, 
year = {2014}, 
title = {{Modified maximum spacings method for generalized extreme value distribution and applications in real data analysis}}, 
author = {Huang, Chao and Lin, Jin-Guan}, 
journal = {Metrika}, 
issn = {00261335}, 
doi = {10.1007/s00184-013-0469-1}, 
abstract = {{This paper analyzes weekly closing price data of the S\&P 500 stock index and electrical insulation element lifetimes data based on generalized extreme value distribution. A new estimation method, modified maximum spacings (MSP) method, is proposed and obtained by using interior penalty function algorithm. The standard error of the proposed method is calculated through Bootstrap method. The asymptotic properties of the modified MSP estimators are discussed. Some simulations are performed, which show that the proposed method is not only available for the whole shape parameter space, but is also of high efficiency. The benchmark risk index, value at risk (VaR), is evaluated according to the proposed method, and the confidence interval of VaR is also calculated through Bootstrap method. Finally, the results are compared with those derived by empirical calculation and some existing methods. © 2013, Springer-Verlag Berlin Heidelberg.}}, 
pages = {867--894}, 
number = {7}, 
volume = {77}
}
@article{10.3390/su11082446, 
year = {2019}, 
title = {{The eco-costs of material scarcity, a resource indicator for LCA, derived from a statistical analysis on excessive price peaks}}, 
author = {Vogtländer, Joost and Peck, David and Kurowicka, Dorota}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su11082446}, 
abstract = {{The availability of resources is crucial for the socio-economic stability of our society. For more than two decades, there was a debate on how to structure this issue within the context of life-Cycle assessment (LCA). The classical approach with LCA is to describe "scarcity" for future generations (100-1000 years) in terms of absolute depletion. The problem, however, is that the long-term availability is simply not known (within a factor of 100-1000). Outside the LCA community, the short-term supply risks (10-30 years) were predicted, resulting in the list of critical raw materials (CRM) of the European Union (EU), and the British risk list. The methodology used, however, cannot easily be transposed and applied into LCA calculations. This paper presents a new approach to the issue of short-term material supply shortages, based on subsequent sudden price jumps, which can lead to socio-economic instability. The basic approach is that each resource is characterized by its own specific supply chain with its specific price volatility. The eco-costs of material scarcity are derived from the so-called value at risk (VAR), a well-known statistical risk indicator in the financial world. This paper provides a list of indicators for 42 metals. An advantage of the system is that it is directly related to business risks, and is relatively easy to understand. A disadvantage is that "statistics of the past" might not be replicated in the future (e.g., when changing from structural oversupply to overdemand, or vice versa, which appeared an issue for two companion metals over the last 30 years). Further research is recommended to improve the statistics. © 2019 by the authors.}}, 
pages = {2446}, 
number = {8}, 
volume = {11}
}
@article{10.1016/j.eneco.2021.105452, 
year = {2021}, 
title = {{Revisiting value-at-risk and expected shortfall in oil markets under structural breaks: The role of fat-tailed distributions}}, 
author = {Patra, Saswat}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2021.105452}, 
abstract = {{Modeling the volatility of oil prices is extremely crucial from a risk management perspective. Value-at-risk (VaR) and Expected Shortfall (ES), the two most popular measures of risk in financial markets, are dependent on the volatility of the oil prices. In this study, Pearson Type-IV and Johnsons Su distributions are explored as two alternate distributions with characteristics, such as asymmetry and heavy tail, to model the volatility and forecast VaR/ES. The estimation is carried out under endogenously determined structural breaks from the data. Various Backtesting methodologies are employed to test the efficacy of the forecasts. The empirical results obtained show that the models with Pearson's Type-IV and Johnson's Su distributions outperform other fat-tailed distributions and the normal distribution especially at the 1\% (for long positions) and 99\% (for short positions) level. This has policy implications for the oil producing companies, market participants, regulators in the energy sector and the government in general. © 2021 Elsevier B.V.}}, 
pages = {105452}, 
number = {NA}, 
volume = {101}
}
@article{10.1109/access.2021.3116869, 
year = {2021}, 
title = {{Landslide Risk Assessment Using Granular Fuzzy Rule-Based Modeling: A Case Study on Earthquake-Triggered Landslides}}, 
author = {Shi, Bingxin and Zeng, Ting and Tang, Chuan and Zhang, Lifang and Xie, Zhuojuan and Lv, Guojun and Wu, Qihong}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2021.3116869}, 
abstract = {{Landslides are one type of destructive and recurring natural calamities in the mountainous regions. The landslide occurrences often lead to immense damage to local infrastructure and loss of land, human lives and livestock. Data-driven risk assessment of landslide risk plays a crucial role in preventing the incoming landslide occurrences and recurrences. In this research, we developed a human-centric framework using information-granules to perform risk assessment of a group of landslides. First, the density-based spatial clustering of applications with noise (DBSCAN) has been selected as the backbone unsupervised learning method to subclusters for landslide risk indication. The clustering outcomes are visualized via t-distributed stochastic neighbor embedding (t-SNE) in the 2-D embedding space. Second, the prototype points within the subclusters produced by DBSCAN are computed for granular construction. Third, interval-based information-granules are constructed and measured via coverage, specificity and area under the coverage-specificity curve (AUC). Last, with the optimal information-granules constructed, two risk measures namely Value-at-Risk (VaR) and Conditional-Value-at-Risk (CVaR) are computed to interpret the rule-based information-granules with respect to the key attributes. Comparative experiments have also been performed against other benchmarking clustering approaches. Computational results indicate that the information-granules constructed from DBSCAN subclusters offered enhanced performance in reveal meaningful information-granules and provide promising results. The proposed approach can capture the main essence of landslide pattern with higher interpretability and help to reduce the computing overhead. © 2013 IEEE.}}, 
pages = {135790--135802}, 
number = {NA}, 
volume = {9}
}
@article{10.1016/j.orl.2014.09.004, 
year = {2014}, 
title = {{Distributionally robust discrete optimization with Entropic Value-at-Risk}}, 
author = {Long, Daniel Zhuoyu and Qi, Jin}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2014.09.004}, 
abstract = {{We study the discrete optimization problem under the distributionally robust framework. We optimize the Entropic Value-at-Risk, which is a coherent risk measure and is also known as Bernstein approximation for the chance constraint. We propose an efficient approximation algorithm to resolve the problem via solving a sequence of nominal problems. The computational results show that the number of nominal problems required to be solved is small under various distributional information sets. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {532--538}, 
number = {8}, 
volume = {42}
}
@article{10.1109/pct.2007.4538440, 
year = {2007}, 
title = {{Effect of risk measures on bilateral trading in electricity markets}}, 
author = {Khatib, Sameh El and Quiles, Catalina Gomez and Galiana, Francisco D.}, 
journal = {2007 IEEE Lausanne Power Tech}, 
issn = {NA}, 
doi = {10.1109/pct.2007.4538440}, 
abstract = {{A scheme is developed and tested to negotiate bilateral contracts in mixed pool/bilateral electricity markets. Each negotiating party can choose any of the following three general measures to assess its profit risk: (i) Regret; (ii) Dispersion-from-the-mean; (iii) Value-at-risk. Similarly, a mix of three measures can be used to describe the benefit: (i) Expected profit; (ii) Expected rate of return; (iii) Expected ratio of the actual to ideal profits. The main results include: An analysis and comparison of different measures of risk and benefit on the outcome of a bilateral negotiation; an analysis of the impact on the outcome of the negotiation of various sources of uncertainty. ©2007 IEEE.}}, 
pages = {932--937}, 
number = {NA}, 
volume = {NA}
}
@article{10.37418/amsj.10.1.2, 
year = {2021}, 
title = {{Statistical modelling of malaysia trading gold price using extreme value theory approach}}, 
author = {Ali, N and Zaimi, N N and Ali, N Mohamed}, 
journal = {Advances in Mathematics: Scientific Journal}, 
issn = {18578365}, 
doi = {10.37418/amsj.10.1.2}, 
abstract = {{The monthly maxima of negative log returns of Malaysia gold prices are modelled using Generalized Extreme Value distribution from Extreme Value Theory. The maximum likelihood estimation and L-moments methods are used to estimate the parameters of the GEV model. The extreme risk measure through Value-at-risk for 10\%, 5\%, and 1\% upper quantile is calculated in order to evaluate maximum losses in investing in a gold market. The assessment of the quantile-quantile plot showed that the GEV model fit the data well using the maximum likelihood estimation. The estimates of VaR showed that for the worst case scenario, the expected market value of gold would not lose more than 0.009424\%, 0.014211\% and 0.028159\% with 90\%, 95\% and 99\% confidence respectively. © 2021, Research Publication. All rights reserved.}}, 
pages = {1857--8365}, 
number = {1}, 
volume = {10}
}
@article{10.1109/icmlc.2006.258609, 
year = {2006}, 
title = {{Estimating Value at Risk of a listed firm in China}}, 
author = {Liu, Rui and Zhan, Yuan-Rui and Liu, Jia-Peng}, 
journal = {2006 International Conference on Machine Learning and Cybernetics}, 
issn = {NA}, 
doi = {10.1109/icmlc.2006.258609}, 
abstract = {{This paper explores the application of the financial engineering techniques in the evaluation of total value risk on a sample-listed firm of China. Merton structural model and an experienced relation function are applied to evaluate the firm's value. GARCH model is also employed to estimate both the return series and the volatility of the equity. Under this situation, the expected distribution of the firm value and the Value at Risk (VaR) can be obtained based on Monte Carlo simulation. Since the paper may be the first one trying to value the potential risk of firm value, it is very helpful for analyzing mergers and acquisitions in the capital market as well as controlling the risk of asset for those large state-owned asset management companies in China. © 2006 IEEE.}}, 
pages = {2137--2141}, 
number = {NA}, 
volume = {2006}
}
@article{10.1016/j.jfds.2016.06.001, 
year = {2016}, 
title = {{Forecasting daily conditional volatility and h-step-ahead short and long Value-at-Risk accuracy: Evidence from financial data}}, 
author = {Mabrouk, Samir}, 
journal = {The Journal of Finance and Data Science}, 
issn = {24059188}, 
doi = {10.1016/j.jfds.2016.06.001}, 
abstract = {{In this article we evaluate the daily conditional volatility and h-step-ahead Value at Risk (VaR) forecasting power of three long memory GARCH-type models (FIGARCH, HYGARCH \& FIAPARCH). The forecasting exercise is done for financial assets including seven stock indices (Dow Jones, Nasdaq100, S\&P 500, DAX30, CAC40, FTSE100 and Nikkei 225) and three exchange rates vis-a-vis the US dollar (the GBP- USD, YEN-USD and Euro-USD). Because all return series are skewed and fat tailed, each conditional volatility model is estimated under a skewed Student distribution. Consistent with the idea that the accuracy of VaR estimates are sensitive to the adequacy of the volatility model used, h-step-ahead VaR forecasts are based on the skewed Student-t AR(1)-FIAPARCH (1,d,1). This model can jointly accounts for the salient features of financial time series. Our findings reveal that the skewed Student AR (1) FIAPARCH (1.d.1) relatively outperforms the other models in out-of-sample forecasts for one, five and fifteen day forecast horizons. However, there is no difference for the AR (1) FIGARCH (1.d.1) and AR (1) HYGARCH (1.d.1) models since they have the same forecasting ability. Results indicate also that skewed Student-t FIAPARCH (1,d,1) model provides more accurate one-day-ahead VaR forecasts for both long and short trading positions than those generated using alternative horizons (5-day and 15-day-ahead). This result holds for each of the financial time series. © 2016 China Science Publishing \& Media Ltd.}}, 
pages = {136--151}, 
number = {2}, 
volume = {2}
}
@article{10.1007/s10287-018-0319-8, 
year = {2019}, 
title = {{Simulation and evaluation of the distribution of interest rate risk}}, 
author = {Hagenbjörk, Johan and Blomvall, Jörgen}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-018-0319-8}, 
abstract = {{We study methods to simulate term structures in order to measure interest rate risk more accurately. We use principal component analysis of term structure innovations to identify risk factors and we model their univariate distribution using GARCH-models with Student’s t-distributions in order to handle heteroscedasticity and fat tails. We find that the Student’s t-copula is most suitable to model co-dependence of these univariate risk factors. We aim to develop a model that provides low ex-ante risk measures, while having accurate representations of the ex-post realized risk. By utilizing a more accurate term structure estimation method, our proposed model is less sensitive to measurement noise compared to traditional models. We perform an out-of-sample test for the U.S. market between 2002 and 2017 by valuing a portfolio consisting of interest rate derivatives. We find that ex-ante Value at Risk measurements can be substantially reduced for all confidence levels above 95\%, compared to the traditional models. We find that that the realized portfolio tail losses accurately conform to the ex-ante measurement for daily returns, while traditional methods overestimate, or in some cases even underestimate the risk ex-post. Due to noise inherent in the term structure measurements, we find that all models overestimate the risk for 10-day and quarterly returns, but that our proposed model provides the by far lowest Value at Risk measures. © 2018, The Author(s).}}, 
pages = {297--327}, 
number = {1-2}, 
volume = {16}
}
@article{10.1016/s0378-4266(02)00263-7, 
year = {2002}, 
title = {{The emperor has no clothes: Limits to risk modelling}}, 
author = {Danı́elsson, Jón}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00263-7}, 
abstract = {{This paper considers the properties of risk measures, primarily value-at-risk (VaR), from both internal and external (regulatory) points of view. It is argued that since market data is endogenous to market behavior, statistical analysis made in times of stability does not provide much guidance in times of crisis. In an extensive survey across data classes and risk models, the empirical properties of current risk forecasting models are found to be lacking in robustness while being excessively volatile. For regulatory use, the VaR measure may give misleading information about risk, and in some cases may actually increase both idiosyncratic and systemic risk. © 2002 Published by Elsevier Science B.V.}}, 
pages = {1273--1296}, 
number = {7}, 
volume = {26}
}
@article{10.1080/15326340600878677, 
year = {2006}, 
title = {{Risk measures and robust optimization problems}}, 
author = {Schied∗, Alexander}, 
journal = {Stochastic Models}, 
issn = {15326349}, 
doi = {10.1080/15326340600878677}, 
abstract = {{These lecture notes give a survey on recent developments in the theory of risk measures. The first part outlines the general representation theory of risk measures in a static one-period setting. In particular, it provides structure theorems for law-invariant risk measures. Examples include Value at Risk, Average Value at Risk, distortion risk measures, and risk measures arising from robust preferences. The second part analyzes risk measures and associated robust optimization problems in the framework of dynamic financial market models. The concept of efficient hedging, as introduced by Föllmer and Leukert[32], is discussed in terms of the more general framework of convex risk measures. The last two sections are devoted to the construction of optimal investment strategies under Knightian uncertainty.}}, 
pages = {753--831}, 
number = {4}, 
volume = {22}
}
@article{10.1088/1742-6596/1725/1/012097, 
year = {2021}, 
title = {{Quota-share and stop-loss reinsurance combination based on Value-at-Risk (VaR) optimization}}, 
author = {Putri, A D and Nurrohmah, S and Fithriani, I}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1725/1/012097}, 
abstract = {{Every insurance companies have a capacity limit related to the maximum claim that can be borne. Therefore, insurance companies need to reinsure risks to reinsurance companies. Besides of quota-share, types of reinsurance contracts that commonly used is stop-loss. The quota-share reinsurance premium is proportional based on the amount claim that is covered, but not safe against a large claim. While for stop-loss, the reinsurance premium is relatively large but safe for a large claim. So, this paper combines both types of reinsurance to cover the shortcomings with their respective strengths. After being combined, it is necessary to determine the optimal quota-share proportion and stop-loss retention. One criterion of determines optimal proportion and retention is based on Value-at-Risk (VaR) optimization. With the reinsurance premium as a constraint, this optimization problem is solved for each type of reinsurance combination, be it quota-share before stop-loss or stop-loss before quota-share. From each of these types combinations, the result is optimal quota-share proportion and stop-loss retention, so as produce a minimum VaR value from the borne risk by insurance companies. by comparing the results of VaR optimization of these combinations, stop-loss before quota-share is obtained resulting in a more minimum VaR value. © 2021 Journal of Physics: Conference Series.}}, 
pages = {012097}, 
number = {1}, 
volume = {1725}
}
@article{10.1017/asb.2018.35, 
year = {2019}, 
title = {{A CONDITIONAL EQUITY RISK MODEL FOR REGULATORY ASSESSMENT}}, 
author = {Floryszczak, A. and Véhel, J. Lévy and Majri, M.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2018.35}, 
abstract = {{We define and study in this work a simple model designed for managing long-term market risk of financial institutions with long-term commitments. It allows the assessment of solvency capital requirements and the allocation of risk budgets. This model allows one to avoid over-assessment of solvency capital requirements specifically after market disruptions. It relies on a dampener component in charge of refining risk assessment after market failures. Rather than aiming at a realistic and thus complex description of equity prices movements, this model concentrates on minimal features enabling accurate computation of capital requirements. It is defined both in a discrete and continuous fashion. In the latter case, we prove the existence, uniqueness and stability of the solution of the stochastic functional differential equation that specifies the model. One difficulty is that the proposed underlying stochastic process has neither stationary nor independent increments. We are however able to perform statistical analyses in view of its validation. Numerical experiments show that our model outperforms more elaborate ones of common use as far as medium-term (between 6 months and 5 years) risk assessment is concerned. © Astin Bulletin 2018.}}, 
pages = {217--242}, 
number = {1}, 
volume = {49}
}
@article{10.1109/tcc.2014.2382099, 
year = {2015}, 
title = {{Clabacus: A Risk-Adjusted Cloud Resources Pricing Model Using Financial Option Theory}}, 
author = {Sharma, Bhanu and Thulasiram, Ruppa K. and Thulasiraman, Parimala and Buyya, Rajkumar}, 
journal = {IEEE Transactions on Cloud Computing}, 
issn = {21687161}, 
doi = {10.1109/tcc.2014.2382099}, 
abstract = {{In Cloud computing, clients would like to pay fair price for the resources while providers would like to make profit for their services. In this study, we propose a Cloud Compute Commodity (C3 ) pricing architecture called Clabacus(Cloud-Abacus) to serve both parties. We use concepts and algorithms from financial option theory to develop Clabacus. We propose a general formula, called compound-Moores law, that captures the technological advances of the resources, rate of inflation and depreciation etc. We map these Cloud parameters to the option pricing parameters to effectively modify the option pricing algorithm in order to compute Cloud resource price. Using financial value-at-risk (VaR) analysis, we adjust the computed resource price to incorporate the inherent risks of the Cloud provider. We propose fuzzy logic and genetic algorithm based approaches to compute the VaR of the provider's resources. We have incorporated this into our Clabacus architecture. Finally, we study the effects of quality of service, rate of depreciation, rate of inflation, capital investment on the Cloud resource price for both client and provider. We show that if the prices are adjusted within a lower and upper bound, SLA can be guaranteed. © 2013 IEEE.}}, 
pages = {332--344}, 
number = {3}, 
volume = {3}
}
@article{10.1080/00036846.2014.1000528, 
year = {2015}, 
title = {{Hedging effectiveness of the hedged portfolio: the expected utility maximization subject to the value-at-risk approach}}, 
author = {Chuang, Chung-Chu and Wang, Yi-Hsien and Yeh, Tsai-Jung and Chuang, Shuo-Li}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2014.1000528}, 
abstract = {{Multivariate volatilities and distribution play an important role in portfolio selection and can be used to calculate the value-at-risk (VaR) of a multiple-asset financial position. This study proposes a new expected utility maximization (EUM) model that accounts for VaR (EUM model with a VaR constraint (EUM–VaR)). Additionally, using the EUM–VaR model, this study investigates the hedging effectiveness of short and long hedged portfolios constructed with multivariate generalized autoregressive conditional heteroscedasticity (GARCH)-type models that feature level effects and multivariate normal (Formula presented.) and skewed (Formula presented.) distributions for stock indexes and their corresponding futures in the Greater China Region. It is found that, all else equal, portfolios constructed using the multivariate skewed (Formula presented.) distribution are far more effective in hedging than those that rely on the other distributions, and the effectiveness of hedged portfolios from the multivariate GARCH-type models with level effects outperform those without level effects. Additionally, the effectiveness of hedged portfolios from multivariate asymmetric GARCH-type models exceeds that of those from multivariate symmetric GARCH-type models. Thus, investors should select the multivariate asymmetry in volatility, multivariate asymmetry in distribution, and EUM–VaR models to construct effectively hedged portfolios. The results of this study can provide useful implications for investors looking to manage risk. © 2015, © 2015 Taylor \& Francis.}}, 
pages = {2040--2052}, 
number = {20}, 
volume = {47}
}
@article{10.1080/24725854.2020.1869352, 
year = {2021}, 
title = {{Solving Bayesian risk optimization via nested stochastic gradient estimation}}, 
author = {Cakmak, Sait and Wu, Di and Zhou, Enlu}, 
journal = {IISE Transactions}, 
issn = {24725854}, 
doi = {10.1080/24725854.2020.1869352}, 
abstract = {{In this article, we aim to solve Bayesian Risk Optimization (BRO), which is a recently proposed framework that formulates simulation optimization under input uncertainty. In order to efficiently solve the BRO problem, we derive nested stochastic gradient estimators and propose corresponding stochastic approximation algorithms. We show that our gradient estimators are asymptotically unbiased and consistent, and that the algorithms converge asymptotically. We demonstrate the empirical performance of the algorithms on a two-sided market model. Our estimators are of independent interest in extending the literature of stochastic gradient estimation to the case of nested risk measures. © Copyright © 2021 “IISE”.}}, 
pages = {1--34}, 
number = {10}, 
volume = {53}
}
@article{10.1109/pes.2008.4596785, 
year = {2008}, 
title = {{Dynamic risk management in electricity portfolio optimization via polyhedral risk functionals}}, 
author = {Eichhorn, Andreas and Römisch, Werner}, 
journal = {2008 IEEE Power and Energy Society General Meeting - Conversion and Delivery of Electrical Energy in the 21st Century}, 
issn = {NA}, 
doi = {10.1109/pes.2008.4596785}, 
abstract = {{We propose a methodology for combining risk management with optimal planning of power production and trading based on probabilistic knowledge about future uncertainties such as demands and spot prices. Typically, such a joint optimization of risk and (expected) revenue yields additional overall efficiency. Our approach is based on stochastic optimization (stochastic programming) with a risk functional as objective. The latter maps an uncertain cash flow to a real number. In particular, we employ socalled polyhedral risk functionals which, though being non-linear mappings, preserve linearity structures of optimization problems. Therefore, these are favorable to the numerical tractability of the optimization problems. The class of polyhedral risk functionals contains well-known risk functionals such as Average-Value-at-Risk and expected polyhedral utility. Moreover, it is also capable to model different dynamic risk mitigation strategies. © 2008 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0161-8938(02)00164-3, 
year = {2002}, 
title = {{Measuring income risk to promote macro markets}}, 
author = {Majumder, Neeta and Majumder, Debasish}, 
journal = {Journal of Policy Modeling}, 
issn = {01618938}, 
doi = {10.1016/s0161-8938(02)00164-3}, 
abstract = {{Volatility of GDP is a most common problem worldwide and one of the remedies might be to share the GDP risk, through the trading of GDP growth rate-related bond, to obtain a mutually preferable allocation of aggregate income. We have experimented the widely used risk assessment technique, Value at Risk (VaR), as a measure of GDP risk. The advantage of VaR involves the estimation of adequate capital as a safeguard and in the case of country-specific income risk the adequate capital may be assured through the inter-country traded GDP growth-related bonds. We have considered Asian developing countries due to their high volatile GDP growth and designed the optimal trading policy of these bonds in such a way that the income risk would be hedged completely. © 2002 Society for Policy Modeling. Published by Elsevier Science Inc. All rights reserved.}}, 
pages = {607--619}, 
number = {6}, 
volume = {24}
}
@article{10.1504/ijram.2007.012743, 
year = {2007}, 
title = {{Equity trading risk management: The case of Casablanca Stock Exchange}}, 
author = {Janabi, Mazin A M Al}, 
journal = {International Journal of Risk Assessment and Management}, 
issn = {14668297}, 
doi = {10.1504/ijram.2007.012743}, 
abstract = {{Despite the fact that emerging markets are characterised in general as illiquid, segmented, politically unstable, with lack of regulations and historical financial databases, they do have enormous advantages for markets' participants. While these emerging markets share some similarities in development patterns, it is often their individual differences that create unique expected return opportunities and embedded risks, which may be addressed through art and science risk management techniques. In this article, key equity trading risk management methods, rules and procedures that financial entities, regulators and policymakers should consider in setting-up their daily equity trading risk management objectives are examined and adapted to the specific needs of emerging markets. In order to illustrate the proper use of Value At Risk (VAR) and stress-testing (scenario analysis) methods, real-world examples and practical reports of equity trading risk management are presented for the Casablanca Stock Exchange (CSE). Copyright © 2007 Inderscience Enterprises Ltd.}}, 
pages = {535}, 
number = {4}, 
volume = {7}
}
@article{10.21314/jor.2018.377, 
year = {2018}, 
title = {{Initial margin with risky collateral}}, 
author = {Shi, Ming and Yu, Xinxin and Zhang, Ke}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.377}, 
abstract = {{The provision of initial margin (IM) for noncentrally cleared derivatives has gained prominence in financial markets as a way to mitigate counterparty credit risk. IM protects transacting parties from the potential increase in future exposure that could arise from the portfolio value change during the time that it takes to close out and replace the portfolio following a counterparty default. The Basel Committee on Banking Supervision prescribes IM as the ten-day value-at-risk (VaR) of the portfolio at the 99th percentile confidence level. Current industry standard VaR approaches such as parametric or historical VaR methods necessitate an assumption that IM is posted in cash or cash-equivalent assets. Although many counterparty-credit-risk-related models exist in the academic literature, there has been little focus on achieving a theoretical basis for calculating margin with consideration of market risk of the collateral. In this paper, we explore the complication of calculating the IM amount required when collateral comprises risky assets in a parametric VaR framework. We show that the required IM amount can be calculated by solving a quadratic inequality. We also introduce a geometric structure to compare the IM amounts calculated using risky and nonrisky collateral. © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {3}, 
volume = {20}
}
@article{10.1007/978-3-642-23023-3_83, 
year = {2011}, 
title = {{A comparative analysis of credit risk management models for banking industry using simulation}}, 
author = {Chen, Hsin-Hung and Shia, Ben-Chang and Lee, Hsiu-Yu}, 
journal = {Communications in Computer and Information Science}, 
issn = {18650929}, 
doi = {10.1007/978-3-642-23023-3\_83}, 
abstract = {{Risk management is an issue that has become increasingly important. Basel II Accord has been widely discussed since it was proposed. However, the comparative analysis of CreditMetrics with Basel II Accord has not been found in previous literatures. The objective of this study is to compare CreditMetrics with Basel II Accord using empirical data and simulation programs. Moreover, the fitness of the standard for Basel II Accord which proposed the minimum requirement of 8\% of capital to risk-weighted assets is discussed in this study. The records of the data system in a bank listed by the Taiwan Stock Exchange Corporation (TSEC) were used as the empirical data in this research. The results showed that the expected loss calculated by the 8\% capital ratio defined in Basel II is clearly lower than the Credit VaR obtained from the CreditMetrics model. © 2011 Springer-Verlag Berlin Heidelberg.}}, 
pages = {554--562}, 
number = {PART 1}, 
volume = {208 CCIS}
}
@article{10.1016/j.cor.2007.12.008, 
year = {2009}, 
title = {{Risk management in uncapacitated facility location models with random demands}}, 
author = {Wagner, Michael R. and Bhadury, Joy and Peng, Steve}, 
journal = {Computers \& Operations Research}, 
issn = {03050548}, 
doi = {10.1016/j.cor.2007.12.008}, 
abstract = {{In this paper we consider a location-optimization problem where the classical uncapacitated facility location model is recast in a stochastic environment with several risk factors that make demand at each customer site probabilistic and correlated with demands at the other customer sites. Our primary contribution is to introduce a new solution methodology that adopts the mean-variance approach, borrowed from the finance literature, to optimize the "Value-at-Risk" (VaR) measure in a location problem. Specifically, the objective of locating the facilities is to maximize the lower limit of future earnings based on a stated confidence level. We derive a nonlinear integer program whose solution gives the optimal locations for the p facilities under the new objective. We design a branch-and-bound algorithm that utilizes a second-order cone program (SOCP) solver as a subroutine. We also provide computational results that show excellent solution times on small to medium sized problems. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {1002--1011}, 
number = {4}, 
volume = {36}
}
@article{10.1109/cso.2009.203, 
year = {2009}, 
title = {{Selecting optimal portfolio on the basis of value at risk}}, 
author = {Peng, Hongmei}, 
journal = {2009 International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2009.203}, 
abstract = {{Within the framework of Markowitz's portfolio theory, this paper analyzes the problem of the optimal portfolio on VaR. By using historical data of return loss to simulate several situations, we built an optimal portfolio model for VaR and proposed the detailed algorithms. The simplicity and the effectiveness of these algorithms were also demonstrated with concrete examples.© 2009 IEEE.}}, 
pages = {558--561}, 
number = {NA}, 
volume = {2}
}
@article{10.1117/12.2585760, 
year = {2021}, 
title = {{Value at Risk strategies for robot swarms in hazardous environments}}, 
author = {Hunt, Edmund R. and Cullen, Conor B. and Hauert, Sabine}, 
journal = {Unmanned Systems Technology XXIII}, 
issn = {0277786X}, 
doi = {10.1117/12.2585760}, 
abstract = {{A promising application of swarm robotics is environmental monitoring, whereby large numbers of self-organizing agents are deployed concurrently to gather information on a short- or long-term basis. The greater resilience and attritability of a swarm system may be particularly valuable in hazardous or adversarial environments where a proportion of agents are likely to be damaged or destroyed. In such contexts the profitable information gathered by sensor payloads on robots is associated with material risk. Relatively little work has been done to consider how to manage this risk autonomously and effectively at the individual and system level. The field of financial risk management provides pre-existing tools and frameworks to get a head-start on this challenge. The method of Value at Risk (VaR) allows the easy quantification of prospective losses over a defined time period and confidence interval. Here, we consider VaR in a multi-agent context where the environment is intrinsically risky, for example containing damaging radiation sources. In agent-based simulations, individuals calculate VaR in real time and broadcast a self-triggered alert to their neighbors when their VaR limit is breached, helping them to avoid the area. This minimal communication system is effective at decreasing overall swarm exposure to hazards, while permitting agents to make risk-weighted explorations of hazardous areas. We further discuss the opportunity for finance-inspired risk management frameworks to be developed in the multi-agent systems context. © 2021 SPIE.}}, 
pages = {117580M--117580M-20}, 
number = {NA}, 
volume = {11758}
}
@article{10.18488/journal.aefr.2020.107.778-789, 
year = {2020}, 
title = {{Risk threshold for sustainable current account balance of payments: An Indonesian case}}, 
author = {}, 
issn = {23052147}, 
doi = {10.18488/journal.aefr.2020.107.778-789}, 
abstract = {{The objective of this study was to model the behavior of the Current Account Balance of Payments (CAB) for Indonesia. It also calculated the conditional Value at Risk (VaR) as a measure of the risk level of the CAB. An ARDL (Autoregressive Distributed Lag) model and an EGARCH (Exponential Generalized Autoregressive Conditional Heteroskedasticity) model were used to estimate the CAB behavior for the annual data 1985-2018. The research found that exchange rates, growth of gross domestic product, inflation, total reserves, and unemployment are essential in determining the behavior and volatility of the CAB. The VaR calculated based on the conditional standard deviation that resulted from the EGARCH estimation shows that most of the time the Indonesian CAB is in safe conditions. However, the VaR has been violated by the actual CAB several times, and the violations coincide with various macroeconomic shocks. The Central Bank of Indonesia could calculate the VaR threshold using this method to evaluate the risky nature of the current account deficit. This study provides an alternative procedure to analyze and assess the current account balance risk to mitigate the impact of macroeconomic shocks. © 2020 AESS Publications. All Rights Reserved.}}, 
number = {7}, 
volume = {10}
}
@article{10.1093/jjfinec/nbr004, 
year = {2011}, 
title = {{Merits and drawbacks of variance targeting in GARCH models}}, 
author = {Francq, C and Horvath, L and Zakoian, J -M}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbr004}, 
abstract = {{Variance targeting estimation (VTE) is a technique used to alleviate the numerical difficulties encountered in the quasi-maximum likelihood estimation (QMLE) of GARCH models. It relies on a reparameterization of the model and a first-step estimation of the unconditional variance. The remaining parameters are estimated by quasi maximum likelihood (QML) in a second step. This paper establishes the asymptotic distribution of the estimators obtained by this method in univariate GARCH models. Comparisons with the standard QML are provided and the merits of the variance targeting method are discussed. In particular, it is shown that when the model is misspecified, the VTE can be superior to the QMLE for long-term prediction or value-at-risk calculation. An empirical application based on stock market indices is proposed. © The Author 2011. Published by Oxford University Press. All rights reserved.}}, 
pages = {619--656}, 
number = {4}, 
volume = {9}
}
@article{10.1007/s10878-013-9678-9, 
year = {2014}, 
title = {{Value-at-risk support vector machine: Stability to outliers}}, 
author = {Tsyurmasto, Peter and Zabarankin, Michael and Uryasev, Stan}, 
journal = {Journal of Combinatorial Optimization}, 
issn = {13826905}, 
doi = {10.1007/s10878-013-9678-9}, 
abstract = {{A support vector machine (SVM) stable to data outliers is proposed in three closely related formulations, and relationships between those formulations are established. The SVM is based on the value-at-risk (VaR) measure, which discards a specified percentage of data viewed as outliers (extreme samples), and is referred to as VaR -SVM. Computational experiments show that compared to the ν -SVM, the VaR-SVM has a superior out-of-sample performance on datasets with outliers. © 2013 Springer Science+Business Media New York.}}, 
pages = {218--232}, 
number = {1}, 
volume = {28}
}
@article{10.1016/j.rfe.2008.03.001, 
year = {2009}, 
title = {{Applying VaR to REITs: A comparison of alternative methods}}, 
author = {Lu, Chiuling and Wu, Sheng‐Ching and Ho, Lan‐Chih}, 
journal = {Review of Financial Economics}, 
issn = {10583300}, 
doi = {10.1016/j.rfe.2008.03.001}, 
abstract = {{This study employs five methods to calculate the VaR of twelve REITs portfolios and evaluates the accuracy of these methods. Firstly, we find that the VaR varies among individual portfolios. The Hotel REITs has consistently the largest VaR. The low-leveraging portfolio tends to have the largest VaR measured by the parametric methods, while the high leveraging portfolio has the largest VaR calculated by the non-parametric methods. Secondly, each method performs differently at different confidence levels, and no method dominates the others. At the 95\% confidence level, the EWMA method performs relatively well. The EQWMA and the two non-parametric methods perform equivalently and slightly overestimate VaRs. The EQWMAT method ranks the bottom and significantly overestimates VaRs for all portfolios. At the 99\% confidence level, the EQWMA method performs the best. The EQWMAT and the two non-parametric methods perform equivalently and may overestimate VaR for all portfolios. The EWMA method turns out to be the worst and tends to underestimate the VaR. These findings may provide more insights for institutional real estate investors. © 2008 Elsevier Inc. All rights reserved.}}, 
pages = {97--102}, 
number = {2}, 
volume = {18}
}
@article{10.1063/1.4887728, 
year = {2014}, 
title = {{Composite weibull-inverse transformed gamma distribution and its actuarial application}}, 
author = {Maghsoudi, Mastoureh and Bakar, Shaiful Anuar Abu and Hamzah, Nor Aishah}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.4887728}, 
abstract = {{This paper introduces a new composite model, namely, composite Weibull-Inverse Transformed Gamma distribution which assumes Weibull distribution for the head up to a specified threshold and inverse transformed gamma distribution beyond it. The closed form of probability density function (pdf) as well as the estimation of parameters by maximum likelihood method is presented. The model is compared with several benchmark distributions and their performances are measured. A well-known data set, Danish fire loss data, is used for this purpose and it's Value at Risk (VaR) using the new model is computed. In comparison to several standard models, the composite Weibull- Inverse Transformed Gamma model proved to be a competitor candidate. © 2014 AIP Publishing LLC.}}, 
pages = {1007--1012}, 
number = {NA}, 
volume = {1605}
}
@article{10.1007/s10687-014-0197-6, 
year = {2014}, 
title = {{Normex, a new method for evaluating the distribution of aggregated heavy tailed risks: Application to risk measures}}, 
author = {Kratz, M.}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-014-0197-6}, 
abstract = {{We develop theoretically as well as numerically a new method, Normex, for the sum of independent heavy tailed distributed random variables, to obtain the most accurate evaluation of its entire distribution. Normex provides sharp results, whatever the number of summands and the tail index are. It is particularly suited when the Central Limit Theorem (CLT) applies but with slow convergence of the mean and with a poor approximation for the tail. Hence, it is filling up a gap in the literature by giving an appropriate limit distribution in this case, in general better than with most standard methods. An application is developed to evaluate the Value-at-Risk of the yearly log returns of financial assets. © 2014, Springer Science+Business Media New York.}}, 
pages = {661--691}, 
number = {4}, 
volume = {17}
}
@article{10.1109/ism46123.2019.00045, 
year = {2019}, 
title = {{Deep Autoencoders with Value-at-Risk Thresholding for Unsupervised Anomaly Detection}}, 
author = {Akhriev, Albert and Marecek, Jakub}, 
journal = {2019 IEEE International Symposium on Multimedia (ISM)}, 
issn = {NA}, 
doi = {10.1109/ism46123.2019.00045}, 
abstract = {{Many real-world monitoring and surveillance applications require non-trivial anomaly detection to be run in the streaming model. We consider an incremental-learning approach, wherein a deep-autoencoding (DAE) model of what is normal is trained and used to detect anomalies at the same time. In the detection of anomalies, we utilise a novel thresholding mechanism, based on value at risk (VaR). We compare the resulting convolutional neural network (CNN) against a number of subspace methods and present results on changedetection.net. © 2019 IEEE.}}, 
pages = {208--2083}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/fuzzy.2011.6007313, 
year = {2011}, 
title = {{Re-scheduling the unit commitment problem in fuzzy environment}}, 
author = {Wang, Bo and Li, You and Watada, Junzo}, 
journal = {2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)}, 
issn = {10987584}, 
doi = {10.1109/fuzzy.2011.6007313}, 
abstract = {{The conventional prediction of future power demands are always made based on the historical data. However, the real power demands are affected by many other factors as weather, temperature and unexpected emergencies. The use of historical information alone cannot well predict real future demands. In this study, the experts' opinions from related fields are taken into consideration. To deal the uncertainty of historical data and imprecise experts' opinions, we employ fuzzy variables to better characterize the forecasted future power loads. The conventional unit commitment problem (UCP) is updated here by considering the spinning reserve costs in a fuzzy environment. As the solution, we proposed a heuristic algorithm called local convergence averse binary particle swarm optimization (LCA-PSO) to solve the UCP. The proposed model and algorithm are used to analyze several test systems. The comparisons between the proposed algorithm and the conventional approaches show that the LCA-PSO performs better in finding the optimal solutions. © 2011 IEEE.}}, 
pages = {1090--1095}, 
number = {NA}, 
volume = {NA}
}
@article{10.1177/097265270900800201, 
year = {2009}, 
title = {{Modified estimators of the Expected Shortfall}}, 
author = {Jadhav, Deepak and Ramanathan, T.V. and Naik-Nimbalkar, U.V.}, 
journal = {Journal of Emerging Market Finance}, 
issn = {09726527}, 
doi = {10.1177/097265270900800201}, 
abstract = {{The coherent risk measure Expected Shortfall is popularly considered as an alternative to Value-at-Risk. We briefly review all existing parametric and non-parametric methods to estimate Expected Shortfall. The historical method is considered as the best method of estimation for the Expected Shortfall, though it has a serious disadvantage of over-estimation in the presence of outliers in the return data. In this article, we propose two non-parametric estimators of Expected Shortfall which are robust to outliers. We estimate the Expected Shortfall corresponding to daily returns of some of the selected assets and indices of the Indian (BSE and NSE) and foreign stock markets (NYSE and LSE). The backtesting procedure boasts in confirming that the proposed non-parametric estimators are the best alternatives to the historical method in avoiding over-estimation of Expected Shortfall.}}, 
pages = {87--107}, 
number = {2}, 
volume = {8}
}
@article{10.1016/j.jbankfin.2014.04.023, 
year = {2014}, 
title = {{The limits of granularity adjustments}}, 
author = {Fermanian, Jean-David}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2014.04.023}, 
abstract = {{We provide a rigorous proof of granularity adjustment (GA) formulas to evaluate loss distributions and risk measures (value-at-risk) in the case of heterogenous portfolios, multiple systematic factors and random recoveries. As a significant improvement with respect to the literature, we detail all the technical conditions of validity and provide an upper bound of the remainder term for finite portfolio sizes. Moreover, we deal explicitly with the case of general loss distributions, possibly with masses. For some simple portfolio models, we prove empirically that the granularity adjustments do not always improve the infinitely granular first-order approximations. This stresses the importance of checking some conditions of regularity before relying on such techniques. Smoothing the underlying loss distributions through random recoveries or exposures improves the GA performances in general. © 2014 Elsevier B.V.}}, 
pages = {9--25}, 
number = {1}, 
volume = {45}
}
@article{10.1016/j.insmatheco.2015.08.005, 
year = {2015}, 
title = {{Optimal retention for a stop-loss reinsurance with incomplete information}}, 
author = {Hu, Xiang and Yang, Hailiang and Zhang, Lianzeng}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.08.005}, 
abstract = {{This paper considers the determination of optimal retention in a stop-loss reinsurance. Assume that we only have incomplete information on a risk X for an insurer, we use an upper bound for the value at risk (VaR) of the total loss of an insurer after stop-loss reinsurance arrangement as a risk measure. The adopted method is a distribution-free approximation which allows to construct the extremal random variables with respect to the stochastic dominance order and the stop-loss order. We derive the optimal retention such that the risk measure used in this paper attains the minimum. We establish the sufficient and necessary conditions for the existence of the nontrivial optimal stop-loss reinsurance. For illustration purpose, some numerical examples are included and compared with the results yielded in Theorem 2.1 of Cai and Tan (2007). © 2015 Elsevier B.V.}}, 
pages = {15--21}, 
number = {NA}, 
volume = {65}
}
@article{10.36244/icj.2019.3.6, 
year = {2019}, 
title = {{Methods for predicting behavior of elephant flows in data center networks}}, 
author = {Alawadi, Aymen Hasan and Zaher, Maiass and Molnár, Sándor}, 
journal = {Infocommunications journal}, 
issn = {20612079}, 
doi = {10.36244/icj.2019.3.6}, 
abstract = {{—Several Traffic Engineering (TE) techniques based on SDN (Software-defined networking) proposed to resolve flow competitions for network resources. However, there is no comprehensive study on the probability distribution of their throughput. Moreover, there is no study on predicting the future of elephant flows. To address these issues, we propose a new stochastic performance evaluation model to estimate the loss rate of two state-of-art flow scheduling algorithms including Equal-cost multi-path routing (ECMP), Hedera besides a flow congestion control algorithm which is Data Center TCP (DCTCP). Although these algorithms have theoretical and practical benefits, their effectiveness has not been statistically investigated and analyzed in conserving the elephant flows. Therefore, we conducted extensive experiments on the fat-tree data center network to examine the efficiency of the algorithms under different network circumstances based on Monte Carlo risk analysis. The results show that Hedera is still risky to be used to handle the elephant flows due to its unstable throughput achieved under stochastic network congestion. On the other hand, DCTCP found suffering under high load scenarios. These outcomes might apply to all data center applications, in particular, the applications that demand high stability and productivity. © 2019 Scientific Association for Infocommunications. All rights reserved.}}, 
pages = {34--41}, 
number = {3}, 
volume = {11}
}
@article{10.1002/for.2726, 
year = {2020}, 
title = {{Dynamic VaR forecasts using conditional Pearson type IV distribution}}, 
author = {Kuang, Wei}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2726}, 
abstract = {{This paper generalizes the exponentially weighted maximum likelihood (EWML) procedure to account for volatility and higher moment dynamics of the returns distribution. Prior research uses EWML to forecast value at risk (VaR) by assuming daily equity returns following a scaled t distribution. This approach does not capture the significant degree of skewness inherent in the data, which potentially leads to an underestimation of VaR. We employ the EWML procedure to estimate a time-varying Pearson IV distribution. Our results show that VaR forecasts based on Pearson IV using the EWML procedure are generally more accurate than those generated by scaled t and generalized autoregressive conditional heteroskedasticity (GARCH)-type models, particularly for assets with high leptokurtosis and negative skewness. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {500--511}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eneco.2006.04.005, 
year = {2006}, 
title = {{Modeling and forecasting petroleum futures volatility}}, 
author = {Sadorsky, Perry}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2006.04.005}, 
abstract = {{Forecasts of oil price volatility are important inputs into macroeconometric models, financial market risk assessment calculations like value at risk, and option pricing formulas for futures contracts. This paper uses several different univariate and multivariate statistical models to estimate forecasts of daily volatility in petroleum futures price returns. The out-of-sample forecasts are evaluated using forecast accuracy tests and market timing tests. The TGARCH model fits well for heating oil and natural gas volatility and the GARCH model fits well for crude oil and unleaded gasoline volatility. Simple moving average models seem to fit well in some cases provided the correct order is chosen. Despite the increased complexity, models like state space, vector autoregression and bivariate GARCH do not perform as well as the single equation GARCH model. Most models out perform a random walk and there is evidence of market timing. Parametric and non-parametric value at risk measures are calculated and compared. Non-parametric models outperform the parametric models in terms of number of exceedences in backtests. These results are useful for anyone needing forecasts of petroleum futures volatility. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {467--488}, 
number = {4}, 
volume = {28}
}
@article{10.1007/s13042-012-0126-4, 
year = {2014}, 
title = {{Random fuzzy bilevel linear programming through possibility-based value at risk model}}, 
author = {Katagiri, Hideki and Uno, Takeshi and Kato, Kosuke and Tsuda, Hiroshi and Tsubaki, Hiroe}, 
journal = {International Journal of Machine Learning and Cybernetics}, 
issn = {18688071}, 
doi = {10.1007/s13042-012-0126-4}, 
abstract = {{This article considers bilevel linear programming problems where random fuzzy variables are contained in objective functions and constraints. In order to construct a new optimization criterion under fuzziness and randomness, the concept of value at risk and possibility theory are incorporated. The purpose of the proposed decision making model is to optimize possibility-based values at risk. It is shown that the original bilevel programming problems involving random fuzzy variables are transformed into deterministic problems. The characteristic of the proposed model is that the corresponding Stackelberg problem is exactly solved by using nonlinear bilevel programming techniques under some convexity properties. A simple numerical example is provided to show the applicability of the proposed methodology to real-world hierarchical problems. © 2012 Springer-Verlag.}}, 
pages = {211--224}, 
number = {2}, 
volume = {5}
}
@article{10.1016/j.procs.2011.04.181, 
year = {2011}, 
title = {{Exploring the value at risk of oil-exporting country portfolio: An empirical analysis from the FSU region}}, 
author = {Sun, Xiaolei and Tang, Ling and He, Wan}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2011.04.181}, 
abstract = {{In the perspective of oil-importers, this paper considers an extension of the Value at Risk approach incorporated with timevarying conditional volatility model to trace the actual dynamic risk of regional oil-importing portfolio caused by the country risk volatility. With an application to oil economies in the Former Soviet Union (FSU) region, empirical results show that the country portfolio risk of oil-imports and country risk volatility in the FSU region has more significant influence on China's oil-importing risk than that on EU's. © 2011 Published by Elsevier Ltd.}}, 
pages = {1675--1680}, 
number = {NA}, 
volume = {4}
}
@article{10.1057/mel.2014.13, 
year = {2014}, 
title = {{Value-at-risk analysis of the asymmetric long-memory volatility process of dry bulk freight rates}}, 
author = {Chang, Chao-Chi and Chou, Heng Chih and Wu, Chun Chou}, 
journal = {Maritime Economics \& Logistics}, 
issn = {14792931}, 
doi = {10.1057/mel.2014.13}, 
abstract = {{This study aims to apply value-at-risk (VaR) models to evaluate the risk of dry bulk freight rates when there is an asymmetric long-memory volatility process. The VaR estimations as well as expected shortfalls for both short and long trading positions are conducted. We use the Fractionally Integrated GARCH, Hyperbolic GARCH and Fractionally Integrated APARCH models to analyse the performance of the VaR models with the normal, Student-t and skewed Student-t distributions. Empirical results suggest that precise VaR estimates may be obtained from an asymmetric long-memory volatility structure with the skewed Student-t distribution. Moreover, the asymmetric FIAPARCH model outperforms than other models in out-of-sampling forecasting. Therefore, our findings provide a more accurate estimation of VaR for dry bulk freight rates. These results present several potential implications for dry bulk freight market risk quantification and hedging strategies. © 2014 Macmillan Publishers Ltd.}}, 
pages = {298--320}, 
number = {3}, 
volume = {16}
}
@article{10.1016/j.csda.2020.107109, 
year = {2021}, 
title = {{Outer power transformations of hierarchical Archimedean copulas: Construction, sampling and estimation}}, 
author = {Górecki, Jan and Hofert, Marius and Okhrin, Ostap}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2020.107109}, 
abstract = {{Outer power (OP) transformations of Archimedean generators are suggested to increase the modeling flexibility and statistical fitting capabilities of classical Archimedean copulas restricted to a single parameter. For OP-transformed Archimedean copulas, a formula for computing tail dependence coefficients is obtained, as well as two feasible OP Archimedean copula estimators are proposed and their properties studied by simulation. For hierarchical extensions of OP-transformed Archimedean copulas under the sufficient nesting condition, a new construction principle, efficient sampling and parameter estimation for models based on a single one-parameter Archimedean family are addressed. Special attention is paid to the case where the sufficient nesting condition simplifies to two types of restrictions on the corresponding parameters. By simulation, the convergence rate and standard errors of the proposed estimator are studied. Excellent tail fitting capabilities of OP-transformed hierarchical Archimedean copula models are demonstrated in a risk management application. The results show that the OP transformation is able to improve the statistical fit of exchangeable Archimedean copulas, particularly of those that cannot capture upper tail dependence or strong concordance, as well as the statistical fit of hierarchical Archimedean copulas, especially in terms of tail dependence and higher dimensions. Given how comparably simple it is to include OP transformations into existing exchangeable and hierarchical Archimedean copula models, OP transformations provide an attractive trade-off between computational effort and statistical improvement. © 2020 The Author(s)}}, 
pages = {107109}, 
number = {NA}, 
volume = {155}
}
@article{10.1080/14697680500467889, 
year = {2006}, 
title = {{On risk management problems related to a coherence property}}, 
author = {Fabozzi, Frank J. and Tunaru, Radu}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680500467889}, 
abstract = {{Value at Risk has lost the battle against Expected Shortfall on theoretical grounds, the latter satisfying all coherence properties while the former may, on carefully constructed cases, lack the sub-additivity property that is in a sense, the most important property a risk measure ought to satisfy. While the superiority of Expected Shortfall is evident as a theoretical tool, little has been researched on the properties of estimators proposed in the literature. Since those estimators are the real tools for calculating bank capital reserves in practice, the natural question that one may ask is whether a given estimator of Expected Shortfall also satisfies the coherence properties. In this paper, we show that it is possible to have estimators of Expected Shortfall that do not satisfy the sub-additivity condition. This finding should motivate risk managers and quantitative asset managers to investigate further the properties of the estimators of the risk measures they are currently utilizing.}}, 
pages = {75--81}, 
number = {1}, 
volume = {6}
}
@article{10.1287/moor.1080.0313, 
year = {2008}, 
title = {{Risk tuning with generalized linear regression}}, 
author = {Rockafellar, R Tyrrell and Uryasev, Stan and Zabarankin, Michael}, 
journal = {Mathematics of Operations Research}, 
issn = {0364765X}, 
doi = {10.1287/moor.1080.0313}, 
abstract = {{A framework is set up in which linear regression, as a way of approximating a random variable by other random variables, can be carried out in a variety of ways, which, moreover, can be tuned to the needs of a particular model in finance, or operations research, more broadly. Although the idea of adapting the form of regression to the circumstances at hand has already found advocates in promoting quantile regression as an alternative to classical least-squares approaches, it is carried here much farther than that. Axiomatic concepts of error measure, deviation measure, and risk measure are coordinated with certain "statistics" that likewise say something about a random variable. Problems of regression utilizing these concepts are analyzed and the character of their solutions is explored in a range of examples. Special attention is paid to parametric forms of regression which arise in connection with factor models. It is argued that when different aspects of risk enter an optimization problem, different forms of regression ought to be invoked for each of those aspects. © 2008 INFORMS.}}, 
pages = {712--729}, 
number = {3}, 
volume = {33}
}
@article{10.1108/03074351211207563, 
year = {2012}, 
title = {{Evaluating value-at-risk models before and after the financial crisis of 2008: International evidence}}, 
author = {Degiannakis, Stavros and Floros, Christos and Livada, Alexandra}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/03074351211207563}, 
abstract = {{Purpose – The purpose of this paper is to focus on the performance of three alternative value-at-risk (VaR) models to provide suitable estimates for measuring and forecasting market risk. The data sample consists of five international developed and emerging stock market indices over the time period from 2004 to 2008. The main research question is related to the performance of widely-accepted and simplified approaches to estimate VaR before and after the financial crisis. Design/methodology/approach – VaR is estimated using daily data from the UK (FTSE 100), Germany (DAX30), the USA (S\&P500), Turkey (ISE National 100) and Greece (GRAGENL). Methods adopted to calculate VaR are: EWMA of Riskmetrics; classic GARCH(1,1) model of conditional variance assuming a conditional normally distributed returns; and asymmetric GARCH with skewed Student-t distributed standardized innovations. Findings – The paper provides evidence that the tools of quantitative finance may achieve their objective. The results indicate that the widely accepted and simplified ARCH framework seems to provide satisfactory forecasts of VaR, not only for the pre-2008 period of the financial crisis but also for the period of high volatility of stock market returns. Thus, the blame for financial crisis should not be cast upon quantitative techniques, used to measure and forecast market risk, alone. Practical implications – Knowledge of modern risk management techniques is required to resolve the next financial crisis. The next crisis can be avoided only when financial risk managers acquire the necessary quantitative skills to measure uncertainty and understand risk. Originality/value – The main contribution of this paper is that it provides evidence that widely accepted/used methods give reliable VaR estimates and forecasts for periods of financial turbulence (financial crises). © 2012, © Emerald Group Publishing Limited.}}, 
pages = {436--452}, 
number = {4}, 
volume = {38}
}
@article{10.1016/j.frl.2008.12.002, 
year = {2009}, 
title = {{Value-at-Risk computation by Fourier inversion with explicit error bounds}}, 
author = {Siven, Johannes Vitalis and Lins, Jeffrey Todd and Szymkowiak-Have, Anna}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2008.12.002}, 
abstract = {{The Value-at-Risk of a delta-gamma approximated derivatives portfolio can be computed by numerical integration of the characteristic function. However, while the choice of parameters in any numerical integration scheme is paramount, in practice it often relies on ad hoc procedures of trial and error. For normal and multivariate t-distributed risk factors, we show how to calculate the necessary parameters for one particular integration scheme as a function of the data (the distribution of risk factors, and delta and gamma) in order to satisfy a given error tolerance. This allows for implementation in a fully automated risk management system. We also demonstrate in simulations that the method is significantly faster than the Monte Carlo method, for a given error tolerance. © 2008 Elsevier Inc. All rights reserved.}}, 
pages = {95--105}, 
number = {2}, 
volume = {6}
}
@article{10.4028/www.scientific.net/amr.452-453.469, 
year = {2012}, 
title = {{A method to quantify risks of financial assets: An empirical analysis of Japanese security prices}}, 
author = {Sato, Aki Hiro}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.452-453.469}, 
abstract = {{This study investigates unconditional distributions of daily log-returns of Japanese security prices from a comprehensive point of view. The purpose of this article is to estimate a risk distribution of stocks in terms of Value-at-Risk (VaR) in order to select low risk securities from many securities. Daily log-return time series of 1,340 Japanese companies listed on the first section of Tokyo Stock Exchange are examined during the last one decade. I develop a method to estimate VaR by both the maximum likelihood estimation procedure under a q-Gaussian assumption and analytical form of its cumulative distribution function. It is confirmed that they are fitted to q-Gaussian distributions (Student t-distributions) with Kolmogorov-Smirnov test. It is found that the complementary cumulative distribution function of VaR has a power-law tail with its characteristic exponent depending on values of the VaR percentile. © (2012) Trans Tech Publications.}}, 
pages = {469--473}, 
number = {NA}, 
volume = {452-453}
}
@article{10.1109/icicic.2007.32, 
year = {2007}, 
title = {{A hybrid importance sampling algorithm for value-at-risk}}, 
author = {Dai, Tian-Shyr and Lin, Shih-Kuei and Liu, Li-Min}, 
journal = {Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)}, 
issn = {NA}, 
doi = {10.1109/icicic.2007.32}, 
abstract = {{Value-at-Risk (VaR) provides a number that measures the risk of a financial portfolio under significant loss. Glasserman et al. suggest that the performance of Mote Calo simulation can be improved by importance sampling [3]. However, their technique might perform poorly for some complex portfolios like shorting straddle options. In this paper, we investigate the hybrid importance sampling algorithm which can efficiently estimate the VaR for complex portfolios. © 2007 IEEE.}}, 
pages = {208--208}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11750-013-0303-y, 
year = {2014}, 
title = {{Robust portfolio choice with CVaR and VaR under distribution and mean return ambiguity}}, 
author = {Paç, A. Burak and Pınar, Mustafa Ç.}, 
journal = {TOP}, 
issn = {11345764}, 
doi = {10.1007/s11750-013-0303-y}, 
abstract = {{We consider the problem of optimal portfolio choice using the Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR) measures for a market consisting of n risky assets and a riskless asset and where short positions are allowed. When the distribution of returns of risky assets is unknown but the mean return vector and variance/covariance matrix of the risky assets are fixed, we derive the distributionally robust portfolio rules. Then, we address uncertainty (ambiguity) in the mean return vector in addition to distribution ambiguity, and derive the optimal portfolio rules when the uncertainty in the return vector is modeled via an ellipsoidal uncertainty set. In the presence of a riskless asset, the robust CVaR and VaR measures, coupled with a minimum mean return constraint, yield simple, mean-variance efficient optimal portfolio rules. In a market without the riskless asset, we obtain a closed-form portfolio rule that generalizes earlier results, without a minimum mean return restriction. © 2013, Sociedad de Estadística e Investigación Operativa.}}, 
pages = {875--891}, 
number = {3}, 
volume = {22}
}
@article{10.1080/15567240600705227, 
year = {2007}, 
title = {{Modeling the estimated time for obtaining environmental permits for industry of oil and gas in Brazil}}, 
author = {Carpio, L. G. Tapia and Guedes, J. C.}, 
journal = {Energy Sources, Part B: Economics, Planning, and Policy}, 
issn = {15567249}, 
doi = {10.1080/15567240600705227}, 
abstract = {{This paper has two aims. The first is to present a prompt method to estimate the necessary time period spent to obtain an environmental permit for pipelines and other oil and gas industrial units, including one category for small units and another for medium and large units, taking into consideration previous experiences in obtaining environmental permits for Petrobras in Brazil. The second aim is to inform the probability of success, whenever one has a previous stipulated period, determined by non-environmental criteria such as financial terms and production need.}}, 
pages = {329--341}, 
number = {4}, 
volume = {2}
}
@article{10.1080/1331677x.2020.1756372, 
year = {2020}, 
title = {{Valuation of real-estate losses via Monte Carlo simulation}}, 
author = {Barañano, Aitor and Peña, J. Iñaki De La and Moreno, Rafael}, 
journal = {Economic Research-Ekonomska Istraživanja}, 
issn = {1331677X}, 
doi = {10.1080/1331677x.2020.1756372}, 
abstract = {{The valuation of the exposure to real estate market risk has traditionally been difficult due to the lack of appropriate data, returns that do not follow a normal distribution and a lack of adequate methodology. However, regulations such as Basel II, Basel III and Solvency II make it possible to assess real estate market risk using an internal model and through Value at Risk. The study develops a procedure to provide an internal model that values real estate market risk and calculates the capital that guarantees it. Monte Carlo simulations are used to calculate Value at Risk. As result, capital requirements can be established from these results to help with portfolio decision-making of insurance companies that hold real estate. Data used in the study is taken from the General Council of Notaries registered dwellings databases from the Spanish National Statistics Institute covering the time period of 2007–2017. This paper contributes to the literature by proposing a model that incorporates the characteristics of investments, allowing a real and market measure of the risk of loss from real estate. © 2020, © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1867--1888}, 
number = {1}, 
volume = {33}
}
@article{10.21314/jrmv.2016.153, 
year = {2016}, 
title = {{Testing value-at-risk models in emerging markets during crises: A case study on South Eastern European countries}}, 
author = {Radivojevic, Nikola and Curcic, Nikola and Milojkovic, Dragana and Miletic, Vuk}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2016.153}, 
abstract = {{This case study examines the applicability of a wide range of value-at-risk (VaR) models in emerging markets, using the South Eastern European countries as examples. The aim of the paper is to get answers to two questions. The first question is whether VaR models that are created and suited to developed markets can be used reliably in emerging markets, such as the South Eastern European countries. The second question is whether modifications can improve their applicability to these markets. The results show that the most popular and widely used VaR models are not well suited to measuring market risk in the South Eastern European countries, and it is necessary to use the most appropriate VaR model for measuring market risk in these markets. © 2016 Incisive Risk Information (IP) Limited.}}, 
number = {2}, 
volume = {10}
}
@article{10.1016/j.ejor.2017.11.062, 
year = {2018}, 
title = {{A value-at-risk approach to optimisation of warranty policy}}, 
author = {Luo, Ming and Wu, Shaomin}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2017.11.062}, 
abstract = {{In the real world, a manufacturer may produce many products, which may have common components installed. Consequently, the frequencies of the warranty claims of those products are statistically dependent. Warranty policy optimisation in the existing research, however, has not considered such statistical dependence, which may increase bias in decision making. This paper is the first attempt to collectively optimises warranty policy for a set of different products, produced by one manufacturer, whose failures are statistically dependent, using tools borrowed from financial mathematics (i.e., value-at-risk theory and copula). We prove the existence of the optimal solutions for different scenarios. Numerical examples are used to validate the applicability of the proposed methods. © 2017 Elsevier B.V.}}, 
pages = {513--522}, 
number = {2}, 
volume = {267}
}
@article{10.17535/crorr.2019.0021, 
year = {2019}, 
title = {{Forecasting portfolio-Value-at-Risk with mixed factorial hidden Markov Models}}, 
author = {{University, College of Business and Economics, Qassim} and Saidane, Mohamed}, 
journal = {Croatian Operational Research Review}, 
issn = {18480225}, 
doi = {10.17535/crorr.2019.0021}, 
abstract = {{This paper is concerned with the statistical modeling of the latent dependence and co-movement structures of multivariate financial data using a new approach based on mixed factorial hidden Markov models, and their applications in Value-at-Risk (VaR) valuation. This approach combines hidden Markov Models (HMM) with mixed latent factor models. The HMM generates a piece-wise constant state evolution process and the observations are produced from the state vectors by a mixture of factor analyzers observation process. This new switching specification provides an alternative, compact, model to handle intra-frame correlation and unobserved heterogeneity in financial data. For maximum likelihood estimation we have proposed an iterative approach based on the Expectation-Maximisation (EM) algorithm. Using a set of historical data, from the Tunisian foreign exchange market, the model parameters are estimated. Then, the fitted model combined with a modified Monte-Carlo simulation algorithm was used to predict the VaR of the Tunisian public debt portfolio. Through a backtesting procedure, we found that this new specification exhibits a good fit to the data, improves the accuracy of VaR predictions and can avoid serious violations when a financial crisis occurs. © 2019 Croatian Operational Research Society.}}, 
pages = {241--255}, 
number = {2}, 
volume = {10}
}
@article{10.1081/sap-200056690, 
year = {2005}, 
title = {{Dynamic portfolio optimization with bounded shortfall risks}}, 
author = {Gabih, A. and Grecksch, W. and Wunderlich, R.}, 
journal = {Stochastic Analysis and Applications}, 
issn = {07362994}, 
doi = {10.1081/sap-200056690}, 
abstract = {{We address a dynamic portfolio optimization problem where the expected utility from terminal wealth has to be maximized. The special feature of this paper is an additional constraint on the portfolio strategy modeling bounded shortfall risks, which are measured by value at risk or expected loss. Using a continuous-time model of a complete financial market and applying martingale methods, analytic expressions for the optimal terminal wealth and the optimal portfolio strategies are given. Finally, some numerical results are presented. Copyright © Taylor \& Francis, Inc.}}, 
pages = {579--594}, 
number = {3}, 
volume = {23}
}
@article{10.1007/s13385-019-00219-9, 
year = {2020}, 
title = {{Optimal risk sharing in insurance networks: An application to asset–liability management}}, 
author = {Hamm, Anna-Maria and Knispel, Thomas and Weber, Stefan}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-019-00219-9}, 
abstract = {{We discuss the impact of risk sharing and asset–liability management on capital requirements. Our analysis contributes to the evaluation of the merits and deficiencies of different risk measures. In particular, we highlight that the class of V@R-based risk measures allows for a substantial reduction of the total capital requirement in corporate networks that share risks between entities. We provide case studies that complement previous theoretical results and demonstrate their practical relevance. For large networks, optimal asset–liability management is often contrary to those strategies that are desirable from a regulatory point of view. © 2019, EAJ Association.}}, 
pages = {203--234}, 
number = {1}, 
volume = {10}
}
@article{10.1016/j.eneco.2013.03.003, 
year = {2013}, 
title = {{Energy risk management through self-exciting marked point process}}, 
author = {Herrera, Rodrigo}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2013.03.003}, 
abstract = {{Crude oil is a dynamically traded commodity that affects many economies. We propose a collection of marked self-exciting point processes with dependent arrival rates for extreme events in oil markets and related risk measures. The models treat the time among extreme events in oil markets as a stochastic process. The main advantage of this approach is its capability to capture the short, medium and long-term behavior of extremes without involving an arbitrary stochastic volatility model or a prefiltration of the data, as is common in extreme value theory applications. We make use of the proposed model in order to obtain an improved estimate for the Value at Risk in oil markets. Empirical findings suggest that the reliability and stability of Value at Risk estimates improve as a result of finer modeling approach. This is supported by an empirical application in the representative West Texas Intermediate (WTI) and Brent crude oil markets. © 2013 Elsevier B.V.}}, 
pages = {64--76}, 
number = {NA}, 
volume = {38}
}
@article{10.1016/j.eneco.2018.03.032, 
year = {2019}, 
title = {{Leverage effects and stochastic volatility in spot oil returns: A Bayesian approach with VaR and CVaR applications}}, 
author = {Chen, Liyuan and Zerilli, Paola and Baum, Christopher F.}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2018.03.032}, 
abstract = {{Crude oil markets have been quite volatile and risky in the past few decades due to the large fluctuations of oil prices. We contribute to the current debate by testing for the existence of the leverage effect when considering daily spot returns in the WTI and Brent crude oil markets and by studying the direct impact of the leverage effect on measures of risk such as VaR and CVaR. More specifically, we model spot crude oil returns using Stochastic Volatility (SV) models with various distributions of the errors. We find that the introduction of the leverage effect in the traditional SV model with Normally distributed errors is capable of adequately estimating risk for speculative oil suppliers in both the WTI and Brent markets. Our results also show that financial regulators' model choice, both on the supply and on the demand side, would not be affected by the introduction of leverage. Focusing instead on firm's internal risk management, our results show that the introduction of leverage would be useful for firms who are on the demand side for oil, who use VaR for risk management and who are particularly worried about the magnitude of the losses exceeding VaR while wanting to minimize the opportunity cost of capital. Using the same logic, firms who are on the supply side would be better off not considering the leverage effect. © 2018 Elsevier B.V.}}, 
pages = {111--129}, 
number = {NA}, 
volume = {79}
}
@article{10.1007/s11156-012-0308-x, 
year = {2013}, 
title = {{Value at risk estimation by quantile regression and kernel estimator}}, 
author = {Huang, Alex YiHou}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-012-0308-x}, 
abstract = {{Risk management has attracted a great deal of attention, and Value at Risk (VaR) has emerged as a particularly popular and important measure for detecting the market risk of financial assets. The quantile regression method can generate VaR estimates without distributional assumptions; however, empirical evidence has shown the approach to be ineffective at evaluating the real level of downside risk in out-of-sample examination. This paper proposes a process in VaR estimation with methods of quantile regression and kernel estimator which applies the nonparametric technique with extreme quantile forecasts to realize a tail distribution and locate the VaR estimates. Empirical application of worldwide stock indices with 29 years of data is conducted and confirms the proposed approach outperforms others and provides highly reliable estimates. © 2012 Springer Science+Business Media, LLC.}}, 
pages = {225--251}, 
number = {2}, 
volume = {41}
}
@article{10.1080/14697681003687569, 
year = {2012}, 
title = {{Probability-unbiased Value-at-Risk estimators}}, 
author = {Francioni, Ivo and Herzog, Florian}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697681003687569}, 
abstract = {{The aim of this paper is to introduce a new property for good quantile or Value-at-Risk (VaR) estimators. We define probability unbiasedness for the α-quantile estimator from a finite amount of data (Q̂ α) for the distribution function F θ with parameter θ such that the estimator also has this probabilistic 'threshold property' in expectation of a F θ distributed random variable X for all θ, i.e. Probability unbiasedness means that the α-quantile estimated from a finite amount of data is only exceeded by probability α for the next observation from the same distribution. Moreover, we show that plug-in estimators for estimating quantiles at a given probability are not unbiased with respect to the probability unbiasedness property, i.e. using a Maximum Likelihood Estimator for the parameters and plugging in the estimated values into the distribution function to obtain the quantile values. Therefore, estimating a quantile needs to be corrected for observing only finitely many samples with a distortion function to obtain a probability-unbiased estimator. In the case of estimating the VaR (quantile) of a normally distributed random variable, the distortion function is calculated via the distribution of the VaR/quantile. Using the distribution derived for the VaR estimate, we also quantify the approximate probability-unbiased confidence bands of the VaR for a finite amount of data. In the last part, the new VaR estimator is tested on a time series. It outperforms the other models examined, all of which are much more complex. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {755--768}, 
number = {5}, 
volume = {12}
}
@article{10.3846/16111699.2011.620168, 
year = {2012}, 
title = {{Systemic risk in Taiwan stock market}}, 
author = {Sheu, Her-Jiun and Cheng, Chien-Ling}, 
journal = {Journal of Business Economics and Management}, 
issn = {16111699}, 
doi = {10.3846/16111699.2011.620168}, 
abstract = {{Recent financial crises resulted from systemic risk caused by idiosyncratic distress. In this research, taking Taiwan stock market as an example and collecting data from 2000 to 2010 which contained the 2001 dot-com bubble and the 2007-2009 financial crisis, we adopt the CoVaR model to empirically explore the impact of sector-specific idiosyncratic risk on the systemic risk of the system and attempt to investigate the links between financial crises, systemic risk and the idiosyncratic risk of a sector-specific anomaly. The result showed sector-specific marginal CoVaR, i.e., ΔCoVaR, perfectly explained Taiwan stock market disturbance during the 2001 dot-com bubble and 2007-2008 financial crisis. Thus, by identifying the larger ΔCoVaR sectors, i.e. the systemic importance sectors, and by exploring the risk indicators, independent variables, of these systemic importance sectors, investors could practically employ the sector-specific ΔCoVaR measure to deepen the systemic risk scrutiny from a macro into a micro prudential perspective. © 2012 Copyright Vilnius Gediminas Technical University (VGTU) Press Technika.}}, 
pages = {895--914}, 
number = {5}, 
volume = {13}
}
@article{10.1145/2558328, 
year = {2014}, 
title = {{Confidence intervals for quantiles using sectioning when applying variance-reduction techniques}}, 
author = {Nakayama, Marvin K.}, 
journal = {ACM Transactions on Modeling and Computer Simulation (TOMACS)}, 
issn = {10493301}, 
doi = {10.1145/2558328}, 
abstract = {{We develop confidence intervals (CIs) for quantiles when applying variance-reduction techniques (VRTs) and sectioning. Similar to batching, sectioning partitions the independent and identically distributed (i.i.d.) outputs into nonoverlapping batches and computes a quantile estimator from each batch. But rather than centering the CI at the average of the quantile estimators across the batches, as in batching, sectioning centers the CI at the overall quantile estimator based on all the outputs. A similar modification is made to the sample variance, which is used to determine the width of the CI. We establish the asymptotic validity of the sectioning CI for importance sampling and control variates, and the proofs rely on first showing that the corresponding quantile estimators satisfy a Bahadur representation, which we have done in prior work. Here, we present some numerical results. © 2014 ACM 1049-3301/2014/03-ART19.}}, 
pages = {19}, 
number = {4}, 
volume = {24}
}
@article{10.1061/40996(330)406, 
year = {2008}, 
title = {{An analysis method of the loan-pledge-rate of warehouse receipts pledge using VAR}}, 
author = {Ma, Hairong and Qiu, Xiaoping}, 
journal = {Logistics}, 
issn = {NA}, 
doi = {10.1061/40996(330)406}, 
abstract = {{In China, warehouse receipts pledge, as a new logistics financial business, allows the achievement of a three-way win-win among production enterprises (businesses), banks, and warehouses. The key to warehouse receipts pledge business is the determination of the loan-pledge-rate, which can be solved by a VAR (value at risk) tool. In this paper, by the basic principles of the VAR method, the author proposes setting weights in classification under confidence interval of sample data, which supports a quantitative analysis of a loan-pledge-rate and achieves a good result. © ASCE.}}, 
pages = {2747--2752}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2021.10.006, 
year = {2021}, 
title = {{Risk aggregation under dependence uncertainty and an order constraint}}, 
author = {Chen, Yuyu and Lin, Liyuan and Wang, Ruodu}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2021.10.006}, 
abstract = {{We study the aggregation of two risks when the marginal distributions are known and the dependence structure is unknown, under the additional constraint that one risk is smaller than or equal to the other. Risk aggregation problems with the order constraint are closely related to the recently introduced notion of the directional lower (DL) coupling. The largest aggregate risk in concave order (thus, the smallest aggregate risk in convex order) is attained by the DL coupling. These results are further generalized to calculate the best-case and worst-case values of tail risk measures. In particular, we obtain analytical formulas for bounds on Value-at-Risk. Our numerical results suggest that the new bounds on risk measures with the extra order constraint can greatly improve those with full dependence uncertainty. © 2021 Elsevier B.V.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10260-015-0332-9, 
year = {2016}, 
title = {{An exponentially weighted quantile regression via SVM with application to estimating multiperiod VaR}}, 
author = {Xu, Qifa and Jiang, Cuixia and He, Yaoyao}, 
journal = {Statistical Methods \& Applications}, 
issn = {16182510}, 
doi = {10.1007/s10260-015-0332-9}, 
abstract = {{The square root of time rule under RiskMetrics has been used as an important tool to estimate multiperiod value at risk (VaR). However, the conditions for the rule are too restrictive to get empirical support in practice since multiperiod VaR is a complex nonlinear function of the holding period and the one-step ahead volatility forecast. In this paper, we propose a new model by considering an exponentially weighted quantile regression via SVM to provide greater accuracy for multiperiod VaR measure. In both numerical simulations and empirical studies on three stock indices, the proposed model outperforms several traditional methods including the volatility models, filtered historical simulation, and linear quantile regression approaches in terms of the value of the number of significant entries, the mean absolute error, and the p value of prediction test in Harvey et al. (Int J Forecast 13:281–291, 1997). © 2015, Springer-Verlag Berlin Heidelberg.}}, 
pages = {285--320}, 
number = {2}, 
volume = {25}
}
@article{10.1016/j.ribaf.2017.01.003, 
year = {2017}, 
title = {{True or spurious long memory in European non-EMU currencies}}, 
author = {Walther, Thomas and Klein, Tony and Thu, Hien Pham and Piontek, Krzysztof}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2017.01.003}, 
abstract = {{We examine the Croatian Kuna, the Czech Koruna, the Hungarian Forint, the Polish Złoty, the Romanian Leu, and the Swedish Krona whether their Euro exchange rates volatility exhibits true or spurious long memory. Recent research reveals long memory in foreign exchange rate volatility and we confirm this finding for these currency pairs by examining the long memory behavior of squared residuals by means of the V/S test. However, by using the ICSS approach we also find structural breaks in the unconditional variance. Literature suggests that structural breaks might lead to spurious long memory behavior. In a refined test strategy, we distinguish true from spurious long memory for the six exchange rates. Our findings suggest that Czech Koruna and Hungarian Forint only feature spurious long memory, while the rest of the series have both structural breaks and true long memory. Lastly, we demonstrate how to extend existing models to jointly model both properties yielding superior fit and better Value-at-Risk forecasts. The results of our work help to avoid misspecification and provide a better understanding of the properties of the foreign exchange rate volatility. © 2017 Elsevier B.V.}}, 
pages = {217--230}, 
number = {NA}, 
volume = {40}
}
@article{10.1080/13518470500377380, 
year = {2006}, 
title = {{Detecting market transitions and energy futures risk management using principal components}}, 
author = {Borovkova, Svetlana}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/13518470500377380}, 
abstract = {{An empirical approach to analysing the forward curve dynamics of energy futures is presented. For non-seasonal commodities-such as crude oil-the forward curve is well described by the first three principal components: the level, slope and curvature. A principal component indicator is described that detects transitions between the two fundamental market states remarkably well. For seasonal commodities-such as electricity and natural gas-it is shown how to extract the seasonal component from the forward curve. The principal component indicator can then be applied to the de-seasoned forward curve to detect significant price deviations that may support profitable trading strategies. A principal component approach to forward curve modelling is applied to computing portfolio value-at-risk. This approach is combined with a new two-step resampling procedure to improve value-at-risk estimates. © 2006 Taylor \& Francis.}}, 
pages = {495--512}, 
number = {6-7}, 
volume = {12}
}
@article{10.1080/17509653.2012.10671219, 
year = {2012}, 
title = {{Decomposition of portfolio var and expected shortfall based on multivariate copula simulation}}, 
author = {Fan, Guobin and Zeng, Yong and Wong, Woon K.}, 
journal = {International Journal of Management Science and Engineering Management}, 
issn = {17509653}, 
doi = {10.1080/17509653.2012.10671219}, 
abstract = {{Portfolio risk-adjusted performance measurement involves the calculation of the risk contribution for each asset it contains. This paper uses multivariate Copula functions to model the dependence structure among the assets in a portfolio, then, based on a simulation, decomposes the portfolio VaR and Expected Shortfall. The research shows this simulation approach provides a way to test if the risk contributions of various assets are significantly different, and also displays results insusceptible to confidence level and risk measures. Furthermore, with this approach, the risk contribution calculated using Expected Shortfall is more robust, and its estimation error can be reduced by increasing the simulation sample size. For the equally-weighted portfolios of five Shanghai industrial stock indices, empirical evidence shows that the Real Estate Index has the largest risk contribution of the whole portfolio. © 2012 Taylor \& Francis Group, LLC.}}, 
pages = {153--160}, 
number = {2}, 
volume = {7}
}
@article{10.1017/asb.2014.14, 
year = {2014}, 
title = {{Spectral methods for the calculation of risk measures for variable annuity guaranteed benefits}}, 
author = {Feng, Runhuan and Volkmer, Hans W.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2014.14}, 
abstract = {{Spectral expansion techniques have been extensively exploited for the pricing of exotic options. In this paper, we present novel applications of spectral methods for the quantitative risk management of variable annuity guaranteed benefits such as guaranteed minimum maturity benefits and guaranteed minimum death benefits. The objective is to find efficient and accurate solution methods for the computation of risk measures, which is the key to determining risk-based capital according to regulatory requirements. Our example calculations show that two spectral methods used in this paper are highly efficient and numerically more stable than conventional known methods. Hence these approaches are more suitable for intensive calculations involving death benefits. Copyright © ASTIN Bulletin 2014.}}, 
pages = {653--681}, 
number = {1}, 
volume = {31}
}
@article{10.1016/j.ejor.2012.03.012, 
year = {2012}, 
title = {{Portfolio value-at-risk optimization for asymmetrically distributed asset returns}}, 
author = {Goh, Joel Weiqiang and Lim, Kian Guan and Sim, Melvyn and Zhang, Weina}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2012.03.012}, 
abstract = {{We propose a new approach to portfolio optimization by separating asset return distributions into positive and negative half-spaces. The approach minimizes a newly-defined Partitioned Value-at-Risk (PVaR) risk measure by using half-space statistical information. Using simulated data, the PVaR approach always generates better risk-return tradeoffs in the optimal portfolios when compared to traditional Markowitz mean-variance approach. When using real financial data, our approach also outperforms the Markowitz approach in the risk-return tradeoff. Given that the PVaR measure is also a robust risk measure, our new approach can be very useful for optimal portfolio allocations when asset return distributions are asymmetrical. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {397--406}, 
number = {2}, 
volume = {221}
}
@article{10.1016/j.econlet.2012.04.074, 
year = {2012}, 
title = {{Sovereign risk contagion in the Eurozone}}, 
author = {Metiu, Norbert}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2012.04.074}, 
abstract = {{This paper extends the canonical model of contagion proposed by Pesaran and Pick [Pesaran, M.H., Pick,A., 2007. Econometric issues in the analysis of contagion. Journal of Economic Dynamics and Control 31, 1245-1277] in order to test for contagion of credit events in Euro area sovereign bond markets. We find evidence for significant contagion effects among long-term bond yield premia between 1, January 2008 and 1, February 2012. © 2012 Elsevier B.V.}}, 
pages = {35--38}, 
number = {1}, 
volume = {117}
}
@article{10.21314/jrmv.2015.145, 
year = {2015}, 
title = {{Risk model validation for BRICS countries: A value-at-risk, expected shortfall and extreme value theory approach}}, 
author = {Wing, Jean Paul Chung and Gonpot, Preethee Nunkoo}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2015.145}, 
abstract = {{In this paper, we employ value-at-risk (VaR) and expected shortfall (ES) as risk measures to assess the competency of several volatility models, based on the stock indexes of the BRICS countries (Brazil, Russia, India, China and South Africa) after the financial crisis. The aim is to determine the most appropriate model for each country and identify a set of common models for the BRICS countries. A family of generalized autoregressive conditional heteroscedasticity (GARCH) models, extreme value theory (EVT) and a dynamic bivariate technique are considered to demonstrate different volatility dynamics. Some models are ruled out under the presented backtesting operations, while the nonrejected models are ranked using two loss functions. In spite of returns displaying large kurtosis, the models with normal innovation generally prove to have better estimates ofVaR and ES. GARCH and fractionally integrated GARCH are preferred by Brazil and Russia, while India favors dual models and the Student t distribution. The Glosten, Jagannathan and Runkle GARCH-N is the best model for South Africa, which is the only country that rejects the stationary EVT approach. Despite these contrasting results, we find that it is possible to obtain more than one model that can be used to model all of the BRICS countries: these are the integrated GARCH-N and exponential GARCH-EVT approaches. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--22}, 
number = {3}, 
volume = {9}
}
@article{10.1016/s0378-4266(03)00113-4, 
year = {2004}, 
title = {{The impact of risk regulation on price dynamics}}, 
author = {Danı́elsson, Jón and Shin, Hyun Song and Zigrand, Jean-Pierre}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(03)00113-4}, 
abstract = {{Most financial risk regulations assume that asset returns are exogenous, where risk is estimated from historical data. This assumption fails to take into account the feedback effect of trading decisions on prices. We investigate the consequences of risk constrained trading by means of simulations of a general equilibrium model with a value-at-risk constraint and compare the results to the case when risk constraints are not present. Prices are lower on average in the presence of risk regulation, while volatility is higher. Risk regulation may have the perverse effect of exacerbating price fluctuations. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {1069--1087}, 
number = {5}, 
volume = {28}
}
@article{10.1109/cis.2013.24, 
year = {2013}, 
title = {{A new evolutionary algorithm for portfolio optimization and its application}}, 
author = {Wang, Weijia and Hu, Jie}, 
journal = {2013 Ninth International Conference on Computational Intelligence and Security}, 
issn = {NA}, 
doi = {10.1109/cis.2013.24}, 
abstract = {{Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) are two of the most widely used and important risk measures in financial risk management models. Because VaR and CVaR portfolio optimization models are often nonlinear and non-convex optimization models, traditional optimization methods usually can not get their global optimal solutions, instead, they often get a local optimal solution. In this paper, the uniform design is integrated into evolutionary algorithm to enhance the search ability of the evolutionary algorithm. The resulted algorithm will has a strong search ability and has more possibility to get the global optimal solution. Based on this idea, a new evolutionary algorithm is proposed for VaR and CVaR optimization models. Computer simulations on ten randomly chosen stocks from Shenzhen Stock Exchange in China are conducted and the analysis to the results is given. The experiment results indicate the proposed algorithm is efficient. © 2013 IEEE.}}, 
pages = {80--84}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.accfor.2007.06.002, 
year = {2007}, 
title = {{Financialized accounts: A stakeholder account of cash distribution in the S\&P 500 (1990-2005)}}, 
author = {Andersson, Tord and Haslam, Colin and Lee, Edward and Tsitsianis, Nick}, 
journal = {Accounting Forum}, 
issn = {01559982}, 
doi = {10.1016/j.accfor.2007.06.002}, 
abstract = {{In this paper, we construct a financialized account of corporate restructuring in S\&P 500 survivor firms where corporate transactions are accounted for at fair value or are marked to market. Accounting practitioners are preoccupied with the technical aspects of fair value reporting, but the outcome of absorbing wealth accumulation into corporate sector balance sheets is not simply a neutral technical issue. In financialized accounts blending current income and expenditure with capital market value amplifies the need to distribute cash to equity holders. In financialized accounts realignments generated by a product market downturn are magnified because value at risk and corrective restructuring will be wired into balance sheet fair value not historic cost. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {217--232}, 
number = {3}, 
volume = {31}
}
@article{10.1504/ijcee.2019.100560, 
year = {2019}, 
title = {{Insurance risk capital and risk aggregation: Bivariate copula approach}}, 
author = {Mejdoub, Hanène and Arab, Mounira Ben}, 
journal = {International Journal of Computational Economics and Econometrics}, 
issn = {17571170}, 
doi = {10.1504/ijcee.2019.100560}, 
abstract = {{This paper discusses the risk aggregation issue in the sphere of the non-life insurance industry. In this context, we attempt to investigate the impact of the dependence structure among losses using copula theory, on the total risk capital estimation measured by the value-at-risk (VaR). First, using numerical illustrations based on a Tunisian insurance company, we apply various copula families that can capture the dependencies across losses that are derived from four lines of business. Then, based on the Monte-Carlo simulation, the total risk capital is deduced by applying VaR on the aggregate loss distributions. We also conduct a comparative analysis between the various types of the copulas. Our findings reveal that there is a regular impact on the capital requirement estimation indicating that a static approach ignoring the real dependencies between different risks can systematically lead to an overestimation of the total capital requirement. Copyright © 2019 Inderscience Enterprises Ltd.}}, 
pages = {202}, 
number = {3}, 
volume = {9}
}
@article{10.1287/opre.1110.0993, 
year = {2011}, 
title = {{Efficient simulation of value at risk with heavy-tailed risk factors}}, 
author = {Fuh, Cheng-Der and Hu, Inchi and Hsu, Ya-Hui and Wang, Ren-Her}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1110.0993}, 
abstract = {{Simulation of small probabilities has important applications in many disciplines. The probabilities considered in value-atrisk (VaR) are moderately small. However, the variance reduction techniques developed in the literature for VaR computation are based on large-deviations methods, which are good for very small probabilities. Modeling heavy-tailed risk factors using multivariate Τ distributions, we develop a new method for VaR computation. We show that the proposed method minimizes the variance of the importance-sampling estimator exactly, whereas previous methods produce approximations to the exact solution. Thus, the proposed method consistently outperforms existing methods derived from large deviations theory under various settings. The results are confirmed by a simulation study. © 2011 INFORMS.}}, 
pages = {1395--1406}, 
number = {6}, 
volume = {59}
}
@article{10.1007/s10690-007-9042-0, 
year = {2006}, 
title = {{Risk management for linear and non-linear assets: A bootstrap method with importance resampling to evaluate value-at-risk}}, 
author = {Lin, Shih-Kuei and Wang, Ren-Her and Fuh, Cheng-Der}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-007-9042-0}, 
abstract = {{Many empirical studies suggest that the distribution of risk factors has heavy tails. One always assumes that the underlying risk factors follow a multivariate normal distribution that is a assumption in conflict with empirical evidence. We consider a multivariate t distribution for capturing the heavy tails and a quadratic function of the changes is generally used in the risk factor for a non-linear asset. Although Monte Carlo analysis is by far the most powerful method to evaluate a portfolio Value-at-Risk (VaR), a major drawback of this method is that it is computationally demanding. In this paper, we first transform the assets into the risk on the returns by using a quadratic approximation for the portfolio. Second, we model the return's risk factors by using a multivariate normal as well as a multivariate t distribution. Then we provide a bootstrap algorithm with importance resampling and develop the Laplace method to improve the efficiency of simulation, to estimate the portfolio loss probability and evaluate the portfolio VaR. It is a very powerful tool that propose importance sampling to reduce the number of random number generators in the bootstrap setting. In the simulation study and sensitivity analysis of the bootstrap method, we observe that the estimate for the quantile and tail probability with importance resampling is more efficient than the naive Monte Carlo method. We also note that the estimates of the quantile and the tail probability are not sensitive to the estimated parameters for the multivariate normal and the multivariate t distribution. © 2007 Springer Science+Business Media, LLC.}}, 
pages = {261--295}, 
number = {3}, 
volume = {13}
}
@article{10.1016/j.mcm.2011.06.053, 
year = {2013}, 
title = {{The impact of distribution on value-at-risk measures}}, 
author = {Olson, David L. and Wu, Desheng}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/j.mcm.2011.06.053}, 
abstract = {{Value at risk is a popular approach to aid financial risk management. Questions about the appropriateness of the measure have arisen since the related 2008 bubble collapses in some US housing markets and the global financial market. These questions include the presence of fat tails and their impact. This paper compares results based upon assumptions of normality and logistic distributions, comparing portfolios generated with various probabilistic models. Computations are applied to real stock data. Optimization models are described, with simulation models evaluating comparative model performance. Chi-square tests indicated that logistic distribution better fit the data than the normal distribution. The error implied by value-at-risk assumptions is demonstrated through Monte Carlo simulation. © 2011 Elsevier Ltd.}}, 
pages = {1670--1676}, 
number = {9-10}, 
volume = {58}
}
@article{10.1061/9780784413135.083, 
year = {2013}, 
title = {{Comprehensive evaluation of chinese listed real estate company based on GARCH models and factor analysis}}, 
author = {Jing, Yan and Wu, Yongxiang}, 
journal = {ICCREM 2013}, 
issn = {NA}, 
doi = {10.1061/9780784413135.083}, 
abstract = {{On the basis of the current comprehensive evaluation system and method both in domestic and abroad, this paper established an evaluation system, which contained value at risk (VaR), market occupancy and 12 financial indexes, to evaluate the comprehensive performance of Chinese listed real estate companies. First, the eviews 6.0 is used to calculate the value at risk of 50 sample companies. Then, SPSS is used to conduct the factor analysis and 5 factors are extracted among 14 indexes. At last, a rank of each factor and comprehensive score are showed and some comments are made according to the result. © 2013 American Society of Civil Engineers.}}, 
pages = {885--893}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2011.08.013, 
year = {2011}, 
title = {{Second order regular variation and conditional tail expectation of multiple risks}}, 
author = {Hua, Lei and Joe, Harry}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2011.08.013}, 
abstract = {{For the purpose of risk management, the study of tail behavior of multiple risks is more relevant than the study of their overall distributions. Asymptotic study assuming that each marginal risk goes to infinity is more mathematically tractable and has also uncovered some interesting performance of risk measures and relationships between risk measures by their first order approximations. However, the first order approximation is only a crude way to understand tail behavior of multiple risks, and especially for sub-extremal risks. In this paper, we conduct asymptotic analysis on conditional tail expectation (CTE) under the condition of second order regular variation (2RV). First, the closed-form second order approximation of CTE is obtained for the univariate case. Then CTE of the form E[X1|g(X1,...,Xd)\&gt;t], as t→∞, is studied, where g is a loss aggregating function and (X1,...,Xd)=(RT1,...,RTd) with R independent of (T1,...,Td) and the survivor function of R satisfying the condition of 2RV. Closed-form second order approximations of CTE for this multivariate form have been derived in terms of corresponding value at risk. For both the univariate and multivariate cases, we find that the first order approximation is affected by only the regular variation index -α of marginal survivor functions, while the second order approximation is influenced by both the parameters for first and second order regular variation, and the rate of convergence to the first order approximation is dominated by the second order parameter only. We have also shown that the 2RV condition and the assumptions for the multivariate form are satisfied by many parametric distribution families, and thus the closed-form approximations would be useful for applications. Those closed-form results extend the study of Zhu and Li (submitted for publication). © 2011 Elsevier B.V.}}, 
pages = {537--546}, 
number = {3}, 
volume = {49}
}
@article{10.3390/econometrics4020024, 
year = {2016}, 
title = {{Bayesian bandwidth selection for a nonparametric regression model with mixed types of regressors}}, 
author = {Zhang, Xibin and King, Maxwell L. and Shang, Han Lin}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics4020024}, 
abstract = {{This paper develops a sampling algorithm for bandwidth estimation in a nonparametric regression model with continuous and discrete regressors under an unknown error density. The error density is approximated by the kernel density estimator of the unobserved errors, while the regression function is estimated using the Nadaraya-Watson estimator admitting continuous and discrete regressors. We derive an approximate likelihood and posterior for bandwidth parameters, followed by a sampling algorithm. Simulation results show that the proposed approach typically leads to better accuracy of the resulting estimates than cross-validation, particularly for smaller sample sizes. This bandwidth estimation approach is applied to nonparametric regression model of the Australian All Ordinaries returns and the kernel density estimation of gross domestic product (GDP) growth rates among the organisation for economic co-operation and development (OECD) and non-OECD countries. © 2016 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {24}, 
number = {2}, 
volume = {4}
}
@article{10.1080/03610918.2014.944658, 
year = {2016}, 
title = {{An R Package for Value at Risk and Expected Shortfall}}, 
author = {Chan, Stephen and Nadarajah, Saralees and Afuecheta, Emmanuel}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2014.944658}, 
abstract = {{Value at risk and expected shortfall are the two most popular measures of financial risk. But the available R packages for their computation are limited. Here, we introduce an R contributed package written by the authors. It computes the two measures for over 100 parametric distributions, including all commonly known distributions. We expect that the R package could be useful to researchers and to the financial community. © 2016, Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {00--00}, 
number = {9}, 
volume = {45}
}
@article{10.1016/j.ejor.2013.05.044, 
year = {2013}, 
title = {{The risk-averse newsvendor problem with random capacity}}, 
author = {Wu, Meng and Zhu, Stuart X. and Teunter, Ruud H.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2013.05.044}, 
abstract = {{We study the effect of capacity uncertainty on the inventory decisions of a risk-averse newsvendor. We consider two well-known risk criteria, namely Value-at-Risk (VaR) included as a constraint and Conditional Value-at-Risk (CVaR). For the risk-neutral newsvendor, we find that the optimal order quantity is not affected by the capacity uncertainty. However, this result does not hold for the risk-averse newsvendor problem. Specifically, we find that capacity uncertainty decreases the order quantity under the CVaR criterion. Under the VaR constraint, capacity uncertainty leads to an order decrease for low confidence levels, but to an order increase for high confidence levels. This implies that the risk criterion should be carefully selected as it has an important effect on inventory decisions. This is shown for the newsvendor problem, but is also likely to hold for other inventory control problems that future research can address. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {328--336}, 
number = {2}, 
volume = {231}
}
@article{10.1109/cis.2011.331, 
year = {2011}, 
title = {{Dynamic risk measurement of futures based on wavelet theory}}, 
author = {Yang, Jianhui and Lin, Peng}, 
journal = {2011 Seventh International Conference on Computational Intelligence and Security}, 
issn = {NA}, 
doi = {10.1109/cis.2011.331}, 
abstract = {{In this paper, we choose consecutive month contract of natural rubber in China futures market for the study. Based on the GARCH class model, we combine the wavelet analysis with extreme value theory to get the approximate distribution of time series and then use rolling time window to predict dynamic value at risk. The empirical results show that the models all have good predictive ability, and the models which using wavelet analysis to estimate the threshold in generalized Pareto distribution achieve a better dynamic prediction. © 2011 IEEE.}}, 
pages = {1484--1487}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2008.12.004, 
year = {2009}, 
title = {{Worst VaR scenarios with given marginals and measures of association}}, 
author = {Kaas, Rob and Laeven, Roger J.A. and Nelsen, Roger B.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2008.12.004}, 
abstract = {{This paper studies the problem of finding best-possible upper bounds on the Value-at-Risk for a function of two random variables when the marginal distributions are known and additional nonparametric information on the dependence structure, such as the value of a measure of association, is available. The same problem for the Tail-Value-at-Risk is also briefly discussed. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {146--158}, 
number = {2}, 
volume = {44}
}
@article{10.1080/00949655.2018.1427240, 
year = {2018}, 
title = {{A new generalization of skew-T distribution with volatility models}}, 
author = {Altun, Emrah and Tatlidil, Huseyin and Ozel, Gamze and Nadarajah, Saralees}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2018.1427240}, 
abstract = {{In this paper, we propose a new generalized alpha-skew-T (GAST) distribution for generalized autoregressive conditional heteroskedasticity (GARCH) models in modelling daily Value-at-Risk (VaR). Some mathematical properties of the proposed distribution are derived including density function, moments and stochastic representation. The maximum likelihood estimation method is discussed to estimate parameters via a simulation study. Then, the real data application on S\&P-500 index is performed to investigate the performance of GARCH models specified under GAST innovation distribution with respect to normal, Student's-t and Skew-T models in terms of the VaR accuracy. Backtesting methodology is used to compare the out-of-sample performance of the VaR models. The results show that GARCH models with GAST innovation distribution outperforms among others and generates the most conservative VaR forecasts for all confidence levels and for both long and short positions. © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--21}, 
number = {7}, 
volume = {88}
}
@article{10.1108/jrf-03-2016-0034, 
year = {2017}, 
title = {{Risk measures computation by Fourier inversion}}, 
author = {Nguyen, Ngoc Quynh Anh and Nguyen, Thi Ngoc Trang}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-03-2016-0034}, 
abstract = {{Purpose: The purpose of this paper is to present the method for efficient computation of risk measures using Fourier transform technique. Another objective is to demonstrate that this technique enables an efficient computation of risk measures beyond value-at-risk and expected shortfall. Finally, this paper highlights the importance of validating assumptions behind the risk model and describes its application in the affine model framework. Design/methodology/approach: The method proposed is based on Fourier transform methods for computing risk measures. The authors obtain the loss distribution by fitting a cubic spline through the points where Fourier inversion of the characteristic function is applied. From the loss distribution, the authors calculate value-at-risk and expected shortfall. As for the calculation of the entropic value-at-risk, it involves the moment generating function which is closely related to the characteristic function. The expectile risk measure is calculated based on call and put option prices which are available in a semi-closed form by Fourier inversion of the characteristic function. We also consider mean loss, standard deviation and semivariance which are calculated in a similar manner. Findings: The study offers practical insights into the efficient computation of risk measures as well as validation of the risk models. It also provides a detailed description of algorithms to compute each of the risk measures considered. While the main focus of the paper is on portfolio-level risk metrics, all algorithms are also applicable to single instruments. Practical implications: The algorithms presented in this paper require little computational effort which makes them very suitable for real-world applications. In addition, the mathematical setup adopted in this paper provides a natural framework for risk model validation which makes the approach presented in this paper particularly appealing in practice. Originality/value: This is the first study to consider the computation of entropic value-at-risk, semivariance as well as expectile risk measure using Fourier transform method. © 2017, © Emerald Publishing Limited.}}, 
pages = {76--87}, 
number = {1}, 
volume = {18}
}
@article{10.1111/1468-036x.00175, 
year = {2002}, 
title = {{Backtesting derivative portfolios with filtered historical simulation (FHS)}}, 
author = {Barone‐Adesi, Giovanni and Giannopoulos, Kostas and Vosper, Les}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/1468-036x.00175}, 
abstract = {{Filtered historical simulation provides the general framework to our backtests of portfolios of derivative securities held by a large sample of financial institutions. We allow for stochastic volatility and exchange rates. Correlations are preserved implicitly by our simulation procedure. Options are repriced at each node. Overall results support the adequacy of our framework, but our VaR numbers are too high for swap portfolios at long horizons and too low for options and futures portfolios at short horizons. © 2002 Blackwell Publishers Ltd.}}, 
pages = {31--58}, 
number = {1}, 
volume = {8}
}
@article{10.1016/j.frl.2021.102388, 
year = {2021}, 
title = {{Managing downside risk of low-risk anomaly portfolios}}, 
author = {Kim, Hyuksoo and Kim, Saejoon}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2021.102388}, 
abstract = {{In this paper, we present a novel risk-scaling strategy based on a measure of downside risk and investigate its performance on underlying portfolios that are formed on low-risk anomaly. The downside risk-scaling strategy addresses two challenges of the volatility-scaling strategy, namely, underestimation of and indirect management of downside risk. We demonstrate that our downside risk-scaled strategy improves the unscaled underlying low-risk anomaly strategy as well as outperforms volatility-scaled strategy in terms of risk-adjusted return and various performance metrics that are related to downside events. © 2021 Elsevier Inc.}}, 
pages = {102388}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s13675-018-0096-8, 
year = {2018}, 
title = {{Portfolio optimization with pw-robustness}}, 
author = {Gabrel, Virginie and Murat, Cécile and Thiele, Aurélie}, 
journal = {EURO Journal on Computational Optimization}, 
issn = {21924406}, 
doi = {10.1007/s13675-018-0096-8}, 
abstract = {{This paper investigates a portfolio optimization problem under uncertainty on the stock returns, where the manager seeks to achieve an appropriate trade-off between the expected portfolio return and the risk of loss. The uncertainty set consists of a finite set of scenarios occurring with equal probability. We introduce a new robustness criterion, called pw-robustness, which seeks to maximize the portfolio return in a proportion p of scenarios and guarantees a minimum return over all scenarios. We model this optimization problem as a mixed-integer programming problem. Through extensive numerical experiments, we identify the instances that can be solved to optimality in an acceptable time using off-the-shelf software. For the instances that cannot be solved to optimality within the time frame, we propose and test a heuristic that exhibits excellent practical performance in terms of computation time and solution quality for the problems we consider. This new criterion and our heuristic methods therefore exhibit great promise to tackle robustness problems when the uncertainty set consists of a large number of scenarios. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature and EURO - The Association of European Operational Research Societies.}}, 
pages = {267--290}, 
number = {3}, 
volume = {6}
}
@article{10.1504/ijbaaf.2021.118588, 
year = {2021}, 
title = {{An assessment of the fundamental review of the trading book: The capital requirement impact on a stylised financial portfolio}}, 
author = {Pederzoli, Chiara and Torricelli, Costanza}, 
journal = {International Journal of Banking, Accounting and Finance}, 
issn = {17553830}, 
doi = {10.1504/ijbaaf.2021.118588}, 
abstract = {{This paper assesses the impact on capital requirements of the Fundamental Review of the Trading Book (FRTB) based on a stylised financial portfolio sensible to the risk factors affected by the review. Our results show the order of magnitude of the increase across the two regulations and the two possible approaches: the standard approach and the internal model approach. We further disentangle the components of the expected increase implied by the FRTB. The most interesting result emerges for the internal model approach, whereby the increase in the capital charge is attributable not only to the change in the risk measure and the inclusion of longer liquidity horizons, but most importantly to the dampening of the diversification benefit. Copyright © 2021 Inderscience Enterprises Ltd.}}, 
pages = {389--403}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.jairtraman.2008.05.001, 
year = {2008}, 
title = {{The performance of Asian airlines in the recent financial turmoil based on VaR and modified Sharpe ratio}}, 
author = {Chuang, I-Yuan and Chiu, Yen-Chen and Wang, C. Edward}, 
journal = {Journal of Air Transport Management}, 
issn = {09696997}, 
doi = {10.1016/j.jairtraman.2008.05.001}, 
abstract = {{Stock performance evaluation is an important subject widely studied for both theoretical and practical purposes. It is particularly relevant for the highly cyclical airline industry now in a financial crisis. Here it is shown that the use of conventional performance measures, such as the Sharpe ratio, could seriously mislead investors regarding stock performances of airline companies. © 2008 Elsevier Ltd. All rights reserved.}}, 
pages = {257--262}, 
number = {5}, 
volume = {14}
}
@article{10.1109/icbk.2018.00060, 
year = {2018}, 
title = {{Mixed-copula VaR for portfolio risk evaluation}}, 
author = {Yin, Lechuan and Chen, Jiebin and Lai, Zhao-Rang}, 
journal = {2018 IEEE International Conference on Big Knowledge (ICBK)}, 
issn = {NA}, 
doi = {10.1109/icbk.2018.00060}, 
abstract = {{This paper proposes a novel Mixed-copula VaR (MCV) model for financial portfolio risk management and a novel investment strategy based on it. VaR (Value at Risk) is a traditional risk metric in computational finance to measure how much a set of investments might lose in a disadvantageous situation. Previous VaR models assume that the yield rates follow a single distribution (e.g. normal distribution) for simplicity, which is far from reality. In order to improve the adaptivity and the extendability of the VaR method, this paper constructs an MCV model with several families of distributions and designs a fast EM algorithm to compute the mixing weights. It further leads to a strategy for portfolio investment. Experiments by Monte Carlo simulation verify the intention of MCV. Besides, experiments on two real-world financial data sets indicate that MCV measures portfolio risk more accurately and adaptively, and delivers superior investing performance. © 2018 IEEE}}, 
pages = {400--408}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.rser.2012.04.019, 
year = {2012}, 
title = {{Risk constrained self-scheduling of hydro/wind units for short term electricity markets considering intermittency and uncertainty}}, 
author = {Ghadikolaei, Hadi Moghimi and Ahmadi, Abdollah and Aghaei, Jamshid and Najafi, Meysam}, 
journal = {Renewable and Sustainable Energy Reviews}, 
issn = {13640321}, 
doi = {10.1016/j.rser.2012.04.019}, 
abstract = {{In view of the intermittency and uncertainty associated with both the electricity production sector of restructured power system and their competitive markets, it is necessary to develop an appropriate risk managing scheme. So that it is desirable to trade-off between optimum utilization of intermittent generation resources (i.e. renewable energy resources), uncertain market prices and related risks in order to maximize participants benefits and minimize the corresponding risks in the multi-product market environment. The main goal of this paper is to investigate risk management by introducing a novel multi-risk index to quantify expected downside risk (EDR) which is caused by both the wind power and market price uncertainties. Value-at-Risk (VaR) method is used to assess the mentioned risk issue by the proposed weighted EDR, so that an optimal trade-off between the profit and risk is made for the system operations. Also, the roulette wheel mechanism is employed for random market price scenario generation wherein the stochastic procedure is converted into its respective deterministic equivalents. Moreover, the autoregressive integrated moving average (ARIMA) model is employed to characterize the stochastic wind farm (WF) generation by predetermined mean level and standard deviation of wind behavior as well as temporal correlation. The problem is formulated as a mixed-integer stochastic framework for a hydro-wind power system scheduling and tested on a generation company (GENCO). © 2012 Elsevier Ltd. All rights reserved.}}, 
pages = {4734--4743}, 
number = {7}, 
volume = {16}
}
@article{10.1016/j.jeconom.2008.12.013, 
year = {2009}, 
title = {{Granger causality in risk and detection of extreme risk spillover between financial markets}}, 
author = {Hong, Yongmiao and Liu, Yanhui and Wang, Shouyang}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2008.12.013}, 
abstract = {{Controlling and monitoring extreme downside market risk are important for financial risk management and portfolio/investment diversification. In this paper, we introduce a new concept of Granger causality in risk and propose a class of kernel-based tests to detect extreme downside risk spillover between financial markets, where risk is measured by the left tail of the distribution or equivalently by the Value at Risk (VaR). The proposed tests have a convenient asymptotic standard normal distribution under the null hypothesis of no Granger causality in risk. They check a large number of lags and thus can detect risk spillover that occurs with a time lag or that has weak spillover at each lag but carries over a very long distributional lag. Usually, tests using a large number of lags may have low power against alternatives of practical importance, due to the loss of a large number of degrees of freedom. Such power loss is fortunately alleviated for our tests because our kernel approach naturally discounts higher order lags, which is consistent with the stylized fact that today's financial markets are often more influenced by the recent events than the remote past events. A simulation study shows that the proposed tests have reasonable size and power against a variety of empirically plausible alternatives in finite samples, including the spillover from the dynamics in mean, variance, skewness and kurtosis respectively. In particular, nonuniform weighting delivers better power than uniform weighting and a Granger-type regression procedure. The proposed tests are useful in investigating large comovements between financial markets such as financial contagions. An application to the Eurodollar and Japanese Yen highlights the merits of our approach. © 2009.}}, 
pages = {271--287}, 
number = {2}, 
volume = {150}
}
@article{10.1016/j.asoc.2017.04.028, 
year = {2017}, 
title = {{An evolving possibilistic fuzzy modeling approach for Value-at-Risk estimation}}, 
author = {Maciel, Leandro and Ballini, Rosangela and Gomide, Fernando}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2017.04.028}, 
abstract = {{Market risk exposure plays a key role in risk management. A way to measure risk exposure is to evaluate the losses likely to incur when the assets prices of a portfolio decline. Most financial institutions rely on Value-at-Risk (VaR) estimates to measure downside market risk. This paper suggests an evolving possibilistic fuzzy modeling (ePFM) approach to estimate VaR. The approach is an extension of the possibilistic fuzzy c-means clustering and functional fuzzy rule-based modeling within the framework of incremental learning. Evolving possibilistic modeling employs memberships and typicalities to update the cluster structure and corresponding fuzzy rules using a statistical control distance-based criterion. A utility measure evaluates the quality of the current cluster structure and associated model. Data from the main global equity market indexes of United States, United Kingdom, Germany, Spain, and Brazil from January 2000 to December 2012 are used to estimate VaR using ePFM. The performance of ePFM is evaluated and compared with traditional VaR benchmarks such as Historical Simulation, GARCH, EWMA, and Extreme Value Theory based VaR, as well as with state of the art evolving approaches. The results suggest that ePFM is a potential candidate for VaR modeling because it achieves better results than the alternative approaches. © 2017 Elsevier B.V.}}, 
pages = {820--830}, 
number = {NA}, 
volume = {60}
}
@article{10.1080/07474930802387985, 
year = {2009}, 
title = {{Regular variation and extremal dependence of GARCH residuals with application to market risk measures}}, 
author = {Laurini, Fabrizio and Tawn, Jonathan A.}, 
journal = {Econometric Reviews}, 
issn = {07474938}, 
doi = {10.1080/07474930802387985}, 
abstract = {{Stock returns exhibit heavy tails and volatility clustering. These features, motivating the use of GARCH models, make it difficult to predict times and sizes of losses that might occur. Estimation of losses, like the Value-at-Risk, often assume that returns, normalized by the level of volatility, are Gaussian. Often under ARMA-GARCH modeling, such scaled returns are heavy tailed and show extremal dependence, whose strength reduces when increasing extreme levels. We model heavy tails of scaled returns with generalized Pareto distributions, while extremal dependence can be reduced by declustering data.}}, 
pages = {146--169}, 
number = {1-3}, 
volume = {28}
}
@article{10.1002/for.1155, 
year = {2010}, 
title = {{Incorporating higher moments into value-at-risk forecasting}}, 
author = {Polanski, Arnold and Stoja, Evarist}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1155}, 
abstract = {{Value-at-risk (VaR) forecasting generally relies on a parametric density function of portfolio returns that ignores higher moments or assumes them constant. In this paper, we propose a simple approach to forecasting of a portfolio VaR. We employ the Gram-Charlier expansion (GCE) augmenting the standard normal distribution with the first four moments, which are allowed to vary over time. In an extensive empirical study, we compare the GCE approach to other models of VaR forecasting and conclude that it provides accurate and robust estimates of the realized VaR. In spite of its simplicity, on our dataset GCE outperforms other estimates that are generated by both constant and time-varying higher-moments models. Copyright © 2009 John Wiley \& Sons, Ltd.}}, 
pages = {523--535}, 
number = {6}, 
volume = {29}
}
@article{10.1108/imefm-05-2019-0204, 
year = {2021}, 
title = {{An artificial neural network augmented GARCH model for Islamic stock market volatility: Do asymmetry and long memory matter?}}, 
author = {Chkili, Walid and Hamdi, Manel}, 
journal = {International Journal of Islamic and Middle Eastern Finance and Management}, 
issn = {17538394}, 
doi = {10.1108/imefm-05-2019-0204}, 
abstract = {{Purpose: The purpose of this study is to investigate the volatility and forecast accuracy of the Islamic stock market for the period 1999–2017. This period is characterized by the occurrence of several economic and political events such as the September 11, 2001, terrorist attack and the 2007–2008 global financial crisis. Design/methodology/approach: This study constructs a new hybrid generalized autoregressive conditional heteroskedasticity (GARCH)-type model based on an artificial neural network (ANN). This model is applied to the daily Dow Jones Islamic Market World Index during the period June 1999–January 2017. Findings: The in-sample results show that the volatility of the Islamic stock market can be better described by the fractionally integrated asymmetric power ARCH (FIAPARCH) approach that takes into account asymmetry and long memory features. Considering the out-of-sample analysis, this paper has applied a hybrid forecasting model, which combines the FIAPARCH approach and the ANN. Empirical results reveal that the proposed hybrid model (FIAPARCH-ANN) outperforms all other single models such as GARCH, fractional integrated GARCH and FIAPARCH in terms of all performance criteria used in the study. Practical implications: The results have some implications for Islamic investors, portfolio managers and policymakers. These implications are related to the optimal portfolio diversification decision, the hedging strategy choice and the risk management analysis. Originality/value: The paper develops a new framework that combines an ANN and FIAPARCH model that introduces two important features of time series, namely, asymmetry and long memory. © 2021, Emerald Publishing Limited.}}, 
pages = {853--873}, 
number = {5}, 
volume = {14}
}
@article{10.1016/j.matcom.2012.05.011, 
year = {2013}, 
title = {{A detailed comparison of value at risk estimates}}, 
author = {Abad, Pilar and Benito, Sonia}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2012.05.011}, 
abstract = {{This work investigates the performance of different models of value at risk. We include several methods (parametric, historical simulation, Monte Carlo, and extreme value theory) and some models to compute the conditional variance. We analyze several international stock indexes and examine two types of periods: stable and volatile periods. To choose the best model, we employ a two-stage selection approach. The result indicates that the best model is a parametric model with conditional variance estimated by an asymmetric GARCH model under Student's t-distribution of returns. This paper shows that parametric models can obtain successful VaR measures if conditional variance is estimated properly. © 2012 IMACS.}}, 
pages = {258--276}, 
number = {NA}, 
volume = {94}
}
@article{10.1108/ijmf-10-2017-0244, 
year = {2018}, 
title = {{Value-at-risk performance in emerging and developed countries}}, 
author = {Gaio, Luiz Eduardo and Júnior, Tabajara Pimenta and Lima, Fabiano Guasti and Passos, Ivan Carlin and Stefanelli, Nelson Oliveira}, 
journal = {International Journal of Managerial Finance}, 
issn = {17439132}, 
doi = {10.1108/ijmf-10-2017-0244}, 
abstract = {{Purpose: The purpose of this paper is to evaluate the predictive capacity of market risk estimation models in times of financial crises. Design/methodology/approach: For this, value-at-risk (VaR) valuation models applied to the daily returns of portfolios composed of stock indexes of developed and emerging countries were tested. The Historical Simulation VaR model, multivariate ARCH models (BEKK, VECH and constant conditional correlation), artificial neural networks and copula functions were tested. The data sample refers to the periods of two international financial crises, the Asian Crisis of 1997, and the US Sub Prime Crisis of 2008. Findings: The results pointed out that the multivariate ARCH models (VECH and BEKK) and Copula-Clayton had similar performance, with good adjustments in 100 percent of the tests. It was not possible to perceive significant differences between the adjustments for developed and emerging countries and of the crisis and normal periods, which was different to what was expected. Originality/value: Previous studies focus on the estimation of VaR by a group of models. One of the contributions of this paper is to use several forms of estimation. © 2018, Emerald Publishing Limited.}}, 
pages = {591--612}, 
number = {5}, 
volume = {14}
}
@article{10.1287/mnsc.1050.0476, 
year = {2006}, 
title = {{Risk management with benchmarking}}, 
author = {Basak, Suleyman and Shapiro, Alex and Teplá, Lucie}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1050.0476}, 
abstract = {{Portfolio theory must address the fact that, in reality, portfolio managers are evaluated relative to a benchmark, and therefore adopt risk management practices to account for the benchmark performance. We capture this risk management consideration by allowing a prespecified shortfall from a target benchmark-linked return, consistent with growing interest in such practice. In a dynamic setting, we demonstrate how a risk-averse portfolio manager optimally under- or overperforms a target benchmark under different economic conditions, depending on his attitude towards risk and choice of the benchmark. The analysis therefore illustrates how investors can achieve their desired performance profile for funds under management through an appropriate combined choice of the benchmark and money manager. We consider a variety of extensions, and also highlight the ability of our setting to shed some light on documented return patterns across segments of the money management industry. © 2006 INFORMS.}}, 
pages = {542--557}, 
number = {4}, 
volume = {52}
}
@article{10.1016/j.econmod.2015.06.004, 
year = {2015}, 
title = {{Value at Risk and expected shortfall of firms in the main European Union stock market indexes: A detailed analysis by economic sectors and geographical situation}}, 
author = {Iglesias, Emma M.}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2015.06.004}, 
abstract = {{We have analyzed extreme movements of the main stocks traded in the Eurozone in the 2000-2012 period. Our results can help future very-risk-averse investors to choose their portfolios in the Eurozone for risk management purposes. We find two main results. First, we can clearly classify firms by economic sector according to their different estimated VaR values in five of the seven countries we analyze. Specially, we find sectors in general where companies have very high (telecommunications and banking) and very low (petroleum, utilities, energy and consumption) estimated VaR values. Second, we only find differences according to the geographical situation of where the stocks are traded in two countries: (1) all firms in the Irish stock market (the only financially rescued country we analyze) have very high estimated VaR values in all sectors; while (2) in Spain all firms have very low estimated VaR values included in the banking and the telecommunication sectors. All our results are supported when we also study the expected shortfall of the firms. © 2015 Elsevier B.V.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {50}
}
@article{10.1145/3149869.3149872, 
year = {2017}, 
title = {{Real-Time Financial Risk Measurement of Dynamic Complex Portfolios with Python and PyOpenCL}}, 
author = {Varela, Javier Alejandro and Wehn, Norbert and Desmettre, Sascha and Korn, Ralf}, 
journal = {Proceedings of the 7th Workshop on Python for High-Performance and Scientific Computing}, 
issn = {NA}, 
doi = {10.1145/3149869.3149872}, 
abstract = {{Risk measures, such as value-at-risk and expected shortfall, are widely used to keep track of the risk at which a financial portfolio is exposed. This analysis is not only a key part of the daily operation of financial institutions worldwide, but it is also strictly enforced by regulators. While nested Monte Carlo simulations are the most flexible approach that can even deal with portfolios containing complicated derivatives, they traditionally suffer from a high computational complexity. This limits their application at certain intervals of time, mostly daily, by temporarily keeping the composition of the portfolio static. In this work, we bring together for the first time nested Monte Carlo simulations with the real-time continuous risk measurement of complex portfolios that dynamically change their composition during intraday operation. By combining the development productivity offered by Python, state-of-the-art mathematical optimizations, and the high performance capabilities offered by PyOpenCL targeting heterogeneous computing systems, our new approach reaches a throughput between 16 and 191 trading orders per second per computing node, which corresponds to the worst-case and best-case scenarios respectively. We have also made use of the Jupyter Notebook, as an interactive interface in an interdisciplinary research environment.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icmse.2006.314034, 
year = {2006}, 
title = {{Wavelet denoised value at risk estimate}}, 
author = {Chi, XIE and Kai-Jian, HE}, 
journal = {2006 International Conference on Management Science and Engineering}, 
issn = {NA}, 
doi = {10.1109/icmse.2006.314034}, 
abstract = {{With the deregulation movement spreading across global electricity market, investors are facing increasing level of price volatility and higher risks. As the proper measurement and management of risks are crucial to both investors and government regulators, this paper attempts to measure risks in the electricity market using Value at Risk (VaR) theory. To estimate VaR at higher accuracy and reliability, this paper proposes Wavelet Denoised Value at Risk (WDNVaR) estimates. Empirical studies based on the traditional ARMA-GARCH approach and the proposed WDNVaR approach are conducted in three Australian electricity markets. Performances of both approaches have been tested and compared using Kupiec backtesting procedures. Experiment results confirm that WDNVaR improves the accuracy and reliability of VaR estimates over traditional ARMA-GARCH approach significantly, which results from its capability to clean up the data and alleviate distortions introduced by outliers.}}, 
pages = {1552--1557}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2014.04.003, 
year = {2014}, 
title = {{Second-order tail asymptotics of deflated risks}}, 
author = {Hashorva, Enkelejd and Ling, Chengxiu and Peng, Zuoxiang}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.04.003}, 
abstract = {{Random deflation of risk models is an interesting topic for both theoretical and practical actuarial problems. In this paper, we investigate second-order tail asymptotics of the deflated risk X = R S under the assumptions of second-order regular variation on the survival functions of the risk R and the deflator S. Our findings are applied to derive second-order expansions of Value-at-Risk. Further we investigate the estimation of small tail probability for deflated risks and then discuss the asymptotics of the aggregated deflated risk. © 2014 Elsevier B.V.}}, 
pages = {88--101}, 
number = {1}, 
volume = {56}
}
@article{10.1111/j.1477-9552.2011.00322.x, 
year = {2012}, 
title = {{Extreme measures of agricultural financial risk}}, 
author = {Morgan, Wyn and Cotter, John and Dowd, Kevin}, 
journal = {Journal of Agricultural Economics}, 
issn = {0021857X}, 
doi = {10.1111/j.1477-9552.2011.00322.x}, 
abstract = {{The agricultural marketing environment is inherently risky. Having accurate measures of risk helps farmers, policy-makers and financial institutions make better informed decisions about how to deal with this risk. This article examines three tail quantile-based risk measures applied to the estimation of extreme agricultural financial risk for corn and soybean production in the US: Value at Risk, Expected Shortfall and Spectral Risk Measures. We use Extreme Value Theory to model the tail returns and present results for these three different risk measures using agricultural futures market returns data. We compare estimated risk measures in terms of size and precision, and find that they are all considerably higher than Gaussian estimates. The estimated risk measures are also quite imprecise, and become more so as the risks involved become more extreme. © 2011 The Agricultural Economics Society.}}, 
pages = {65--82}, 
number = {1}, 
volume = {63}
}
@article{10.1016/j.najef.2012.06.012, 
year = {2013}, 
title = {{Downside risk management and VaR-based optimal portfolios for precious metals, oil and stocks}}, 
author = {Hammoudeh, Shawkat and Santos, Paulo Araújo and Al-Hassan, Abdullah}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2012.06.012}, 
abstract = {{Value-at-Risk (VaR) is used to analyze the market downside risk associated with investments in six key individual assets including four precious metals, oil and the S\&P 500 index, and three diversified portfolios. Using combinations of these assets, three optimal portfolios and their efficient frontiers within a VaR framework are constructed and the returns and downside risks for these portfolios are also analyzed. One-day-ahead VaR forecasts are computed with nine risk models including calibrated RiskMetrics, asymmetric GARCH type models, the filtered Historical Simulation approach, methodologies from statistics of extremes and a risk management strategy involving combinations of models. These risk models are evaluated and compared based on the unconditional coverage, independence and conditional coverage criteria. The economic importance of the results is also highlighted by assessing the daily capital charges under the Basel Accord rule. The best approaches for estimating the VaR for the individual assets under study and for the three VaR-based optimal portfolios and efficient frontiers are discussed. The VaR-based performance measure ranks the most diversified optimal portfolio (Portfolio \#2) as the most efficient and the pure precious metals (Portfolio \#1) as the least efficient. © 2012 Elsevier Inc.}}, 
pages = {318--334}, 
number = {NA}, 
volume = {25}
}
@article{10.1109/ieem.2015.7385963, 
year = {2016}, 
title = {{Selection of supplier portfolio in the presence of operational risk and disruption risk}}, 
author = {Liao, Xiangxiang and Fang, Chao}, 
journal = {2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
issn = {21573611}, 
doi = {10.1109/ieem.2015.7385963}, 
abstract = {{In a make-to-order environment, manufactures should make preparations for production to fulfill stochastic customers' demand at low costs and risk levels. This research focuses on two risk levels: operational risks and disruption risks, and correspondingly, two kinds of evaluation models-value at risk and conventional value at risk-is presented to qualify them. Then, conventional criteria are incorporated into a multi-objective model with hybrid risks. The proposed GA-based approach is able to balance these objectives during supplier portfolio selection and order allocation. Finally, computational examples are presented to illustrate how the decision is made. © 2015 IEEE.}}, 
pages = {1825--1829}, 
number = {NA}, 
volume = {2016-January}
}
@article{10.1111/1477-9552.12083, 
year = {2015}, 
title = {{Agricultural Financial Risks Resulting from Extreme Events}}, 
author = {Xouridas, Stergios}, 
journal = {Journal of Agricultural Economics}, 
issn = {0021857X}, 
doi = {10.1111/1477-9552.12083}, 
abstract = {{Decision-makers in the agricultural sector operate in a volatile and risky environment. The statistical assessment of agricultural commodity prices is necessary to deduce the stylised facts of agricultural markets and guide the action of market participants. This article examines the kurtosis values of 60 agricultural commodities and presents evidence that the distributions of their returns are fat-tailed. We use power-law distributions to model the tail returns and the possible time-varying extreme event risks in commodity markets. Our results suggest that the usefulness of the value at risk and expected shortfall as risk management tools is questionable. © 2014 The Agricultural Economics Society.}}, 
pages = {192--220}, 
number = {1}, 
volume = {66}
}
@article{10.1108/jiabr-07-2019-0122, 
year = {2020}, 
title = {{Selection of Value-at-Risk models for MENA Islamic indices}}, 
author = {Ayed, Wassim Ben and Fatnassi, Ibrahim and Maatoug, Abderrazak Ben}, 
journal = {Journal of Islamic Accounting and Business Research}, 
issn = {17590817}, 
doi = {10.1108/jiabr-07-2019-0122}, 
abstract = {{Purpose: The purpose of this study is to investigate the performance of Value-at-Risk (VaR) models for nine Middle East and North Africa Islamic indices using RiskMetrics and VaR parametric models. Design/methodology/approach: The authors test the performance of several VaR models using Kupiec and Engle and Manganelli tests at 95 and 99 per cent levels for long and short trading positions, respectively, for the period from August 10, 2006 to December 14, 2014. Findings: The authors’ findings show that the VaR under Student and skewed Student distribution are preferred at a 99 per cent level VaR. However, at 95 per cent level, the VaR forecasts obtained under normal distribution are more accurate than those generated using models with fat-tailed distributions. These results suggest that VaR is a good tool for measuring market risk. The authors support the use of RiskMetrics during calm periods and the asymmetric models (Generalized Autoregressive Conditional Heteroskedastic and the Asymmetric Power ARCH model) during stressed periods. Practical implications: These results will be useful to investors and risk managers operating in Islamic markets, because their success depends on the ability to forecast stock price movements. Therefore, because a few Islamic financial institutions use internal models for their capital calculations, the regulatory committee should enhance market risk disclosure. Originality/value: This study contributes to the knowledge in this area by improving our understanding of market risk management for Islamic assets during the stress periods. Then, it highlights important implications regarding financial risk management. Finally, this study fills a gap in the literature, as most empirical studies dealing with evaluating VaR prediction models have focused on quantifying the model risk in the conventional market. © 2020, Emerald Publishing Limited.}}, 
pages = {1689--1708}, 
number = {9}, 
volume = {11}
}
@article{10.18803/capsi.v14.195-205, 
year = {2014}, 
title = {{Managing expected returns and downside risk with information from technical analysis}}, 
author = {Santos, Araújo and Matos, Carraca}, 
journal = {Atas da 14ª Conferência da Associação Portuguesa de Sistemas de Informação}, 
issn = {2183489X}, 
doi = {10.18803/capsi.v14.195-205}, 
abstract = {{With the growing amount of data and information, one of the biggest challenges of information science is to transform information into useful knowledge. This paper presents an example of how we can use the scientific method to transform information from stock market data in useful knowledge, using the bootstrap methodology. We measure downside risk with a Value-at-Risk (VaR) model and take into account the joint performance in terms of returns and risks, with Sharpe type ratios. Empirical evidence presented here confirms results from previous studies that show consistently higher returns during an uptrend and the opposite during a downward trend. Additionally, provides very strong statistical evidence that downside risk is much lower during a primary uptrend than during a downward trend and performance is better in terms of Sharpe type ratios. Empirical results show that information from the primary trend obtained with a 200-days moving average is useful. © 2014 Associacao Portuguesa de Sistemas de Informacao. All rights reserved.}}, 
pages = {195--205}, 
number = {NA}, 
volume = {14}
}
@article{10.1016/j.eswa.2015.02.002, 
year = {2015}, 
title = {{A news event-driven approach for the historical value at risk method}}, 
author = {Hogenboom, Frederik and Winter, Michael de and Frasincar, Flavius and Kaymak, Uzay}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2015.02.002}, 
abstract = {{Value at Risk (VaR) is a tool widely used in financial applications to assess portfolio risk. The historical stock return data used in calculating VaR may be sensitive to rare news events that occur during the sampled period and cause trend disruptions. Therefore, in this paper, we measure the effects of various news events on stock prices. Subsequently, we identify irregular events using a Poisson distribution, and we examine whether VaR accuracy can be improved by considering news events as an additional input in the calculation. Our experiments demonstrate that VaR predictions for rare event occurrences can be improved by removing the event-generated disturbance from the stock prices for a small, optimized time window. © 2015 Elsevier Ltd.}}, 
pages = {4667--4675}, 
number = {10}, 
volume = {42}
}
@article{10.1287/moor.2020.1072, 
year = {2021}, 
title = {{A theory for measures of tail risk}}, 
author = {Liu, Fangda and Wang, Ruodu}, 
journal = {Mathematics of Operations Research}, 
issn = {0364765X}, 
doi = {10.1287/moor.2020.1072}, 
abstract = {{The notion of “tail risk” has been a crucial consideration in modern risk management and financial regulation, as very well documented in the recent regulatory documents. To achieve a comprehensive understanding of the tail risk, we carry out an axiomatic study for risk measures that quantify the tail risk, that is, the behaviour of a risk beyond a certain quantile. Such risk measures are referred to as tail risk measures in this paper. The two popular classes of regulatory risk measures in banking and insurance, value at risk (VaR) and expected shortfall, are prominent, yet elementary, examples of tail risk measures. We establish a connection between a tail risk measure and a corresponding law-invariant risk measure, called its generator, and investigate their joint properties. A tail risk measure inherits many properties from its generator, but not subadditivity or convexity; nevertheless, a tail risk measure is coherent if and only if its generator is coherent. We explore further relevant issues on tail risk measures, such as bounds, distortion risk measures, risk aggregation, elicitability, and dual representations. In particular, there is no elicitable tail convex risk measure other than the essential supremum, and under a continuity condition, the only elicitable and positively homogeneous monetary tail risk measures are the VaRs. Copyright: © 2021 INFORMS}}, 
pages = {1109--1128}, 
number = {3}, 
volume = {46}
}
@article{10.1007/s11146-010-9252-5, 
year = {2012}, 
title = {{Extreme Risk Measures for International REIT Markets}}, 
author = {Zhou, Jian and Anderson, Randy I.}, 
journal = {The Journal of Real Estate Finance and Economics}, 
issn = {08955638}, 
doi = {10.1007/s11146-010-9252-5}, 
abstract = {{Extreme risks associated with extraordinary market conditions are catastrophic for all investors. The ongoing financial crisis has perfectly exemplified this point. Surprisingly, there are few studies exploring this issue for REITs. This study aims to close the knowledge gap. We conduct a comprehensive study by utilizing all three methodological categories to examine their forecasting performances of VaR and ES for nine major global REIT markets. Our findings indicate that there is no universally adequate method to model extreme risks across global markets. Also, estimating risks for the stock and REIT markets may require different methods. In addition, we compare the risk profiles between the stock and REIT markets, and find that the extreme risks for REITs are generally higher than those of stock markets. The fluctuations of risk levels are well synchronized between the two types of markets. The current crisis has significantly increased the extreme risk exposure for both REIT and stock investors. In all, our results have significant implications for REIT risk management, portfolio selection, and evaluation. © 2010 Springer Science+Business Media, LLC.}}, 
pages = {152--170}, 
number = {1}, 
volume = {45}
}
@article{10.1109/fuzz45933.2021.9494528, 
year = {2021}, 
title = {{A Novel Data Driven Machine Learning Algorithm for Fuzzy Estimates of Optimal Portfolio Weights and Risk Tolerance Coefficient}}, 
author = {Thavaneswaran, Aerambamoorthy and Liang, You and Paseka, Alex and Hoque, Md. Erfanul and Thulasiram, Ruppa K.}, 
journal = {2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
issn = {10987584}, 
doi = {10.1109/fuzz45933.2021.9494528}, 
abstract = {{Recently, there has been a growing interest in portfolio optimization using graphical LASSO (GL) machine learning method, by assuming normality for asset returns. However, a major drawback is that most of the asset returns follow non-normal distributions and sample percentiles are used to study the portfolio optimization with Value-at-Risk (VaR) as a risk measure. In this paper, a data-driven random weights algorithm (RWA) and a sign correlation based portfolio return distribution are used to study the fuzzy portfolio optimization. The superiority of RWA over the commonly used genetic algorithm (GA) in computing the optimal portfolio weights is demonstrated by comparing the computing time. When comparing the estimate of the risk tolerance coefficient and the theoretical value for tangency portfolios with volatility as a risk measure, RWA outperforms (smaller absolute error) the GA. The novelty of this paper is the use of RWA and GA to calculate the fuzzy estimates (interval estimates) of the risk tolerance coefficient/optimal weights and using the sign correlation to obtain the data-driven distribution of the portfolio returns. More specifically the novelty is to obtain the fuzzy estimates of the risk tolerance coefficient and portfolio weights by modelling the portfolio volatility as an asymmetric triangular fuzzy number from the data-driven observed portfolio volatilities. In particular, the proposed RWA as well as GA lead to machine learning solutions for the portfolio optimization problems without a closed form solution and provide fuzzy estimates of the risk tolerance coefficient and the optimal portfolio weights. © 2021 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {2021-July}
}
@article{10.1007/s41775-021-00112-x, 
year = {2021}, 
title = {{Central bank equity: facts and analytics}}, 
author = {Bandyopadhyay, Sujan and Devnani, Rishab and Ghosh, Sudipta and Lahiri, Amartya}, 
journal = {Indian Economic Review}, 
issn = {00194670}, 
doi = {10.1007/s41775-021-00112-x}, 
abstract = {{How much capital should the central bank of a country hold? There is no consensus on this matter. We review the balance sheets of 45 central banks from around the world to describe actual practices. The principal findings are: (a) the average capital-asset ratio of central banks globally (net of revaluation capital which is purely an accounting entry) is 6.56 percent while the number in emerging economies is 6.96 percent; and (b) our Value-at-Risk estimates for the RBI excluding exchange rate risk indicate that the current level of the core capital of the RBI as mandated by the Jalan committee may be too low. We also discuss the policy moral hazards associated with mandating RBI equity payouts to the government. © 2021, Editorial Office, Indian Economic Review.}}, 
pages = {255--279}, 
number = {1}, 
volume = {56}
}
@article{10.1287/mnsc.1040.0201, 
year = {2004}, 
title = {{A comparison of VaR and CVaR constraints on portfolio selection with the mean-variance model}}, 
author = {Alexander, Gordon J and Baptista, Alexandre M}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1040.0201}, 
abstract = {{In this paper, we analyze the portfolio selection implications arising from imposing a value-at-risk (VaR) constraint on the mean-variance model, and compare them with those arising from the imposition of a conditional value-at-risk (CVaR) constraint. We show that for a given confidence level, a CVaR constraint is tighter than a VaR constraint if the CVaR and VaR bounds coincide. Consequently, a CVaR constraint is more effective than a VaR constraint as a tool to control slightly risk-averse agents, but in the absence of a risk-free security, has a perverse effect in that it is more likely to force highly risk-averse agents to select portfolios with larger standard deviations. However, when the CVaR bound is appropriately larger than the VaR bound or when a risk-free security is present, a CVaR constraint "dominates" a VaR constraint as a risk management tool.}}, 
pages = {1261--1273}, 
number = {9}, 
volume = {50}
}
@article{10.4018/ijban.2017100101, 
year = {2017}, 
title = {{The risk of optimization in marketing campaigns}}, 
author = {Paetz, Jürgen}, 
journal = {International Journal of Business Analytics (IJBAN)}, 
issn = {23344547}, 
doi = {10.4018/ijban.2017100101}, 
abstract = {{In marketing one of the most common important tasks is to assign campaigns to sets of customers. These sets of customers, the target groups, consist of persons with similar properties, for example a high buying affinity for a certain product. Database marketers would not only assign a campaign by general economic or promotional consideration, but they take into account learning from databases by algorithms. The basic assumptions are already determined clusters to which campaigns, representing the products, should be assigned. The assignment can be done in the most optimal way by formal optimization, which is usually stochastic due to unknown campaign success in the future. The authors model the financial risk of the campaign success for enterprise practice. Their proposal is to use triangular distributions, known from financial and supply chain management applications. In an example, they demonstrate the benefits of the proposed procedure for the marketing task. © 2017, IGI Global.}}, 
pages = {1--20}, 
number = {4}, 
volume = {4}
}
@article{10.1590/0100-2945-204/14, 
year = {2015}, 
title = {{Including risk in economic feasibility analysis: A stochastic simulation model for blueberry investment decisions in Chile [Incluyendo el riesgo en el análisis de viabilidad económica: Un modelo de simulación estocástica para decisiones de inversión en arándanos en Chile]}}, 
author = {LOBOS, GERMÁN and MORA, MARCOS and SAENS, RODRIGO and MUÑOZ, TRISTÁN and SCHNETTLER, BERTA}, 
journal = {Revista Brasileira de Fruticultura}, 
issn = {01002945}, 
doi = {10.1590/0100-2945-204/14}, 
abstract = {{The traditional method of net present value (NPV) to analyze the economic profitability of an investment (based on a deterministic approach) does not adequately represent the implicit risk associated with different but correlated input variables. Using a stochastic simulation approach for evaluating the profitability of blueberry (Vaccinium corymbosum L.) production in Chile, the objective of this study is to illustrate the complexity of including risk in economic feasibility analysis when the project is subject to several but correlated risks. The results of the simulation analysis suggest that the non-inclusion of the intratemporal correlation between input variables underestimate the risk associated with investment decisions. The methodological contribution of this study illustrates the complexity of the interrelationships between uncertain variables and their impact on the convenience of carrying out this type of business in Chile. The steps for the analysis of economic viability were: First, adjusted probability distributions for stochastic input variables (SIV) were simulated and validated. Second, the random values of SIV were used to calculate random values of variables such as production, revenues, costs, depreciation, taxes and net cash flows. Third, the complete stochastic model was simulated with 10,000 iterations using random values for SIV. This result gave information to estimate the probability distributions of the stochastic output variables (SOV) such as the net present value, internal rate of return, value at risk, average cost of production, contribution margin and return on capital. Fourth, the complete stochastic model simulation results were used to analyze alternative scenarios and provide the results to decision makers in the form of probabilities, probability distributions, and for the SOV probabilistic forecasts. The main conclusion shown that this project is a profitable alternative investment in fruit trees in Chile. © 2015, Sociedade Brasileira de Fruticultura. All rights reserved.}}, 
pages = {870--882}, 
number = {4}, 
volume = {37}
}
@article{10.1016/j.estger.2017.02.003, 
year = {2017}, 
title = {{Measuring the value at risk of fixed-income portfolios using interest rate multi-factor dynamic models [Medição do valor em risco de carteiras de renda fixa usando modelos multifatoriais dinâmicos de taxas de juro] [Medición del valor en riesgo de portafolios de renta fija usando modelos multifactoriales dinámicos de tasas de interés]}}, 
author = {Álvarez-Franco, Sara Isabel and Restrepo-Tobón, Diego Alexander and Velásquez-Giraldo, Mateo}, 
journal = {Estudios Gerenciales}, 
issn = {01235923}, 
doi = {10.1016/j.estger.2017.02.003}, 
abstract = {{In this article we assess the performance of three interest rate dynamic term structure models in order to estimate the Value at Risk (VaR) of fixed-income portfolios. We find that that the model proposed by Diebold, Rudebusch and Aruoba performs appropriately in VaR backtesting statistical tests, while the model from Diebold and Li and a no-arbitrage akin term structure model display serious problems. The three models assume that the variance-covariance matrix for their underlying factors is constant, which limits their usefulness in estimating the VaR. Therefore, those models that relax this assumption should perform better and be more adequate for risk-management of fixed-income portfolios. © 2017 Universidad ICESI. Published by Elsevier España, S.L.U. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).}}, 
pages = {52--63}, 
number = {142}, 
volume = {33}
}
@article{10.1198/jbes.2009.07238, 
year = {2010}, 
title = {{The gaussian mixture dynamic conditional correlation model: Parameter estimation, value at risk calculation, and portfolio selection}}, 
author = {Galeano, Pedro and Ausín, M Concepción}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/jbes.2009.07238}, 
abstract = {{A multivariate generalized autoregressive conditional heteroscedasticity model with dynamic conditional correlations is proposed, in which the individual conditional volatilities follow exponential generalized autoregressive conditional heteroscedasticity models and the standardized innovations follow a mixture of Gaussian distributions. Inference on the model parameters and prediction of future volatilities are addressed by both maximum likelihood and Bayesian estimation methods. Estimation of the Value at Risk of a given portfolio and selection of optimal portfolios under the proposed specification are addressed. The good performance of the proposed methodology is illustrated via Monte Carlo experiments and the analysis of the daily closing prices of the Dow Jones and NASDAQ indexes. © 2010 American Statistical Association.}}, 
pages = {559--571}, 
number = {4}, 
volume = {28}
}
@article{10.1214/09-aos719, 
year = {2009}, 
title = {{Efficient estimation of copula-based semiparametric Markov models}}, 
author = {Chen, Xiaohong and Wu, Wei Biao and Yi, Yanping}, 
journal = {The Annals of Statistics}, 
issn = {00905364}, 
doi = {10.1214/09-aos719}, 
eprint = {0901.0751}, 
abstract = {{This paper considers the efficient estimation of copula-based semiparametric strictly stationary Markov models. These models are characterized by nonparametric invariant (one-dimensional marginal) distributions and parametric bivariate copula functions where the copulas capture temporal dependence and tail dependence of the processes. The Markov processes generated via tail dependent copulas may look highly persistent and are useful for financial and economic applications. We first show that Markov processes generated via Clayton, Gumbel and Student's t copulas and their survival copulas are all geometrically ergodic. We then propose a sieve maximum likelihood estimation (MLE) for the copula parameter, the invariant distribution and the conditional quantiles. We show that the sieve MLEs of any smooth functional is root-n consistent, asymptotically normal and efficient and that their sieve likelihood ratio statistics are asymptotically chi-square distributed. Monte Carlo studies indicate that, even for Markov models generated via tail dependent copulas and fat-tailed marginals, our sieve MLEs perform very well. © Institute of Mathematical Statistics, 2009.}}, 
pages = {4214--4253}, 
number = {6 B}, 
volume = {37}
}
@article{10.1016/j.frl.2018.08.009, 
year = {2019}, 
title = {{Regime changes in Bitcoin GARCH volatility dynamics}}, 
author = {Ardia, David and Bluteau, Keven and Rüede, Maxime}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2018.08.009}, 
abstract = {{We test the presence of regime changes in the GARCH volatility dynamics of Bitcoin log–returns using Markov–switching GARCH (MSGARCH) models. We also compare MSGARCH to traditional single–regime GARCH specifications in predicting one–day ahead Value–at–Risk (VaR). The Bayesian approach is used to estimate the model parameters and to compute the VaR forecasts. We find strong evidence of regime changes in the GARCH process and show that MSGARCH models outperform single–regime specifications when predicting the VaR. © 2018 The Authors}}, 
pages = {266--271}, 
number = {NA}, 
volume = {29}
}
@article{10.1007/s11156-017-0652-y, 
year = {2018}, 
title = {{How accurate are modern Value-at-Risk estimators derived from extreme value theory?}}, 
author = {Mögel, Benjamin and Auer, Benjamin R.}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-017-0652-y}, 
abstract = {{In this study, we compare the out-of-sample forecasting performance of several modern Value-at-Risk (VaR) estimators derived from extreme value theory (EVT). Specifically, in a multi-asset study covering 30 years of stock, bond, commodity and currency market data, we analyse the accuracy of the classic generalised Pareto peak over threshold approach and three recently proposed methods based on the Box–Cox transformation, L-moment estimation and the Johnson system of distributions. We find that, in their unconditional form, some of the estimators may be acceptable under current regulatory assessment rules but none of them can continuously pass more advanced tests of forecasting accuracy. In their conditional forms, forecasting power is significantly increased and the Box–Cox method proves to be the most promising estimator. However, it is also important to stress that the traditional historical simulation approach, which is currently the most frequently used VaR estimator in commercial banks, can not only keep up with the EVT-based methods but occasionally even outperforms them (depending on the setting: unconditional versus conditional). Thus, recent claims to generally replace this simple method by theoretically more advanced EVT-based methods may be premature. © 2017, Springer Science+Business Media, LLC.}}, 
pages = {979--1030}, 
number = {4}, 
volume = {50}
}
@article{10.1016/j.eneco.2019.03.019, 
year = {2019}, 
title = {{Oil price risk evaluation using a novel hybrid model based on time-varying long memory}}, 
author = {Zhao, Lu-Tao and Liu, Kun and Duan, Xin-Lei and Li, Ming-Fang}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2019.03.019}, 
abstract = {{The volatility of crude oil price has a great influence on the world economy. In order to measure the crude oil price risk (VaR) and explain the dynamic relationship between investment income and risk in the oil market more clearly, this paper uses a variety of fractional GARCH models to describe typical volatility characteristics like long memory, volatility clustering, asymmetry and thick tail. The autoregressive conditional heteroscedasticity in the mean model (ARCH-M) and peaks-over-threshold model of extreme value theory (EVT-POT) are taken into account to develop a hybrid time-varying long memory GARCH-M-EVT model for calculation of static and dynamic VaR. Empirical results show that the WTI crude oil has a significantly long memory feature. All the fractional integration GARCH models can describe the long memory appropriately and the FIAPARCH model is the best in regression and out of sample one-step-ahead VaR forecasting. Back-testing results show that the FIAPARCH-M-EVT model is superior to other GARCH-type models which only consider oil price fluctuation characteristics partially and traditional methods including Variance-Covariance and Monte Carlo in price risk measurement. Our conclusions confirm that considering long memory, asymmetry and fat tails in the behavior of energy commodity return combined with effectively dynamic time-varying risk reflection such as the ARCH-M model and reliable tail extreme filter processes such as EVT can improve the accuracy of crude oil price risk measurement, provide an effective tool for analyzing the extreme risk of the tail of the oil market and facilitate the risk management for oil market investors. © 2019 Elsevier B.V.}}, 
pages = {70--78}, 
number = {NA}, 
volume = {81}
}
@article{10.1016/j.najef.2017.07.016, 
year = {2017}, 
title = {{Modeling Latin-American stock and Forex markets volatility: Empirical application of a model with random level shifts and genuine long memory}}, 
author = {Rodríguez, Gabriel}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2017.07.016}, 
abstract = {{Following Varneskov and Perron (2017a,b), I apply the RLS-ARFIMA(p,d,q) models to the daily stock and Forex market returns volatility of Argentina, Brazil, Mexico and Peru. Further, two sets of high-frequency data are also used. The model is a parametric state-space model with an estimation framework that combines long memory and level shifts by decomposing the underlying process into a mixture model and ARFIMA dynamics. The results of the estimates are not conclusive as those obtained in Varneskov and Perron (2017a,b). In fact, the very small magnitudes of the fractional parameter estimates suggest that only the high-frequency series could be modeled as RLS-ARFIMA models. The other (daily) series would be modeled as RLS-ARMA models with measurement errors except in the case of the Forex market of Brazil where there is no evidence of measurement errors. Another possibility is to accept the small magnitudes of the estimates of the fractional parameter as evidence of genuine long memory and, in that case, a larger group of series can be modeled as RLS-ARFIMA models. The forecasts are evaluated from two perspectives: one using 10\% of the Model Confidence Set of Hansen, Lunde, and Nason (2011) and the other using a recent statistic proposed by Knüppel (2015) to evaluate density forecasts. The results favor the RLS-ARMA and RLS-ARFIMA models although we found some differences between the two approaches for the cases of Brazil and Mexico (stocks) and Argentina (Forex). Finally, forecasts are used to calculate the VaR at 1\%, 5\% and 10\%. The results support broadly the RLS-ARFIMA models with one or two exceptions. © 2017 Elsevier Inc.}}, 
pages = {393--420}, 
number = {NA}, 
volume = {42}
}
@article{10.1007/978-3-319-45348-4_21, 
year = {2016}, 
title = {{Value at risk within business processes: An automated it risk governance approach}}, 
author = {González-Rojas, Oscar and Lesmes, Sebastian}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-45348-4\_21}, 
abstract = {{Business processes are core operational assets to control firms’ efficiency in value generation. However, the execution and control of business processes is increasingly dependent on Information Technology (IT). Therefore, the risks that arise from relying on IT in business processes must be quantified. This paper proposes the adaptation of the Value at Risk (VaR) financial technique to measure the level of risk within a process portfolio. This is done by quantifying the impact resulting from changes in the performance of IT services. The probability of IT risks is measured daily in order to model the volatility of IT services, especially when they are flexible and changeable. The proposed method enables predicting and estimating the losses of IT risks and their effect on dependent business processes over a time horizon. The incorporation of risk management mechanisms enriches business processes with organizational management capabilities. © Springer International Publishing Switzerland 2016.}}, 
pages = {365--380}, 
number = {NA}, 
volume = {9850 LNCS}
}
@article{10.1002/for.2485, 
year = {2018}, 
title = {{Measuring the market risk of freight rates: A forecast combination approach}}, 
author = {Argyropoulos, Christos and Panopoulou, Ekaterini}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2485}, 
abstract = {{This paper addresses the issue of freight rate risk measurement via value at risk (VaR) and forecast combination methodologies while focusing on detailed performance evaluation. We contribute to the literature in three ways: First, we reevaluate the performance of popular VaR estimation methods on freight rates amid the adverse economic consequences of the recent financial and sovereign debt crisis. Second, we provide a detailed and extensive backtesting and evaluation methodology. Last, we propose a forecast combination approach for estimating VaR. Our findings suggest that our combination methods produce more accurate estimates for all the sectors under scrutiny, while in some cases they may be viewed as conservative since they tend to overestimate nominal VaR. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {201--224}, 
number = {2}, 
volume = {37}
}
@article{10.1016/j.insmatheco.2006.06.005, 
year = {2007}, 
title = {{Bayesian graduation of mortality rates: An application to reserve evaluation}}, 
author = {Neves, César da Rocha and Migon, Helio S.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2006.06.005}, 
abstract = {{This paper presents Bayesian graduation models of mortality rates, using Markov chain Monte Carlo (MCMC) techniques. Graduated annual death probabilities are estimated through the predictive distribution of the number of deaths, which is assumed to follow a Poisson process, considering that all individuals in the same age class die independently and with the same probability. The resulting mortality tables are formulated through dynamic Bayesian models. Calculation of adequate reserve levels is exemplified, via MCMC, making use of the value at risk concept, demonstrating the importance of using "true" observed mortality figures for the population exposed to risk in determining the survival coverage rate. © 2006 Elsevier Ltd. All rights reserved.}}, 
pages = {424--434}, 
number = {3}, 
volume = {40}
}
@article{10.1093/jjfinec/nbp011, 
year = {2009}, 
title = {{CHICAGO: A fast and accurate method for portfolio risk calculation}}, 
author = {Broda, S A and Paolella, M S}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbp011}, 
abstract = {{This paper shows how independent component analysis can be used to estimate the generalized orthogonal GARCH model in a fraction of the time otherwise required. The proposed method is a two-step procedure, separating the estimation of the correlation structure from that of the univariate dynamics, thus facilitating the incorporation of non-Gaussian innovations distributions in a straightforward manner. The generalized hyperbolic distribution provides an excellent parametric description of financial returns data and is used for the univariate fits, but its convolutions, necessary for portfolio risk calculations, are intractable. This restriction is overcome by saddlepoint approximations for the Value at Risk and expected shortfall, which are computationally cheap and retain excellent accuracy far into the tails. It is further shown that the mean-expected shortfall portfolio optimization problem can be solved efficiently in the context of the model. A simulation study and an application to stock returns demonstrate the validity of the procedure. © The Author 2009. Published by Oxford University Press. All rights reserved.}}, 
pages = {412--436}, 
number = {4}, 
volume = {7}
}
@article{10.21314/jop.2015.154, 
year = {2015}, 
title = {{Approximations of value-at-risk as an extreme quantile of a random sum of heavy-tailed random variables}}, 
author = {Hannah, Lincoln and Puza, Borek}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2015.154}, 
abstract = {{This paper studies the approximation of extreme quantiles of random sums of heavytailed random variables, or, more specifically, subexponential random variables.Akey application of this approximation is the calculation of operational value-at-risk (VaR) for financial institutions in order to determine operational risk capital requirements. This paper follows work by Böcker, Klüppelberg and Sprittulla and makes several advances. These include two new approximations ofVaR and an extension to multiple loss types where the VaR relates to a sum of random sums, each of which is defined by different distributions. The proposed approximations are assessed via a simulation study. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--21}, 
number = {2}, 
volume = {10}
}
@article{10.3390/en14144147, 
year = {2021}, 
title = {{Tail dependence between crude oil volatility index and WTI oil price movements during the COVID-19 pandemic}}, 
author = {Echaust, Krzysztof and Just, Małgorzata}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en14144147}, 
abstract = {{This study investigates the dependence between extreme returns of West Texas Intermediate (WTI) crude oil prices and the Crude Oil Volatility Index (OVX) changes as well as the predictive power of OVX to generate accurate Value at Risk (VaR) forecasts for crude oil. We focus on the COVID-19 pandemic period as the most violate in the history of the oil market. The static and dynamic conditional copula methodology is used to measure the tail dependence coefficient (TDC) between the variables. We found a strong relationship in the tail dependence between negative returns on crude oil and OVX changes and the tail independence for positive returns. The time-varying copula discloses the strongest tail dependence of negative oil price shocks and the index changes during the COVID-19 health crisis. The findings indicate the ability of the OVX index to be a fear gauge with respect to the oil market. However, we cannot confirm the ability of OVX to improve one day-ahead forecasts of the Value at Risk. The impact of investors’ expectations embedded in OVX on VaR forecasts seems to be negligible. © 2021 by the authors.}}, 
pages = {4147}, 
number = {14}, 
volume = {14}
}
@article{10.1007/s00780-012-0200-5, 
year = {2013}, 
title = {{Bounds for the sum of dependent risks and worst Value-at-Risk with monotone marginal densities}}, 
author = {Wang, Ruodu and Peng, Liang and Yang, Jingping}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-012-0200-5}, 
abstract = {{In quantitative risk management, it is important and challenging to find sharp bounds for the distribution of the sum of dependent risks with given marginal distributions, but an unspecified dependence structure. These bounds are directly related to the problem of obtaining the worst Value-at-Risk of the total risk. Using the idea of complete mixability, we provide a new lower bound for any given marginal distributions and give a necessary and sufficient condition for the sharpness of this new bound. For the sum of dependent risks with an identical distribution, which has either a monotone density or a tail-monotone density, the explicit values of the worst Value-at-Risk and bounds on the distribution of the total risk are obtained. Some examples are given to illustrate the new results. © 2012 Springer-Verlag Berlin Heidelberg.}}, 
pages = {395--417}, 
number = {2}, 
volume = {17}
}
@article{10.1109/compsac.2019.10210, 
year = {2019}, 
title = {{Fuzzy value-at-risk forecasts using a novel data-driven neuro volatility predictive model}}, 
author = {Thavaneswaran, A. and Thulasiram, Ruppa K. and Zhu, Zimo and Hoque, Md. Erfanul and Ravishanker, Nalini}, 
journal = {2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)}, 
issn = {07303157}, 
doi = {10.1109/compsac.2019.10210}, 
abstract = {{Quantitative finance has been evolving over last several decades and combining randomness and fuzziness of the parameters has found growing interest among researchers to solve forecasting problems. Superiority of the fuzzy forecasting method over the minimum mean square forecasting had been demonstrated for fuzzy coefficient (linear as well as nonlinear) time series models in Thavaneswaran et al. [5]. However, many proposed fuzzy forecasting methods remain difficult to use in practice and there is a need for data-driven approach to fit the fuzzy coefficient volatility models. A neural network (NN) system can uniformly approximate any real nonlinear function on a compact domain to any degree of accuracy. Artificial NN (ANNs) have been applied to finance problems such as stock index prediction and bankruptcy prediction. In this paper, we introduce a novel direct data-driven neuro predictive model for conditional volatility and study the fuzzy value-at-risk (VaR) forecasts. We apply this model to forecast VaR with actual financial data. Our model shows considerable promise as a decision making and risk managing tool. © 2019 IEEE.}}, 
pages = {221--226}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.najef.2018.12.002, 
year = {2019}, 
title = {{International portfolio of stock indices with spatiotemporal correlations: Can investors still benefit from portfolio, when and where?}}, 
author = {Mo, Guoli and Tan, Chunzhi and Zhang, Weiguo and Liu, Fang}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2018.12.002}, 
abstract = {{This paper revisits the topic of international portfolio of stock indices under spatiotemporal correlations, to help people to get better portfolio performance in international stock markets. We firstly develop a mean-VaR framework as well as a mean-CVaR one, where both of which are with spatiotemporal correlation and other constraints. Then we apply these two frameworks to investigate whether investors can gain in international stock markets or not under various constraints. Our empirical results find that 1) Investors can still benefit from the international portfolio with spatiotemporal correlations, either in the view of avoiding risk or pursuing the profit; 2) the spatiotemporal correlation and exchange rate contribute to the performance of portfolio significantly, and transaction cost and fixed income rarely have effect on the portfolio, both in crisis and calm periods. Additionally, in the period of calm, the skewness of each single return series has some significant impact on the portfolio performance; 3) the portfolio with lowest spatiotemporal correlation with other markets is the optimal choice. In addition, in the calm period, another suitable area can be the one with positive mean and negative skewness of returns, such as in the U.K. market; 4) the mean-CVaR framework outperforms the mean-VaR one in financial calm period, but equals to the latter in crisis time. Our results demonstrate that the proposed mean-CVaR programming framework with spatiotemporal correlation provides a more flexible and effective decision support tool for international portfolio. © 2018 Elsevier Inc.}}, 
pages = {168--183}, 
number = {NA}, 
volume = {47}
}
@article{10.1109/iccasm.2010.5623247, 
year = {2010}, 
title = {{Minimizing Value-at-risk methodology under fuzzy-stochastic approach}}, 
author = {He, Ying-Yu}, 
journal = {2010 International Conference on Computer Application and System Modeling (ICCASM 2010)}, 
issn = {NA}, 
doi = {10.1109/iccasm.2010.5623247}, 
abstract = {{This paper describes new models for portfolio selection under uncertainty with risk and vagueness aspects which are solved by fuzzy-stochastic methodology. And the corresponding optimal portfolio is derived using minimizing Value-at-risk(VaR). The VaR concept has been extended to the portfolio value-at-risk measure used for managing risk and returns under a multiple-asset portfolio. An approach to modeling risks by VaR under imprecise and fuzzy conditions is discussed. It is supposed that the input data and problem conditions is difficult to determine as real numbers or as some precise distribution functions. Thus, vagueness is modeling through the fuzzy numbers. A numerical example of a portfolio selection problem is given to illustrate our proposed approaches. © 2010 IEEE.}}, 
pages = {V11--120-V11-124}, 
number = {NA}, 
volume = {11}
}
@article{10.25115/eea.v39i3.3711, 
year = {2021}, 
title = {{An Approximation Scheme for Value at Risk under Mean Reverting Stochastic Volatility Model}}, 
author = {Neisy, Abdolsadeh}, 
journal = {Studies of Applied Economics}, 
issn = {11333197}, 
doi = {10.25115/eea.v39i3.3711}, 
abstract = {{In this paper, a stochastic differential equation is provided for the stock price in which not only the volatility of returns is stochastic as in the Hull and White model but also, it has the mean-reverting property. Then, to analyze the probability distribution of this model, the Fokker-Plank equation is applied and the resulting problem is solved numerically. The well-known numerical scheme, θ-method is applied to approximate the solution of the resulting equation. Finally, a practical application of the provided model is shown within a numerical example. In this example, the proposed model and numerical results are used to approximate the Value at Risk (VaR) of the Tehran stock exchange total index. © 2021 Ascociacion Internacional de Economia Aplicada. All rights reserved.}}, 
number = {3}, 
volume = {39}
}
@article{10.1214/17-aos1580, 
year = {2018}, 
title = {{A smooth block bootstrap for quantile regression with time series}}, 
author = {Gregory, Karl B. and Lahiri, Soumendra N. and Nordman, Daniel J.}, 
journal = {The Annals of Statistics}, 
issn = {00905364}, 
doi = {10.1214/17-aos1580}, 
abstract = {{Quantile regression allows for broad (conditional) characterizations of a response distribution beyond conditional means and is of increasing interest in economic and financial applications. Because quantile regression estimators have complex limiting distributions, several bootstrap methods for the independent data setting have been proposed, many of which involve smoothing steps to improve bootstrap approximations. Currently, no similar advances in smoothed bootstraps exist for quantile regression with dependent data. To this end, we establish a smooth tapered block bootstrap procedure for approximating the distribution of quantile regression estimators for time series. This bootstrap involves two rounds of smoothing in resampling: individual observations are resampled via kernel smoothing techniques and resampled data blocks are smoothed by tapering. The smooth bootstrap results in performance improvements over previous unsmoothed versions of the block bootstrap as well as normal approximations based on Powell's kernel variance estimator, which are common in application. Our theoretical results correct errors in proofs for earlier and simpler versions of the (unsmoothed) moving blocks bootstrap for quantile regression and broaden the validity of block bootstraps for this problem under weak conditions. We illustrate the smooth bootstrap through numerical studies and examples. © Institute of Mathematical Statistics, 2018.}}, 
pages = {1138--1166}, 
number = {3}, 
volume = {46}
}
@article{10.1016/j.insmatheco.2017.11.003, 
year = {2018}, 
title = {{From Concentration Profiles to Concentration Maps. New tools for the study of loss distributions}}, 
author = {Fontanari, Andrea and Cirillo, Pasquale and Oosterlee, Cornelis W.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2017.11.003}, 
abstract = {{We introduce a novel approach to risk management, based on the study of concentration measures of the loss distribution. We show that indices like the Gini index, especially when restricted to the tails by conditioning and truncation, give us an accurate way of assessing the variability of the larger losses – the most relevant ones – and the reliability of common risk management measures like the Expected Shortfall. We first present the Concentration Profile, which is formed by a sequence of truncated Gini indices, to characterize the loss distribution, providing interesting information about tail risk. By combining Concentration Profiles and standard results from utility theory, we develop the Concentration Map, which can be used to assess the risk attached to potential losses on the basis of the risk profile of a user, her beliefs and historical data. Finally, with a sequence of truncated Gini indices as weights for the Expected Shortfall, we define the Concentration Adjusted Expected Shortfall, a measure able to capture additional features of tail risk. Empirical examples and codes for the computation of all the tools are provided. © 2017 Elsevier B.V.}}, 
pages = {13--29}, 
number = {NA}, 
volume = {78}
}
@article{10.1016/j.jempfin.2016.01.006, 
year = {2016}, 
title = {{Realizing the extremes: Estimation of tail-risk measures from a high-frequency perspective}}, 
author = {Bee, Marco and Dupuis, Debbie J. and Trapin, Luca}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2016.01.006}, 
abstract = {{This article applies realized volatility forecasting to Extreme Value Theory (EVT). We propose a two-step approach where returns are first pre-whitened with a high-frequency based volatility model, and then an EVT based model is fitted to the tails of the standardized residuals. This realized EVT approach is compared to the conditional EVT of McNeil \& Frey (2000). We assess both approaches' ability to filter the dependence in the extremes and to produce stable out-of-sample VaR and ES estimates for one-day and ten-day time horizons. The main finding is that GARCH-type models perform well in filtering the dependence, while the realized EVT approach seems preferable in forecasting, especially at longer time horizons. © 2016 Elsevier B.V.}}, 
pages = {86--99}, 
number = {NA}, 
volume = {36}
}
@article{10.3130/jaabe.17.63, 
year = {2018}, 
title = {{Determining value at risk for estimating renovation building projects by application of probability-based fuzzy set theory}}, 
author = {Cha, HeeSung and Lee, DongGun}, 
journal = {Journal of Asian Architecture and Building Engineering}, 
issn = {13467581}, 
doi = {10.3130/jaabe.17.63}, 
abstract = {{Building renovation projects are mainly affected by not only the design constraints of existing buildings but also various influential factors (e.g., building age, structural stability, site condition). However, conventional estimation methods deal with renovation projects similar to new construction and do not effectively consider potential risk factors. Thus, unexpected problems can make it difficult to proceed with the initial project execution plan. A three-phase framework is proposed for a probabilistic cost estimation process model that reflects the uncertainties of residential building renovation: (1) roughly estimating the cost based on industry means data; (2) converting the estimated costs to probabilistic values to consider the project characteristics; and (3) adjusting the converted probabilistic values according to risk factors. The value at risk of a specific construction can then be calculated to improve the reliability of the cost estimation. The proposed model was applied to a case study to demonstrate its feasibility. The results showed that the distribution of cost estimates reflecting the project characteristics and risk factors could be objectively confirmed at the initial stage. Thus, the method can help facilitate the decision-making process for homeowners, especially those reluctant to choose renovation. © 2018, Architectural Institute of Japan. All rights reserved.}}, 
pages = {63--70}, 
number = {1}, 
volume = {17}
}
@article{10.1111/itor.12652, 
year = {2021}, 
title = {{Robust reward–risk ratio portfolio optimization}}, 
author = {Sehgal, Ruchika and Mehra, Aparna}, 
journal = {International Transactions in Operational Research}, 
issn = {09696016}, 
doi = {10.1111/itor.12652}, 
abstract = {{In this paper, we propose robust portfolio optimization models for reward–risk ratios utilizing Omega, semi-mean absolute deviation ratio, and weighted stable tail adjusted return ratio (STARR). We address the uncertainty in returns on assets by varying them in symmetric bounded intervals. The proposed robust reward–risk ratios preserve linearity in the resulting models, and hence are tractable. However, the robust models involve a sizably voluminous number of constraints, especially when the number of assets and scenarios is large. We employ the cutting plane algorithm to solve the proposed models in a much reduced time efficiently. We evaluate the performance of the robust reward–risk ratio models on the listed stocks of FTSE 100, Nikkei 225, S\&P 500, and S\&P BSE 500. The robust portfolio optimization models are found to outperform their conventional counterpart models in terms of statistics measured by the standard deviation, value at risk (VaR), conditional value at risk (CVaR), Sharpe ratio, and STARR ratio. © 2019 The Authors. International Transactions in Operational Research © 2019 International Federation of Operational Research Societies}}, 
pages = {2169--2190}, 
number = {4}, 
volume = {28}
}
@article{10.21314/jor.2018.384, 
year = {2018}, 
title = {{Impact of D-vine structure on risk estimation}}, 
author = {Bolance, Catalina and Alemany, Ramon and Barreto, Alemar E Padilla}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.384}, 
abstract = {{In this paper, a sensitivity analysis using pair–copula decomposition of multivariate dependency models is performed on estimates of value-at-risk (VaR) and conditional value-at-risk (CVaR). To illustrate the results, we use four financial share portfolios selected to exemplify this purpose. For each share, we calculate filtered log returns using autoregressive moving average–generalized autoregressive conditional hetero-scedasticity models and study their dependence. We analyze how selecting pairs of assets to define vines prior to pair–copula decomposition affects the estimated VaR and CVaR. Further, using bootstrap confidence intervals, we compare the results of different risk measures obtained by employing alternative measures of dependence to select the order in which the drawable vine (D-vine) is defined in different portfolios. Moreover, we carry out a simulation study to analyze the finite sample properties of the different criteria for selecting the pair–copula decomposition associated with the D-vine. We find some differences between the results obtained for VaR and CVaR. © Infopro Digital Limited 2018. All rights reserved.}}, 
number = {5}, 
volume = {20}
}
@article{10.1093/jjfinec/nbq025, 
year = {2011}, 
title = {{Backtesting value-at-risk: A GMM duration-based test}}, 
author = {Candelon, B and Colletaz, G and Hurlin, C and Tokpavi, S}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbq025}, 
abstract = {{This paper proposes a new duration-based backtesting procedure for value-at-risk (VaR) forecasts. The GMM test framework proposed by Bontemps (2006) to test for the distributional assumption (i.e., the geometric distribution) is applied to the case of the VaR forecasts validity. Using simple J-statistic based on the moments defined by the orthonormal polynomials associated with the geometric distribution, this new approach tackles most of the drawbacks usually associated to duration-based backtesting procedures. An empirical application for Nasdaq returns confirms that using GMMtest leads to major consequences for the expost evaluation of the risk by regulation authorities. © The Author 2010. Published by Oxford University Press. All rights reserved.}}, 
pages = {314--343}, 
number = {2}, 
volume = {9}
}
@article{10.1016/j.laa.2010.10.023, 
year = {2011}, 
title = {{Random orthogonal matrix simulation}}, 
author = {Ledermann, Walter and Alexander, Carol and Ledermann, Daniel}, 
journal = {Linear Algebra and its Applications}, 
issn = {00243795}, 
doi = {10.1016/j.laa.2010.10.023}, 
abstract = {{This paper introduces a method for simulating multivariate samples that have exact means, covariances, skewness and kurtosis. We introduce a new class of rectangular orthogonal matrix which is fundamental to the methodology and we call these matrices L matrices. They may be deterministic, parametric or data specific in nature. The target moments determine the L matrix then infinitely many random samples with the same exact moments may be generated by multiplying the L matrix by arbitrary random orthogonal matrices. This methodology is thus termed "ROM simulation". Considering certain elementary types of random orthogonal matrices we demonstrate that they generate samples with different characteristics. ROM simulation has applications to many problems that are resolved using standard Monte Carlo methods. But no parametric assumptions are required (unless parametric L matrices are used) so there is no sampling error caused by the discrete approximation of a continuous distribution, which is a major source of error in standard Monte Carlo simulations. For illustration, we apply ROM simulation to determine the value-at-risk of a stock portfolio. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {1444--1467}, 
number = {6}, 
volume = {434}
}
@article{10.1016/j.iref.2018.03.026, 
year = {2018}, 
title = {{Dynamic hedging performance and downside risk: Evidence from Nikkei index futures}}, 
author = {Ubukata, Masato}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2018.03.026}, 
abstract = {{This paper assesses the incremental value of dynamic futures hedging models in minimizing downside risks including value-at-risk, expected shortfall, exponential spectral risk measure and lower partial moment over unconditional hedging approaches. We estimate hedge ratios using dynamic conditional correlation models incorporating high-frequency measures of volatility and correlation under multivariate skewed t-distributions. In the out-of-sample analysis with daily rebalancing and a portfolio hedged with Nikkei 225 futures, the unconditional minimum downside risk approaches perform worse than the proposed conditional approaches. The use of high-frequency measures possibly improves performance of the conditional downside-risk hedging models. © 2018 Elsevier Inc.}}, 
pages = {270--281}, 
number = {NA}, 
volume = {58}
}
@article{10.1016/j.qref.2015.10.009, 
year = {2016}, 
title = {{On animal spirits and economic decisions: Value-at-Risk and Value-within-Reach as measures of risk and return}}, 
author = {Joaquin, Domingo Castelo}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2015.10.009}, 
abstract = {{Let us suppose that presently unimagined is possible, that "the unexpected may happen" (Marshall (1920). Principles of economics, McMillan, London, p. 347). Then "human decisions affecting the future, whether personal, political or economic, cannot depend on strict mathematical expectation since the basis for making such calculations does not exist" (Keynes (1936). The general theory of employment, interest and money, Harcourt Brace, New York, NY, pp. 162-163) and "individual initiative will only be adequate when reasonable calculation is supplemented and supported by animal spirits" (Keynes (1936). The general theory of employment, interest and money, Harcourt Brace, New York, NY, p. 162)-by "a spontaneous urge to action rather than inaction" (Keynes (1936). The general theory of employment, interest and money, Harcourt Brace, New York, NY, p. 161). It is intended here to examine an investment's Value-at-Risk as a reasonable calculation of the worst threat an action appears to make possible, and its return counterpart, referred to as the investment's Value-within-Reach, as a reasonable calculation of the best hope an action appears to offer. In exploring the extension of the Value-at-Risk approach from applications to investments in financial assets to applications to investments in real assets, the properties of Value-at-Risk as a risk measure are reviewed. Recognizing that Value-at-Risk focuses exclusively on downside risk, a complementary set of properties is specified which is shown to be necessary and sufficient for the acceptance of Value-within-Reach as a measure of return. This note concludes with remarks on a distribution's focus-values, consisting of the distribution's Value-at-Risk and Value-within-Reach, as reasonable calculation of a course of action's risk and return. © 2015 The Board of Trustees of the University of Illinois.}}, 
pages = {231--233}, 
number = {NA}, 
volume = {60}
}
@article{10.1016/s0378-4266(02)00281-9, 
year = {2002}, 
title = {{Spectral measures of risk: A coherent representation of subjective risk aversion}}, 
author = {Acerbi, Carlo}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00281-9}, 
abstract = {{We study a space of coherent risk measures Mφ obtained as certain expansions of coherent elementary basis measures. In this space, the concept of "risk aversion function" φ naturally arises as the spectral representation of each risk measure in a space of functions of confidence level probabilities. We give necessary and sufficient conditions on φ for Mφ to be a coherent measure. We find in this way a simple interpretation of the concept of coherence and a way to map any rational investor's subjective risk aversion onto a coherent measure and vice-versa. We also provide for these measures their discrete versions Mφ(N) acting on finite sets of N independent realizations of a r.v. which are not only shown to be coherent measures for any fixed N, but also consistent estimators of Mφ for large N. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {1505--1518}, 
number = {7}, 
volume = {26}
}
@article{10.1016/j.jbankfin.2009.08.009, 
year = {2010}, 
title = {{The level and quality of Value-at-Risk disclosure by commercial banks}}, 
author = {Pérignon, Christophe and Smith, Daniel R.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2009.08.009}, 
abstract = {{In this paper we study both the level of Value-at-Risk (VaR) disclosure and the accuracy of the disclosed VaR figures for a sample of US and international commercial banks. To measure the level of VaR disclosures, we develop a VaR Disclosure Index that captures many different facets of market risk disclosure. Using panel data over the period 1996-2005, we find an overall upward trend in the quantity of information released to the public. We also find that Historical Simulation is by far the most popular VaR method. We assess the accuracy of VaR figures by studying the number of VaR exceedances and whether actual daily VaRs contain information about the volatility of subsequent trading revenues. Unlike the level of VaR disclosure, the quality of VaR disclosure shows no sign of improvement over time. We find that VaR computed using Historical Simulation contains very little information about future volatility. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {362--377}, 
number = {2}, 
volume = {34}
}
@article{10.1007/s10614-015-9493-8, 
year = {2016}, 
title = {{Bank Capital Shock Propagation via Syndicated Interconnectedness}}, 
author = {Nirei, Makoto and Sushko, Vladyslav and Caballero, Julián}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-015-9493-8}, 
abstract = {{Loan syndication increases bank interconnectedness through co-lending relationships. We study the financial stability implications of such dependency on syndicate partners in the presence of shocks to banks’ capital. Model simulations in a network setting show that such shocks can produce rare events in this market when banks have shared loan exposures while also relying on a common risk management tool such as value-at-risk (VaR). This is because a withdrawal of a bank from a syndicate can cause ripple effects through the market, as the loan arranger scrambles to commit more of its own funds by also pulling back from other syndicates or has to dissolve the syndicate it had arranged. However, simulations also show that the core-periphery structure observed in the empirical network may reduce the probability of such contagion. In addition, simulations with tighter VaR constraints show banks taking on less risk ex-ante. © 2015, Springer Science+Business Media New York.}}, 
pages = {67--96}, 
number = {1}, 
volume = {47}
}
@article{10.1016/j.econmod.2012.11.032, 
year = {2013}, 
title = {{Fuzzy possibilistic portfolio selection model with VaR constraint and risk-free investment}}, 
author = {Li, Ting and Zhang, Weiguo and Xu, Weijun}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2012.11.032}, 
abstract = {{We propose a possibilistic portfolio model with VaR constraint and risk-free investment based on the possibilistic mean and variance, while assuming that the expected rate of returns is a fuzzy number. The model shows more clearly that, in the financial market affected by several non-probabilistic factors, risk-averse investors wish not only to reach the expected rate of returns in their actual investment, but also to assure that the maximum of their possible future risk is lower than an expected loss. Under the condition that the expected rate of returns is a normal distribution fuzzy variable, we proposed a theorem as the solution, and derive a crisp equivalent form of the possibilistic portfolio under constraints of VaR and risk-free investment. This model is an expansion of the fuzzy possibilistic mean-variance model by Zhang (2007). Finally, an empirical study is carried out using the data concerning some stocks of various industries listed at the Shanghai Stock Exchange. A conclusion is reached that the investors are able to choose a portfolio more suitable to them under the VaR constraint. © 2012 Elsevier B.V.}}, 
pages = {12--17}, 
number = {1}, 
volume = {31}
}
@article{10.2298/csis121024017r, 
year = {2014}, 
title = {{The mean-Value at Risk static portfolio optimization using genetic algorithm}}, 
author = {Ranković, Vladimir and Drenovak, Mikica and Stojanović, Boban and Kalinić, Zoran and Arsovski, Zora}, 
journal = {Computer Science and Information Systems}, 
issn = {18200214}, 
doi = {10.2298/csis121024017r}, 
abstract = {{In this paper we solve the problem of static portfolio allocation based on historical Value at Risk (VaR) by using genetic algorithm (GA). VaR is a predominantly used measure of risk of extreme quantiles in modern finance. For estimation of historical static portfolio VaR, calculation of time series of portfolio returns is required. To avoid daily recalculations of proportion of capital invested in portfolio assets, we introduce a novel set of weight parameters based on proportion of shares. Optimal portfolio allocation in the VaR context is computationally very complex since VaR is not a coherent risk metric while number of local optima increases exponentially with the number of securities. We presented two different single-objective and a multiobjective technique for generating mean-VaR efficient frontiers. Results document good risk/reward characteristics of solution portfolios while there is a trade-off between the ability to control diversity of solutions and computation time.}}, 
pages = {89--109}, 
number = {1}, 
volume = {11}
}
@article{10.1109/compsac48688.2020.00-75, 
year = {2020}, 
title = {{Portfolio Optimization Using a Novel Data-Driven EWMA Covariance Model with Big Data}}, 
author = {Zhu, Zimo and Thavaneswaran, A. and Paseka, Alexander and Frank, Julieta and Thulasiram, Ruppa K.}, 
journal = {2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
issn = {NA}, 
doi = {10.1109/compsac48688.2020.00-75}, 
abstract = {{Recently there has been a growing interest in using machine learning methods with empirical variance covariance matrix of returns to study Markovitz portfolio optimization. The statistical technique of graphical LASSO (GL) for stock selection in the portfolio assumes that the asset returns are normally distributed, independent random variables with constant variance. In this paper sign correlations and the autocorrelations of the absolute values of the returns are used to show that the returns are non-normal with time-varying volatility. We use the recently proposed data-driven exponentially weighted moving average (DDEWMA) volatility model to estimate the covariance matrix of asset returns in Markowitz portfolio optimization. Empirical results with big data (consists of 444 stocks for a period of 7 years downloaded from Yahoo Finance) show that the proposed DDEWMA variance covariance matrix model outperforms (larger Sharpe ratio) the model with empirical variance covariance matrix. © 2020 IEEE.}}, 
pages = {1308--1313}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2015.03.035, 
year = {2015}, 
title = {{Intraday Value-at-Risk: An asymmetric autoregressive conditional duration approach}}, 
author = {Liu, Shouwei and Tse, Yiu-Kuen}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2015.03.035}, 
abstract = {{We propose to compute the Intraday Value-at-Risk (IVaR) for stocks using real-time transaction data. Tick-by-tick data filtered by price duration are modeled using a two-state asymmetric autoregressive conditional duration (AACD) model, and the IVaR is calculated using Monte Carlo simulation based on the estimated AACD model. Backtesting results for the New York Stock Exchange (NYSE) show that the IVaR calculated using the AACD method outperforms those using the Dionne et al. (2009) and Giot (2005) methods. © 2015 Elsevier B.V.}}, 
pages = {437--446}, 
number = {2}, 
volume = {189}
}
@article{10.1002/for.1116, 
year = {2009}, 
title = {{Risk factor beta conditional value-at-risk}}, 
author = {Semenov, Andrei}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1116}, 
abstract = {{We propose a new approach to the estimation of the portfolio Value-at-Risk. Based on the assumption that the same macroeconomic factors affect returns of all assets in a portfolio, this methodology allows the generation of thesequence of hypothetical future equilibrium portfolio returns given the historicalvalues of the underlying macroeconomic factors and the asset betas withrespect to these factors. Value-at-Risk is then found as an appropriate percentileof the corresponding hypothetical distribution of the portfolio profi ts and losses. The backtesting results for the six Fama-French benchmark portfolios and the S\&P500 index show that this approach yields reasonably accurate estimates of the portfolio Value-at-Risk. © 2008 John Wiley \& Sons, Ltd.}}, 
pages = {549--558}, 
number = {6}, 
volume = {28}
}
@article{10.22495/rcgv6i3c1art6, 
year = {2016}, 
title = {{Concentration risk: Setting credit limits in loan portfolios, case of Morocco}}, 
author = {Mehdi, Bazzi and Hassan, Chhaiba and Hasna, Chamlal}, 
journal = {Risk Governance and Control: Financial Markets \& Institutions}, 
issn = {2077429X}, 
doi = {10.22495/rcgv6i3c1art6}, 
abstract = {{The latest biggest financial crisis reveals different weakness points over the global financial system. The concentration risk is one of many different risks that figured out by the regulators after the 2008 financial crisis. To deal with such a risk the regulators set up a dispositive of measures to control it. Therefore, we suggest in this paper a version of a mathematical model that optimize the allocation of capitals for a credit portfolio of a bank with taking into consideration the Moroccan regulatory environment. © 2016, Virtus Interpress. All rights reserved.}}, 
pages = {48--56}, 
number = {3Continued1SpecialIssue}, 
volume = {6}
}
@article{10.3390/risks7040112, 
year = {2019}, 
title = {{Market risk analysis of energy in Vietnam}}, 
author = {Tran, Ngoc Phu and Nguyen, Thang Cong and Vo, Duc Hong and McAleer, Michael}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks7040112}, 
abstract = {{The purpose of this paper is to evaluate and estimate market risk for the ten major industries in Vietnam. The focus of the empirical analysis is on the energy sector, which has been designated as one of the four key industries, together with services, food, and telecommunications, targeted for economic development by the Vietnam Government through to 2020. The oil and gas industry is a separate energy-related major industry, and it is evaluated separately from energy. The data set is from 2009 to 2017, which is decomposed into two distinct sub-periods after the Global Financial Crisis (GFC), namely the immediate post-GFC (2009–2011) period and the normal (2012–2017) period, in order to identify the behavior of market risk for Vietnam’s major industries. For the stock market in Vietnam, the website used in this paper provided complete and detailed data for each stock, as classified by industry. Two widely used approaches to measure and analyze risk are used in the empirical analysis, namely Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR). The empirical findings indicate that Energy and Pharmaceuticals are the least risky industries, whereas oil and gas and securities have the greatest risk. In general, there is strong empirical evidence that the four key industries display relatively low risk. For public policy, the Vietnam Government’s proactive emphasis on the targeted industries, including energy, to achieve sustainable economic growth and national economic development, seems to be working effectively. This paper presents striking empirical evidence that Vietnam’s industries have substantially improved their economic performance over the full sample, moving from relatively higher levels of market risk in the immediate post-GFC period to a lower risk environment in a normal period several years after the end of the calamitous GFC. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {112}, 
number = {4}, 
volume = {7}
}
@article{10.1287/mnsc.1090.1090, 
year = {2009}, 
title = {{Conditional monte carlo estimation of quantile sensitivities}}, 
author = {Fu, Michael C and Hong, L Jeff and Hu, Jian-Qiang}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1090.1090}, 
abstract = {{Estimating quantile sensitivities is important in many optimization applications, from hedging in financial engineering to service-level constraints in inventory control to more general chance constraints in stochastic programming. Recently, Hong (Hong, L. J. 2009. Estimating quantile sensitivities. Oper. Res. 57 118-130) derived a batched infinitesimal perturbation analysis estimator for quantile sensitivities, and Liu and Hong (Liu, G., L. J. Hong. 2009. Kernel estimation of quantile sensitivities. Naval Res. Logist. 56 511-525) derived a kernel estimator. Both of these estimators are consistent with convergence rates bounded by n-1/3 and n -2/5, respectively. In this paper, we use conditional Monte Carlo to derive a consistent quantile sensitivity estimator that improves upon these convergence rates and requires no batching or binning. We illustrate the new estimator using a simple but realistic portfolio credit risk example, for which the previous work is inapplicable. ©2009 INFORMS.}}, 
pages = {2019--2027}, 
number = {12}, 
volume = {55}
}
@article{10.1016/j.jbankfin.2013.04.015, 
year = {2013}, 
title = {{Market capitalization and Value-at-Risk}}, 
author = {Dias, Alexandra}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.04.015}, 
abstract = {{The potential of economic variables for financial risk measurement is an open field for research. This article studies the role of market capitalization in the estimation of Value-at-Risk (VaR). We test the performance of different VaR methodologies for portfolios with different market capitalization. We perform the analysis considering separately financial crisis periods and non-crisis periods. We find that VaR methods perform differently for portfolios with different market capitalization. For portfolios with stocks of different sizes we obtain better VaR estimates when taking market capitalization into account. We also find that it is important to consider crisis and non-crisis periods separately when estimating VaR across different sizes. This study provides evidence that market fundamentals are relevant for risk measurement. © 2013 Elsevier B.V.}}, 
pages = {5248--5260}, 
number = {12}, 
volume = {37}
}
@article{10.1016/s0378-4266(00)00124-2, 
year = {2001}, 
title = {{Parameterizing credit risk models with rating data}}, 
author = {Carey, Mark and Hrycay, Mark}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(00)00124-2}, 
abstract = {{Estimates of average default probabilities for borrowers assigned to each of a financial institution's internal credit risk rating grades are crucial inputs to portfolio credit risk models. Such models are increasingly used in setting financial institution capital structure, in internal control and compensation systems, in asset-backed security design, and are being considered for use in setting regulatory capital requirements for banks. This paper empirically examines properties of the major methods currently used to estimate average default probabilities by grade. Evidence of potential problems of bias, instability, and gaming is presented. With care, and perhaps judicious application of multiple methods, satisfactory estimates may be possible. In passing, evidence is presented about other properties of internal and rating-agency ratings. © 2001 Elsevier Science B.V.}}, 
pages = {197--270}, 
number = {1}, 
volume = {25}
}
@article{10.7500/aeps20151130012, 
year = {2016}, 
title = {{Load restoration optimization during the last stage of network reconfiguration considering load fuzzy uncertainty}}, 
author = {}, 
issn = {10001026}, 
doi = {10.7500/aeps20151130012}, 
abstract = {{Owing to the various uncertain factors during power system restoration, there are always deviations between the actual recovered load amounts and the predicted values. In order to overcome the shortcomings of existing deterministic load restoration model, trapezoidal fuzzy parameters are used to express the injected recovered load amounts. And the deterministic safety constraints are substituted by the chance constraints based on fuzzy parameters. By taking into account the influences of load restoration benefit and overload risk on load optimization, a load restoration optimization model for the last stage of network reconfiguration is proposed. To calculate the load weights, the fuzzy entropy is introduced to quantify the load uncertainty, and the load importance and load uncertainty are used as load evaluation indices. Finally, clear equivalent forms are used to change the uncertain load restoration model into a deterministic 0-1 programming problem which can be solved by the mixed integer programming method. Case studies show that the proposed model is able to balance the restoration benefit and the overload risk, and the decisions obtained are better adapted to load fuzzy uncertainty. © 2016 Automation of Electric Power Systems Press.}}, 
number = {20}, 
volume = {40}
}
@article{10.1088/1367-2630/12/7/075030, 
year = {2010}, 
title = {{Recurrence interval analysis of high-frequency financial returns and its application to risk estimation}}, 
author = {Ren, Fei and Zhou, Wei-Xing}, 
journal = {New Journal of Physics}, 
issn = {13672630}, 
doi = {10.1088/1367-2630/12/7/075030}, 
eprint = {0909.0123}, 
abstract = {{We investigate the probability distributions of the recurrence intervals x between consecutive 1-min returns above a positive threshold q \&gt; 0 or below a negative threshold q \&lt; 0 of two indices and 20 individual stocks in China's stock market. The distributions of recurrence intervals for positive and negative thresholds are symmetric, and display power-law tails tested by three goodness-of-fit measures, including the Kolmogorov-Smirnov (KS) statistic, the weighted KS statistic and the Cramér-von Mises criterion. Both longterm and shot-term memory effects are observed in the recurrence intervals for positive and negative thresholds q. We further apply the recurrence interval analysis to the risk estimation for the Chinese stock markets based on the probability Wq(Δt, t), value-at-risk (VaR) analysis and VaR analysis conditioned on preceding recurrence intervals. © IOP Publishing Ltd and Deutsche Physikalische Gesellschaft.}}, 
pages = {075030}, 
number = {NA}, 
volume = {12}
}
@article{10.1016/j.jbankfin.2015.01.012, 
year = {2015}, 
title = {{Forecasting portfolio-Value-at-Risk with nonparametric lower tail dependence estimates}}, 
author = {Siburg, Karl Friedrich and Stoimenov, Pavel and Weiß, Gregor N.F.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2015.01.012}, 
abstract = {{We propose to forecast the Value-at-Risk of bivariate portfolios using copulas which are calibrated on the basis of nonparametric sample estimates of the coefficient of lower tail dependence. We compare our proposed method to a conventional copula-GARCH model where the parameter of a Clayton copula is estimated via Canonical Maximum-Likelihood. The superiority of our proposed model is exemplified by analyzing a data sample of nine different bivariate and one nine-dimensional financial portfolio. A comparison of the out-of-sample forecasting accuracy of both models confirms that our model yields economically significantly better Value-at-Risk forecasts than the competing parametric calibration strategy. © 2015 Elsevier B.V.}}, 
pages = {129--140}, 
number = {NA}, 
volume = {54}
}
@article{10.1016/j.enpol.2010.03.006, 
year = {2010}, 
title = {{Promoting energy efficiency investments with risk management decision tools}}, 
author = {Jackson, Jerry}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2010.03.006}, 
abstract = {{This paper reviews current capital budgeting practices and their impact on energy efficiency investments. The prevalent use of short payback ′rule-of-thumb′ requirements to screen efficiency projects for risk is shown to bias investment choices towards ′sure bet′ investments bypassing many profitable efficiency investment options. A risk management investment strategy is presented as an alternative to risk avoidance practices applied with payback thresholds. The financial industry risk management tool Value-at-Risk is described and extended to provide an Energy-Budgets-at-Risk or EBaR risk management analysis to convey more accurate energy efficiency investment risk information. The paper concludes with recommendations to expand the use of Value-at-Risk-type energy efficiency analysis. © 2010 Elsevier Ltd.}}, 
pages = {3865--3873}, 
number = {8}, 
volume = {38}
}
@article{10.1109/ths.2009.5168077, 
year = {2009}, 
title = {{Analysis of detection systems for outdoor chemical or biological attacks}}, 
author = {Barter, Garrett E. and Purvis, Liston K. and Teclemariam, Nerayo P. and West, Todd H.}, 
journal = {2009 IEEE Conference on Technologies for Homeland Security}, 
issn = {NA}, 
doi = {10.1109/ths.2009.5168077}, 
abstract = {{This paper presents Sandia National Laboratories' Out-door Weapons of Mass Destruction Decision Analysis Center (Out-DAC) and, through an example case study, derives lessons for its use. This tool, related to similar capabilities at Sandia, can be used to determine functional requirements for a detection system of aerosol-released threats outdoors. Essential components of OutDAC are a population database, a meteorological dataset, an atmospheric transport and dispersion model and an optimization toolkit. Detector placement is done through optimization against a library of hypothe-sized attack scenarios by minimizing either the mean or value-at-risk of undetected infections. These scenarios are the product of a Monte Carlo simulation intended to characterize the uncertainty associated with the threat. An example case study illustrates that Monte Carlo convergence is dependent on the statistic of interest. Furthermore, the quality of the detector placement optimization may be tied to the convergence level of the Monte Carlo simulation. © 2009 IEEE.}}, 
pages = {485--492}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/etep.2641, 
year = {2018}, 
title = {{Optimal energy management of community microgrids: A risk-based multi-criteria approach}}, 
author = {Abniki, Hassan and Taghvaei, Seyed Masoud and Mohammadi‐Hosseininejad, Seyed Mohsen}, 
journal = {International Transactions on Electrical Energy Systems}, 
issn = {20507038}, 
doi = {10.1002/etep.2641}, 
abstract = {{Community microgrids (CMGs) have been developing nowadays as an initiative to operate modern electric distribution systems in a more economical, reliable, and environmentally friendly manner than the existing centralized electricity grid which benefited both distribution system operator and consumers. In this paper, the optimal energy management of CMGs is formulated considering distributed energy resources and thermal and electrical demands in CMGs. The objective function of the proposed methodology consists of the total cost of CMGs operation, the total cost of energy not supplied in CMGs, and total cost of emission produced in CMGs in order to comprehensively manage the CMG operation. Moreover, the uncertainty of renewable energy resources, electricity price, and demanded power of CMGs are considered in the proposed methodology. In addition, two risk evaluation measures are employed to cope with existing risks in the CMG operation. Furthermore, a number of sensitivity studies are accomplished to investigate the effects of important parameters on the performance of the proposed approach. The effectiveness of the proposed methodology is examined on a test system. © 2018 John Wiley \& Sons, Ltd.}}, 
pages = {e2641}, 
number = {12}, 
volume = {28}
}
@article{10.1016/j.jempfin.2017.03.004, 
year = {2017}, 
title = {{Displaced relative changes in historical simulation: Application to risk measures of interest rates with phases of negative rates}}, 
author = {Fries, Christian P. and Nigbur, Tobias and Seeger, Norman}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2017.03.004}, 
abstract = {{In this paper we introduce the displaced historical simulation model which is designed to handle negative and close-to-zero risk factors. This is an issue of recent and major interest to the financial sector, both from a regulatory and financial institutions perspective, especially in light of observed negative values for major bond yield and interest rate spread time series. In historical simulation a common approach is to consider log returns (that is, relative changes), given that the risk factors remain positive. If a risk factor allows for negative values, log returns cannot be applied and one either ignores such scenarios or switches to considering absolute changes. The latter approach implies an abrupt model change. Our displaced historical simulation model strongly improves the historical simulation by “displacing” the shifts such that negative values can be handled, smoothly moving to the limit case of using absolute shifts instead of relative shifts of the original data. Our empirical results show that compared to other models presented in the literature, models equipped with our proposed displacement feature handle situations of close-to-zero or negative risk variables particularly well. © 2017 Elsevier Ltd}}, 
pages = {175--198}, 
number = {NA}, 
volume = {42}
}
@article{10.3390/econometrics4010003, 
year = {2016}, 
title = {{Forecasting value-at-risk under different distributional assumptions}}, 
author = {Braione, Manuela and Scholtes, Nicolas K.}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics4010003}, 
abstract = {{Financial asset returns are known to be conditionally heteroskedastic and generally non-normally distributed, fat-tailed and often skewed. These features must be taken into account to produce accurate forecasts of Value-at-Risk (VaR). We provide a comprehensive look at the problem by considering the impact that different distributional assumptions have on the accuracy of both univariate and multivariate GARCH models in out-of-sample VaR prediction. The set of analyzed distributions comprises the normal, Student, Multivariate Exponential Power and their corresponding skewed counterparts. The accuracy of the VaR forecasts is assessed by implementing standard statistical backtesting procedures used to rank the different specifications. The results show the importance of allowing for heavy-tails and skewness in the distributional assumption with the skew-Student outperforming the others across all tests and confidence levels. © 2016 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {3}, 
number = {1}, 
volume = {4}
}
@article{10.1007/s11579-020-00280-z, 
year = {2021}, 
title = {{Preferences over rich sets of random variables: on the incompatibility of convexity and semicontinuity in measure}}, 
author = {Zimper, Alexander and Assa, Hirbod}, 
journal = {Mathematics and Financial Economics}, 
issn = {18629679}, 
doi = {10.1007/s11579-020-00280-z}, 
abstract = {{This paper considers a decision maker whose preferences are locally upper- or/and lower-semicontinuous in measure. We introduce the notion of a rich set which encompasses any standard vector space of random variables but also much smaller sets containing only random variables with at most two different outcomes in their support. Whenever preferences are complete on a rich set of random variables, lower- (resp. upper-) semicontinuity in measure becomes incompatible with convexity of strictly better (resp. worse) sets. We discuss implications for utility representations and risk-measures. In particular, we show that the value-at-risk criterion violates convexity exactly because it is lower-semicontinuous in measure. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {353--380}, 
number = {2}, 
volume = {15}
}
@article{10.1016/s1049-0078(02)00188-4, 
year = {2003}, 
title = {{Open economy and financial burden of corruption: Theory and application to Asia}}, 
author = {Vinod, Hrishikesh D}, 
journal = {Journal of Asian Economics}, 
issn = {10490078}, 
doi = {10.1016/s1049-0078(02)00188-4}, 
abstract = {{We discuss why corruption remains high and show that corruption contributes to the Banking distress and to the rapid transmission across international stock and currency markets. Undeveloped 'derivative securities' markets make the risk from stress-induced volatility difficult to manage. Vinod's (1999) closed economy model is extended to indicate the asymmetry of 'home bias' and the effect of corruption on the value at risk (VaR). Our theory predicts that capital flight controls will be many, foreign direct investment (FDI) will be low and cost of capital will be high in corrupt developing countries, which is supported by Asian data. We include some policy recommendations regarding financial institutions and markets. © 2002 Elsevier Science Inc. All rights reserved.}}, 
pages = {873--890}, 
number = {6}, 
volume = {13}
}
@article{10.1111/mafi.12160, 
year = {2018}, 
title = {{Risk management with weighted VaR}}, 
author = {Wei, Pengyu}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12160}, 
abstract = {{This article studies the optimal portfolio selection of expected utility-maximizing investors who must also manage their market-risk exposures. The risk is measured by a so-called weighted value-at-risk (WVaR) risk measure, which is a generalization of both value-at-risk (VaR) and expected shortfall (ES). The feasibility, well-posedness, and existence of the optimal solution are examined. We obtain the optimal solution (when it exists) and show how risk measures change asset allocation patterns. In particular, we characterize three classes of risk measures: the first class will lead to models that do not admit an optimal solution, the second class can give rise to endogenous portfolio insurance, and the third class, which includes VaR and ES, two popular regulatory risk measures, will allow economic agents to engage in “regulatory capital arbitrage,” incurring larger losses when losses occur. © 2017 Wiley Periodicals, Inc.}}, 
pages = {1020--1060}, 
number = {4}, 
volume = {28}
}
@article{10.1016/j.jmva.2010.08.004, 
year = {2011}, 
title = {{On directional multiple-output quantile regression}}, 
author = {Paindaveine, Davy and Šiman, Miroslav}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2010.08.004}, 
abstract = {{This paper sheds some new light on projection quantiles. Contrary to the sophisticated set analysis used in Kong and Mizera (2008) [13], we adopt a more parametric approach and study the subgradient conditions associated with these quantiles. In this setup, we introduce Lagrange multipliers which can be interpreted in various interesting ways, in particular in a portfolio optimization context. The corresponding projection quantile regions were already shown to coincide with the halfspace depth ones in Kong and Mizera (2008) [13], but we provide here an alternative proof (completely based on projection quantiles) that has the advantage of leading to an exact computation of halfspace depth regions from projection quantiles. Above all, we systematically consider the regression case, which was barely touched in Kong and Mizera (2008) [13]. We show in particular that the regression quantile regions introduced in Hallin, Paindaveine, and Šiman (2010) [6,7] can also be obtained from projection (regression) quantiles, which may lead to a faster computation of those regions in some particular cases. © 2010 Elsevier Inc.}}, 
pages = {193--212}, 
number = {2}, 
volume = {102}
}
@article{10.1109/tdc-la.2012.6319144, 
year = {2012}, 
title = {{Differences between the operations of the generation power system of Uruguay operated minimizing the expected value vs. minimizing the value at risk of the future operating costs}}, 
author = {Larrosa, D and Coppes, E and Casaravilla, G and Chaer, R}, 
journal = {2012 Sixth IEEE/PES Transmission and Distribution: Latin America Conference and Exposition (T…D-LA)}, 
issn = {NA}, 
doi = {10.1109/tdc-la.2012.6319144}, 
abstract = {{Hydrothermal systems optimal operation includes a step of optimizing the resources that are valued system storable. This optimization is performed traditionally by a stochastic dynamic programming in which the objective function to minimize is the expected value of future cost of operation, also known as Bellman function. While in theory, minimize the expected value of future cost of operation is "objective" in practice there are many reasons why the actual operation includes additional precautions, sometimes actually taken by operators who are the ones which have responsibility for the consequences of the operation or sometimes made based on safety considerations were not introduced in the optimization of the operation. This work shows the implementation on the platform Simulation of Electric Power Systems of stochastic dynamic programming algorithm for specifying the objective function cost reduction future with a certain probability of exceedance. This work was performed as part of the draft platform enhancements SimSEE with funding from the Energy Sector A(II. The paper presents the results of the operation to minimize the expected value of future cost and minimizing the risk value of 5\% of being exceeded. Both operations are compared both costs achieved as in the qualitative aspects. The results allow evaluating the cost of being introduced by risk averse and also identify situations where there are major differences. It also discusses the impact on the marginal cost of system operation with a slogan risk averse. This value is relevant because it is the basis for calculating the Uruguayan market spot price. These scenarios correspond to the operation in 2017 with high penetration of wind energy in the system. © 2012 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10614-006-9068-9, 
year = {2007}, 
title = {{Managing value-at-risk for a bond using bond put options}}, 
author = {Deelstra, Griselda and Ezzine, Ahmed and Heyman, Dries and Vanmaele, Michèle}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-006-9068-9}, 
abstract = {{This paper studies a strategy that minimizes the Value-at-Risk (VaR) of a position in a zero-coupon bond by buying a percentage of a put option, subject to a fixed budget available for hedging. We elaborate a formula for determining the optimal strike price for this put option in case of a Vasicek stochastic interest rate model. We demonstrate the relevance of searching the optimal strike price, since moving away from the optimum implies a loss, either due to an increased VaR or due to an increased hedging expenditure. In this way, we extend the results of [Ahn, Boudoukh, Richardson, and Whitelaw (1999). Journal of Finance, 54, 359-375] who minimize VaR for a position in a share. In addition, we look at the alternative risk measure Tail Value-at-Risk. © Springer Science+Business Media, LLC 2005.}}, 
pages = {139--149}, 
number = {2}, 
volume = {29}
}
@article{10.1016/j.eneco.2012.10.004, 
year = {2013}, 
title = {{Modeling EU allowances and oil market interdependence. Implications for portfolio management}}, 
author = {Reboredo, Juan C.}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2012.10.004}, 
abstract = {{This paper examines the dependence structure between European Union allowances (EUAs) and crude oil markets during the second commitment period of the European Union Emissions Trading Scheme and the implications for portfolio management. Using different copula models, our findings suggest positive average dependence and extreme symmetric independence that is consistent with interdependence and no contagion effects between the EUA and crude oil markets. The implication of this result for EUA-oil portfolios points to the existence of diversification benefits, hedging effectiveness, and value-at-risk reductions. The EUA market is therefore an attractive market for investors in terms of diversifying market risk and reducing downside risk in crude oil markets. © 2012 Elsevier B.V.}}, 
pages = {471--480}, 
number = {NA}, 
volume = {36}
}
@article{10.1080/03461230903424199, 
year = {2011}, 
title = {{Folded and log-folded-t distributions as models for insurance loss data}}, 
author = {Brazauskas, Vytaras and Kleefeld, Andreas}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461230903424199}, 
abstract = {{A rich variety of probability distributions has been proposed in the actuarial literature for fitting of insurance loss data. Examples include: lognormal, log-t, various versions of Pareto, loglogistic, Weibull, gamma and its variants, and generalized beta of the second kind distributions, among others. In this paper, we supplement the literature by adding the log-folded-normal and log-folded-t families. Shapes of the density function and key distributional properties of the 'folded' distributions are presented along with three methods for the estimation of parameters: method of maximum likelihood; method of moments; and method of trimmed moments. Further, large and small-sample properties of these estimators are studied in detail. Finally, we fit the newly proposed distributions to data which represent the total damage done by 827 fires in Norway for the year 1988. The fitted models are then employed in a few quantitative risk management examples, where point and interval estimates for several value-at-risk measures are calculated. © 2011 Taylor \& Francis.}}, 
pages = {59--74}, 
number = {1}, 
volume = {NA}
}
@article{10.1016/j.frl.2018.09.014, 
year = {2019}, 
title = {{Bitcoin returns and risk: A general GARCH and GAS analysis}}, 
author = {Troster, Victor and Tiwari, Aviral Kumar and Shahbaz, Muhammad and Macedo, Demian Nicolás}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2018.09.014}, 
abstract = {{This paper performs a general GARCH and GAS analysis for modelling and forecasting bitcoin returns and risk. Since Bitcoin trading exhibits excess volatility compared with other securities, it is important to model its risk and returns. We consider heavy-tailed GARCH models as well as GAS models based on the score function of the predictive conditional density of the bitcoin returns. We compare out-of-sample 1\%-Value-at-Risk (VaR) forecasts under 45 different specifications using three backtesting procedures. We find that GAS models with heavy-tailed distributions provide the best out-of-sample forecast and goodness-of-fit properties to bitcoin returns and risk modelling. Normally-distributed GARCH models are always outperformed by heavy-tailed GARCH or GAS models. Besides, heavy-tailed GAS models provide the best conditional and unconditional coverage for 1\%-VaR forecasts, illustrating the importance of modelling excess kurtosis for bitcoin returns. Hence, our findings have important implications for risk managers and investors for using bitcoin in optimal hedging or investment strategies. © 2018 Elsevier Inc.}}, 
pages = {187--193}, 
number = {NA}, 
volume = {30}
}
@article{10.1007/s10690-008-9077-x, 
year = {2008}, 
title = {{A method of calculating the downside risk by multivariate nonnormal distributions}}, 
author = {Nagahara, Yuichi}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-008-9077-x}, 
abstract = {{A method of calculating the downside risk by fitting multivariate nonnormal distributions to financial data is proposed. Firstly, maximum likelihood method by using the random numbers of the Pearson distribution system are introduced. The rates of returns of the stock index are fitted to the multivariate nonnormal distributions by this method. Secondly, the cases of calculating the downside risk by the standard deviation, the percentile of historical simulation method and this method, are compared. © 2009 Springer Science+Business Media, LLC.}}, 
pages = {175--184}, 
number = {3-4}, 
volume = {15}
}
@article{10.1504/ijmef.2011.040922, 
year = {2011}, 
title = {{GARCH-class models estimations and value-at-risk analysis for exchange rate}}, 
author = {Mabrouk, Samir and Aloui, Chaker}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2011.040922}, 
abstract = {{In this paper, we focus on three daily exchange rate returns dynamics. Indeed, we have assessed five GARCH-class models under three alternative distributions. Our findings confirm that the skewed Student-t FIAPARCH model performs very well. Then, we have computed short and long Value-at-Risk and Expected Shortfall based on AR (1) - FIAPARCH under normal, Student-t and skewed Student-t distributions. More precisely, we have investigated the estimation performance by computing both In-sample and Out-of-sample VaR for one-day-ahead horizon. Results reveal that VaR and ES estimations based on skewed Student-t FIAPARCH models outperform other models for both long and short trading positions. Copyright © 2011 Inderscience Enterprises Ltd.}}, 
pages = {254}, 
number = {3}, 
volume = {4}
}
@article{10.1007/978-3-540-72584-8_73, 
year = {2007}, 
title = {{Modeling VaR in crude oil market: A multi scale nonlinear ensemble approach incorporating wavelet analysis and ANN}}, 
author = {Lai, Kin Keung and He, Kaijian and Yen, Jerome}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-540-72584-8\_73}, 
abstract = {{Price fluctuations in the crude oil markets worldwide have attracted significant attentions from both, industries and academics, due to their profound impact on businesses and governments. Proper measurement and management of risks due to unexpected price movements in the markets has been crucial from both, operational and strategic perspectives. However, risk measurements from current approaches offer insufficient explanatory power and performance due to the complicated non-linear nature of risk evolutions. This paper adopts a VaR approach to measure risks and proposes multi-scale non-linear ensemble approaches to model the risk evolutions in WTI crude oil market. The proposed WDNEVaR follows a semi-parametric paradigm, incorporating both, wavelet analysis and artificial neural network techniques. Experiment results from empirical studies suggest that the proposed WDNEVaR is superior to traditional approaches. It provides VaR estimates of higher reliability and accuracy. It also brings significantly more flexibility during the modeling attempts. © Springer-Verlag Berlin Heidelberg 2007.}}, 
pages = {554--561}, 
number = {NA}, 
volume = {4487 LNCS}
}
@article{10.1016/j.procs.2015.07.115, 
year = {2015}, 
title = {{EMD copula based value at risk estimates for electricity markets}}, 
author = {Wang, Xuan and Cai, Junling and He, Kaijian}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2015.07.115}, 
abstract = {{In this paper we propose an Empirical Mode Decomposition Copula based approach for analyzing the portfolio risk and estimating VaR at Risk. The copula theory is introduced to analyze the time-varying microscopic dependence structure between New South Wales (NSW) and Queensland (QLD) in Australlian electricity market, and test the existence of the symmetrical dependence structure between them. The EMD algorithm is combined with copula theory to construct a new based EMDCopula model for estimating Value-at-Risk (VaR) of electricity market. Results from the empirical analysis show that compared with the benchmark DCC-GARCH model, the proposed model outperforms the DCC-GARCH model, in terms of estimation reliability. ©2015 Published by Elsevier B.V.}}, 
pages = {1318--1324}, 
number = {NA}, 
volume = {55}
}
@article{10.1016/j.ejor.2011.09.012, 
year = {2012}, 
title = {{Minimizing loss probability bounds for portfolio selection}}, 
author = {Gotoh, Jun-ya and Takeda, Akiko}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2011.09.012}, 
abstract = {{In this paper, we derive a portfolio optimization model by minimizing upper and lower bounds of loss probability. These bounds are obtained under a nonparametric assumption of underlying return distribution by modifying the so-called generalization error bounds for the support vector machine, which has been developed in the field of statistical learning. Based on the bounds, two fractional programs are derived for constructing portfolios, where the numerator of the ratio in the objective includes the value-at-risk (VaR) or conditional value-at-risk (CVaR) while the denominator is any norm of portfolio vector. Depending on the parameter values in the model, the derived formulations can result in a nonconvex constrained optimization, and an algorithm for dealing with such a case is proposed. Some computational experiments are conducted on real stock market data, demonstrating that the CVaR-based fractional programming model outperforms the empirical probability minimization. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {371--380}, 
number = {2}, 
volume = {217}
}
@article{10.1504/ijsom.2015.069376, 
year = {2015}, 
title = {{Using VAR for strategic capacity allocation: An airline perspective}}, 
author = {Leon, Steven and Szmerekovsky, Joseph and Tolliver, Denver}, 
journal = {International Journal of Services and Operations Management}, 
issn = {17442370}, 
doi = {10.1504/ijsom.2015.069376}, 
abstract = {{We consider a value-at-risk (VAR) approach to allocating seat miles for airlines. The US global airline industry is used to demonstrate this approach. Using OLS regression, we estimate the expected profit and the variance of profit based on the seat miles allocation. A non-linear optimisation model is then used to devise a portfolio of available seat miles distributed to global regions using the mean-value-at-risk technique. A comparison between the results and actual airline operating profits is conducted. Given the substantial operating profit improvements observed, there is promise in pursing this method for strategic airline seat allocation. Copyright © 2015 Inderscience Enterprises Ltd.}}, 
pages = {127}, 
number = {2}, 
volume = {21}
}
@article{10.1080/1540496x.2019.1609442, 
year = {2020}, 
title = {{Regime-Switching Processes and Mean-Reverting Volatility Models in Value-at-Risk Estimation: Evidence from the Taiwan Stock Index}}, 
author = {Chen, Yi-Wen and Lin, Chu-Bin and Tu, Anthony H.}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2019.1609442}, 
abstract = {{This article develops a model that can accurately forecast the volatility of Taiwan stock returns and efficiently estimate value-at-risk (VaR). Because the volatility in the Taiwan stock market has been shown to die down and shift quickly, we find that the model able to outperform others is one that allows the parameters of the volatility models to switch between regimes and conditional volatility to revert quickly to near-normal levels following extremely volatile periods. Compared with nested models, this model has the best performance in terms of the statistical fit of in-sample data and out-of-sample volatility forecasts and VaR estimates. © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--18}, 
number = {12}, 
volume = {56}
}
@article{10.1016/j.insmatheco.2016.06.018, 
year = {2016}, 
title = {{Credible risk measures with applications in actuarial sciences and finance}}, 
author = {Pitselis, Georgios}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2016.06.018}, 
abstract = {{In this paper, we introduce a general framework for obtaining a new type of risk measures, the so called credible risk measures, as a result of incorporating credibility methodology with some well known risk measures, such as the value at risk (VaR) and the conditional tail expectation (CTE). The resulting credible risk measures are more informative than the usual risk measures (i.e. VaR, CTE) in capturing the risk of individual insurer's contract (or returns of an individual asset) as well as the portfolio risk consisting of several similar but not identical contracts (or returns of a portfolio of similar assets), which are grouped together to share the risk. These credible risk measures are: the credible value at risk, the credible conditional tail expectation, the credible tail conditional median and the credible quantile tail expectation. Two examples of credible risks measures are presented, one with insurance loss data and the other with industry financial data. The advantages and disadvantages of these new credible measures are also discussed. © 2016 Elsevier B.V.}}, 
pages = {373--386}, 
number = {NA}, 
volume = {70}
}
@article{10.1111/1468-036x.00090, 
year = {1999}, 
title = {{A multi-factor model for the risk management of portfolios}}, 
author = {Peterson, Sandra and Stapleton, Richard C.}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/1468-036x.00090}, 
abstract = {{We propose a methodology for modelling the value at risk of a complex portfolio, based on an extension of the Ho, Stapleton and Subrahmanyam technique. We model the variance-covariance structure of up to seven variables. These could represent four country indices and three exchange rates, for example. In addition, the effect of an arbitrary number of orthogonal factors can be analysed. The system is illustrated by estimating the value at risk for a portfolio of international stocks where the factors are stock market indices and exchange rates, a portfolio of international bonds where the factors are interest rates as well as exchange rates, and a portfolio of interest rate derivatives in different currencies. In this last case, we model a two-factor term structure of interest rates in each of the currencies, valuing the derivatives at a future date using these term structures and the Black model. The model is applied for different fineness of the binomial density and computational accuracy and efficiency are estimated. G13, G15, G21. © 1999 Blackwell Publishers Ltd.}}, 
pages = {223--239}, 
number = {2}, 
volume = {5}
}
@article{10.1093/jjfinec/nbs001, 
year = {2012}, 
title = {{Converting Tail-VaR to VaR: An econometric study}}, 
author = {Gourieroux, C and Liu, W}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbs001}, 
abstract = {{This paper studies the link between two popular measures of risk, that are the Value-at-Risk (VaR) and the Tail-VaR (TVaR). We study how the TVaR and VaR are related through their risk levels and characterize the underlying distributions under which this relationship is linear. A large portion of this paper is devoted to the related econometric analysis, such as the estimation and test of this relationship. We apply the results to currency portfolios and observe that this linearity relationship between the TVaR and VaR is a surprisingly common phenomenon for the portfolios considered for both historical and conditional risk measures. © The Author 2012. Published by Oxford University Press. All rights reserved.}}, 
pages = {233--264}, 
number = {2}, 
volume = {10}
}
@article{10.1109/ihmsc.2017.50, 
year = {2017}, 
title = {{Short-Term Power Load Forecasting with Deep Belief Network and Copula Models}}, 
author = {He, Yusen and Deng, Jiahao and Li, Huajin}, 
journal = {2017 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
issn = {NA}, 
doi = {10.1109/ihmsc.2017.50}, 
abstract = {{The complexity and uncertainty in scheduling and operation of the power system are prominently increasing with the penetration of smart grid. An essential task for the effective operation of power systems is the power load forecasting. In this paper, a tandem data-driven method is studied in this research based on deep learning. A deep belief network (DBN) embedded with parametric Copula models is proposed to forecast the hourly load of a power grid. Data collected over a whole year from an urbanized area in Texas, United States is utilized. Forecasting hourly power load in four different seasons in a selected year is examined. Two forecasting scenarios, day-Ahead and week-Ahead forecasting are conducted using the proposed methods and compared with classical neural networks (NN), support vector regression machine (SVR), extreme learning machine (ELM), and classical deep belief networks (DBN). The accuracy of the forecasted power load is assessed by mean absolute percentage error (MAPE) and root mean square error (RMSE). Computational results confirm the effectiveness of the proposed semi-parametric data-driven method. © 2017 IEEE.}}, 
pages = {191--194}, 
number = {NA}, 
volume = {1}
}
@article{10.3390/risks6020061, 
year = {2018}, 
title = {{On exactitude in financial regulation: Value-at-risk, expected shortfall, and expectiles}}, 
author = {Chen, James Ming}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks6020061}, 
abstract = {{This article reviews two leading measures of financial risk and an emerging alternative. Embraced by the Basel accords, value-at-risk and expected shortfall are the leading measures of financial risk. Expectiles offset the weaknesses of value-at-risk (VaR) and expected shortfall. Indeed, expectiles are the only elicitable law-invariant coherent risk measures. After reviewing practical concerns involving backtesting and robustness, this article more closely examines regulatory applications of expectiles. Expectiles are most readily evaluated as a special class of quantiles. For ease of regulatory implementation, expectiles can be defined exclusively in terms of VaR, expected shortfall, and the thresholds at which those competing risk measures are enforced. Moreover, expectiles are in harmony with gain/loss ratios in financial risk management. Expectiles may address some of the flaws in VaR and expected shortfall—subject to the reservation that no risk measure can achieve exactitude in regulation. © 2018 by the author. Licensee MDPI, Basel, Switzerland.}}, 
pages = {61}, 
number = {2}, 
volume = {6}
}
@article{10.31269/vol8iss2pp237-250, 
year = {2010}, 
title = {{Calculating the unknown. Rationalities of operational risk in financial institutions}}, 
author = {Greif, Hajo and Werner, Matthias}, 
journal = {tripleC: Communication, Capitalism \& Critique. Open Access Journal for a Global Sustainable Information Society}, 
issn = {1726670X}, 
doi = {10.31269/vol8iss2pp237-250}, 
abstract = {{In this paper, findings of a study on the perception and policing of information-technology (ICT) related operational risks in banking are presented, with a view on identifying some part of the role that these technologies, and the specific organisational settings in which they are embedded, may have played in the making of the 2007+ financial crisis. The study's findings concern, firstly, biases in risk perception that turn a blind eye towards certain operational risks; secondly, competing, qualitative vs. quantitative norms and methods of risk analysis and management and their significance for the governance of financial institutions; and thirdly, the role of ICTs as organisational technologies that work both as sources and as remedies of operational risks. The use of ICTs in financial institutions, it is concluded, while not being fully acknowledged in its organisational role, caters to the calculative rationality to which the analysis, management and governance of operational and other risks are increasingly subjected. Presuming that all kinds of risk can be made calculable and computable, this calculative rationality either misses out or obscures one important risk category: low frequency/ high magnitude risks, which tend to cross the boundary between calculable risk and genuine uncertainty of knowledge.}}, 
pages = {237--250}, 
number = {2}, 
volume = {8}
}
@article{10.1111/j.1467-629x.2011.00422.x, 
year = {2012}, 
title = {{It pays to violate: How effective are the Basel accord penalties in encouraging risk management?}}, 
author = {Veiga, Bernardo da and Chan, Felix and McAleer, Michael}, 
journal = {Accounting \& Finance}, 
issn = {08105391}, 
doi = {10.1111/j.1467-629x.2011.00422.x}, 
abstract = {{The internal models amendment to the Basel Accord allows banks to use internal models to forecast Value-at-risk (VaR) thresholds, which are used to calculate the required capital that banks must hold in reserve as a protection against negative changes in the value of their trading portfolios. As capital reserves lead to an opportunity cost to banks, it is likely that banks could be tempted to use models that underpredict risk and hence lead to low capital charges. To avoid this problem the Basel Accord introduced a backtesting procedure, whereby banks using models that led to excessive violations are penalised through higher capital charges. This paper investigates the performance of five popular volatility models that can be used to forecast VaR thresholds under a variety of distributional assumptions. The results suggest that, within the current constraints and the penalty structure of the Basel Accord, the lowest capital charges arise when using models that lead to excessive violations, thereby suggesting the current penalty structure is not severe enough to encourage adequate risk management. In addition, this paper suggests an alternative penalty structure that is more effective at aligning the interests of banks and regulators. © 2011 The Authors. Accounting and Finance © 2011 AFAANZ.}}, 
pages = {95--116}, 
number = {1}, 
volume = {52}
}
@article{10.1007/978-3-030-77445-5_61, 
year = {2021}, 
title = {{Portfolio Optimization Model for Asset Allocation Problem Based on Alternative Risk Measures}}, 
author = {Malakhova, Anna Andreevna and Sochneva, Elena Nikolaevna and Yarkova, Svetlana Anatolyevna and Yarkova, Anastasiya Vladimirovna and Starova, Olga Valeryevna and Kravtsov, Dmitry Ivanovitch and Zyablikov, Dmitry Valeryevitch}, 
journal = {Lecture Notes in Networks and Systems}, 
issn = {23673370}, 
doi = {10.1007/978-3-030-77445-5\_61}, 
abstract = {{In this paper new approaches to synthesize investment portfolios are proposed based on the introduced alternative risk measures for portfolio. It is shown that the difference between the portfolio mean return and ‘low-mean’ return, the difference between the portfolio mean return and ‘value-at-risk’ return may be considered as alternative risk-measures. The algorithm for portfolio ‘low-mean’ return and ‘value-at-risk’ return evaluation is formulated. The experimental study of the traditional models and of the alternative risk measures has been performed on the basis of common stocks traded via Moscow Exchange. The feasible sets for the proposed models have been constructed. It has been shown that the feasible sets for the developed investment portfolio models are conceptually similar to the ‘mean – variance’ set of the Markowitz model. The efficient frontiers for the Markowitz model, for the Sharpe index model and for the proposed alternative risk measures have been constructed; the optimal weight distributions have been obtained. The efficiency of the portfolios based on traditional risk measures and of the portfolios with alternative risk measures have been evaluated and compared. It has been shown that models based on the ‘left-side’ risk measures have produced better results for the market under consideration. These findings confirm the validity of application of alternative measures for risk assessment under financial market conditions. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.}}, 
pages = {673--693}, 
number = {NA}, 
volume = {229}
}
@article{10.1016/j.ijforecast.2010.09.007, 
year = {2011}, 
title = {{Decay factor optimisation in time weighted simulation - evaluating var performance}}, 
author = {Žiković, Saša and Aktan, Bora}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2010.09.007}, 
abstract = {{We propose an optimisation approach for determining the optimal decay factor in time weighted (BRW) simulation. The backtesting of the BRW simulation, which involves different decay factors, together with a broad range of competing VaR models, has been performed on a sample of seven stock indexes and two commodities: gold and WTI oil. The results obtained show that the BRW simulation with an optimised decay factor relative to the Lopez (1998) size-adjusted function is among the best performing VaR models, second only to the conditional extreme value approach (McNeil \& Frey, 2000). The optimised decay factors are sufficiently stable over time, giving economic justification to the optimisation because they do not change over longer time periods. Unlike most of the VaR models tested, in the large majority of cases, the optimised BRW model passes the Basel II criteria but yields significantly lower VaR forecasts than the extreme value approaches, thus resulting in a lower idle capital, i.e. lower costs. © 2011 International Institute of Forecasters.}}, 
pages = {1147--1159}, 
number = {4}, 
volume = {27}
}
@article{10.1177/21582440211005758, 
year = {2021}, 
title = {{Value-at-Risk Analysis for Measuring Stochastic Volatility of Stock Returns: Using GARCH-Based Dynamic Conditional Correlation Model}}, 
author = {Afzal, Fahim and Haiying, Pan and Afzal, Farman and Mahmood, Asif and Ikram, Amir}, 
journal = {SAGE Open}, 
issn = {21582440}, 
doi = {10.1177/21582440211005758}, 
abstract = {{To assess the time-varying dynamics in value-at-risk (VaR) estimation, this study has employed an integrated approach of dynamic conditional correlation (DCC) and generalized autoregressive conditional heteroscedasticity (GARCH) models on daily stock return of the emerging markets. A daily log-returns of three leading indices such as KSE100, KSE30, and KSE-ALL from Pakistan Stock Exchange and SSE180, SSE50 and SSE-Composite from Shanghai Stock Exchange during the period of 2009–2019 are used in DCC-GARCH modeling. Joint DCC parametric results of stock indices show that even in the highly volatile stock markets, the bivariate time-varying DCC model provides better performance than traditional VaR models. Thus, the parametric results in the DCC-GRACH model indicate the effectiveness of the model in the dynamic stock markets. This study is helpful to the stockbrokers and investors to understand the actual behavior of stocks in dynamic markets. Subsequently, the results can also provide better insights into forecasting VaR while considering the combined correlational effect of all stocks. © The Author(s) 2021.}}, 
pages = {21582440211005758}, 
number = {1}, 
volume = {11}
}
@article{10.1109/icmss.2010.5577942, 
year = {2010}, 
title = {{A comparative study on the performance of the value-at-risk using realized volatility and GARCH model}}, 
author = {Xiong, Zhengde and Zhang, Jie}, 
journal = {2010 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2010.5577942}, 
abstract = {{The risk of the financial market is the focus of global financing institution and supervisory authorities. Correspondingly, the accurate measure of volatility is central to the measure of the Value-at-Risk. VaR is a popular method to computer the finance risk at present, and the key of calculating VaR is predicting volatility exactly. About the measurements of volatility, the first method is the initial classic model that the volatility is estimated from the financial analysis model (such as Black-Scholes). The second method is the ARCH model and the SV model. The third method is realized volatility which is based upon high frequency data. From these, we can see the measure of volatility is developed quickly. In this paper, we compare the performance of the value-at-risk based on realized volatility which is based upon high frequency data and GARCH(1,1) model under different distributions , and the results show the performance of value-at-risk using realized volatility is more effective. © 2010 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-030-51971-1_49, 
year = {2020}, 
title = {{Reward-to-Variability Ratio as a Key Performance Indicator in Financial Manager Efficiency Assessment}}, 
author = {Malakhova, Anna Andreevna and Starova, Olga Valeryevna and Yarkova, Svetlana Anatolyevna and Danilova, Albina Sergeevna and Zdanovich, Marina Yuryevna and Kravtsov, Dmitry Ivanovitch and Zyablikov, Dmitry Valeryevitch}, 
journal = {Advances in Intelligent Systems and Computing}, 
issn = {21945357}, 
doi = {10.1007/978-3-030-51971-1\_49}, 
abstract = {{In this paper computational techniques to process financial data and to assess management efficiency are proposed. Personnel evaluation process is formalized on the basis of the proposed key performance indicators based on portfolio efficiency criteria. Personnel efficiency is assessed via the excessive portfolio return over average market performance indicators per unit of risk. Alternative measures to evaluate risk are formulated. The proposed downside risk measures are implemented into portfolio performance evaluation criteria. Comparative analysis of the introduced portfolio performance evaluation criteria is held. Case study via the Trading Organiser ‘Moscow Exchange’ is performed. The experimental results prove that the introduced portfolio performance evaluation criteria yield better results than the coefficients which do not take into account downside risk measures. It is concluded that the proposed modified ‘reward-to-variability’ ratio can be incorporated into the system of key performance indicators for assessing financial management efficiency. \#CSOC1120. © 2020, Springer Nature Switzerland AG.}}, 
pages = {598--613}, 
number = {NA}, 
volume = {1225 AISC}
}
@article{10.1016/j.jclepro.2021.128023, 
year = {2021}, 
title = {{Modelling extreme risks for carbon emission allowances — Evidence from European and Chinese carbon markets}}, 
author = {Fang, Sheng and Cao, Guangxi}, 
journal = {Journal of Cleaner Production}, 
issn = {09596526}, 
doi = {10.1016/j.jclepro.2021.128023}, 
abstract = {{Modelling the dynamics of carbon emission allowance prices has been of keen interest to academicians from around the world. The distribution of tails is different from that of centres for carbon allowance prices, invalidating regular methods and conventions applied in quantitative finance. Value-at-Risk (VaR) and Expected Shortfall (ES) are traditionally useful tools to measure the extreme risk of carbon markets. On this basis, this study uses the semiparametric method to estimate the time-varying VaR and ES of European Union Allowance (EUA) and Chinese carbon emission allowance (CEA) in downside and upside events. Our results show that the GAS model outperforms the rolling estimate in describing the time-varying downside and upside VaR and ES. We find two breaks in VaR and ES for EUA whilst no break for CEA. We also find the evidence of asymmetry in CEA and reversed asymmetry in EUA, but it differs across periods and between VaR and ES. In addition, the results reveal that the downside CEA VaR and ES are significantly greater than those of EUA. Meanwhile, the relationship between upside EUA and CEA series varies across periods as well as between VaR and ES. These findings have important implications for entrepreneurs, investors, risk managers, and policy makers. © 2021}}, 
pages = {128023}, 
number = {NA}, 
volume = {316}
}
@article{10.1017/asb.2015.23, 
year = {2016}, 
title = {{Optimal Reinsurance from the Perspectives of Both an Insurer and a Reinsurer}}, 
author = {Cai, Jun and Lemieux, Christiane and Liu, Fangda}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2015.23}, 
abstract = {{Optimal reinsurance from an insurer's point of view or from a reinsurer's point of view has been studied extensively in the literature. However, as two parties of a reinsurance contract, an insurer and a reinsurer have conflicting interests. An optimal form of reinsurance from one party's point of view may be not acceptable to the other party. In this paper, we study optimal reinsurance designs from the perspectives of both an insurer and a reinsurer and take into account both an insurer's aims and a reinsurer's goals in reinsurance contract designs. We develop optimal reinsurance contracts that minimize the convex combination of the Value-at-Risk (VaR) risk measures of the insurer's loss and the reinsurer's loss under two types of constraints, respectively. The constraints describe the interests of both the insurer and the reinsurer. With the first type of constraints, the insurer and the reinsurer each have their limit on the VaR of their own loss. With the second type of constraints, the insurer has a limit on the VaR of his loss while the reinsurer has a target on his profit from selling a reinsurance contract. For both types of constraints, we derive the optimal reinsurance forms in a wide class of reinsurance policies and under the expected value reinsurance premium principle. These optimal reinsurance forms are more complicated than the optimal reinsurance contracts from the perspective of one party only. The proposed models can also be reduced to the problems of minimizing the VaR of one party's loss under the constraints on the interests of both the insurer and the reinsurer. © Copyright 2016 Astin Bulletin.}}, 
pages = {815--849}, 
number = {3}, 
volume = {46}
}
@article{10.24818/18423264/53.4.19.17, 
year = {2019}, 
title = {{Probability modeling and estimation of risk measures for fire loss severity in pakistan: An application of extreme value theory}}, 
author = {ISHFAQ, AHMAD and MUFRAH, ALMANJAHIE IBRAHIM and MUHAMMAD, ASGHER and HAQUL, EHTSHAM}, 
journal = {ECONOMIC COMPUTATION AND ECONOMIC CYBERNETICS STUDIES AND RESEARCH}, 
issn = {0424267X}, 
doi = {10.24818/18423264/53.4.19.17}, 
abstract = {{Extreme events are increasing in the insurance and financial markets, causing large losses and ultimately huge insurance claims. Commercial fire loss severity has the largest value among the major insurance claims. The goal of our study is modeling the commercial fire loss severity and estimating the risk of extreme fire losses by using Extreme Value Theory (EVT). In the present study, we utilize the EVT (point over threshold modeling) for modeling the tail of fire loss data. We find that the Generalized Pareto distribution (GPD) gives more satisfactory fit to commercial fire loss data as compared to other parametric distributions including exponential, Pareto, gamma, logistic and generalized extreme value (GEV) distribution. In the empirical study, we determine the peaks over threshold of the GPD with the help of Mean Excess plots and Hill plots. We also estimate the risk measures like value at risk (VaR) and expected shortfall (ES). These estimates are helpful for pricing and risk management of non-insurance companies for their policy implications. © 2019, Bucharest University of Economic Studies. All rights reserved.}}, 
pages = {275--292}, 
number = {4}, 
volume = {53}
}
@article{10.3923/itj.2013.6184.6190, 
year = {2013}, 
title = {{Research on risk measure of electricity market based on armax-garch model with conditional skewed-t distribution and extreme value theory}}, 
author = {Wang, Ruiqing}, 
journal = {Information Technology Journal}, 
issn = {18125638}, 
doi = {10.3923/itj.2013.6184.6190}, 
abstract = {{How to effectively evaluate price of volatility risk is the basis of risk management in electricity market. An ARMAX-GARCH model imposing a skewedt-t distribution with time-varying skewness and degree of freedom over the error terms (ARMAX-GARCH-ST) is proposed and used to filter electricity price series in order to capture the dependencies, seasonalities, heteroscedasticities, skewnesses, leptokurtosises, volatility-clustering and relationship to system loads. In this way, an approximately independently and identically distributed residual series with better statistical properties is acquired. Then Extreme Value Theory (EVT) is adopted to explicitly model the tails of the normalized residuals of ARMAX-GARCH-ST model and accurate estimates of electricity market Value-at-Risk (VaR) can be produced. The empirical analysis shows that the ARMAX-GARCH-EVT models can be rapidly reflect the most recent and relevant changes of spot electricity prices and can produce accurate forecasts of VaR at all confidence levels, showing better dynamic characteristics. These results present several potential implications for electricity markets risk quantifications and hedging strategies. © 2013 Asian Network for Scientific Information.}}, 
pages = {6184--6190}, 
number = {21}, 
volume = {12}
}
@article{10.1007/s11424-020-8037-z, 
year = {2020}, 
title = {{First and Second Order Asymptotics of the Spectral Risk Measure for Portfolio Loss Under Multivariate Regular Variation}}, 
author = {Xing, Guodong and Yang, Shanchao}, 
journal = {Journal of Systems Science and Complexity}, 
issn = {10096124}, 
doi = {10.1007/s11424-020-8037-z}, 
abstract = {{In the context of multivariate regular variation, the authors establish the first-order asymptotics of the spectral risk measure of portfolio loss. Furthermore, by the notion of second-order regular variation, the second-order asymptotics of the spectral risk measure of portfolio loss is also presented. In order to illustrate the derived results, a numerical example with Monte Carlo simulation is carried out. © 2020, The Editorial Office of JSSC \& Springer-Verlag GmbH Germany.}}, 
pages = {1533--1544}, 
number = {5}, 
volume = {33}
}
@article{10.1061/(asce)co.1943-7862.0000457, 
year = {2012}, 
title = {{Quantitative method for updating cost contingency throughout project execution}}, 
author = {Xie, Hua and AbouRizk, Simaan and Zou, Junhao}, 
journal = {Journal of Construction Engineering and Management}, 
issn = {07339364}, 
doi = {10.1061/(asce)co.1943-7862.0000457}, 
abstract = {{This paper presents a method for project cost contingency forecasting and updating based on Value at Risk at a certain confidence level during project execution. The method is applied and demonstrated through an ongoing tunnel construction project lasting for several years. The method makes use of newly available information as the project progresses, and project daily cost and daily progress are analyzed and used as inputs for the quantitative model. The forecasts compare favorably with actual contingencies. The method provides opportunities to update contingencies at project milestones and to allocate appropriate contingencies at different project phases. This is beneficial for major companies which run multiple projects lasting for years, allowing them to set appropriate contingencies on certain timelines and to improve resource utilization. © 2012 American Society of Civil Engineers.}}, 
pages = {759--766}, 
number = {6}, 
volume = {138}
}
@article{10.1057/grir.2011.5, 
year = {2012}, 
title = {{Enhancing insurer value using reinsurance and value-at-risk criterion}}, 
author = {Tan, Ken Seng and Weng, Chengguo}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1057/grir.2011.5}, 
abstract = {{The quest for optimal reinsurance design has remained an interesting problem among insurers, reinsurers, and academicians. An appropriate use of reinsurance could reduce the underwriting risk of an insurer and thereby enhance its value. This paper complements the existing research on optimal reinsurance by proposing another model for the determination of the optimal reinsurance design. The problem is formulated as a constrained optimization problem with the objective of minimizing the value-at-risk of the net risk of the insurer while subjecting to a profitability constraint. The proposed optimal reinsurance model, therefore, has the advantage of exploiting the classical tradeoff between risk and reward. Under the additional assumptions that the reinsurance premium is determined by the expectation premium principle and the ceded loss function is confined to a class of increasing and convex functions, explicit solutions are derived. Depending on the risk measure's level of confidence, the safety loading for the reinsurance premium, and the expected profit guaranteed for the insurer, we establish conditions for the existence of reinsurance. When it is optimal to cede the insurer's risk, the optimal reinsurance design could be in the form of pure stop-loss reinsurance, quota-share reinsurance, or a combination of stop-loss and quota-share reinsurance. © 2012 The International Association for the Study of Insurance Economics.}}, 
pages = {109--140}, 
number = {1}, 
volume = {37}
}
@article{10.1016/j.cma.2014.07.021, 
year = {2014}, 
title = {{Uncertainty quantification by geometric characterization of sensitivity spaces}}, 
author = {Mohammadi, Bijan}, 
journal = {Computer Methods in Applied Mechanics and Engineering}, 
issn = {00457825}, 
doi = {10.1016/j.cma.2014.07.021}, 
abstract = {{We propose a systematic procedure for both aleatory and epistemic uncertainty quantification of numerical simulations through geometric characteristics of global sensitivity spaces. Two mathematical concepts are used to characterize the geometry of these spaces and to identify possible impacts of variability in data or changes in the models or solution procedures: the dimension of the maximal free generator subspace in vector spaces and the principal angles between subspaces. We show how these characters can be used as indications on the aleatory and epistemic uncertainties. In the case of large dimensional parameter spaces, these characterizations are established for quantile-based extreme scenarios and a multi-point moment-based sensitivity direction permits to propose a directional uncertainty quantification concept for directional extreme scenarios (DES). The approach is non-intrusive and exploits in parallel the elements of existing mono-point gradient-based design platforms. The ingredients of the paper are illustrated on a model problem with the Burgers equation with control and on a constrained aerodynamic performance analysis problem. •Non-intrusive parallel geometric procedure for aleatory and epistemic uncertainty quantification.•Analysis of the dimension of the maximal free generator subspace in the global multi-point sensitivity spaces.•Principal angles between subspaces by SVD of projection matrices to quantify the deviation between sensitivity spaces.•Directional uncertainty quantification using quantile-based extreme scenarios.•Illustrations on a model problem and a full aircraft analysis in a range of transverse winds. © 2014.}}, 
pages = {197--221}, 
number = {NA}, 
volume = {280}
}
@article{10.1109/allerton.2017.8262843, 
year = {2018}, 
title = {{Transition-based versus state-based reward functions for MDPs with Value-At-Risk}}, 
author = {Ma, Shuai and Yu, Jia Yuan}, 
journal = {2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
issn = {NA}, 
doi = {10.1109/allerton.2017.8262843}, 
abstract = {{In reinforcement learning, the reward function on current state and action is widely used. When the objective is about the expectation of the (discounted) total reward only, it works perfectly. However, if the objective involves the total reward distribution, the result will be wrong. This paper studies Value-At-Risk (VaR) problems in short-and long-horizon Markov decision processes (MDPs) with two reward functions, which share the same expectations. Firstly we show that with VaR objective, when the real reward function is transition-based (with respect to action and both current and next states), the simplified (state-based, with respect to action and current state only) reward function will change the VaR. Secondly, for long-horizon MDPs, we estimate the VaR function with the aid of spectral theory and the central limit theorem. Thirdly, since the estimation method is for a Markov reward process with the reward function on current state only, we present a transformation algorithm for the Markov reward process with the reward function on current and next states, in order to estimate the VaR function with an intact total reward distribution. © 2017 IEEE.}}, 
pages = {974--981}, 
number = {NA}, 
volume = {2018-January}
}
@article{10.1007/s10584-011-0353-9, 
year = {2012}, 
title = {{A methodology for the risk assessment of climate variability and change under uncertainty. A case study: Coffee production in Veracruz, Mexico}}, 
author = {Estrada, Francisco and Gay, Carlos and Conde, Cecilia}, 
journal = {Climatic Change}, 
issn = {01650009}, 
doi = {10.1007/s10584-011-0353-9}, 
abstract = {{Existing methods for the assessment of the potential impacts of climate change in productive activities and sectors are usually limited to point estimates that do not consider the inherent variability and uncertainty of climatic and socioeconomic variables. This is a major drawback given that only a limited and potentially misleading estimation of risk can be expected when ignoring such determinant factors. In this paper, a new methodology is introduced that is capable of integrating the agent's beliefs and expert judgment into the assessment of the potential impacts of climate change in a quantitative manner by means of an objective procedure. The goal is to produce tailor-made information to assist decision-making under uncertainty in a way that is consistent with the current state of knowledge and the available subjective "expert" information. Time-charts of the evolution of different risk measures, that can be relevant for assisting decision-making and planning, can be constructed using this new methodology. This methodology is illustrated with a case study of coffee production in Mexico. Time-dependent probabilistic scenarios for coffee production and income, conditional on the agent's beliefs and expert judgment, are developed for the average producer under uncertain future conditions. It is shown that variability in production and income, generated by introducing climate variability and uncertainty are important factors affecting decision-making and the assessment of economic viability that are frequently ignored. The concept of Value at Risk, commonly applied in financial risk management, is introduced as a means for estimating the maximum expected loss for a previously chosen confidence level. Results are tailor-made for agents that have incomplete information and different beliefs. In this case study, the costs of climate change for coffee production in Veracruz are estimated to have a present value representing from 3 to 14 times the current annual value of coffee production in the state. © 2011 The Author(s).}}, 
pages = {455--479}, 
number = {2}, 
volume = {113}
}
@article{10.1016/j.ijforecast.2010.01.007, 
year = {2010}, 
title = {{Bayesian forecasting of Value at Risk and Expected Shortfall using adaptive importance sampling}}, 
author = {Hoogerheide, Lennart and Dijk, Herman K. van}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2010.01.007}, 
abstract = {{An efficient and accurate approach is proposed for forecasting the Value at Risk (VaR) and Expected Shortfall (ES) measures in a Bayesian framework. This consists of a new adaptive importance sampling method for the Quick Evaluation of Risk using Mixture of t approximations (QERMit). As a first step, the optimal importance density is approximated, after which multi-step 'high loss' scenarios are efficiently generated. Numerical standard errors are compared in simple illustrations and in an empirical GARCH model with Student-t errors for daily S\&P 500 returns. The results indicate that the proposed QERMit approach outperforms alternative approaches, in the sense that it produces more accurate VaR and ES estimates given the same amount of computing time, or, equivalently, that it requires less computing time for the same numerical accuracy. © 2010 International Institute of Forecasters.}}, 
pages = {231--247}, 
number = {2}, 
volume = {26}
}
@article{10.1002/for.868, 
year = {2003}, 
title = {{Selection of value-at-risk models}}, 
author = {Sarma, Mandira and Thomas, Susan and Shah, Ajay}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.868}, 
abstract = {{Value-at-Risk (VaR) is widely used as a tool for measuring the market risk of asset portfolios. However, alternative VaR implementations are known to yield fairly different VaR forecasts. Hence, every use of VaR requires choosing among alternative forecasting models. This paper undertakes two case studies in model selection, for the S\&P 500 index and India's NSE-50 index, at the 95\% and 99\% levels. We employ a two-stage model selection procedure. In the first stage we test a class of models for statistical accuracy. If multiple models survive rejection with the tests, we perform a second stage filtering of the surviving models using subjective loss functions. This two-stage model selection procedure does prove to be useful in choosing a VaR model, while only incompletely addressing the problem. These case studies give us some evidence about the strengths and limitations of present knowledge on estimation and testing for VaR. © 2003 John Wiley \& Sons, Ltd.}}, 
pages = {337--358}, 
number = {4}, 
volume = {22}
}
@article{10.1109/csse.2008.229, 
year = {2008}, 
title = {{Risk measure of shibor based on VAR and EGARCH}}, 
author = {Qi-Zhi, He}, 
journal = {2008 International Conference on Computer Science and Software Engineering}, 
issn = {NA}, 
doi = {10.1109/csse.2008.229}, 
abstract = {{There is a great significance to research the interest rate risk based on the method of value at risk on the background of Chinas gradual marketization of interest rates. The paper takes the overnight shibor as the target. First, introduce the calculating method for value at risk. Second, give the sample characters and the dynamic model of the yield rate of the overnight shibor. Third, using the GARCH and EGARCH model, at 99\% confidence level and 95\% confidence level, calculate the value at risk and the exception rate for the overnight shibor. The empirical results show that the value at risk of the overnight shibor has positive correlation with the level of interest rates, and whatever at 95\% confidence level or at 99\% confidence level, the EGARCH model is better than the GARCH model. © 2008 IEEE.}}, 
pages = {1333--1336}, 
number = {NA}, 
volume = {5}
}
@article{10.2143/ast.31.2.1007, 
year = {2001}, 
title = {{Empirical Issues in Value-at-Risk}}, 
author = {Bams, Dennis and Wielhouwer, Jacco L.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.31.2.1007}, 
abstract = {{For the purpose of Value-at-Risk (VaR) analysis, a model for the return distribution is important because it describes the potential behavior of a financial security in the future. What is primarily, is the behavior in the tail of the distribution since VaR analysis deals with extreme market situations. We analyze the extension of the normal distribution function to allow for fatter tails and for time-varying volatility. Equally important to the distribution function are the associated parameter values. We argue that parameter uncertainty leads to uncertainty in the reported VaR estimates. There is a tradeoff between more complex tail-behavior and this uncertainty. The “best estimate”-VaR should be adjusted to take account of the uncertainty in the VaR. Finally, we consider the VaR forecast for a portfolio of securities. We propose a method to treat the modeling in a univariate, rather than a multivariate, framework. Such a choice allows us to reduce parameter uncertainty and to model directly the relevant variable. © 2001, International Actuarial Association. All rights reserved.}}, 
pages = {299--315}, 
number = {2}, 
volume = {31}
}
@article{10.3923/jeasci.2017.4846.4850, 
year = {2017}, 
title = {{Quadratic investment portfolio without a risk-free asset based on value-at-risk}}, 
author = {}, 
issn = {1816949X}, 
doi = {10.3923/jeasci.2017.4846.4850}, 
abstract = {{This study will discuss the problems of quadratic investment portfolio without a risk-free asset based on value-at-risk. It is assumed that the risk of an investment portfolio measured by value-at-risk. The resolution of problems that do include: first, formulate models the trade-off problem. Secondly, formulate expectation maximization model of the problem. Third, formulate model minimization of value-at-risk problem. Based on the results of the discussions can be concluded that the trade-off between risk and expected return does not only depend on the type of investor but also on the size of the investment. In a realistic investment situation, it is likely that more constraints, e.g., restriction on short-selling, need to be considered. © Medwell Journals, 2017.}}, 
number = {19}, 
volume = {12}
}
@article{10.1007/s10690-005-6009-x, 
year = {2003}, 
title = {{Is volatility the best predictor of market crashes?}}, 
author = {Tsuji, Chikashi}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-005-6009-x}, 
abstract = {{The objective of this paper is to determine the best predictor of equity market crashes by focusing particularly on volatility and market liquidity. In finance, volatility has traditionally been regarded as the best measure of market risk. However, this paper shows that the forecast value of market liquidity, in particular our modified calculated market depth, predicts equity market crashes much more accurately than does the forecast values of EGARCH or Implied Volatility. © Springer 2005.}}, 
pages = {163--185}, 
number = {2-3}, 
volume = {10}
}
@article{10.1080/03461238.2020.1863856, 
year = {2021}, 
title = {{Two-step risk analysis in insurance ratemaking}}, 
author = {Kang, Seul Ki and Peng, Liang and Golub, Andrew}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2020.1863856}, 
abstract = {{Recently, Heras et al. (2018. An application of two-stage quantile regression to insurance ratemaking. Scandinavian Actuarial Journal9, 753–769) propose a two-step inference to forecast the Value-at-Risk of aggregated losses in insurance ratemaking by combining logistic regression and quantile regression without discussing the critical issue of uncertainty quantification. This paper proposes a random weighted bootstrap method to quantify the estimation uncertainty and an alternative two-step inference via weighted quantile regression. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--11}, 
number = {6}, 
volume = {2021}
}
@article{10.1061/(asce)wr.1943-5452.0001293, 
year = {2020}, 
title = {{Risk-based models for optimal sensor location problems in water networks}}, 
author = {Hooshmand, F. and Amerehi, F. and MirHassani, S. A.}, 
journal = {Journal of Water Resources Planning and Management}, 
issn = {07339496}, 
doi = {10.1061/(asce)wr.1943-5452.0001293}, 
abstract = {{Water distribution networks (WDNs), as vital systems of any city, are vulnerable to the intrusion of contamination, and the security of WDNs is of great importance. A promising approach for the protection of WDNs against such threats is to deploy contaminant detection sensors in the network. One of the modeling strategies to identify an appropriate configuration of sensors is the expectation-based model. In this model, the location of sensors is determined so that the expected impact of contamination events is minimized. In this work, we consider two existing expectation-based models that have been presented in the literature. Although both models identified the same optimal sensor placements, choosing the better model in practical implementation may not be clear to practitioners, who must consider complexity and resolution times. This work first answers this question and proves that both models have the same quality - and hence, in terms of solution time, it makes no difference which model is used. As the second contribution of this work, the expectation-based model is extended to incorporate worst-case, value-at-risk (VaR), and conditional VaR (CVaR) measures. Computational results compare the damage of risk-based models with real-world WDNs, and indicate that the CVaR-based model may be an excellent approach to address risk measures in this problem. The CVaR-based model optimizes the CVaR measure and, at the same time, does not cause a significant increase in the optimal value of other risk measures. © 2020 American Society of Civil Engineers.}}, 
pages = {04020086}, 
number = {11}, 
volume = {146}
}
@article{10.1016/j.jbankfin.2012.04.009, 
year = {2012}, 
title = {{Short-horizon regulation for long-term investors}}, 
author = {Shi, Zhen and Werker, Bas J.M.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.04.009}, 
abstract = {{We study the effects of imposing repeated short-horizon regulatory constraints on long-term investors. We show that Value-at-Risk and Expected Shortfall constraints, when imposed dynamically, lead to similar optimal portfolios and wealth distributions. We also show that, in utility terms, the costs of imposing these constraints can be sizeable. For a 96\% funded pension plan, both an annual Value-at-Risk constraint and an annual Expected Shortfall constraint can lead to an economic cost of about 2.5-3.8\% of initial wealth over a 15-year horizon. © 2012 Elsevier B.V.}}, 
pages = {3227--3238}, 
number = {12}, 
volume = {36}
}
@article{10.1016/j.jbankfin.2012.08.011, 
year = {2012}, 
title = {{High-frequency financial data modeling using Hawkes processes}}, 
author = {Chavez-Demoulin, V. and McGill, J.A.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.08.011}, 
abstract = {{Intraday Value-at-Risk (VaR) is one of the risk measures used by market participants involved in high-frequency trading. High-frequency log-returns feature important kurtosis (fat tails) and volatility clustering (extreme log-returns appear in clusters) that VaR models should take into account. We propose a marked point process model for the excesses of the time series over a high threshold that combines Hawkes processes for the exceedances with a generalized Pareto distribution model for the marks (exceedance sizes). The conditional approach features intraday clustering of extremes and is used to calculate instantaneous conditional VaR. The models are backtested on real data and compared to a competitor approach that proposes a nonparametric extension of the classical peaks-over-threshold method. Maximum likelihood estimation is computationally intensive; we use a differential evolution genetic algorithm to find adequate starting values for the optimization process. © 2012 Elsevier B.V.}}, 
pages = {3415--3426}, 
number = {12}, 
volume = {36}
}
@article{10.1016/j.csda.2015.08.018, 
year = {2016}, 
title = {{Linking Tukey's legacy to financial risk measurement}}, 
author = {Vijverberg, Chu-Ping C. and Vijverberg, Wim P.M. and Taşpınar, Süleyman}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2015.08.018}, 
abstract = {{Financial data are often thick-tailed and exhibit skewness. The versatile Generalized Tukey Lambda (GTL) distribution is able to capture varying degrees of skewness in thin- or thick-tailed data. Such versatility makes the GTL distribution potentially useful in the area of financial risk measurement. Moreover, for GTL-distributed random variables, the familiar risk measures of Value at Risk (VaR) and Expected Shortfall (ES) may be expressed in simple analytical forms. It turns out that, both analytically and through Monte Carlo simulations, GTL's VaR and ES differ significantly from other flexible distributions. The asymptotic properties of the maximum likelihood estimator of the GTL parameters are also examined. In order to study risk in financial data, the GTL distribution is inserted into a GARCH model. This GTL-GARCH model is estimated with data on daily returns of GE stock, demonstrating that, for certain data sets, GTL may capture risk measurements better than other distributions. © 2015}}, 
pages = {595--615}, 
number = {NA}, 
volume = {100}
}
@article{10.1007/s10700-015-9216-6, 
year = {2016}, 
title = {{A unit commitment-based fuzzy bilevel electricity trading model under load uncertainty}}, 
author = {Wang, Bo and Zhou, Xian-zhong and Watada, Junzo}, 
journal = {Fuzzy Optimization and Decision Making}, 
issn = {15684539}, 
doi = {10.1007/s10700-015-9216-6}, 
abstract = {{In this study, we establish a bilevel electricity trading model where fuzzy set theory is applied to address future load uncertainty, system reliability as well as human imprecise knowledge. From the literature, there have been some studies focused on this bilevel problem while few of them consider future load uncertainty and unit commitment optimization which handles the collaboration of generation units. Then, our study makes the following contributions: First, the future load uncertainty is characterized by fuzzy set theory, as the various factors that affect the load forecasting are often assessed with some non-statistical uncertainties. Second, the generation costs are obtained by solving complicated unit commitment problems, rather than approximate calculations used in existing studies. Third, this model copes with the optimizations of both the generation companies and the market operator, where the unexpected load risk is particularly analyzed by using fuzzy value-at-risk as a quantitative risk measurement. Forth, a mechanism to encourage the convergence of the bilevel model is proposed based on fuzzy maxmin approach, and a bilevel particle swarm optimization algorithm is developed to solve the problem in a proper runtime. To illustrate the effectiveness of this research, we provide a test system-based numerical example and discuss about the experimental results according to the principle of social welfare maximization. Finally, we also compare the model and algorithm with conventional methods. © 2015, Springer Science+Business Media New York.}}, 
pages = {103--128}, 
number = {1}, 
volume = {15}
}
@article{10.1016/j.iimb.2012.09.001, 
year = {2012}, 
title = {{Evaluation of Basel III revision of quantitative standards for implementation of internal models for market risk}}, 
author = {Sharma, Meera}, 
journal = {IIMB Management Review}, 
issn = {09703896}, 
doi = {10.1016/j.iimb.2012.09.001}, 
abstract = {{This paper studies revisions under Basel III for market risk which allow conservative combination of short and long period Value-at-Risks (VaRs). This is the first study that examines this issue. The performance of the combination method is evaluated through regulatory back tests, unconditional and conditional coverage tests. The combination improves performance in regulatory back tests and tests of unconditional coverage. A common trend is the superior performance of long (1000/750 day) in combination with short (190/125 days) VaR methods. The combination does not enhance conditional coverage performance. This is the first study on this topic. © 2012.}}, 
pages = {234--244}, 
number = {4}, 
volume = {24}
}
@article{10.1007/0-387-23394-6_2, 
year = {2005}, 
title = {{An overview of probabilistic and time series models in finance}}, 
author = {Balbás, Alejandro and Romera, Rosario and Ruiz, Esther}, 
issn = {NA}, 
doi = {10.1007/0-387-23394-6\_2}, 
abstract = {{In this paper, we partially review probabilistic and time series models in finance. Both discrete and continuous-time models are described. The characterization of the No-Arbitrage paradigm is extensively studied in several financial market contexts. As the probabilistic models become more and more complex to be realistic, the Econometrics needed to estimate them are more difficult. Consequently, there is still much research to be done on the link between probabilistic and time series models. © 2005 Springer Science + Business Media, Inc.}}, 
pages = {27--63}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1368-423x.2007.00213.x, 
year = {2007}, 
title = {{Bayesian inference for the mixed conditional heteroskedasticity model}}, 
author = {Bauwens, L. and Rombouts, J.V.K.}, 
journal = {The Econometrics Journal}, 
issn = {13684221}, 
doi = {10.1111/j.1368-423x.2007.00213.x}, 
abstract = {{We estimate by Bayesian inference the mixed conditional heteroskedasticity model of Haas et al. (2004a Journal of Financial Econometrics 2, 211-50). We construct a Gibbs sampler algorithm to compute posterior and predictive densities. The number of mixture components is selected by the marginal likelihood criterion. We apply the model to the SP500 daily returns. © Royal Economic Society 2007.}}, 
pages = {408--425}, 
number = {2}, 
volume = {10}
}
@article{10.1016/j.econmod.2018.11.023, 
year = {2019}, 
title = {{Overnight exchange rate risk based on multi-quantile and joint-shock CAViaR models}}, 
author = {Peng, Wei and Zeng, Yufeng}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2018.11.023}, 
abstract = {{Overnight risk of exchange rate is more and more important because the exchange rate trading time of various countries is inconsistent. Drawing on the multi-quantile CAViaR model for two markets, this study proposes a multi-quantile CAViaR model for three markets and a multi-quantile CAViaR model for joint shock. The two new models are used to measure the impact of the U.S. Dollar index and the Euro on the overnight risk for the exchange rate of the Japanese Yen, Hong Kong Dollar, and Chinese Renminbi. The results show that, first, a lag risk affects the overnight risk of the three exchange rates, of which the Renminbi exchange rate is subject to the largest risk. Second, the U.S. Dollar index and Euro exchange rate risks impact the overnight risk of the three exchange rates and this effect is highest for the overnight risk of the Yen's exchange rate. In addition, the impact of the U.S.Dollar index risk is greater than that of the Euro. Third, the Euro and U.S.Dollar index produce a joint shock on the overnight risk of the three exchange rates, and here, the Yen's exchange rate suffers the biggest shock. Finally, the multi-quantile CAViaR model for joint shock is more accurate than that for three markets, particularly when the Hong Kong Dollar exchange rate has a 5\% VaR. These empirical results have meaningful implications for regulatory authorities. © 2018 Elsevier B.V.}}, 
pages = {392--399}, 
number = {NA}, 
volume = {80}
}
@article{10.1016/j.econmod.2013.07.030, 
year = {2013}, 
title = {{Foreign exchange risk in a managed float regime: A case study of Pakistani rupee}}, 
author = {Mudakkar, Syeda Rabab and Uppal, Jamshed Y. and Zaman, Khalid and Naseem, Imran and Shah, Ghias Ud Din}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2013.07.030}, 
abstract = {{The study examines applicability and performance of Value-at-Risk (VaR) models with respect to foreign exchange risk assessment within a managed float regime. Pakistani rupee offers an instructive case as it seems to manage its currency mainly against the US dollar, but to a lesser extent against the euro. We find that the distributional characteristics are quite different for the two currencies. We also find that the dynamic processes are remarkably different for the two exchange rates. The results indicate that compared with alternative competing models, the foreign exchange risk is better modeled using VaR based on Extreme Value Theory. Our findings underscore the importance of correctly specifying the return model in a dynamic framework. © 2013 Elsevier B.V.}}, 
pages = {409--417}, 
number = {NA}, 
volume = {35}
}
@article{10.1016/j.frl.2021.102197, 
year = {2021}, 
title = {{Risk management of Bitcoin futures with GARCH models}}, 
author = {Guo, Zi-Yi}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2021.102197}, 
abstract = {{In this article, we investigate the quantitative risk management of Bitcoin futures by using the GARCH models. We first found that it is crucial to introduce a heavy-tailed distribution into the GARCH models to explain return volatilities of Bitcoin futures. Then, we compare the VaR estimates based on the parametric methods, namely the GARCH model with the normal distribution (GARCH-Normal) and the GARCH model with the normal inverse Gaussian distribution (GARCH-NIG), and the nonparametric method. Our results illustrate that although the VaR estimates based on the nonparametric method are overall accurate and even more accurate than the VaR estimates based on the GARCH-Normal model, the VaR estimates based on the GARCH-NIG model perform the best. Overall, we conclude that the GARCH-NIG model could generate accurate VaR estimates for the Bitcoin futures return series. In addition, we found that in contrast to Bitcoin cash, the return volatilities of the Bitcoin futures do not increase by more in response to positive shocks than in response to negative shocks. © 2021 Elsevier Inc.}}, 
pages = {102197}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0927-538x(97)00003-6, 
year = {1997}, 
title = {{Derivatives and bank regulation}}, 
author = {Chen, Andrew H.}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/s0927-538x(97)00003-6}, 
abstract = {{Recent years have seen the emergence of U.S. financial institutions as major players in the worldwide market for financial derivatives. Over-the-counter (OTC) derivatives, such as interest rate swaps, have proven to be an important tool for the management of risk in banking. This paper provides some guidelines on the proper measurement of fair market value and risk for banks' derivatives positions. Proper assessment of market value and risk, it is argued, is essential for effective internal control and external regulation. © 1997 Elsevier Science B.V.}}, 
pages = {157--165}, 
number = {2}, 
volume = {5}
}
@article{10.1080/1350486042000297243, 
year = {2005}, 
title = {{Modelling specific interest rate risk with estimation of missing data}}, 
author = {Siegl, Thomas and Quell, Peter}, 
journal = {Applied Mathematical Finance}, 
issn = {1350486X}, 
doi = {10.1080/1350486042000297243}, 
abstract = {{For the treatment of specific interest rate risk, a risk model is suggested, quantifying and combining both market and credit risk components consistently. The market risk model is based on credit spreads derived from traded bond prices. Though traded bond prices reveal a maximum amount of issuer specific information, illiquidity problems do not allow for classical parameter estimation in this context. To overcome this difficulty an efficient multiple imputation method is proposed that also quantifies the amount of risk associated with missing data. The credit risk component is based on event risk caused by correlated rating migrations of individual bonds using a Copula function approach. © 2005 Taylor \& Francis.}}, 
pages = {283--309}, 
number = {3}, 
volume = {12}
}
@article{10.1111/jtsa.12524, 
year = {2020}, 
title = {{Backtesting portfolio value-at-risk with estimated portfolio weights}}, 
author = {Du, Zaichao and Pei, Pei}, 
journal = {Journal of Time Series Analysis}, 
issn = {01439782}, 
doi = {10.1111/jtsa.12524}, 
abstract = {{This article theoretically and empirically analyzes backtesting portfolio value-at-risk (VaR) with estimation risk in an intrinsically multi-variate framework. It particularly takes into account the estimation of portfolio weights in forecasting portfolio VaR and its impact on backtesting. It shows that the estimation risk from estimating portfolio weights and that from estimating the multi-variate dynamic model make the existing methods in a univariate framework inapplicable. It proposes a general theory to quantify estimation risk applicable to the present problem and suggests practitioners a simple but effective way to implement valid inference to overcome the effect of estimation risk in backtesting portfolio VaR. In particular, we apply our theory to the efficient mean-variance-skewness portfolio for a multi-variate generalized autoregressive conditional heteroscedasticity model with multi-variate general hyperbolic distributed innovations. Some Monte Carlo simulations and an empirical application demonstrate the merits of our method. © 2020 John Wiley \& Sons Ltd}}, 
pages = {605--619}, 
number = {5}, 
volume = {41}
}
@article{10.1108/s1571-038620190000026014, 
year = {2019}, 
title = {{Explaining systemic risk in Latin American banking industry over 2002 - 2015}}, 
author = {Pellegrini, Carlo Bellavite and Pellegrini, Laura and Sironi, Emiliano}, 
journal = {International Symposia in Economic Theory and Econometrics}, 
issn = {15710386}, 
doi = {10.1108/s1571-038620190000026014}, 
abstract = {{Systemic risk has been one of the most interesting issues in banking and financial literature during the last years, particularly in evaluating its effects on the stability of the whole financial system during crises. Differently from other studies which analyze systemic risk focusing on European countries, we explore the determinant of systemic risk in other regional or continental banking systems, as Latin America. Using the CoVaR approach proposed by Adrian and Brunnermeier (2016), we study the impact of corporate variables on systemic risk on a sample of 30 Latin American banks belonging to seven countries, continuously listed from 2002Q1 to 2015Q4. We investigate the contribution of the corporate variables over different economic periods: the Subprime crisis (2007Q3 2008Q3), the European Great Financial Depression (2008Q4 2010Q2), and the Sovereign debt crisis (2010Q3 2012Q3). © 2019 by Emerald Publishing Limited. All rights reserved.}}, 
pages = {287--309}, 
number = {NA}, 
volume = {26}
}
@article{10.1007/s13385-017-0160-4, 
year = {2017}, 
title = {{Solvency II solvency capital requirement for life insurance companies based on expected shortfall}}, 
author = {Boonen, Tim J.}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-017-0160-4}, 
pmid = {29323358}, 
pmcid = {PMC5744639}, 
abstract = {{This paper examines the consequences for a life annuity insurance company if the solvency II solvency capital requirements (SCR) are calibrated based on expected shortfall (ES) instead of value-at-risk (VaR). We focus on the risk modules of the SCRs for the three risk classes equity risk, interest rate risk and longevity risk. The stress scenarios are determined using the calibration method proposed by EIOPA in 2014. We apply the stress-scenarios for these three risk classes to a fictitious life annuity insurance company. We find that for EIOPA’s current quantile 99.5\% of the VaR, the stress scenarios of the various risk classes based on ES are close to the stress scenarios based on VaR. Might EIOPA choose to calibrate the stress scenarios on a smaller quantile, the longevity SCR is relatively larger and the equity SCR is relatively smaller if ES is used instead of VaR. We derive the same conclusion if stress scenarios are determined with empirical stress scenarios. © 2017, The Author(s).}}, 
pages = {405--434}, 
number = {2}, 
volume = {7}
}
@article{10.1016/j.cam.2016.10.012, 
year = {2017}, 
title = {{Impact of foreign exchange rate on oil companies risk in stock market: A Markov-switching approach}}, 
author = {Zolfaghari, Mehdi and Sahabi, Bahram}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2016.10.012}, 
abstract = {{During the recent years, the importance of effective risk management has become extremely crucial. Value at Risk (VaR) is a standard downside measure to explain the behavior of financial series. As the GARCH models have been successfully applied in modeling the volatility structure of securities and other financial assets, the VaR of GARCH models has turned into an important quantity to study. In addition to GARCH family models, the Markov Regime-Switching GARCH (MRS-GARCH) models are a widely used approach to model financial volatility with potential structural breaks. In this paper, we evaluated the performance of the MRS-GARCH models with the traditional ARMA-GARCH family models for estimating VaR of the Stock Return of Operating Companies in the Oil Industry (SROCOI) in Iran's stock market under the normal, student-t and GED distributions. Our findings showed that the MRS-GARCH models outperform the ARMA-GARCH family models to capture the characteristics of the SROCOIs volatility. Additionally, we evaluated impact of the Foreign Exchange Rate fluctuations on the VaR of SROCOIs that were calculated from the optimal models under two regimes using ARDL model. The results further demonstrated that foreign exchange rate fluctuations have significant and different impacts on the VaR of SROCOIs across regimes. © 2016 Elsevier B.V.}}, 
pages = {274--289}, 
number = {NA}, 
volume = {317}
}
@article{10.1016/j.eswa.2011.09.048, 
year = {2012}, 
title = {{A comparison of GARCH models for VaR estimation}}, 
author = {Orhan, Mehmet and Köksal, Bülent}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2011.09.048}, 
abstract = {{This study is an attempt to compare a comprehensive list of GARCH models in quantifying risks of VaR under stress times. We gather data of stock market indices from both emerging (Brazil and Turkey) and developed (Germany and the USA) markets, over the period of global financial crisis and make use of numerous GARCH specifications to return VaR values. Then we compare the assessments of VaR with the realized returns by Kupiec and Christoffersen Tests. Besides, we calculate Quadratic Losses to evaluate the GARCH specifications in calculating VaR. The results reveal that the ARCH specification is the best performer followed by GARCH(1, 1) and the Student's t distribution is slightly better than the Normal. The other outcome of the paper is that the worst performers are Non-Linear Power GARCH and Non-Linear Power GARCH with a shift. All GARCH estimations are carried out with STATA that uses the Maximum Likelihood method of estimation. © 2011 Elsevier Ltd. All rights reserved.}}, 
pages = {3582--3592}, 
number = {3}, 
volume = {39}
}
@article{10.1007/978-3-319-99719-3_46, 
year = {2018}, 
title = {{Optimal Selection of Assets and Portfolios}}, 
author = {Hu, Bowen and Makarov, Roman N.}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-99719-3\_46}, 
abstract = {{In this paper, we propose a new method that allows an investor to rank available financial securities such as equities and exchange-traded funds (ETFs) in accordance with his or her risk preferences. We have demonstrated that using a linear combination of several risk measures and performance metrics as a ranking function can help us to select the most suitable efficient portfolio that is meeting risk preferences of an investor. We use three different methods to evaluate long-term values of metrics for each asset. After applying the ranking system to select most suitable assets from a large pool of securities, an optimal portfolio is formed by maximizing the ranking function. Past 5–10 years data with U.S. ETFs and S\&P500 stocks have been extracted using a Bloomberg terminal. © 2018, Springer Nature Switzerland AG.}}, 
pages = {509--519}, 
number = {NA}, 
volume = {259}
}
@article{10.1016/j.physa.2019.123207, 
year = {2020}, 
title = {{Setting the margins of Hang Seng Index Futures on different positions using an APARCH-GPD Model based on extreme value theory}}, 
author = {Chen, Yan and Yu, Wenqiang}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.123207}, 
abstract = {{An asymmetric power autoregressive conditional heteroscedasticity with generalized Pareto distribution (APARCH-GPD) model is proposed in this study to determine the optimal margin level for the Hang Seng Index futures contracts on the Hong Kong Futures Exchange. This method requires two steps. First, the APARCH model is used to measure the time-varying volatility of futures contract returns. Then, the tail distribution of the residuals from APARCH model is estimated by the GPD on the basis of the extreme value theory. Value-at-risk is finally estimated and predicted by the APARCH-GPD model, and this is compared with the APARCH-t and EWMA models by backtesting historical return series. The experimental results show that the long trading position of the Hang Seng Index futures contract undertakes more risk than the short trading position. Moreover, the APARCH-GPD model offers better one-day forecasting on both positions than the other models. The findings of this study provide important implications for making decisions on financial risk management. © 2019 Elsevier B.V.}}, 
pages = {123207}, 
number = {NA}, 
volume = {544}
}
@article{10.1504/ijads.2021.112928, 
year = {2021}, 
title = {{Portfolio management strategies of cryptocurrencies}}, 
author = {Mills, Ebenezer Fiifi Emire Atta and Zeng, Kailin}, 
journal = {International Journal of Applied Decision Sciences}, 
issn = {17558077}, 
doi = {10.1504/ijads.2021.112928}, 
abstract = {{This study explores the portfolio management of cryptocurrencies by assessing the out-of-sample performance of selected portfolio strategies in the literature. Using daily data from 500 randomly selected cryptocurrencies with monthly and weekly revision, the scaled and stable mean-variance-entropic (MVE) value-at-risk portfolios outperform other portfolio strategies closely followed by 1/N portfolios. The mean Sharpe ratio with transaction costs of both MVE and 1/N was higher than that of benchmark, Coinbase index. Indeed, diversification across cryptocurrencies does improve investment results and mitigates risk exposure. The findings of this research are crucial for practitioners as they showcase a coherent manner to aid fund managers and investors in their investment practices. Copyright © 2021 Inderscience Enterprises Ltd.}}, 
pages = {43--54}, 
number = {1}, 
volume = {14}
}
@article{10.1016/j.jbankfin.2003.06.008, 
year = {2005}, 
title = {{Comment on "Optimal portfolio selection in a value-at-risk framework"}}, 
author = {Huang, Hung-Hsi}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2003.06.008}, 
abstract = {{This comment discusses some errors in [Journal of Banking and Finance 25 (2001) 1789]. Given the portfolio rate of return is normally distributed, the following can be inferred. First, taking expected portfolio return rate as the benchmark of value-at-risk (VaR), the risk-return ratio collapses to a multiple of the Sharpe index. However, using risk-free rate as the benchmark, then above inference does not hold. Second, whether the benchmark of VaR is expected portfolio return rate or the risk-free rate, the optimal asset allocations for maximizing the risk-return ratio and Sharpe index are identical. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {3181--3185}, 
number = {12}, 
volume = {29}
}
@article{10.1515/demo-2015-0009, 
year = {2015}, 
title = {{Seven proofs for the subadditivity of expected shortfall}}, 
author = {Embrechts, Paul and Wang, Ruodu}, 
journal = {Dependence Modeling}, 
issn = {23002298}, 
doi = {10.1515/demo-2015-0009}, 
abstract = {{Subadditivity is the key property which distinguishes the popular risk measures Value-at-Risk and Expected Shortfall (ES). In this paper we oer seven proofs of the subadditivity of ES, some found in the literature and some not. One of the main objectives of this paper is to provide a general guideline for instructors to teach the subadditivity of ES in a course. We discuss the merits and suggest appropriate contexts for each proof. With different proofs, different important properties of ES are revealed, such as its dual representation, optimization properties, continuity, consistency with convex order, and natural estimators. © 2015 Paul Embrechts and Ruodu Wang, published by De Gruyter Open.}}, 
number = {1}, 
volume = {3}
}
@article{10.1007/s10687-016-0245-5, 
year = {2016}, 
title = {{Diversification limit of quantiles under dependence uncertainty}}, 
author = {Bignozzi, Valeria and Mao, Tiantian and Wang, Bin and Wang, Ruodu}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-016-0245-5}, 
abstract = {{In this paper, we investigate the asymptotic behavior of the portfolio diversification ratio based on Value-at-Risk (quantile) under dependence uncertainty, which we refer to as “worst-case diversification limit”. We show that the worst-case diversification limit is equal to the upper limit of the worst-case diversification ratio under mild conditions on the portfolio marginal distributions. In the case of regularly varying margins, we provide explicit values for the worst-case diversification limit. Under the framework of dependence uncertainty the worst-case diversification limit is significantly higher compared to classic results obtained in the literature of multivariate regularly varying distributions. The results carried out in this paper bring together extreme value theory and dependence uncertainty, two popular topics in the recent study of risk aggregation. © 2016, Springer Science+Business Media New York.}}, 
pages = {143--170}, 
number = {2}, 
volume = {19}
}
@article{10.1016/j.ribaf.2021.101567, 
year = {2022}, 
title = {{Semi-nonparametric risk assessment with cryptocurrencies}}, 
author = {Jiménez, Inés and Mora-Valencia, Andrés and Perote, Javier}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2021.101567}, 
abstract = {{This paper establishes a brand-new perspective of analyzing the risk of crypto assets through a semi-nonparametric approach, discussing its theoretical advantages and testing its performance compared to parametric approaches and in terms of backtesting techniques and different risk measures: Value-at-Risk, Expected Shortfall and Median Shortfall. Our comprehensive analysis for six cryptocurrencies shows that flexible semi-nonparametric approaches outperform risk measures of most crypto assets (particularly Bitcoin) and tend to provide the most conservative risk assessment. Furthermore, we propose the Median Shortfall as a robust-to-outliers and reliable risk measure for cryptocurrencies and discuss on the choice of the appropriate probability levels according to the assumed distribution. The evidence supports that Median Shortfall at 98.31 \% and 98.51 \% confidence levels as accurate alternatives to Value-at-Risk at 99 \% and Expected Shortfall at 97.5 \%. © 2021 The Author(s)}}, 
pages = {101567}, 
number = {NA}, 
volume = {59}
}
@article{10.1088/1742-6596/1324/1/012098, 
year = {2019}, 
title = {{Risk measurement of global stock markets: A factor copula-based GJR-GARCH approach}}, 
author = {Song, Quanrui and Liu, Jianxu and Sriboonchitta, Songsak}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1324/1/012098}, 
abstract = {{Financial crisis in 2008 caused huge loss and one of the accusations is the misprediction of risk measurement. Considering the important role the stock markets play, and the trend of globalization in economy, we propose forecasting Value at Risk of G20's (except European Union) stock indexes in three periods, pre-crisis, during crisis and post-crisis, via factor copula model. Unlike those models based on multivariate normality, factor copula is based on the assumption that there exists a or several common factors which lead to the change of stock prices. In this paper, different levels of dependence among 19 countries are presented and the results indicate that, during crisis countries with higher values of coefficients tend to have larger loss than others. Also, the large numbers of violations to VaR may be an indicator of the upcoming financial crisis. © 2019 IOP Publishing Ltd. All rights reserved.}}, 
pages = {012098}, 
number = {1}, 
volume = {1324}
}
@article{10.1109/pesgm40551.2019.8973967, 
year = {2019}, 
title = {{Probabilistic Quantification of Power Distribution System Operational Resilience}}, 
author = {Poudel, Shiva and Dubey, Anamika and Bose, Anjan}, 
journal = {2019 IEEE Power \& Energy Society General Meeting (PESGM)}, 
issn = {19449925}, 
doi = {10.1109/pesgm40551.2019.8973967}, 
abstract = {{High-impact, low-probability (HILP) events resulting from extreme weather conditions have a significant impact on the aging power distribution infrastructures. It is of growing concern to minimize the impacts of such catastrophic events on critical infrastructures by appropriately hardening the infrastructure and implementing new operational procedures. This calls for a quantitative assessment of distribution system resilience that can not only predict the impacts of the future events but also be used to evaluate different planning measures taken to minimize the impacts of extreme events. In this paper, we propose a probabilistic metric to quantify the operational resilience of the distribution grid. The metric is based on Conditional Value-at-Risk (CVaR) measure where resilience is defined as the conditional expectation of loss of energy in MWh for events beyond a prespecified risk threshold. A simulation-based framework to evaluate the proposed metric for resilience under different weather scenarios is presented. The impacts of restorative actions, specifically using distributed generators and remote-controlled switches (RCS) and the impacts of infrastructure hardening on resilience metric is also investigated. Numerical simulations are performed on the IEEE 123-bus test system. © 2019 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {2019-August}
}
@article{10.1016/j.eneco.2018.07.009, 
year = {2018}, 
title = {{Selection of Value at Risk Models for Energy Commodities}}, 
author = {Laporta, Alessandro G. and Merlo, Luca and Petrella, Lea}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2018.07.009}, 
abstract = {{In this paper we investigate different VaR forecasts for daily energy commodities returns using GARCH, EGARCH, GJR-GARCH, Generalized Autoregressive Score (GAS) and the Conditional Autoregressive Value at Risk (CAViaR) models. We further develop a Dynamic Quantile Regression (DQR) one where the parameters evolve over time following a first order stochastic process. The models considered are selected employing the Model Confidence Set procedure of Hansen et al. (2011) which provides a superior set of models by testing the null hypothesis of Equal Predictive Ability. Successively information coming from each model is pooled together using a weighted average approach. The empirical analysis is conducted on seven energy commodities. The results show that the quantile approach i.e. the CAViaR and the DQR outperform all the others for all the series considered and that, generally, VaR aggregation yields better results. © 2018 Elsevier B.V.}}, 
pages = {628--643}, 
number = {NA}, 
volume = {74}
}
@article{10.1287/opre.2017.1716, 
year = {2018}, 
title = {{Quantile-based risk sharing}}, 
author = {Embrechts, Paul and Liu, Haiyan and Wang, Ruodu}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.2017.1716}, 
abstract = {{We address the problem of risk sharing among agents using a two-parameter class of quantile-based risk measures, the so-called range-value-at-risk (RVaR), as their preferences. The family of RVaR includes the value-at-risk (VaR) and the expected shortfall (ES), the two popular and competing regulatory risk measures, as special cases. We first establish an inequality for RVaR-based risk aggregation, showing that RVaR satisfies a special form of subadditivity. Then, the Pareto-optimal risk sharing problem is solved through explicit construction. To study risk sharing in a competitive market, an Arrow-Debreu equilibrium is established for some simple yet natural settings. Furthermore, we investigate the problem of model uncertainty in risk sharing and show that, in general, a robust optimal allocation exists if and only if none of the underlying risk measures is a VaR. Practical implications of our main results for risk management and policy makers are discussed, and several novel advantages of ES over VaR from the perspective of a regulator are thereby revealed. © 2018, INFORMS.}}, 
pages = {936--949}, 
number = {4}, 
volume = {66}
}
@article{10.21314/jor.2017.356, 
year = {2017}, 
title = {{A new bootstrap test for multiple assets joint risk testing}}, 
author = {Ardia, David and Gatarek, Lukasz and Hoogerheide, Lennart}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2017.356}, 
abstract = {{In this paper, a novel simulation-based methodology is proposed to test the validity of a set of marginal time series models, where the dependence structure between the time series is taken directly from the observed data. The procedure is useful when one wants to summarize the test results for several time series in one joint test statistic and p value. The proposed test method can have higher statistical power than a test for a univariate time series, especially for short time series. Therefore, our test for multiple time series is particularly useful if one wants to assess value-at-risk (or expected shortfall) predictions within a small time frame (eg, a crisis period). We apply our method to test generalized autoregressive conditional heteroscedasticity (GARCH) model specifications for a large panel data set of stock returns. © 2017 Incisive Risk Information (IP) Limited.}}, 
pages = {1--22}, 
number = {4}, 
volume = {19}
}
@article{10.1007/s00180-020-00990-4, 
year = {2020}, 
title = {{A dominance approach for comparing the performance of VaR forecasting models}}, 
author = {Garcia-Jorcano, Laura and Novales, Alfonso}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-020-00990-4}, 
abstract = {{We introduce three dominance criteria to compare the performance of alternative value at risk (VaR) forecasting models. The three criteria use the information provided by a battery of VaR validation tests based on the frequency and size of exceedances, offering the possibility of efficiently summarizing a large amount of statistical information. They do not require the use of any loss function defined on the difference between VaR forecasts and observed returns, and two of the criteria are not conditioned by the choice of a particular significance level for the VaR tests. We use them to explore the potential for 1-day ahead VaR forecasting of some recently proposed asymmetric probability distributions for return innovations, as well as to compare the asymmetric power autoregressive conditional heteroskedasticity (APARCH) and the family of generalized autoregressive conditional heteroskedasticity (FGARCH) volatility specifications with more standard alternatives. Using 19 assets of different nature, the three criteria lead to similar conclusions, suggesting that the unbounded Johnson SU, the skewed Student-t and the skewed Generalized-t distributions seem to produce the best VaR forecasts. The unbounded Johnson SU distribution performs remarkably well, while symmetric distributions seem clearly inappropriate for VaR forecasting. The added flexibility of a free power parameter in the conditional volatility in the APARCH and FGARCH models leads to a better fit to return data, but it does not improve upon the VaR forecasts provided by GARCH and GJR-GARCH volatilities. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {1411--1448}, 
number = {3}, 
volume = {35}
}
@article{10.1108/ijesm-02-2018-0009, 
year = {2018}, 
title = {{Renewable energy investments with storage: a risk-return analysis}}, 
author = {Krömer, Sarah and Gatzert, Nadine}, 
journal = {International Journal of Energy Sector Management}, 
issn = {17506220}, 
doi = {10.1108/ijesm-02-2018-0009}, 
abstract = {{Purpose: The purpose of this paper is to study investments in renewable energy projects which are jointly operated with an energy storage system, with particular focus on risk-return characteristics from the perspective of private and institutional investors, taking into account resource risk, energy price risk, inflation risk and policy risk. Design/methodology/approach: To this end, this paper presents a stochastic discounted cash flow model which is then applied to a wind farm with a pumped hydro storage system. Findings: The results show that energy storage systems have the potential to increase the expected present value of future investment cash flows and to hedge (downside) risk. However, to realize this potential, storage systems have to be cost-effective in terms of fixed operation, maintenance, staffing and insurance costs. Also, several key factors are identified which have a considerable influence on the performance of the operation strategy. Originality/value: The paper contributes to the literature by conducting an analysis of (downside) risk and return of renewable energy investments with a storage system taking into account stochastic policy, resource, inflation and energy price risk. © 2018, Emerald Publishing Limited.}}, 
pages = {714--736}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.pacfin.2021.101498, 
year = {2021}, 
title = {{Value at risk and the cross-sect 10ion of expected returns: Evidence from China}}, 
author = {Gui, Pingshu and Zhu, Yifeng}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2021.101498}, 
abstract = {{In the Chinese stock market, we find that the cross-sectional relation between value-at-risk (VaR) and expected returns is unclear, which is different from the recent findings in the United States. Additionally, VaR is negatively related with expected returns and cannot be explained by idiosyncratic volatility, momentum, short-term reversal, or maximum daily return during a high consumer confidence period. In contrast, no significant relation is observed between VaR and expected returns during a period of low consumer confidence. © 2021 Elsevier B.V.}}, 
pages = {101498}, 
number = {NA}, 
volume = {66}
}
@article{10.1080/00036846.2011.593501, 
year = {2012}, 
title = {{An analysis of extreme movements of exchange rates of the main currencies traded in the foreign exchange market}}, 
author = {Iglesias, Emma M.}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2011.593501}, 
abstract = {{This article analyses the extreme movements of exchange rates of the seven main currencies traded in the Foreign Exchange market against the US dollar: Euro, British pound, Canadian dollar, Japanese Yen, Swiss franc, Australian dollar and New Zealand dollar by using tail index indicators. Payaslioǧ lu (2009) considers the case of the Turkish exchange rate using the traditional Hill (1975) estimator as a tool. In this article, we employ also an alternative estimator proposed in Iglesias and Linton (2009) that is shown to have, in some cases, improved finite sample properties and it provides substantially different results versus the Hill estimator. We find that for the Euro, Japanese Yen, Swiss franc, Canadian, Australian and New Zealand dollars, the Hill estimator provides a better measure to analyse the extreme behaviour; while for the British pound, the Iglesias and Linton alternative estimator is superior by using Hausman-type tests of misspecification. Measures of value at risk are also provided for the seven markets. We also find that the largest estimated value at risk by far is for the Japanese Yen, followed by the Swiss franc, the Canadian dollar, the Euro, the New Zealand dollar and the Australian dollar. The UK pound has the smallest value at risk when extreme movements occur. © 2012 Taylor \& Francis.}}, 
pages = {4631--4637}, 
number = {35}, 
volume = {44}
}
@article{10.4028/www.scientific.net/amr.143-144.1, 
year = {2011}, 
title = {{Value-at-Risk estimation based on empirical likelihood method}}, 
author = {Meng, Zhao Wei and Yu, Pei Chao}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.143-144.1}, 
abstract = {{Value at Risk (VaR ) is a method using statistical knowledge to measure financial risks, and its calculating core is to estimate or predicate fluctuation of the financial assets price. In recent years, the main method of estimating and predicating fluctuation of the financial assets price is the GARCH model. So to determine a reasonable GARCH model becomes the crux of VaR calculating. In this paper, we proposed using empirical likelihood method to estimate VaR , and we also proved that the empirical likelihood method is more effective and more concise than other current methods by simulation analysis. © (2011) Trans Tech Publications.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {143-144}
}
@article{10.1002/asmb.2561, 
year = {2021}, 
title = {{Asymptotics for value at risk and conditional tail expectation of a portfolio loss}}, 
author = {Su, Xiaonan and Wang, Xinzhi and Yang, Yang}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2561}, 
abstract = {{Consider a risk model in which X1,…, Xn are n potential losses from different risky assets at the terminal time, and (Formula presented.) are n discount factors over the period. In this paper, we establish some asymptotic formulas for the value at risk and conditional tail expectation of the total discounted loss (Formula presented.) of an investment portfolio. We also demonstrate our obtained results through Monte Carlo simulations with asymptotics. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {266--281}, 
number = {2}, 
volume = {37}
}
@article{10.1007/s12597-021-00531-7, 
year = {2021}, 
title = {{Solving mean-VaR portfolio selection model with interval-typed random parameter using interval analysis}}, 
author = {Kumar, P. and Behera, Jyotirmayee and Bhurjee, A. K.}, 
journal = {OPSEARCH}, 
issn = {00303887}, 
doi = {10.1007/s12597-021-00531-7}, 
abstract = {{Portfolio optimization encompasses the optimal assignment of limited capital to different available financial assets to achieve a reasonable trade-off between profit and risk. This paper focuses on a portfolio selection model with interval-typed random parameters considering risk measures as value-at-risk (VaR). The value-at-risk is expressed by means of the interval-typed of random parameters and associated with Markowitz’s model. The purpose of this opinion is to design an interval mean-VaR portfolio optimization model with the objective of minimization of VaR. A methodology is developed to obtain an efficient investment strategy using interval analysis with the parametric representation of the interval. The theoretical developments are illustrated based on a historical data set taken from the National Stock Exchange, India. © 2021, Operational Research Society of India.}}, 
pages = {1--37}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.irfa.2013.05.006, 
year = {2013}, 
title = {{Forecasting VaR using analytic higher moments for GARCH processes}}, 
author = {Alexander, Carol and Lazar, Emese and Stanescu, Silvia}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2013.05.006}, 
abstract = {{It is widely accepted that some of the most accurate Value-at-Risk (VaR) estimates are based on an appropriately specified GARCH process. But when the forecast horizon is greater than the frequency of the GARCH model, such predictions have typically required time-consuming simulations of the aggregated returns distributions. This paper shows that fast, quasi-analytic GARCH VaR calculations can be based on new formulae for the first four moments of aggregated GARCH returns. Our extensive empirical study compares the Cornish-Fisher expansion with the Johnson SU distribution for fitting distributions to analytic moments of normal and Student t, symmetric and asymmetric (GJR) GARCH processes to returns data on different financial assets, for the purpose of deriving accurate GARCH VaR forecasts over multiple horizons and significance levels. © 2013 Elsevier Inc.}}, 
pages = {36--45}, 
number = {NA}, 
volume = {30}
}
@article{10.1016/j.energy.2018.04.110, 
year = {2018}, 
title = {{A comparison of risk measures for accidents in the energy sector and their implications on decision-making strategies}}, 
author = {Spada, Matteo and Paraschiv, Florentina and Burgherr, Peter}, 
journal = {Energy}, 
issn = {03605442}, 
doi = {10.1016/j.energy.2018.04.110}, 
abstract = {{Within the broader context of energy security and critical infrastructure protection, the comprehensive assessment of accidents and their related consequences are of high priority for many stakeholders. The risk of accidents is commonly assessed by aggregated risk indicators, allowing for a consistent and direct comparison between energy chains and country groups. However, these indicators do not explicitly evaluate consequences at selected probability levels and/or consider risk aversion aspects. Furthermore, in risk-informed decision-making it is important to account for risk preferences of different stakeholders. To overcome these potential drawbacks, in this study, Value-at-Risk, Expected Shortfall and the Spectral Risk Measures, which are commonly used in the financial realm, are applied within an energy security perspective. In particular, fatality risk indicators are calculated for different country groups of three fossil data sets (coal, oil, natural gas) extracted from the Energy-related Severe Accident Database (ENSAD). The use of these risk measures facilitates a direct comparison and a better understanding of energy accident risks to insurers and other industry stakeholders that normally focus on financial and less infrastructure-related aspects. Furthermore, the usefulness of the risk measures and their pros and cons in the evaluation of accident risks in the energy sector has been discussed. © 2018 Elsevier Ltd}}, 
pages = {277--288}, 
number = {NA}, 
volume = {154}
}
@article{10.1016/j.insmatheco.2013.09.017, 
year = {2013}, 
title = {{Complete mixability and asymptotic equivalence of worst-possible VaR and ES estimates}}, 
author = {Puccetti, Giovanni and Wang, Bin and Wang, Ruodu}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.09.017}, 
abstract = {{We give a new sufficient condition for a continuous distribution to be completely mixable, and we use this condition to show that the worst-possible value-at-risk for the sum of d inhomogeneous risks is equivalent to the worst-possible expected shortfall under the same marginal assumptions, in the limit as d → ∞. Numerical applications show that this equivalence holds also for relatively small dimensions d. © 2013.}}, 
pages = {821--828}, 
number = {3}, 
volume = {53}
}
@article{10.1016/j.insmatheco.2011.10.008, 
year = {2012}, 
title = {{Extreme value behavior of aggregate dependent risks}}, 
author = {Chen, Die and Mao, Tiantian and Pan, Xiaoqing and Hu, Taizhong}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2011.10.008}, 
abstract = {{Consider a portfolio of n identically distributed risks with dependence structure modeled by an Archimedean survival copula. Wüthrich (2003) and Alink etal. (2004) proved that the probability of a large aggregate loss scales like the probability of a large individual loss, times a proportionality factor. This factor depends on the dependence strength and the tail behavior of the individual risk, denoted by qnF, qnW and qnG according to whether the tail behavior belongs to the maximum domain of attraction of the Fréchet, the Weibull or the Gumbel distribution, respectively. We investigate properties of the factors qnW and qnG with respect to the dependence parameter and/or the tail behavior parameter, and revisit the asymptotic behavior of conditional tail expectations of aggregate risks for the Weibull and the Gumbel cases by using a different method. The main results strengthen and complement some results in Alink etal. (2004, 2005)Barbe etal. (2006), and Embrechts etal. (2009). © 2011 Elsevier B.V.}}, 
pages = {99--108}, 
number = {1}, 
volume = {50}
}
@article{10.1080/1351847x.2012.744763, 
year = {2015}, 
title = {{Modelling commodity value at risk with Psi Sigma neural networks using open–high–low–close data}}, 
author = {Sermpinis, Georgios and Laws, Jason and Dunis, Christian L.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2012.744763}, 
abstract = {{The motivation for this paper is to investigate the use of a promising class of neural network models, Psi Sigma (PSI), when applied to the task of forecasting the one-day ahead value at risk (VaR) of the oil Brent and gold bullion series using open–high–low–close data. In order to benchmark our results, we also consider VaR forecasts from two different neural network designs, the multilayer perceptron and the recurrent neural network, a genetic programming algorithm, an extreme value theory model along with some traditional techniques such as an ARMA-Glosten, Jagannathan, and Runkle (1,1) model and the RiskMetrics volatility. The forecasting performance of all models for computing the VaR of the Brent oil and the gold bullion is examined over the period September 2001–August 2010 using the last year and half of data for out-of-sample testing. The evaluation of our models is done by using a series of backtesting algorithms such as the Christoffersen tests, the violation ratio and our proposed loss function that considers not only the number of violations but also their magnitude. Our results show that the PSI outperforms all other models in forecasting the VaR of gold and oil at both the 5\% and 1\% confidence levels, providing an accurate number of independent violations with small magnitude. © 2013 Taylor \& Francis.}}, 
pages = {316--336}, 
number = {4}, 
volume = {21}
}
@article{10.1002/ijfe.1577, 
year = {2017}, 
title = {{How fat are the tails of equity market indices?}}, 
author = {Stoyanov, Stoyan and Loh, Lixia and Fabozzi, Frank J.}, 
journal = {International Journal of Finance \&amp; Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.1577}, 
abstract = {{Using a generalized autoregressive conditional heteroskedasticity model to explain away the volatility clustering of volatility effect and extreme value theory to analyse the residuals' left and right tails, we study the tail thickness of 22 developed and 19 emerging equity market indices. In-sample and out-of-sample tests indicate that exponential tails of the residuals cannot be strongly rejected. We study the dispersion of extremes of developed and emerging markets, and we report a statistically significant tail asymmetry in both types of markets and a significant change in both tail risk and tail asymmetry of emerging markets after the financial crisis of 2008. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {181--200}, 
number = {3}, 
volume = {22}
}
@article{10.1007/978-981-13-3441-2_15, 
year = {2019}, 
title = {{Forecasting value at risk of foreign exchange rate by integrating geometric brownian motion}}, 
author = {Karim, Siti Noorfaera and Jaffar, Maheran Mohd}, 
journal = {Communications in Computer and Information Science}, 
issn = {18650929}, 
doi = {10.1007/978-981-13-3441-2\_15}, 
abstract = {{Foreign exchange is one of the most important financial assets for all countries around the world including Malaysia. After recovering from the Asian financial crisis, Malaysia tried to build a strong currency in order to maintain the economic performance. The study focuses on Malaysia foreign exchange rate and foreign exchange risk between ten currencies, which are CNY, SGD, JPY, EUR, USD, THB, KRW, IDR, TWD and AUD. Unpredictability of the foreign exchange rate makes the traders hard to forecast the future rate and the future risk. The study implements the parametric approach in the Value at Risk (VaR) method and the geometric Brownian motion (GBM) model. The objectives of the study are to integrate the VaR model with the GBM model in order to compute or forecast the VaR. By using parametric approach, the study successfully computes the VaR of foreign exchange rate for different confidence levels. The GBM model is suitable to forecast the foreign exchange rate accurately using less than one year input data and using the log volatility formula. Lastly, the study verifies the feasibility of the integrated model for a one month holding period using the data shifting technique. In conclusion, the prediction of future foreign exchange rate and foreign exchange risk is important in order to know the performance of a country and to make better decision on investment. © Springer Nature Singapore Pte Ltd. 2019.}}, 
pages = {186--198}, 
number = {NA}, 
volume = {937}
}
@article{10.1111/j.1467-9965.2009.00378.x, 
year = {2009}, 
title = {{Maximizing the growth rate under risk constraints}}, 
author = {Pirvu, Traian A. and Žitković, Gordan}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/j.1467-9965.2009.00378.x}, 
abstract = {{We investigate the ergodic problem of growth-rate maximization under a class of risk constraints in the context of incomplete, Itô-process models of financial markets with random ergodic coefficients. Including value-at-risk, tail-value-at-risk, and limited expected loss, these constraints can be both wealth-dependent (relative) and wealth-independent (absolute). The optimal policy is shown to exist in an appropriate admissibility class, and can be obtained explicitly by uniform, state-dependent scaling down of the unconstrained (Merton) optimal portfolio. This implies that the risk-constrained wealth-growth optimizer locally behaves like a constant relative risk aversion (CRRA) investor, with the relative risk-aversion coefficient depending on the current values of the market coefficients. © Copyright the Authors. Journal Compilation © 2009 Wiley Periodicals, Inc.}}, 
pages = {423--455}, 
number = {3}, 
volume = {19}
}
@article{10.1016/j.spl.2016.09.005, 
year = {2017}, 
title = {{CoVaR of families of copulas}}, 
author = {Bernardi, M. and Durante, F. and Jaworski, P.}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2016.09.005}, 
abstract = {{We revisit the notion of Conditional Value-at-Risk (shortly, CoVaR) by weakening the usual assumptions on the joint distribution function of the involved random variables. The new approach exploits the copula methodology and uses the concept of Dini derivatives. A directory of CoVaR values for different families of copulas is provided. © 2016 Elsevier B.V.}}, 
pages = {8--17}, 
number = {NA}, 
volume = {120}
}
@article{10.1002/ijfe.280, 
year = {2006}, 
title = {{An analysis of the distribution of extremes in indices of share returns in the US, UK and Japan from 1963 to 2000}}, 
author = {Gettinby, G. D. and Sinclair, C. D. and Power, D. M. and Brown, R. A.}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.280}, 
abstract = {{This paper seeks to characterize the distribution of extreme returns for US, UK and Japanese equity indices over the years 1963-2000. In particular, the suitability of the following distributions is investigated: Normal, Frechet, Gumbel, Weibull, Generalized Extreme Value (GEV), Generalized Pareto and Generalized Logistic (GL). Daily returns were obtained for each of the countries, and the minima over a variety of selection intervals were calculated. Plots of higher moment statistics for the minima on statistical distribution maps suggested that the best fitting distribution would be either the GEV or the GL. The results from fitting each of these distributions to extremes of a series of US, UK and Japanese share returns supported the preliminary evidence that the GL distribution best fitted the data in all three countries over the period of study. The GL distribution has fatter tails than the GEV distribution; hence this finding is of importance to investors who are concerned with assessing the risk of a portfolio. The paper highlights the important finance implicapons and in particular the potential for underestimation of risk if distributions without fat enough tails are employed. Copyright © 2006 John Wiley \& Sons, Ltd.}}, 
pages = {97--113}, 
number = {2}, 
volume = {11}
}
@article{10.1093/jjfinec/nbq021, 
year = {2011}, 
title = {{Robust backtesting tests for value-at-risk models}}, 
author = {Escanciano, J C and Olmo, J}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbq021}, 
abstract = {{Backtesting methods are statistical tests designed to uncover value-at-risk (VaR) models not capable of reporting the correct unconditional coverage probability or filtering the serial dependence in the data. We show in this paper that these methods are subject to the presence of model risk produced by the incorrect specification of the conditional VaR model and derive its effect in the asymptotic distribution of the relevant out-of-sample tests. We also show that in the absence of estimation risk, the unconditional backtest is affected by model misspecification but the independence test is not. We propose using resampling methods to implement robust backtests. Our experiments suggest that block-bootstrap outperforms subsampling methods in size accuracy. We carry out a Monte Carlo study to see the importance of model risk in finite samples for location-scale models that are incorrectly specified but correct on "average". An application to Dow-Jones Index shows the impact of correcting for model risk on backtesting procedures for different dynamic VaR models measuring risk exposure. © The Author 2010. Published by Oxford University Press. All rights reserved.}}, 
pages = {132--161}, 
number = {1}, 
volume = {9}
}
@article{10.1155/2015/143739, 
year = {2015}, 
title = {{Optimal Limited Stop-Loss Reinsurance under VaR, TVaR, and CTE Risk Measures}}, 
author = {Zhou, Xianhua and Zhang, Huadong and Fan, Qingquan}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2015/143739}, 
abstract = {{This paper aims to provide a practical optimal reinsurance scheme under particular conditions, with the goal of minimizing total insurer risk. Excess of loss reinsurance is an essential part of the reinsurance market, but the concept of stop-loss reinsurance tends to be unpopular. We study the purchase arrangement of optimal reinsurance, under which the liability of reinsurers is limited by the excess of loss ratio, in order to generate a reinsurance scheme that is closer to reality. We explore the optimization of limited stop-loss reinsurance under three risk measures: value at risk (VaR), tail value at risk (TVaR), and conditional tail expectation (CTE). We analyze the topic from the following aspects: (1) finding the optimal franchise point with limited stop-loss coverage, (2) finding the optimal limited stop-loss coverage within a certain franchise point, and (3) finding the optimal franchise point with limited stop-loss coverage. We provide several numerical examples. Our results show the existence of optimal values and locations under the various constraint conditions. © 2015 Xianhua Zhou et al.}}, 
pages = {1--12}, 
number = {NA}, 
volume = {2015}
}
@article{10.1016/j.ijforecast.2021.04.004, 
year = {2021}, 
title = {{Nonparametric expected shortfall forecasting incorporating weighted quantiles}}, 
author = {Storti, Giuseppe and Wang, Chao}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2021.04.004}, 
abstract = {{A new semi-parametric expected shortfall (ES) estimation and forecasting framework is proposed. The proposed approach is based on a two-step estimation procedure. The first step involves the estimation of value at risk (VaR) at different quantile levels through a set of quantile time series regressions. Then, the ES is computed as a weighted average of the estimated quantiles. The quantile weighting structure is parsimoniously parameterized by means of a beta weight function whose coefficients are optimized by minimizing a joint VaR and ES loss function of the Fissler–Ziegel class. The properties of the proposed approach are first evaluated with an extensive simulation study using two data generating processes. Two forecasting studies with different out-of-sample sizes are then conducted, one of which focuses on the 2008 Global Financial Crisis period. The proposed models are applied to seven stock market indices, and their forecasting performances are compared to those of a range of parametric, non-parametric, and semi-parametric models, including GARCH, conditional autoregressive expectile (CARE), joint VaR and ES quantile regression models, and a simple average of quantiles. The results of the forecasting experiments provide clear evidence in support of the proposed models. © 2021 International Institute of Forecasters}}, 
number = {NA}, 
volume = {NA}
}
@article{10.5004/dwt.2017.21285, 
year = {2017}, 
title = {{Developing lcc and risk analysis method for dewatering facility based on its management history}}, 
author = {Nam, Youngwook and Choi, Jihoon and Kim, Dooil}, 
journal = {DESALINATION AND WATER TREATMENT}, 
issn = {19443994}, 
doi = {10.5004/dwt.2017.21285}, 
abstract = {{Operation and management of wastewater treatment facilities are critical for facility owners because of limited budget and aging equipments. The assets in a facility are huge in number and amount and complicated, which make owners to adopt an asset management system to their facility. In this research, we compared proactive management cost (PMC) and follow-up management cost (FMC) using life cycle cost (LCC) and risk management, which were major part of asset management for the wastewater treatment plant located in South Korea. For this, we developed a method to forecast future maintenance cost of a dewatering facility in a wastewater treatment plant. We also developed a method to obtain average occurrence probability and average occurrence cost using data grouping method with equal width of the valid data of USD 273.0. And then, we predicted cumulated maintenance cost (CMC) and cumulative distribution function (CDF). LCC results of dewatering facility were obtained from maintenance cost data and Bernoulli trials. It was expected that the maintenance cost reaches \$234,778 after 385 months from its installation. We calculated value at risk (VaR) using average occurrence probability and repair cost. We calculated PMC and FMC using VaR and LCC. Proactive maintenance was beneficial to facility owners because the maximum difference at 285 months between PMC and FMC was USD 37,530. © 2017 Desalination Publications. All rights reserved.}}, 
pages = {211--218}, 
number = {NA}, 
volume = {96}
}
@article{10.1145/1645413.1645422, 
year = {2009}, 
title = {{Cost vs. performance of VaR on accelerator platforms}}, 
author = {Majmudar, Mukul and Docan, Ciprian and Parashar, Manish and Marty, Christopher}, 
journal = {Proceedings of the 2nd Workshop on High Performance Computational Finance - WHPCF '09}, 
issn = {NA}, 
doi = {10.1145/1645413.1645422}, 
abstract = {{The computation of value at risk (VaR) can be parallelized to boost performance, but different parallel platforms entail different gains in performance, as well as different costs. This paper explores the cost and performance tradeoffs inherent in the computation of VaR when implemented on different parallel platforms. Copyright 2009 ACM.}}, 
pages = {9}, 
number = {NA}, 
volume = {NA}
}
@article{10.3846/16111699.2017.1342272, 
year = {2017}, 
title = {{Scaled and stable mean-variance-EVaR portfolio selection strategy with proportional transaction costs}}, 
author = {Mills, Ebenezer Fiifi Emire Atta and Yu, Bo and Yu, Jie}, 
journal = {Journal of Business Economics and Management}, 
issn = {16111699}, 
doi = {10.3846/16111699.2017.1342272}, 
abstract = {{This paper studies a portfolio optimization problem with variance and Entropic Value-at-Risk (EVaR) as risk measures. As the variance measures the deviation around the expected return, the introduction of EVaR in the mean-variance framework helps to control the downside risk of portfolio returns. This study utilized the squared l2-norm to alleviate estimation risk problems arising from the mean estimate of random returns. To adequately represent the variance-EVaR risk measure of the resulting portfolio, this study pursues rescaling by the capital accessible after payment of transaction costs. The results of this paper extend the classical Markowitz model to the case of proportional transaction costs and enhance the efficiency of portfolio selection by alleviating estimation risk and controlling the downside risk of portfolio returns. The model seeks to meet the requirements of regulators and fund managers as it represents a balance between short tails and variance. The practical implications of the findings of this study are that the model when applied, will increase the amount of capital for investment, lower transaction cost and minimize risk associated with the deviation around the expected return at the expense of a small additional risk in short tails. © 2017 Vilnius Gediminas Technical University (VGTU) Press.}}, 
pages = {561--584}, 
number = {4}, 
volume = {18}
}
@article{10.1198/jasa.2009.0102, 
year = {2009}, 
title = {{Nonparametric quantile estimations for dynamic smooth coefficient models}}, 
author = {Cai, Zongwu and Xu, Xiaoping}, 
journal = {Journal of the American Statistical Association}, 
issn = {01621459}, 
doi = {10.1198/jasa.2009.0102}, 
abstract = {{In this article, quantile regression methods are suggested for a class of smooth coefficient time series models.We use both local polynomial and local constant fitting schemes to estimate the smooth coefficients in a quantile framework. We establish the asymptotic properties of both the local polynomial and local constant estimators for a-mixing time series. Also, a bandwidth selector based on the nonparametric version of the Akaike information criterion is suggested, together with a consistent estimate of the asymptotic covariance matrix. Furthermore, the asymptotic behaviors of the estimators at boundaries are examined. A comparison of the local polynomial quantile estimator with the local constant estimator is presented. A simulation study is carried out to illustrate the performance of estimates. An empirical application of the model to real data further demonstrates the potential of the proposed modeling procedures.}}, 
pages = {371--383}, 
number = {485}, 
volume = {104}
}
@article{10.1080/14697688.2015.1019357, 
year = {2016}, 
title = {{Conditional higher order moments in metal asset returns}}, 
author = {Cochran, Steven J. and Mansur, Iqbal and Odusami, Babatunde}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2015.1019357}, 
abstract = {{This study examines the role of higher order moments in the returns of four important metals, aluminium, copper, gold and silver, using the asymmetric GARCH (AGARCH) model with a conditional skewed generalized-t (SGT) distribution. Implications of higher order moments in metal returns are evaluated by comparing the performances of conditional value-at-risk measures obtained from the AGARCH models with SGT distributions to those obtained from the AGARCH models with normal and student-t distributions. With the exception of gold, the AGARCH model with the SGT distribution appears to have the best fit for all metals examined. © 2015 Taylor \& Francis.}}, 
pages = {151--167}, 
number = {1}, 
volume = {16}
}
@article{10.5755/j01.ee.29.2.17405, 
year = {2018}, 
title = {{The impact made on project portfolio optimisation by the selection of various risk measures}}, 
author = {Yousefi, Vahidreza and Yakhchali, Siamak Haji and Šaparauskas, Jonas and Kiani, Sarmad}, 
journal = {Engineering Economics}, 
issn = {13922785}, 
doi = {10.5755/j01.ee.29.2.17405}, 
abstract = {{This study addresses the effect of selecting an appropriate risk measure and the impact of this choice on the efficient frontier of the project portfolio of an organisation. The appropriate choice of a firm’s project portfolio has a great impact on the organisational success. Each portfolio manager selects the best projects with different criteria and consistent with firm’s strategic objectives. We used the Markowitz efficient frontier method to select the best projects of the organisation. The choice of proper measures impacts on this decision and can change the organisation’s portfolio. The standard deviation was applied, and the relevant optimisation was made for this purpose. Then, the semi-standard deviation was used to differentiate between favourable and unfavourable opportunities. Afterwards, Value at Risk and Expected Shortfall were applied as appropriate risk measures to make a better estimate of the tail risks. All these risk measures were used to select the best possible projects. Managers should select the appropriate risk measures according to their objectives, estimation of their project distribution, and characteristics of the projects. This research studied the best measures consistent with construction projects and the effect of changes in these measures. © 2018, Kauno Technologijos Universitetas. All rights reserved.}}, 
pages = {168 -- 175}, 
number = {2}, 
volume = {29}
}
@article{10.3390/en9100848, 
year = {2016}, 
title = {{Revenue risk of U.S. Tight-oil firms}}, 
author = {Abadie, Luis Mª and Chamorro, José M.}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en9100848}, 
abstract = {{American U.S. crude oil prices have dropped significantly of late down to a low of less than \$30 a barrel in early 2016. At the same time price volatility has increased and crude in storage has reached record amounts in the U.S. America. Low oil prices in particular pose quite a challenge for the survival of U.S. America's tight-oil industry. In this paper we assess the current profitability and future prospects of this industry. The question could be broadly stated as: should producers stop operation immediately or continue in the hope that prices will rise in the medium term? Our assessment is based on a stochastic volatility model with three risk factors, namely the oil spot price, the long-term oil price, and the spot price volatility; we allow for these sources of risk to be correlated and display mean reversion. We then use information from spot and futures West Texas Intermediate (WTI) oil prices to estimate this model. Our aim is to show how the development of the oil price in the future may affect the prospective revenues of firms and hence their operation decisions at present. With the numerical estimates of the model's parameters we can compute the value of an operating tight-oil field over a certain time horizon. Thus, the present value (PV) of the prospective revenues up to ten years from now is \$37.07/bbl in the base case. Consequently, provided that the cost of producing a barrel of oil is less than \$37.07 production from an operating field would make economic sense. Obviously this is just a point estimate. We further perform a Monte Carlo (MC) simulation to derive the risk profile of this activity and calculate two standard measures of risk, namely the value at risk (VaR) and the expected shortfall (ES) (for a given confidence level). In this sense, the PV of the prospective revenues will fall below \$22.22/bbl in the worst 5\% of the cases; and the average value across these worst scenarios is \$19.77/bbl. Last we undertake two sensitivity analyses with respect to the spot price and the long-term price. The former is shown to have a stronger impact on the field's value than the latter. This bodes well with the usual time profile of tight oil production: intense depletion initially, followed by steep decline thereafter. © 2016 by the authors; licensee MDPI.}}, 
pages = {848}, 
number = {10}, 
volume = {9}
}
@article{10.1080/03610918.2012.756911, 
year = {2014}, 
title = {{Alternative approximations to value-at-risk: A comparison}}, 
author = {Lien, Donald and Yang, Xiaobin and Ye, Keying}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2012.756911}, 
abstract = {{This article compares three value-at-risk (VaR) approximation methods suggested in the literature: Cornish and Fisher (1937), Sillitto (1969), and Liu (2010). Simulation results are obtained for three families of distributions: student-t, skewed-normal, and skewed-t. We recommend the Sillitto approximation as the best method to evaluate the VaR when the financial return has an unknown, skewed, and heavy-tailed distribution. Copyright © 2014 Taylor \& Francis Group, LLC.}}, 
pages = {2225--2240}, 
number = {10}, 
volume = {43}
}
@article{10.1007/s10696-018-9316-z, 
year = {2019}, 
title = {{A branch-and-bound approach for the single machine maximum lateness stochastic scheduling problem to minimize the value-at-risk}}, 
author = {Urgo, M. and Váncza, J.}, 
journal = {Flexible Services and Manufacturing Journal}, 
issn = {19366582}, 
doi = {10.1007/s10696-018-9316-z}, 
abstract = {{The research in the field of robust scheduling aims at devising schedules which are not sensitive—to a certain extent—to the disruptive effects of unexpected events. Nevertheless, the protection of the schedule from rare events causing heavy losses is still a challenging aim. The paper presents a novel approach for protecting the quality of a schedule by assessing the risk associated to the different scheduling decisions. The approach is applied to a stochastic scheduling problem with a set of jobs to be sequenced on a single machine. The release dates and processing times of the jobs are generally distributed independent random variables, while the due dates are deterministic. A branch-and-bound approach is taken to minimise the value-at-risk of the distribution of the maximum lateness. The viability of the approach is demonstrated through a computational experiment and the application to an industrial problem in the tool making industry. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {472--496}, 
number = {2}, 
volume = {31}
}
@article{10.1007/978-3-642-01044-6-44, 
year = {2010}, 
title = {{The analysis of power for some chosen VaR backtesting procedures: Simulation approach}}, 
author = {}, 
issn = {14318814}, 
doi = {10.1007/978-3-642-01044-6-44}, 
abstract = {{Everyone who measures the market risk using the Value at Risk (VaR) approach should test if the assumed model is correct. This procedure is called back-testing. There are many different tests available, but usually risk managers are not concerned about their power. The aim of this paper is to analyze some chosen backtesting methods focusing on the problem of power of the tests and limited data sets. The paper is organized as follows. At the beginning a financial aspect of the analyzed problem is presented very briefly. The second part gives information about some chosen, but (in the author's opinion) the most popular backtests. The main attention is paid to tests based on the frequency of failures and on multiple VaR levels. Next, the results of the simulations are presented. The last part summarizes obtained results and gives hints for the optimal backtesting. © Springer-Verlag Berlin Heidelberg 2010.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jfineco.2020.05.010, 
year = {2021}, 
title = {{Volatility, intermediaries, and exchange rates}}, 
author = {Fang, Xiang and Liu, Yang}, 
journal = {Journal of Financial Economics}, 
issn = {0304405X}, 
doi = {10.1016/j.jfineco.2020.05.010}, 
abstract = {{We propose and estimate a quantitative model of exchange rates in which participants in the foreign exchange market are intermediaries subject to value-at-risk (VaR) constraints. Higher volatility translates into tighter VaR constraints, and intermediaries require higher returns to hold foreign assets. Therefore, the foreign currency is expected to appreciate. The model quantitatively resolves the Backus–Smith puzzle, the forward premium puzzle, and the exchange rate volatility puzzle and explains deviations from the covered interest rate parity. Moreover, the model implies both contemporaneous and predictive relations between proxies of leverage constraint tightness and exchange rates. These implications are supported in the data. © 2021 Elsevier B.V.}}, 
pages = {217--233}, 
number = {1}, 
volume = {141}
}
@article{10.1002/oca.2061, 
year = {2014}, 
title = {{Value at risk for confidence level quantifications in robust engineering optimization}}, 
author = {Mohammadi, Bijan}, 
journal = {Optimal Control Applications and Methods}, 
issn = {01432087}, 
doi = {10.1002/oca.2061}, 
abstract = {{We show how to introduce the Value at Risk (VaR) concept in optimization algorithms with emphasis in calculation complexity issues. To do so, we assume known the PDF of the uncertainties. Our aim is to quantify our confidence on the optimal solution at low complexity without sampling of the control space. The notion of over-solving appears naturally where it becomes useless to solve accurately near an optimum when the variations in control parameters fall below the uncertainties. Examples show the behavior of this VaR-based correction and link the approach to momentum-based optimization where the mean and variance of a functional are considered. The approach is then applied to an inverse problem with fluids with uncertainties in the definition of the injection devices. It is shown that an optimization problem with an admissible solution in the control space in the deterministic case can lose its solution in the presence of uncertainties on the control parameters, which suggests that the control space itself should be redefined in such a situation to recover an admissible problem. This permits to evaluate the cost of making reliable a system that has been deterministically designed but has uncertain parameters. A shape optimization problem closes the paper showing the importance of including VaR information during the design iterations and not only at the end of the procedure. Copyright © 2013 John Wiley \& Sons, Ltd.}}, 
pages = {179--190}, 
number = {2}, 
volume = {35}
}
@article{10.1016/j.physa.2019.121277, 
year = {2019}, 
title = {{On a new Pareto-type distribution with applications in the study of income inequality and risk analysis}}, 
author = {Sarabia, José María and Jordá, Vanesa and Prieto, Faustino}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.121277}, 
abstract = {{A recent paper published in this journal (Bourguignon et al., 2016) introduces and studies a new Pareto-type distribution, deriving some of its probabilistic and inferential properties. In this paper, we obtain additional and important properties of this distribution and derive simpler expressions for relevant inequality indices and risk measures. One important feature of this distribution is that many of these expressions are simple functions of the incomplete beta function. Using this result, we obtain closed expressions for the moments and for several inequality measures including the Gini index and the Lorenz curve, the Donaldson–Weymark–Kakwani index, the Pietra index and the generalized entropy measures. Basic risk measures are also obtained. Finally, empirical applications with real income data are given. © 2019 Elsevier B.V.}}, 
pages = {121277}, 
number = {NA}, 
volume = {527}
}
@article{10.1016/j.cam.2015.12.008, 
year = {2016}, 
title = {{Portfolio selection problem with Value-at-Risk constraints under non-extensive statistical mechanics}}, 
author = {Zhao, Pan and Xiao, Qingxian}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2015.12.008}, 
abstract = {{The optimal portfolio selection problem is a major issue in the financial field in which the process of asset prices is usually modeled by a Wiener process. That is, the return distribution of the asset is normal. However, several empirical results have shown that the return distribution of the asset has the characteristics of fat tails and aiguilles and is not normal. In this work, we propose an optimal portfolio selection model with a Value-at-Risk (VaR) constraint in which the process of asset prices is modeled by the non-extensive statistical mechanics instead of the classical Wiener process. The model can describe the characteristics of fat tails and aiguilles of returns. Using the dynamic programming principle, we derive a Hamilton-Jacobi-Bellman (HJB) equation. Then, employing the Lagrange multiplier method, we obtain closed-form solutions for the case of logarithmic utility. Moreover, the empirical results show that the price process can more accurately fit the empirical distribution of returns than the familiar Wiener process. In addition, as the time increases, the constraint becomes binding. That is, to control the risk the agent reduces the proportion of the wealth invested in the risky asset. Furthermore, at the same confidence level, the agent reduces the proportion of the wealth invested in the risky asset more quickly under our model than under the model based on the Wiener process. This can give investors a good decision-making reference. © 2015 Elsevier B.V. All rights reserved.}}, 
pages = {64--71}, 
number = {NA}, 
volume = {298}
}
@article{10.1080/1351847x.2014.946528, 
year = {2016}, 
title = {{Pension plan solvency and extreme market movements: a regime switching approach}}, 
author = {Abourashchi, Niloufar and Clacher, Iain and Freeman, Mark C. and Hillier, David and Kemp, Malcolm and Zhang, Qi}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2014.946528}, 
abstract = {{We develop and test a new approach to assess defined benefit (DB) pension plan solvency risk in the presence of extreme market movements. Our method captures both the ‘fat-tailed’ nature of asset returns and their correlation with discount rate changes. We show that the standard assumption of constant discount rates leads to dramatic underestimation of future projections of pension plan solvency risk. Failing to incorporate leptokurtosis into asset returns also leads to downward biased estimates of risk, but this is less pronounced than the time-varying discount rate effect. Further modifying the model to capture the correlation between asset returns and the discount rate provides additional improvements in the projection of future pension plan solvency. This reduces the perceived future risk of underfunding because of the negative correlation between interest rate changes and asset returns. These results have important implications for those with responsibility for balancing risk against expected return when seeking to improve the current poor funding positions of DB pension schemes. © 2014 The Author(s). Published by Taylor \& Francis.}}, 
pages = {1--28}, 
number = {13}, 
volume = {22}
}
@article{10.1002/for.2381, 
year = {2016}, 
title = {{Multiple Hypothesis Testing of Market Risk Forecasting Models}}, 
author = {Esposito, Francesco P. and Cummins, Mark}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2381}, 
abstract = {{Extending previous risk model backtesting literature, we construct multiple hypothesis testing (MHT) with the stationary bootstrap. We conduct multiple tests which control for the generalized confidence level and employ the bootstrap MHT to design multiple comparison testing. We consider absolute and relative predictive ability to test a range of competing risk models, focusing on value-at-risk and expected shortfall (ExS). In devising the test for the absolute predictive ability, we take the route of recent literature and construct balanced simultaneous confidence sets that control for the generalized family-wise error rate, which is the joint probability of rejecting true hypotheses. We implement a step-down method which increases the power of the MHT in isolating false discoveries. In testing for the ExS model predictive ability, we design a new simple test to draw inference about recursive model forecasting capability. In the second suite of statistical testing, we develop a novel device for measuring the relative predictive ability in the bootstrap MHT framework. The device, which we coin multiple comparison mapping, provides a statistically robust instrument designed to answer the question: ‘Which model is the best model?’ Copyright © 2016 John Wiley \& Sons, Ltd. Copyright © 2016 John Wiley \& Sons, Ltd.}}, 
pages = {381--399}, 
number = {5}, 
volume = {35}
}
@article{10.1007/978-3-319-03395-2_16, 
year = {2014}, 
title = {{A vine copula approach for analyzing financial risk and co-movement of the Indonesian, Philippine and Thailand stock markets}}, 
author = {Sriboonchitta, Songsak and Liu, Jianxu and Kreinovich, Vladik and Nguyen, Hung T.}, 
journal = {Advances in Intelligent Systems and Computing}, 
issn = {21945357}, 
doi = {10.1007/978-3-319-03395-2\_16}, 
abstract = {{This paper aims at analyzing the financial risk and co-movement of stock markets in three countries: Indonesia, Philippine and Thailand. It consists of analyzing the conditional volatility and test the leverage effect in the stock markets of the three countries. To capture the pairwise and conditional dependence between the variables, we use the method of vine copulas. In addition, we illustrate the computations of the value at risk and the expected shortfall using Monte Carlo simulation with copula based GJR-GARCH model. The empirical evidence shows that all the leverage effects add much to the capacity for explanation of the three stock returns, and that the D-vine structure is more appropriate than the C-vine one for describing the dependence of the three stock markets. In addition, the value at risk and ES provide the evidence to confirm that the portfolio may avoid risk in significant measure. © Springer International Publishing Switzerland 2014.}}, 
pages = {245--257}, 
number = {NA}, 
volume = {251}
}
@article{10.1109/ieom.2015.7093732, 
year = {2015}, 
title = {{Maintenance schedule optimization based on failure probability distribution}}, 
author = {Tezuka, Masaru and Munakata, Satoshi and Sawada, Mikiko}, 
journal = {2015 International Conference on Industrial Engineering and Operations Management (IEOM)}, 
issn = {NA}, 
doi = {10.1109/ieom.2015.7093732}, 
abstract = {{Organizations related to infrastructure, such as utilities and railway companies, manage a large number of facilities, the failure of which can have a huge impact on society. The cost of maintaining these facilities is a combination of regular maintenance costs and urgent recovery costs. Generally, the urgent costs are much higher than regular costs. Regular maintenance work should result in fewer sudden failures, and thus reduce these urgent costs. However, if the regular maintenance is too frequent, its cost becomes too high. Therefore, it is important to balance the regular and urgent costs to minimize the overall maintenance cost. We propose a maintenance schedule optimization method based on the failure probability distribution of the facilities. The total cost is mathematically modeled, with the regular maintenance schedule included via decision variables and the occurrence of failures modeled as stochastic variables. The stochastic total maintenance costs are evaluated using a Monte Carlo method, and a genetic algorithm is employed to optimize the maintenance schedule. The proposed method is evaluated using data provided by a Japanese railway company, and our results confirm that the method produces an excellent maintenance schedule. A statistical test shows there is a significant difference between the proposed and conventional methods. © 2015 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.econmod.2014.07.010, 
year = {2014}, 
title = {{A semiparametric approach to value-at-risk, expected shortfall and optimum asset allocation in stock-bond portfolios}}, 
author = {Chen, Xiangjin B. and Silvapulle, Param and Silvapulle, Mervyn}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2014.07.010}, 
abstract = {{This paper investigates stock-bond portfolios' tail risks such as value-at-risk (VaR) and expected shortfall (ES), and the way in which these measures have been affected by the global financial crisis. The semiparametric t-copulas adequately model stock-bond returns joint distributions of G7 countries and Australia. Empirical results show that the (negative) weak stock-bond returns dependence has increased significantly for seven countries after the crisis, except for Italy. However, both VaR and ES have increased for all eight countries. Before the crisis, the minimum portfolio VaR and ES were achieved at an interior solution only for the US, the UK, Australia, Canada and Italy. After the crisis, the corner solution was found for all eight countries. Evidence of "flight to quality" and "safety first" investor behaviour was strong, after the global financial crisis. The semiparametric t-copula adequately forecasts the outer-sample VaR. These findings have implications for global financial regulators and the Basel Committee, whose central focus is currently on increasing the capital requirements as a consequence of the recent global financial crisis. © 2014.}}, 
pages = {230--242}, 
number = {NA}, 
volume = {42}
}
@article{10.1016/j.ijpe.2008.09.014, 
year = {2009}, 
title = {{Multi-product newsvendor problem with value-at-risk considerations}}, 
author = {Özler, Aysun and Tan, Barış and Karaesmen, Fikri}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2008.09.014}, 
abstract = {{We consider the single period stochastic inventory (newsvendor) problem with downside risk constraints. The aim in the classical newsvendor problem is maximizing the expected profit. This formulation does not take into account the risk of earning less than a desired target profit or losing more than an acceptable level due to the randomness of demand. We utilize Value at Risk (VaR) as the risk measure in a newsvendor framework and investigate the multi-product newsvendor problem under a VaR constraint. To this end, we first derive the exact distribution function for the two-product newsvendor problem and develop an approximation method for the profit distribution of the N-product case (N>2). A mathematical programming approach is used to determine the solution of the newsvendor problem with a VaR constraint. This approach allows us to handle a wide range of cases including the correlated demand case that yields new results and insights. The accuracy of the approximation method and the effects of the system parameters on the solution are investigated numerically. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {244--255}, 
number = {2}, 
volume = {117}
}
@article{10.1007/s11156-007-0038-7, 
year = {2007}, 
title = {{Value relevance of value-at-risk disclosure}}, 
author = {Lim, Chee Yeow and Tan, Patricia Mui-Siang}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-007-0038-7}, 
abstract = {{The SEC issued FRR No. 48 in 1997 to enhance public disclosure of firms' exposures to market risk. We examine whether the quantitative value-at-risk (VAR) estimates disclosed by 81 non-financial firms during the period 1997-2002 are value-relevant using the earnings-returns relation. The empirical results indicate that high VAR is associated with weaker earnings-returns relation. Further analysis shows that VAR is positively and significantly associated with future stock return volatility. Our evidence suggests that investors perceive the earnings of firms with substantial market risk exposure to be less persistent, and adjust the future abnormal earnings for the higher risk exposure. Thus, this results in a lower expected rate of return. © 2007 Springer Science+Business Media, LLC.}}, 
pages = {353--370}, 
number = {4}, 
volume = {29}
}
@article{10.1007/978-3-662-47200-2_12, 
year = {2015}, 
title = {{Quantile estimation using a combination of stratified sampling and control variates}}, 
author = {Nakayama, Marvin K.}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-3-662-47200-2\_12}, 
abstract = {{Quantiles are used to measure risk in many application areas. We consider simulation methods for estimating a quantile using a variance-reduction technique that combines stratified sampling and control variates. We provide an asymptotically valid confidence interval for the quantile. © Springer-Verlag Berlin Heidelberg 2015.}}, 
pages = {105--114}, 
number = {NA}, 
volume = {349}
}
@article{10.1080/03610926.2018.1528364, 
year = {2019}, 
title = {{Pareto-optimal reinsurance for both the insurer and the reinsurer with general premium principles}}, 
author = {Fang, Ying and Wang, Xia and Liu, Hongli and Li, Tong}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2018.1528364}, 
abstract = {{In this paper, we study Pareto-optimal reinsurance policies from the perspectives of an insurer and a reinsurer, assuming reinsurance premium principles satisfy risk loading and stop-loss ordering preserving. By geometric approach, we determine the forms of the optimal policies among two classes of ceded loss functions, the class of increasing convex ceded loss functions and the class that the constraints on both ceded and retained loss functions are relaxed to increasing functions. Then we demonstrate the applicability of our results by giving the parameters of the optimal ceded loss functions under Dutch premium principle and Wang’s premium principle. © 2018, © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--21}, 
number = {24}, 
volume = {48}
}
@article{10.1093/jjfinec/nbm019, 
year = {2008}, 
title = {{Nonparametric estimation of expected shortfall}}, 
author = {Chen, Song Xi}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbm019}, 
abstract = {{The expected shortfall is an increasingly popular risk measure in financial risk management and it possesses the desired sub-additivity property, which is lacking for the value at risk (VaR). We consider two nonparametric expected shortfall estimators for dependent financial losses. One is a sample average of excessive losses larger than a VaR. The other is a kernel smoothed version of the first estimator (Scaillet, 2004 Mathematical Finance), hoping that more accurate estimation can be achieved by smoothing. Our analysis reveals that the extra kernel smoothing does not produce more accurate estimation of the shortfall. This is different from the estimation of the VaR where smoothing has been shown to produce reduction in both the variance and the mean square error of estimation. Therefore, the simpler ES estimator based on the sample average of excessive losses is attractive for the shortfall estimation. © The Author 2007. Published by Oxford University Press. All rights reserved.}}, 
pages = {87--107}, 
number = {1}, 
volume = {6}
}
@article{10.1002/for.1183, 
year = {2011}, 
title = {{Forecasting time-varying covariance with a robust Bayesian threshold model}}, 
author = {Wu, Chih‐Chiang and Lee, Jack C.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1183}, 
abstract = {{This paper proposes a robust multivariate threshold vector autoregressive model with generalized autoregressive conditional heteroskedasticities and dynamic conditional correlations to describe conditional mean, volatility and correlation asymmetries in financial markets. In addition, the threshold variable for regime switching is formulated as a weighted average of endogenous variables to eliminate excessively subjective belief in the threshold variable decision and to serve as the proxy in deciding which market should be the price leader. The estimation is performed using Markov chain Monte Carlo methods. Furthermore, several meaningful criteria are introduced to assess the forecasting performance in the conditional covariance matrix. The proposed methodology is illustrated using daily S\&P500 futures and spot prices. © 2010 John Wiley \& Sons, Ltd.}}, 
pages = {451--468}, 
number = {5}, 
volume = {30}
}
@article{10.1109/clei47609.2019.235095, 
year = {2019}, 
title = {{Value-at-risk prediction for the Brazilian stock market: A comparative study between Parametric Method, Feedforward and LSTM Neural Network}}, 
author = {Reghin, Daniel and Lopes, Fábio}, 
journal = {2019 XLV Latin American Computing Conference (CLEI)}, 
issn = {NA}, 
doi = {10.1109/clei47609.2019.235095}, 
abstract = {{Value-at-Risk (VaR) as a risk quantification mechanism has more than one calculation method, one of which is the parametric method. In the bibliographic study, it was identified that the parametric method is not effective for all market moments, such as those of crisis or abrupt changes in behavior. This study, therefore, seeks to verify whether other methods of calculation are more efficient, such as the use of neural networks. This study compared the VaR calculation using the parametric method against the use of Feedforward neural networks and Long Short-Term Memory (LSTM) recurrent networks. It used the B3 São Paulo Stock Exchange, IBOVESPA, as the index of study. For the parametric method, volatility models such as standard deviation, Exponentially Weighted Moving Average (EWMA) and Generalized Autoregressive Conditional Heteroskedasticity (GARCH) were tested. For neural networks, different layers and amounts of neurons, activation functions, use of different predictors and incorporation of macroeconomic data were explored. The result of the experiment showed that LSTM networks had a better performance when comparing the exception rate generated by the entire model. When analyzing periods of crisis or abrupt changes in behavior, LSTM and Feedforward networks were less efficient in predicting VaR compared to the parametric method. © 2019 IEEE.}}, 
pages = {1--11}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eneco.2008.12.006, 
year = {2009}, 
title = {{CAViaR-based forecast for oil price risk}}, 
author = {Huang, Dashan and Yu, Baimin and Fabozzi, Frank J. and Fukushima, Masao}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2008.12.006}, 
abstract = {{As a benchmark for measuring market risk, value-at-risk (VaR) reduces the risk associated with any kind of asset to just a number (amount in terms of a currency), which can be well understood by regulators, board members, and other interested parties. This paper employs a new VaR approach due to Engle and Manganelli [Engle, R.F., Manganelli, S., 2004. CAViaR: Conditional Autoregressive Value at Risk by Regression Quantiles. Journal of Business and Economic Statistics 22, 367-381] to forecasting oil price risk. In doing so, we provide two original contributions by introducing a new exponentially weighted moving average CAViaR model and developing a mixed data regression model for multi-period VaR prediction. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {511--518}, 
number = {4}, 
volume = {31}
}
@article{10.32479/ijeep.7253, 
year = {2019}, 
title = {{Do long-memory GARCH-type-value-at-risk models outperform none-and semi-parametric value-at-risk models?}}, 
author = {}, 
issn = {21464553}, 
doi = {10.32479/ijeep.7253}, 
abstract = {{As a result of the 2007-2008 global financial crisis, traditional value-at-risk (VaR) models used to measure the market risk have been criticised for their inaccuracy. Therefore, alternative models such as long-memory GARCH-type based VaR models have been receiving increased attention in recent literature. In this regard, this study compares the one-day-ahead out-of-sample VaR forecasting performances of FIGARCH, HYGARCH, and FIAPARCH models under normal, student t, and skewed student t distribution assumptions with FHS and HS model performances, which are the most commonly applied models especially by commercial banks in practice, for eight different financial variables including energy commodities (West intermediate crude oil and New York Harbour Conventional Gasoline regular (NYHCGR)), stock indices (NIKKEI 225 stock market index and TSEC weighted stock index), foreign exchange rates (Euro/US Dollar (EUR/USD) and Japanese Yen/USD (JPY/USD)), and precious metals (gold and copper). Results clearly show that the FHS model is the most appropriate model for long trading positions, to which the relevant literature has paid more attention, whereas for short trading positions the HYGARCH model under skewed student t distribution assumption should be preferred. © 2019, Econjournals. All rights reserved.}}, 
number = {2}, 
volume = {9}
}
@article{10.1142/s0219024907004184, 
year = {2007}, 
title = {{The relative risk performance of Islamic finance: A new guide to less risky investments}}, 
author = {AL-ZOUBI, HAITHAM A and MAGHYEREH, AKTHAM I}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024907004184}, 
abstract = {{We examine the relative risk performance of the Dow Jones Islamic Index (DJIS) and find that the index outperforms the Dow Jones (DJIM) WORLD Index in terms of risk. Using the most recent Value-at-Risk (VaR) methodologies (RiskMetrics, Student-t APARCH, and skewed Student-t APARCH) on the 1996-2005 period, and assuming one-day holding period for both indices with a moving window of 500 day data, we show that the value of VaR is greater for DJIM WORLD than for DJIS Islamic. We interpret the results mainly to the profit-and-loss sharing principle of Islamic finance where banks share the profits and bear losses (Mudarabah) or share both profits and losses (Musharaka) with the firm. © World Scientific Publishing Company.}}, 
pages = {235--249}, 
number = {2}, 
volume = {10}
}
@article{10.1016/j.jbankfin.2005.10.002, 
year = {2006}, 
title = {{On time-scaling of risk and the square-root-of-time rule}}, 
author = {Daníelsson, Jón and Zigrand, Jean-Pierre}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.10.002}, 
abstract = {{Many financial applications, such as risk analysis, and derivatives pricing, depend on time scaling of risk. A common method for this purpose is the square-root-of-time rule where an estimated quantile of a return distribution is scaled to a lower frequency by the square root of the time horizon. This paper examines time scaling of quantiles when returns follow a jump diffusion process. We demonstrate that when jumps represent losses, the square-root-of-time rule leads to a systematic underestimation of risk, whereby the degree of underestimation worsens with the time horizon, the jump intensity and the confidence level. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {2701--2713}, 
number = {10}, 
volume = {30}
}
@article{10.2298/pan1201059o, 
year = {2012}, 
title = {{An analysis of exchange rate risk exposure related to the public debt portfolio of Tunisia: Beyond vaR approach [Une analyse de l'exposition au risque de change du portefeuille de la dette publique de la Tunisie: Application de l'approche VaR]}}, 
author = {Omrane, Samia}, 
journal = {Panoeconomicus}, 
issn = {1452595X}, 
doi = {10.2298/pan1201059o}, 
abstract = {{The aim of this study is to assess the exchange rate risk associated with the Tunisian public debt portfolio through Value-at-Risk (VaR) methodology. We use daily spot exchange rates of the Tunisian dinar against the three main debt currencies, the dollar, the euro and the yen. Our period of interest is from 02/01/2004 to 31/12/2008. Thetas and Marginal VaR analysis reveal that Japanese yen is the most risky currency constituting the Tunisian public debt portfolio. American dollar appears as a source of risk for the Tunisian external debt but remains less risky than the yen, while, the euro constitutes a hedge currency for exchange risk management associated with the Tunisian public debt portfolio.}}, 
pages = {59--87}, 
number = {1}, 
volume = {59}
}
@article{10.3390/su13137402, 
year = {2021}, 
title = {{Study on the value model of urban green infrastructure development—a case study of the central district of taichung city}}, 
author = {Hsu, Kuo-Wei and Chao, Jen-Chih}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su13137402}, 
abstract = {{Urban green infrastructure has become an important concept for sustainable urban devel-opment. Regarding the joining up of green spaces into green networks, it can have major positive impacts on the environment, societies and economies, and ecology. This study proposes a value model for investing in urban green infrastructure, with impact factors including land use value, energy conservation value, carbon reduction, and disaster prevention value. It establishes that through the interaction between all four of these factors, urban green infrastructure investment increases net operating income. Additionally, as disaster prevention value increases, urban disaster risk declines, and this has an important positive effect on overall value. Our modeling also indicates that in the face of climatic extremes, the construction of urban green infrastructure is increasingly important, particularly in terms of energy value and disaster prevention value. Specific incentives and catalysts for promoting investment in urban green infrastructure are proposed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {7402}, 
number = {13}, 
volume = {13}
}
@article{10.1016/j.jempfin.2006.02.001, 
year = {2007}, 
title = {{Value-at-Risk analysis for long-term interest rate futures: Fat-tail and long memory in return innovations}}, 
author = {Wu, Ping-Tsung and Shieh, Shwu-Jane}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2006.02.001}, 
abstract = {{This article uses the FIGARCH(1,d,1) models to calculate daily Value-at-Risk (VaR) for T-bond interest rate futures returns of long and short trading positions based on the normal, Student-t, and skewed Student-t innovations distributions. The empirical results show that based on Kupiec LR failure rate tests, in-sample and out-of-sample VaR values calculated using FIGARCH(1,d,1) model with skewed Student-t innovations are more accurate than those generated using traditional GARCH(1,1) models. Moreover, we find that the in-sample values of VaR are subject to a significant positive bias, as pointed out by Inui et al. [Inui, K., Kijima, M., Kitano, A., 2003. VaR is subject to a significant positive bias, working paper]. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {248--259}, 
number = {2}, 
volume = {14}
}
@article{10.1016/j.econmod.2013.02.008, 
year = {2013}, 
title = {{The analysis of bank business performance and market risk-Applying Fuzzy DEA}}, 
author = {Chen, Yu-Chuan and Chiu, Yung-Ho and Huang, Chin-Wei and Tu, Chien Heng}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2013.02.008}, 
abstract = {{In the fast changing financial circumstances of nowadays, in avoiding the crisis of closing down, financial institutions are concerned about the efficiency and risk strictly in the meantime. Therefore, efficiency and risk management are goals for a financial institution administrator. Data Envelopment Analysis (DEA) is a non-parameter approach to evaluate the performance of DMU's efficiency and the variables used in the DEA are all accurate values. However, when the input or output variables are fuzzy, the performance of DMUs must proceed by the Fuzzy-DEA. On the basis of risk uncertainty, this research plans to apply the expanding model of Fuzzy Slack-Based Measurement (Fuzzy SBM). The efficiency scores estimated by Fuzzy SBM model are subordinate to functional form, which provides efficiency value region in different degrees of confidence, conforms to the characteristic of risk anticipation, and estimates the management achievement of Taiwan banking under market risk. © 2013 Elsevier B.V.}}, 
pages = {225--232}, 
number = {1}, 
volume = {32}
}
@article{10.1016/j.jclepro.2017.07.101, 
year = {2017}, 
title = {{Power demand risk models on milling machines}}, 
author = {Jeon, Hyun Woo and Lee, Seokgi and Kargarian, Amin and Kang, Yuncheol}, 
journal = {Journal of Cleaner Production}, 
issn = {09596526}, 
doi = {10.1016/j.jclepro.2017.07.101}, 
abstract = {{The measurement of power demand risks in manufacturing power systems will benefit manufacturers and the wider society. If the risks can be characterized using manufacturing parameters, manufacturers can better control risks originating in those parameters, select less power-risky production plans, and reduce utility costs and resource consumption. The measurement of risk can also help manufacturers and power suppliers to protect their power systems from unexpected disturbances. Existing measures of risk, however, do not consider time duration, and thus cannot accurately quantify the risks in manufacturing power systems; the risks of a period of high power demand must be evaluated with the duration of the surge. Therefore, new methods of measuring power demand risks are proposed, adapting measures drawn from the field of finance. With a focus on milling operations, processing power is shown to be a function of processing amount (A) and processing time (T), and a power demand distribution is directly derived as a joint distribution of A and T. A bivariate random variable model with copulas is applied to examine the correlation in the joint distribution. Then, based on evaluation of a probability distribution of power demand from A and T, new risk measures are introduced. Illustrative examples are provided to show how the proposed measures can quantify the power demand risks from milling machines, based on manufacturing parameters. Certain manufacturing parameters are found to affect overall power demand risks, including i) raw material type, ii) variability in processing time, and iii) correlation between A and T. In the examples, these three factors increase power demand risks by up to 108\%, 67\%, and 1\% respectively. © 2017}}, 
pages = {1215--1228}, 
number = {NA}, 
volume = {165}
}
@article{10.1016/j.ecosta.2020.12.001, 
year = {2021}, 
title = {{Bayesian inference for a single factor copula stochastic volatility model using Hamiltonian Monte Carlo}}, 
author = {Kreuzer, Alexander and Czado, Claudia}, 
journal = {Econometrics and Statistics}, 
issn = {24523062}, 
doi = {10.1016/j.ecosta.2020.12.001}, 
abstract = {{For modeling multivariate financial time series a single factor copula model with stochastic volatility margins is proposed. It generalizes single factor models based on the multivariate normal distribution by allowing for symmetric and asymmetric tail dependence. A joint Bayesian approach using Hamiltonian Monte Carlo (HMC) within Gibbs sampling is developed. Thus, the information loss caused by the two-step approach for margins and dependence is avoided. Further, the Bayesian approach is tractable in high dimensional parameter spaces in addition to uncertainty quantification through credible intervals. By introducing indicators for different copula families the copula families are selected automatically in the Bayesian framework. In a first simulation study the performance of HMC for the copula part is compared to a procedure based on adaptive rejection Metropolis sampling within Gibbs sampling. It is shown that HMC considerably outperforms this alternative approach in terms of effective sample size per minute. In a second simulation study satisfactory performance is seen for the full HMC within Gibbs procedure. The approach is illustrated for a portfolio of financial assets with respect to one-day ahead value at risk forecasts. Comparison to a two-step estimation procedure and to relevant benchmark models, such as a multivariate factor stochastic volatility model, shows superior performance of the proposed approach. © 2021 EcoSta Econometrics and Statistics}}, 
pages = {130--150}, 
number = {NA}, 
volume = {19}
}
@article{10.1080/13518470600763737, 
year = {2007}, 
title = {{Extreme risk and value-at-risk in the German stock market}}, 
author = {Tolikas, Konstantinos and Koulakiotis, Athanasios and Brown, Richard A.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/13518470600763737}, 
abstract = {{Extreme Value Theory methods are used to investigate the distribution of the extreme minima in the German stock market over the period 1973 to 2001. Innovative aspects of this paper include (i) a wide set of distributions considered, (ii) L-moment diagrams employed to identify the most appropriate distribution/s, (iii) 'probability weighted moments' used to estimate the parameters of these distribution/s and (iv) the Anderson-Darling goodness of fit test employed to test the adequacy of fit. The 'generalized logistic' distribution is found to provide adequate descriptions of the extreme minima of the German stock market over the period studied. VaR analysis results show that the EVT methods used in this study can be particularly useful for market risk measurement since they produce estimates that outperform those derived by traditional methods at high confidence levels.}}, 
pages = {373--395}, 
number = {4}, 
volume = {13}
}
@article{10.1007/s00780-003-0105-4, 
year = {2004}, 
title = {{Optimal portfolios when stock prices follow an exponential Lévy process}}, 
author = {Emmer, Susanne and Klüppelberg, Claudia}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-003-0105-4}, 
abstract = {{We investigate some portfolio problems that consist of maximizing expected terminal wealth under the constraint of an upper bound for the risk, where we measure risk by the variance, but also by the Capital-at-Risk (CaR). The solution of the mean-variance problem has the same structure for any price process which follows an exponential Lévy process. The CaR involves a quantile of the corresponding wealth process of the portfolio. We derive a weak limit law for its approximation by a simpler Levy process, often the sum of a drift term, a Brownian motion and a compound Poisson process. Certain relations between a Lev́y process and its stochastic exponential are investigated. © Springer-Verlag 2004.}}, 
pages = {17--44}, 
number = {1}, 
volume = {8}
}
@article{10.1016/j.fuel.2021.120631, 
year = {2021}, 
title = {{Impact of the forecast price on economic results for methanol production from olive waste}}, 
author = {Puig-Gamero, M. and Trapero, J.R. and Pedregal, D.J. and Sánchez, P. and Sanchez-Silva, L.}, 
journal = {Fuel}, 
issn = {00162361}, 
doi = {10.1016/j.fuel.2021.120631}, 
abstract = {{The development of circular economies due to the limitation of natural resources is becoming a common strategy of paramount importance among different countries. In Spain, given the strategic nature of its olive industry, trying to value one of its main residuals (olive pomace) alone or together with other residues through its chemical transformation in methanol is a promising research line. One of the key variables that would make the investment advisable or not is the future value of the methanol price. However, most of the literature do not consider its future price volatility on the economic evaluation of the chemical processes. This work bridges that gap by proposing three econometric models based on Unobserved Components to forecast the methanol price over the life cycle plant. Those probabilistic forecasts feed a Monte Carlo simulation that provides an exhaustive investment risk assessment in terms of Net Present Value, Internal Rate of Return and Value at Risk metrics. The results showed the relationship between forecasting models and the investment profitability with an average Internal Rate of Return ranging from 23\% to 31\%. Additionally, the previous analysis was completed by adding other variables subject to uncertainty (olive pomace feed, capital investment, feedstock price, labor costs and discount rate). In this case, assuming a potential underestimation error up to 100\% of the capital cost, the probability of obtaining a profitable investment was significantly reduced ranging the Value at Risk from 48\% to 98\%. © 2021}}, 
pages = {120631}, 
number = {NA}, 
volume = {295}
}
@article{10.1002/for.2796, 
year = {2021}, 
title = {{Singular spectrum analysis for value at risk in stochastic volatility models}}, 
author = {Arteche, Josu and García‐Enríquez, Javier}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2796}, 
abstract = {{Estimation of the value at risk (VaR) requires prediction of the future volatility. Whereas this is a simple task in ARCH and related models, it becomes much more complicated in stochastic volatility (SV) processes where the volatility is a function of a latent variable that is not observable. In-sample (present and past values) and out-of-sample (future values) predictions of that unobservable variable are thus necessary. This paper proposes singular spectrum analysis (SSA), which is a fully nonparametric technique that can be used for both purposes. A combination of traditional forecasting techniques and SSA is also considered to estimate the VaR. Their performance is assessed in an extensive Monte Carlo and with an application to a daily series of S\&P500 returns. © 2021 John Wiley \& Sons, Ltd.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/itap.2011.6006180, 
year = {2011}, 
title = {{Research of the RMB exchange rate fluctuations and the value at risk}}, 
author = {Yun-sheng, Qiao and Zuo-xing, Huang}, 
journal = {2011 International Conference on Internet Technology and Applications}, 
issn = {NA}, 
doi = {10.1109/itap.2011.6006180}, 
abstract = {{This paper selects the middle exchange rate of RMB/Dollar as the research object; the daily returns are computed using GARCH model, and the value at risk is calculated using parameters and half parameters. The results show that, IGARCH model is a good description of the characteristics of volatility of the middle exchange rate of RMB/Dollar; and the semi-parametric method is better than the parameter method at the measurement of VaR. This will help us choose the right methods to monitor the risk of exchange rate fluctuations. © 2011 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jempfin.2013.05.002, 
year = {2013}, 
title = {{Value at risk forecasts by extreme value models in a conditional duration framework}}, 
author = {Herrera, Rodrigo and Schipp, Bernhard}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2013.05.002}, 
abstract = {{The analysis of extremes in financial return series is often based on the assumption of independent and identically distributed observations. However, stylized facts such as clustered extremes and serial dependence typically violate the assumption of independence. This has been the main motivation to propose an approach that is able to overcome these difficulties by considering the time between extreme events as a stochastic process. One of the advantages of the method consists in its capability to capture the short-term behavior of extremes without involving an arbitrary stochastic volatility model or a prefiltration of the data, which would certainly affect the estimate. We make use of the proposed model to obtain an improved estimate for the value at risk (VaR). The model is then compared to various competing approaches such as Engle and Marianelli's CAViaR and the GARCH-EVT model. Finally, we present a comparative empirical illustration with transaction data from Bayer AG, a typical blue chip stock from the German stock market index DAX, the DAX index itself and a hypothetical portfolio of international equity indexes already used by other authors. © 2013 Elsevier B.V.}}, 
pages = {33--47}, 
number = {NA}, 
volume = {23}
}
@article{10.1016/j.irfa.2007.06.003, 
year = {2007}, 
title = {{Ratings-based credit risk modelling: An empirical analysis}}, 
author = {Nickell, Pamela and Perraudin, William and Varotto, Simone}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2007.06.003}, 
abstract = {{Banks have recently developed new techniques for gauging the credit risk associated with portfolios of illiquid and defaultable instruments. These techniques could revolutionise banks' management of credit risk and could in the longer term serve as a more risk-sensitive basis for calculating regulatory capital on banks' loan books than in Basel 2, the new regulatory capital framework. In this paper we implement a popular credit risk model that exploits the information in credit ratings to determine a portfolio's value-at-risk. Using price data on large eurobond portfolios, we assess, on an out-of-sample basis, how well the model tracks the risks it is supposed to measure. © 2007 Elsevier Inc. All rights reserved.}}, 
pages = {434--451}, 
number = {5}, 
volume = {16}
}
@article{10.1016/j.jbusres.2018.01.004, 
year = {2018}, 
title = {{Backtesting an equity risk model under Solvency II}}, 
author = {Santomil, Pablo Durán and González, Luís Otero and Cunill, Onofre Martorell and Lindahl, José M. Merigó}, 
journal = {Journal of Business Research}, 
issn = {01482963}, 
doi = {10.1016/j.jbusres.2018.01.004}, 
abstract = {{Backtesting is a technique for validating internal models under Solvency II, which allows for evaluating the discrepancies between the results provided by a model and real observations. This paper aims to establish various backtesting tests and to show their applications to equity risk in Solvency II. Normal and empirical models with a rolling window are used to determine VaR at the 99.5\% confidence level over a one-year time horizon. The proposed methodology performs the backtesting of annualized returns arising from the accumulation of daily returns. The results show that even if a model is conservative when tested out of a sample, it may be inadequate when evaluated in a sample, thereby highlighting the problems inherent in the out-of-sample backtesting proposed by the regulator. © 2018 Elsevier Inc.}}, 
pages = {216--222}, 
number = {NA}, 
volume = {89}
}
@article{10.1142/s0219024908004889, 
year = {2008}, 
title = {{Measuring the market risk of freight rates: A value-at-risk approach}}, 
author = {ANGELIDIS, TIMOTHEOS and SKIADOPOULOS, GEORGE}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024908004889}, 
abstract = {{The fluctuation of shipping freight rates (freight rate risk) is an important source of market risk for all participants in the freight markets including hedge funds, commodity and energy producers. We measure the freight rate risk by the Value-at-Risk (VaR) approach. A range of parametric and non-parametric VaR methods is applied to various popular freight markets for dry and wet cargoes. Backtesting is conducted in two stages by means of statistical tests and a subjective loss function that uses the Expected Shortfall, respectively. We find that the simplest non-parametric methods should be used to measure freight rate risk. In addition, freight rate risk is greater in the wet cargoes markets. The margins in the growing freight derivatives markets should be set accordingly. © 2008 World Scientific Publishing Company.}}, 
pages = {447--469}, 
number = {5}, 
volume = {11}
}
@article{10.1007/s00181-018-1435-6, 
year = {2019}, 
title = {{Measuring expected time to default under stress conditions for corporate loans}}, 
author = {Górajski, Mariusz and Serwa, Dobromił and Wośko, Zuzanna}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-018-1435-6}, 
abstract = {{We present a new measure of extreme credit risk in the time domain, namely the conditional expected time to default (CETD). This measure has a clear interpretation and can be applied in a straightforward way to the analyses of loan performance in time. In contrast to the probability of default, CETD provides direct information on the timing of a potential loan default under some stress scenarios. We apply a novel method to compute CETD using Markov probability transition matrices, a popular approach in the survival analysis literature. We employ the new measure to the analysis of changing credit risk in a large portfolio of corporate loans. CETD changes through time in line with other measures of credit risk and is positively related to output growth. © 2018, The Author(s).}}, 
pages = {31--52}, 
number = {1}, 
volume = {57}
}
@article{10.1016/j.jbankfin.2016.07.014, 
year = {2016}, 
title = {{Evaluating Value-at-Risk forecasts: A new set of multivariate backtests}}, 
author = {Wied, Dominik and Weiß, Gregor N.F. and Ziggel, Daniel}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2016.07.014}, 
abstract = {{We propose two new tests for detecting clustering in multivariate Value-at-Risk (VaR) forecasts. First, we consider CUSUM-tests to detect non-constant expectations in the matrix of VaR-violations. Second, we propose χ2-tests for detecting cross-sectional and serial dependence in the VaR-forecasts. Moreover, we combine our new backtests with a test of unconditional coverage to yield two new backtests of multivariate conditional coverage. Results from a simulation study underline the usefulness of our new backtests for controlling portfolio risks across a bank's business lines. In an empirical study, we show how our multivariate backtests can be employed by regulators to backtest a banking system. © 2016 Elsevier B.V.}}, 
pages = {121--132}, 
number = {NA}, 
volume = {72}
}
@article{10.1142/s0218488503002156, 
year = {2003}, 
title = {{Imprecise previsions for risk measurement}}, 
author = {PELESSONI, RENATO and VICIG, PAOLO}, 
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems}, 
issn = {02184885}, 
doi = {10.1142/s0218488503002156}, 
abstract = {{In this paper the theory of coherent imprecise previsions is applied to risk measurement. We introduce the notion of coherent risk measure defined on an arbitrary set of risks, showing that it can be considered a special case of coherent upper prevision. We also prove that our definition generalizes the notion of coherence for risk measures defined on a linear space of random numbers, given in literature. Consistently properties of Value-at-Risk (VaR), currently one of the most used risk measures, are investigated too, showing that it does not necessarily satisfy a weaker notion of consistency called 'avoiding sure loss'. We introduce sufficient conditions for V aR to avoid sure loss and to be coherent. Finally we discuss ways of modifying incoherent risk measures into coherent ones.}}, 
pages = {393--412}, 
number = {4}, 
volume = {11}
}
@article{10.1108/s1571-03862021000029b032, 
year = {2021}, 
title = {{Impact of covid-19 pandemic risk and lockdown on the indian economy*}}, 
author = {Bhadury, Soumya and Kamate, Vidya and Nath, Siddhartha}, 
journal = {International Symposia in Economic Theory and Econometrics}, 
issn = {15710386}, 
doi = {10.1108/s1571-03862021000029b032}, 
abstract = {{The study provides medium-term estimates of recovery paths for Indian economy using a dynamic factor (DF)-based approach that employs data on high-frequency indicators à la Bhadury, Ghosh, and Kumar (2020). The DFs are used to analyze the post-pandemic recovery and convergence with its pre-COVID-19 trend for India between March 2021 and March 2022. A broad sectoral assessment of the impact of COVID-19 is also conducted. In addition, forward-looking measures based on stock returns are used to analyze the transmission of additional banking sector risks to the real sectors by constructing daily delta conditional value-at-risk (CoVaR) estimates. Our estimates based on the DFs suggest that the aggregate economic activities may catch up to the estimated pre-COVID trend by March 2021 predominantly driven by the growth in services sector. The industrial sector and consumer goods sector continue to show moderate signs of recovery. Our CoVaR estimates corroborate these findings. Banking sector transmission risk is among the lowest for services such as healthcare and information technology (IT), for both the lockdown period between March 25 and June 8, 2020, and for the latter months. The transmission risk continues to remain high for metal, oil and gas, and capital goods sector. Broadly, the evidence on forward-looking banking sector risk transmission for major sectors is in alignment with our finding on their recovery based on DF models, after easing of COVID-19 lockdown. © 2022 by Emerald Publishing Limited.}}, 
pages = {169--188}, 
number = {NA}, 
volume = {29B}
}
@article{10.1007/s10287-004-0031-8, 
year = {2005}, 
title = {{Distribution assumptions and risk constraints in portfolio optimization}}, 
author = {Maringer, Dietmar G.}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-004-0031-8}, 
abstract = {{Empirical distributions are often claimed to be superior to parametric distributions, yet to also increase the computational complexity and are therefore hard to apply in portfolio optimization. In this paper, we approach the portfolio optimization problem under constraints on the portfolio's Value at Risk and Expected Tail Loss, respectively, under empirical distributions for the Standard and Poor's 100 stocks. We apply a heuristic optimization method which has been found to overcome the restrictions of traditional optimization techniques. Our results indicate that empirical distributions might turn into a Pandora's Box: Though highly reliable for predicting the assets' risks, employing these distributions in the optimization process might result in severe mis-estimations of the resulting portfolios' actual risk. It is found that even a simple mean-variance approach can be superior despite its known specification errors. © Springer-Verlag Berlin 2005.}}, 
pages = {139--153}, 
number = {2}, 
volume = {2}
}
@article{10.1016/j.jbankfin.2005.04.023, 
year = {2006}, 
title = {{The magnitude of a market crash can be predicted}}, 
author = {Novak, S.Y. and Beirlant, J.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.04.023}, 
abstract = {{Could the magnitude of the stock market crash of 19.10.1987 be predicted on the base of the data available on the eve "black Monday" How far can the financial market fall, say, once in 40 years? We demonstrate that modern methods of Extreme Value Theory can help in answering these questions. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {453--462}, 
number = {2}, 
volume = {30}
}
@article{10.1109/hicss.2009.135, 
year = {2009}, 
title = {{Cost-effective firm investments in customer information privacy}}, 
author = {Kauffman, Robert J. and Lee, Yong J. and Sougstad, Ryan}, 
journal = {2009 42nd Hawaii International Conference on System Sciences}, 
issn = {NA}, 
doi = {10.1109/hicss.2009.135}, 
abstract = {{Extensive personal information is gathered explicitly or implicitly when a customer interacts with a firm. Significant risks are associated with handling such personal information. Providing protection may reduce risk of misuse or loss of private information, but it imposes some costs on the firm and its customers. Risk is associated with improper handling of sensitive customer information. Profits from e-commerce that are earned when there is improper use of private customer information are subject to lawsuits, restitution and other undesirable outcomes. So a firm will want to ensure appropriate privacy protections are in place to safeguard customer information. We present a profit optimization model for customer privacy protection investments considering the potential value implications that arise. We employ a profit-at-risk approach based on value-at-risk methods from financial economics. © 2009 IEEE.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/powercon.2010.5666520, 
year = {2010}, 
title = {{Unit commitment and risk management based on wind power penetrated system}}, 
author = {Li, Xiaohu and Jiang, Chuanwen}, 
journal = {2010 International Conference on Power System Technology}, 
issn = {NA}, 
doi = {10.1109/powercon.2010.5666520}, 
abstract = {{With the advent of an era of the high fossil energy cost and growing environment consciousness, wind power is gaining great favor over other traditional energy sources. Large scale usage of wind energy can significantly reduce the pollutions and carbon emission otherwise caused by fossil fuel. In many cases, with tax incentive, investment in wind energy can also reduce the operational cost of generating companies. One difficulty of integrating large scale wind power is related to the uncertainty in wind power. Unlike hydro energy which is also renewable and intermittent, wind power output is hard to predict precisely due to the indeterminacy in wind speed. The uncertainty not only causes difficulty in scheduling, but also system stability and security concerns in operation. This work proposed a short-term(24 hours) optimal economical dispatch model and developed a risk evaluation method for the short-term operation of power systems with high wind penetration, considering the wind variability. We use the Particle Swarm Optimization(PSO) algorithm with constraints to solve the dispatch problem above. The Value at Risk(VaR) and Utility function(UF) are used to evaluate the risk and make a optimal tradeoff between the profit and risk for the system operators. The algorithms are tested on the standard IEEE 30-bus power system to validate the applicability. ©2010 IEEE.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {NA}
}
@article{10.17010//2015/v9i8/74560, 
year = {2015}, 
title = {{A test of alternative value-at-risk models during volatile periods}}, 
author = {Bhat, Aparna Prasad}, 
journal = {Indian Journal of Finance}, 
issn = {09738711}, 
doi = {10.17010//2015/v9i8/74560}, 
abstract = {{This paper compared the performance of alternative models for estimating Value at Risk (VaR) of four different currencies against the Indian rupee. I examined whether incorporating a volatility estimate capturing the ARCH effects in the normal linear VaR model yielded a better estimate of market risk than the traditional models based on historical simulation and historical moving average volatility. I tested the effectiveness of different VaR models during the volatile period of June-September 2013 and found that VaR models based on an estimate of time-varying volatility performed better than traditional models during turbulent times. © 2015.}}, 
pages = {19--33}, 
number = {8}, 
volume = {9}
}
@article{10.1109/icit.2009.4939607, 
year = {2009}, 
title = {{Electricity auction market risk analysis based on EGARCH-EVT-CVaR model}}, 
author = {Gong, Xiusong and Luo, Xia and Wu, Jiajie}, 
journal = {2009 IEEE International Conference on Industrial Technology}, 
issn = {NA}, 
doi = {10.1109/icit.2009.4939607}, 
abstract = {{In the competitive power market of generation side, the bidding strategies with taking into account the profit and risk are essential for generation companies. This paper proposes a dynamic risk model of bidding strategy of generation companies based on the EGARCH-EVT-CVaR method. In this model, the tail of return is modeled by the extreme value theory (EVT). The EGARCH model is used to achieve auto-regression weekly and seasonally in both the conditional mean and conditional volatility of return as well as leverage effect. In addition, theconditional value at risk (CVaR) is adopted as a risk measurement tool. Taking the California electricity market as an example, the price return at the same hour in every day is modeled. The empirical analysis results show that the proposed EGARCH-EVT-based model rationally forecasts dynamic VaR and CVaR in the electricity auction market. In addition, the results indicate that the proposed model is a useful technique for generation companies to deal with market risks.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.red.2008.10.002, 
year = {2009}, 
title = {{Model uncertainty and liquidity}}, 
author = {Routledge, Bryan R. and Zin, Stanley E.}, 
journal = {Review of Economic Dynamics}, 
issn = {10942025}, 
doi = {10.1016/j.red.2008.10.002}, 
abstract = {{Extreme market outcomes are often followed by a lack of liquidity and a lack of trade. This market collapse seems particularly acute for markets where traders rely heavily on a specific empirical model such as in derivative markets like the market for mortgage backed securities or credit derivatives. Moreover, the observed behavior of traders and institutions that places a large emphasis on "worst-case scenarios" through the use of "stress testing" and "Value-at-Risk" seems different than Savage expected utility would suggest. In this paper, we capture model-uncertainty using an Epstein and Wang [Epstein, L.G., Wang, T., 1994. Intertemporal asset pricing under Knightian uncertainty. Econometrica 62, 283-322] uncertainty-averse utility function with an ambiguous underlying asset-returns distribution. To explore the connection of uncertainty with liquidity, we specify a simple market where a monopolist financial intermediary makes a market for a propriety derivative security. The market-maker chooses bid and ask prices for the derivative, then, conditional on trade in this market, chooses an optimal portfolio and consumption. We explore how uncertainty can increase the bid-ask spread and, hence, reduces liquidity. Our infinite-horizon example produces short, dramatic decreases in liquidity even though the underlying environment is stationary. We show how these liquidity crises are closely linked to the uncertainty aversion effect on the optimal portfolio. Effectively, the uncertainty aversion can, at times, limit the ability of the market-maker to hedge a position and thus reduces the desirability of trade, and hence, liquidity. © 2008.}}, 
pages = {543--566}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.enpol.2011.06.052, 
year = {2011}, 
title = {{Spatial dependence in wind and optimal wind power allocation: A copula-based analysis}}, 
author = {Grothe, Oliver and Schnieders, Julius}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2011.06.052}, 
abstract = {{The investment decision on the placement of wind turbines is, neglecting legal formalities, mainly driven by the aim to maximize the expected annual energy production of single turbines. The result is a concentration of wind farms at locations with high average wind speed. While this strategy may be optimal for single investors maximizing their own return on investment, the resulting overall allocation of wind turbines may be unfavorable for energy suppliers and the economy because of large fluctuations in the overall wind power output. This paper investigates to what extent optimal allocation of wind farms in Germany can reduce these fluctuations. We analyze stochastic dependencies of wind speed for a large data set of German on- and offshore weather stations and find that these dependencies turn out to be highly nonlinear but constant over time. Using copula theory we determine the value at risk of energy production for given allocation sets of wind farms and derive optimal allocation plans. We find that the optimized allocation of wind farms may substantially stabilize the overall wind energy supply on daily as well as hourly frequency. © 2011 Elsevier Ltd.}}, 
pages = {4742--4754}, 
number = {9}, 
volume = {39}
}
@article{10.1016/s0304-4076(03)00100-3, 
year = {2003}, 
title = {{An MCMC approach to classical estimation}}, 
author = {Chernozhukov, Victor and Hong, Han}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/s0304-4076(03)00100-3}, 
abstract = {{This paper studies computationally and theoretically attractive estimators called here Laplace type estimators (LTEs), which include means and quantiles of quasi-posterior distributions defined as transformations of general (nonlikelihood-based) statistical criterion functions, such as those in GMM, nonlinear IV, empirical likelihood, and minimum distance methods. The approach generates an alternative to classical extremum estimation and also falls outside the parametric Bayesian approach. For example, it offers a new attractive estimation method for such important semi-parametric problems as censored and instrumental quantile regression, nonlinear GMM and value-at-risk models. The LTEs are computed using Markov Chain Monte Carlo methods, which help circumvent the computational curse of dimensionality. A large sample theory is obtained and illustrated for regular cases. © 2003 Elsevier Science B.V. All rights reserved.}}, 
pages = {293--346}, 
number = {2}, 
volume = {115}
}
@article{10.1109/bife.2009.93, 
year = {2009}, 
title = {{The study on hedging model based on risk tolerance of hedgers}}, 
author = {Sui, Cong and Chi, Guotai and Yang, Zhongyuan}, 
journal = {2009 International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2009.93}, 
abstract = {{In this paper, Value at Risk of hedging portfolio is adopted to measure the risk of futures hedging. The control constraint based on risk tolerance of hedgers is established. The futures optimal hedge ratio is presented by maximizing the return of hedging portfolio under the control constraint. The contributions of the model are as follows: Firstly that we use VaR to construct the control constraint which reflects risk tolerance of hedgers. This method effectively avoids the huge losses suffered by hedging. Secondly, we prove the minimum variance hedging ratio and VaR hedging ratio are special cases of this model. © 2009 IEEE.}}, 
pages = {378--380}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/9780470972571.ch7, 
year = {2010}, 
title = {{Scoring Models for Operational Risks}}, 
author = {Giudici, Paolo}, 
issn = {NA}, 
doi = {10.1002/9780470972571.ch7}, 
abstract = {{The chapter deals with the problem of analyzing and integrating qualitative and quantitative data. In particular it shows how, on the basis of the experience and opinions of internal company "experts", a scorecard is derived producing a ranking of different risks and a prioritized list of improvement areas and related controls. Scorecard models represent a first step in risk analysis. The chapter presents advanced approaches and statistical models for implementing such models. © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {125--135}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-319-02499-8_7, 
year = {2014}, 
title = {{Fitting financial returns distributions: A mixture normality approach}}, 
author = {Bramante, Riccardo and Zappa, Diego}, 
issn = {NA}, 
doi = {10.1007/978-3-319-02499-8\_7}, 
abstract = {{An important research field in finance is the identification of probability distribution model that fits at the best the empirical distribution of time series returns. In this paper we propose the use of mixtures of truncated normal distributions in modelling returns. An optimization algorithm has been developed to obtain the best fit by using the minimum distance approach. Empirical results show evidence of the capability of the method to fit return distributions at a satisfactory level, completely maintaining local normality properties in the model. Moreover, the model provides a good tail fit thus improving the accuracy of Value at Risk estimates. © Springer International Publishing Switzerland 2014.}}, 
pages = {81--88}, 
number = {NA}, 
volume = {NA}
}
@article{10.1214/17-aoas1041, 
year = {2017}, 
title = {{Elicitability and backtesting: Perspectives for banking regulation}}, 
author = {Nolde, Natalia and Ziegel, Johanna F.}, 
journal = {The Annals of Applied Statistics}, 
issn = {19326157}, 
doi = {10.1214/17-aoas1041}, 
abstract = {{Conditional forecasts of risk measures play an important role in internal risk management of financial institutions as well as in regulatory capital calculations. In order to assess forecasting performance of a risk measurement procedure, risk measure forecasts are compared to the realized financial losses over a period of time and a statistical test of correctness of the procedure is conducted. This process is known as backtesting. Such traditional backtests are concerned with assessing some optimality property of a set of risk measure estimates. However, they are not suited to compare different risk estimation procedures.We investigate the proposal of comparative backtests, which are better suited for method comparisons on the basis of forecasting accuracy, but necessitate an elicitable risk measure.We argue that supplementing traditional backtests with comparative backtests will enhance the existing trading book regulatory framework for banks by providing the correct incentive for accuracy of risk measure forecasts. In addition, the comparative backtesting framework could be used by banks internally as well as by researchers to guide selection of forecasting methods. The discussion focuses on three risk measures, Value at Risk, expected shortfall and expectiles, and is supported by a simulation study and data analysis. © Institute of Mathematical Statistics, 2017.}}, 
pages = {1833--1874}, 
number = {4}, 
volume = {11}
}
@article{10.1007/978-3-540-71805-5_24, 
year = {2007}, 
title = {{Comparison of evolutionary techniques for value-at-risk calculation}}, 
author = {Uludag, Gonul and Uyar, A. Sima and Senel, Kerem and Dag, Hasan}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-540-71805-5\_24}, 
abstract = {{The Value-at-Risk (VaR) approach has been used for measuring and controlling the market risks in financial institutions. Studies show that the t-distribution is more suited to representing the financial asset returns in VaR calculations than the commonly used normal distribution. The frequency of extremely positive or extremely negative financial asset returns is higher than that is suggested by normal distribution. Such a leptokurtic distribution can better be approximated by a t-distribution. The aim of this study is to asses the performance of a real coded Genetic Algorithm (GA) with Evolutionary Strategies (ES) approach for Maximum Likelihood (ML) parameter estimation. Using Monte Carlo (MC) simulations, we compare the test results of VaR simulations using the t-distribution, whose optimal parameters are generated by the Evolutionary Algorithms (EAs), to that of the normal distribution. It turns out that the VaR figures calculated with the assumption of normal distribution significantly understate the VaR figures computed from the actual historical distribution at high confidence levels. On the other hand, for the same confidence levels, the VaR figures calculated with the assumption of t-distribution are very close to the results found using the actual historical distribution. Finally, in order to speed up the MC simulation technique, which is not commonly preferred in financial applications due to its time consuming algorithm, we implement a parallel version of it. © Springer-Verlag Berlin Heidelberg 2007.}}, 
pages = {218--227}, 
number = {NA}, 
volume = {4448 LNCS}
}
@article{10.1016/s0378-4266(02)00266-2, 
year = {2002}, 
title = {{Saddlepoint approximation of CreditRisk}}, 
author = {Gordy, Michael B.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00266-2}, 
abstract = {{CreditRisk+ is an influential and widely implemented model of portfolio credit risk. As a close variant of models long used for insurance risk, it retains the analytical tractability for which the insurance models were designed. Value-at-risk (VaR) can be obtained via a recurrence-rule algorithm, so Monte Carlo simulation can be avoided. Little recoganized, however, is that the algorithm is fragile. Under empirically realistic conditions, numerical error can accumulate in the execution of the recurrence rule and produce wildly inaccurate results for VaR. This paper provides new tools for users of CreditRisk+ based on the cumulant generating function (cgf) of the portfolio loss distribution. Direct solution for the moments of the loss distribution from the cgf is almost instantaneous and is computationally robust. Thus, the moments provide a convenient, quick and independent diagnostic on the implementation and execution of the standard solution algorithm. Better still, with the cgf in hand we have an alternative to the standard algorithm. I show how tail percentiles of the loss distribution can be calculated quickly and easily by saddlepoint approximation. On a large and varied sample of simulated test portfolios, I find a natural complementarity between the two algorithms: Saddlepoint approximation is accurate and robust in those situations for which the standard algorithm performs least well, and is less accurate in those situations for which the standard algorithm is fast and reliable. Published by Elsevier Science B.V.}}, 
pages = {1335--1353}, 
number = {7}, 
volume = {26}
}
@article{10.2139/ssrn.1884084, 
year = {2011}, 
title = {{Peas in a pod: Canadian and Australian banks before and during a Global Financial Crisis}}, 
author = {Allen, David E. and Boffey, Ray and Powell, Robert J.}, 
journal = {SSRN Electronic Journal}, 
issn = {NA}, 
doi = {10.2139/ssrn.1884084}, 
abstract = {{In the aftermath of the Global Financial Crisis (GFC), the Canadian and Australian banking systems have been singled out by some commentators as having performed better than many other banking systems, particularly those in Europe, America and the United Kingdom. Banks in both Canada and Australia, for instance, have continued to report enviable earnings, sound capital levels, and high credit ratings both before and during the GFC. The G-20 and the European Union have tried to identify the features of the Canadian and Australian financial systems which have underpinned this success in order to use them in shaping a revised international regulatory framework. One area of focus has been the regulations governing "quality of capital". Despite these apparent successes, there is some evidence that both Canadian and Australian banks experienced considerable deterioration in the market value of their assets during the GFC. In this paper we use the KMV / Merton structural methodology, which incorporates market asset values, to examine default probabilities of 9 listed Canadian banks and 13 Australian listed banks in both a pre-GFC period (2000-2006) and a GFC period (2007-2008). We also modify the model to incorporate conditional probability of default which measures extreme credit risk. This paper finds that bank risk was significantly similar for Australian and Canadian Banks during the GFC period. This includes an assessment of impaired assets, Value at Risk (VaR) and Distance to Default (DD), as well as the extreme measures of Conditional VaR (CVaR), and Conditional Distance to Default (CDD); metrics which confirm the two countries similarities in terms of a significant increase in credit risk between pre-GFC and GFC periods. The extent of this increase was, however, far more pronounced for Australia, which was coming off a lower base. Bank risk for both countries was found to be far lower than for global counterparts due to factors such as sound regulatory control and low levels of involvement in sub-prime lending. This could provide lessons for global banks on risk management. A key conclusion of the paper is that it is important that fluctuating market values, especially the extreme fluctuations which are measured by CVaR and CDD, are a key consideration when determining risk management criteria such as capital adequacy.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1300/j130v09n02_04, 
year = {2004}, 
title = {{Value-at-risk (var) computations under various var models and stress testing}}, 
author = {Teker, Suat and Akçay, M. Baris}, 
journal = {Journal of Transnational Management Development}, 
issn = {10686061}, 
doi = {10.1300/j130v09n02\_04}, 
abstract = {{Bank for International Settlements (BIS) proposes that all banks calculate and report amount of market risk they incur and allocate sufficient amount of capital starting at the beginning of year 2002. BIS also suggests that value-at-risk (VaR) models in computing market risk should be used. The Turkish Bank Regulation and Supervision Agency already required all Turkish banks to compute and periodically report market risk and reserve adequate amount of capital since January, 2002. This study mimics an average trading marketable securities portfolio subject to market risk of the four largest Turkish banks. The publicly available quarterly financial reports of year 2001 for Isbank, Garanti, Yapi Kredi and Akbank are examined, and a mimicking portfolio composition is determined as bond investments; 60\% in Turkish currency (TRL), 20\% in American dollar (USD) and 20\%in Euro (EUR). The VaR amounts of the mimicking portfolio are computed by applying Historical Simulation, Monte Carlo Simulation, Delta-Normal and Standard Methods. Finally, stress test is applied for each of the models by using crisis scenarios. The Turkish financial crises of November 2000 and February 2001 are simulated as stress scenarios. The results of stress testing reveal that all methods except standard method can stand the crisis in November 2000, but none of the models can stand the crisis in February 2001. © 2004 Taylor \& Francis Group, LLC.}}, 
pages = {47--67}, 
number = {2-3}, 
volume = {9}
}
@article{10.1108/ijis-06-2016-011, 
year = {2016}, 
title = {{An innovative approach to mitigating horizon mismatch: A multi-resolution investigation of ELSS}}, 
author = {Chakrabarty, Anindya and Dubey, Rameshwar and De, Anupam}, 
journal = {International Journal of Innovation Science}, 
issn = {17572223}, 
doi = {10.1108/ijis-06-2016-011}, 
abstract = {{Purpose -This paper aims to propose an innovative approach to risk measurement for the abolition of selection bias arising from the specious selection of different horizons for investment and risk computation of equity-linked-saving schemes (ELSS). Design/methodology/approach - ELSS has a lock-in period of three years, but shorter horizons' (daily/weekly/monthly) return data are preferred, in practice, for risk computation. This results in horizon mismatch. This paper studies the consequences of this mismatch and provides a noble solution to diminish its effect on investors' decision-making. To accomplish this objective, the paper uses an innovative methodology, maximal overlap discrete wavelet transformation, to segregate the price movements across different horizons. Risk across all horizons is measured using Cornish-Fisher expected shortfall and Cornish-Fisher value-At-risk methods. Findings -The degree of consistency of risk-based rankings across horizons is examined by means of the Spearman and Kendall's rank correlation tests. The risk-based ranking of ELSS is found to vary significantly with the change in investor's horizon. Precisely, the rankings formulated using daily net asset values are significantly different from the rankings developed using fluctuations over longer horizons (two-four and four-eight years). Originality/value -This finding indicates that the ranking exercise may mislead investors if horizon correction is not done while developing such rankings. © Emerald Group Publishing Limited.}}, 
pages = {161--180}, 
number = {2}, 
volume = {8}
}
@article{10.22495/rgcv1i1art3, 
year = {2011}, 
title = {{The dance of duplicity in emerging markets: Using bank regulation and deposit insurance protection to enrich the elite}}, 
author = {Dew, Kurt}, 
journal = {Risk Governance and Control: Financial Markets \& Institutions}, 
issn = {2077429X}, 
doi = {10.22495/rgcv1i1art3}, 
abstract = {{We seek to identify the culpability of banks in resource misallocation in Mexico, Thailand and Turkey. Specifically we provide evidence of an agency problem in the government and banking systems of the three countries. Where governments pass laws and regulations consistent with modern capitalism for the purpose of deceiving investors and others, the door is opened to the use of deposit insurance and repeated promises of regulatory reform to transfer wealth from the efficient to the corrupt. © 2011, Virtus Interpress. All rights reserved.}}, 
pages = {37--51}, 
number = {1}, 
volume = {1}
}
@article{10.1016/j.insmatheco.2020.08.001, 
year = {2020}, 
title = {{Pareto-optimal insurance contracts with premium budget and minimum charge constraints}}, 
author = {Asimit, Alexandru V. and Cheung, Ka Chun and Chong, Wing Fung and Hu, Junlei}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.08.001}, 
abstract = {{In view of the fact that minimum charge and premium budget constraints are natural economic considerations in any risk-transfer between the insurance buyer and seller, this paper revisits the optimal insurance contract design problem in terms of Pareto optimality with imposing these practical constraints. Pareto optimal insurance contracts, with indemnity schedule and premium payment, are solved in the cases when the risk preferences of the buyer and seller are given by Value-at-Risk or Tail Value-at-Risk. The effect of our constraints and the relative bargaining powers of the buyer and seller on the Pareto optimal insurance contracts are highlighted. Numerical experiments are employed to further examine these effects for some given risk preferences. © 2020 Elsevier B.V.}}, 
pages = {17--27}, 
number = {NA}, 
volume = {95}
}
@article{10.21314/jop.2018.209, 
year = {2018}, 
title = {{Modeling very large losses}}, 
author = {Gzyl, Henryk}, 
journal = {Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2018.209}, 
abstract = {{In this paper, we present a simple probabilistic model for aggregating very large losses into a loss collection. This supposes that “standard” losses come in various possible sizes – small, moderate and large – which, fortunately, seem to occur with decreasing frequency. Standard modeling allows us to infer a probability distribution describing their occurrence. From the historical record, we know that very large losses do occur, albeit very rarely, yet they are not usually included in the available data sets. Such losses should be made part of the distribution for computation purposes. For example, to a bank they may helpful in the computation of economic or regulatory capital, while to an insurance company they may be useful in the computation of premiums of losses due to catastrophic events. We develop a simple modeling procedure that allows us to include very large losses in a loss distribution obtained from moderately sized loss data. We say that a loss is large when it is larger than the value-at-risk (VaR) at a high confidence level. The original and extended distributions will have the same VaR but quite different values of tail VaR (TVaR). © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {2}, 
volume = {13}
}
@article{10.1016/s0378-4266(00)00120-5, 
year = {2001}, 
title = {{Standard \& Poor's official response to the Basel Committee's proposal}}, 
author = {Griep, Clifford and Stefano, Michael De}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(00)00120-5}, 
abstract = {{The following text is Standard \& Poor's official response to the Basel Committee's proposal, which was presented to the regulators in December 1999. Standard \& Poor's welcomes the opportunity to respond to the committee's proposal. The risk adjustment of capital, introduced in a simplified format in the 1988 Basel accord, was adopted by most of the world's bank regulatory regimes. The current proposal could meaningfully advance this process, a development that Standard \& Poor's strongly supports.}}, 
pages = {149--169}, 
number = {1}, 
volume = {25}
}
@article{10.1109/tpec48276.2020.9042563, 
year = {2020}, 
title = {{Risk-driven planning for system upgrades to enhance resilience of distribution systems}}, 
author = {Mukherjee, Monish and Poudel, Shiva and Dubey, Anamika and Bose, Anjan}, 
journal = {2020 IEEE Texas Power and Energy Conference (TPEC)}, 
issn = {NA}, 
doi = {10.1109/tpec48276.2020.9042563}, 
abstract = {{Resilience to high impact low probability (HILP) weather events is an increasing concern to address the vulnerabilities of the aging power distribution infrastructures. It is essential to improve distribution system's resilience using appropriate planing measures such as infrastructure hardening or standby resources. Absence of regulatory policies have made it extremely challenging for distribution system operators (DSOs) to plan system upgrades for enhancing resilience. This paper presents a novel framework to plan investments for resilience-driven system upgrades. The framework quantifies the value of service for the upgrades in terms of the monetary value-at-risk due to customer interruption. The socio-economic optimal level for resilience-driven investments is computed through the marginal investment cost for system upgrades and the resulting marginal value of service. The proposed framework is validated for deploying distributed generators (DGs) as a planning measure to enhance resilience using a modified IEEE 123-bus test system. Results from the test-case demonstrate the effectiveness of the framework to facilitate DSOs in investment decisions for enhancing resilience that is driven by the risks posed by the HILP events and the socio-economic costs of planning measures. © 2020 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10479-015-2046-7, 
year = {2018}, 
title = {{Ex-ante real estate Value at Risk calculation method}}, 
author = {Amédée-Manesme, Charles-Olivier and Barthélémy, Fabrice}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-015-2046-7}, 
abstract = {{The computation of Value at Risk (VaR) has long been a problematic issue in commercial real estate. Difficulties mainly arise from the lack of appropriate data, the lack of transactions, the non-normality of returns, and the inapplicability of many of the traditional methodologies. In addition, specific risks remain latent in investors’ portfolios and thus risk measurements based on market index do not represent the risks of a specific portfolio. Following a spate of new regulations such as Basel II, Basel III, NAIC and Solvency II, financial institutions have increasingly been required to estimate and control their exposure to market risk. Hence, financial institutions now commonly use “internal” VaR (or Expected Shortfall) models in order to assess their market risk exposure. This paper proposes the first model designed especially for static real estate VaR computation. The proposal accounts for specific real estate characteristics such that the lease structures or the vacancies. The paper contributes to the real estate risk management literature by proposing for the first time a model that incorporates characteristics of real estate investments. It allows more precise real estate risk measurements and is derived from a regulators’ approach. © 2015, Springer Science+Business Media New York.}}, 
pages = {257--285}, 
number = {2}, 
volume = {262}
}
@article{10.1080/00207160.2018.1447666, 
year = {2019}, 
title = {{Quantifying credit portfolio losses under multi-factor models}}, 
author = {Colldeforns-Papiol, Gemma and Ortiz-Gracia, Luis and Oosterlee, Cornelis W.}, 
journal = {International Journal of Computer Mathematics}, 
issn = {00207160}, 
doi = {10.1080/00207160.2018.1447666}, 
abstract = {{In this work, we investigate the challenging problem of estimating credit risk measures of portfolios with exposure concentration under the multi-factor Gaussian and multi-factor t-copula models. It is well-known that Monte Carlo (MC) methods are highly demanding from the computational point of view in the aforementioned situations. We present efficient and robust numerical techniques based on the Haar wavelets theory for recovering the cumulative distribution function of the loss variable from its characteristic function. To the best of our knowledge, this is the first time that multi-factor t-copula models are considered outside the MC framework. The analysis of the approximation error and the results obtained in the numerical experiments section show a reliable and useful machinery for credit risk capital measurement purposes in line with Pillar II of the Basel Accords. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--22}, 
number = {11}, 
volume = {96}
}
@article{10.1287/opre.1110.0950, 
year = {2011}, 
title = {{Tight bounds for some risk measures, with applications to robust portfolio selection}}, 
author = {Chen, Li and He, Simai and Zhang, Shuzhong}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1110.0950}, 
abstract = {{In this paper we develop tight bounds on the expected values of several risk measures that are of interest to us. This work is motivated by the robust optimization models arising from portfolio selection problems. Indeed, the whole paper is centered around robust portfolio models and solutions. The basic setting is to find a portfolio that maximizes (respectively, minimizes) the expected utility (respectively, disutility) values in the midst of infinitely many possible ambiguous distributions of the investment returns fitting the given mean and variance estimations. First, we show that the single-stage portfolio selection problem within this framework, whenever the disutility function is in the form of lower partial moments (LPM), or conditional value-at-risk (CVaR), or value-at-risk (VaR), can be solved analytically. The results lead to the solutions for single-stage robust portfolio selection models. Furthermore, the results also lead to a multistage adjustable robust optimization (ARO) solution when the disutility function is the second-order LPM. Exploring beyond the confines of convex optimization, we also consider the so-called S-shaped value function, which plays a key role in the prospect theory of Kahneman and Tversky. The nonrobust version of the problem is shown to be NP-hard in general. However, we present an efficient procedure for solving the robust counterpart of the same portfolio selection problem. In this particular case, the consideration of the robustness actually helps to reduce the computational complexity. Finally, we consider the situation whereby we have some additional information about the chance that a quadratic function of the random distribution reaches a certain threshold. That information helps to further reduce the ambiguity in the robust model. We show that the robust optimization problem in that case can be solved by means of semidefinite programming (SDP), if no more than two additional chance inequalities are to be incorporated. © 2011 INFORMS.}}, 
pages = {847--865}, 
number = {4}, 
volume = {59}
}
@article{10.1016/j.econlet.2014.06.028, 
year = {2014}, 
title = {{Estimation of extreme value-at-risk: An EVT approach for quantile GARCH model}}, 
author = {Yi, Yanping and Feng, Xingdong and Huang, Zhuo}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2014.06.028}, 
abstract = {{We proposed a method to estimate extreme conditional quantiles by combining quantile GARCH model of Xiao and Koenker (2009) and extreme value theory (EVT) approach. We first estimate the latent volatility process using the information of intermediate quantiles. We then apply EVT to the tail observations to obtain a sound estimate of the likelihood of experiencing an extreme event. Quantile autoregression and EVT together improve efficiency in estimation of extreme quantiles, by borrowing information from neighbor quantiles. Monte Carlo simulation indicates that, the proposed method is promising to provide more accurate estimates for VaR of a financial portfolio, where non-Gaussian tail is present. © 2014 Elsevier B.V.}}, 
pages = {378--381}, 
number = {3}, 
volume = {124}
}
@article{10.1080/03081060.2015.1059121, 
year = {2015}, 
title = {{Incorporating uncertainty and risk in transportation investment decision-making}}, 
author = {Mishra, Sabyasachee and Khasnabis, Snehamay and Swain, Subrat}, 
journal = {Transportation Planning and Technology}, 
issn = {03081060}, 
doi = {10.1080/03081060.2015.1059121}, 
abstract = {{This paper presents a framework for addressing uncertainty and risk for large-scale transportation investments involving public–private participation. Demand, fare/toll and demand responsive costs are considered in the uncertainty analysis. Uncertainty analysis provides information on economic feasibility of the project. A set of relaxation policies is proposed to form various Ownership, Tenure and Governance (OTG) strategies reflecting the nature and level of participation by the public and private entity. A Monte Carlo Simulation-based Value at Risk is used to quantify risk. Finally, a methodology is proposed to integrate uncertainty and risk. The framework is tested on the proposed multibillion dollar Detroit River International Crossing connecting the cities of Detroit in the USA with Windsor in Canada. The analysis provides insights to probable outcomes for this transportation infrastructure investment under different OTG scenarios. © 2015 Taylor \& Francis.}}, 
pages = {738--760}, 
number = {7}, 
volume = {38}
}
@article{10.1016/j.irfa.2009.09.007, 
year = {2009}, 
title = {{The effect of downside risk reduction on UK equity portfolios included with Managed Futures Funds}}, 
author = {Tee, Kai-Hong}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2009.09.007}, 
abstract = {{The concept of asymmetric risk estimation has become more widely applied in risk management in recent years with the increased use of Value-at-risk (VaR) methodologies. This paper uses the n-degree lower partial moment (LPM) models, of which VaR is a special case, to empirically analyse the effect of downside risk reduction on UK portfolio diversification and returns. Data on Managed Futures Funds are used to replicate the increasingly popular preference of investors for including hedge funds and fund-of-funds type investments in the UK equity portfolios. The result indicates, however that the potential benefits of fund diversification may deteriorate following reductions in downside risk tolerance levels. These results appear to reinforce the importance of risk (tolerance) perception, particularly downside risk, when making decisions to include Managed Futures Funds in UK equity portfolios as the empirical analysis suggests that this could negatively affect portfolio returns. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {303--310}, 
number = {5}, 
volume = {18}
}
@article{10.1088/1742-6596/1616/1/012070, 
year = {2020}, 
title = {{Value at risk of the exchange rate in southeast ASEAN-3 based on bayesian Markov-switching GARCH approach}}, 
author = {Li, Mingyang and Liao, Ruofan and Sriboonchitta, Songsak}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1616/1/012070}, 
abstract = {{This study analyzes Bayesian Markov-Switching of the single regime and the two regimes to forecast the risk of the exchange rate in three ASEAN countries, and various GARCH family and distribution are selected by DIC to find the best fitting models. This study will help governments to prevent the recurrence of events like the 1997 financial crisis. The study finds that Thailand has the best exchange rate stability and the lowest risk and is most suitable for foreign investors seeking stability. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012070}, 
number = {1}, 
volume = {1616}
}
@article{10.2478/v10033-007-0009-x, 
year = {2007}, 
title = {{Measuring Market Risk for Commercial Banks in the Volatile Environment of an Emerging Market Economy}}, 
author = {Grum, Andraz}, 
journal = {South East European Journal of Economics and Business}, 
issn = {1840118X}, 
doi = {10.2478/v10033-007-0009-x}, 
abstract = {{Slovenian commercial banks have two possibilities for calculating capital charges for the market risks to which they are exposed. Due to the capital decree legislated by the Bank of Slovenia, they can use standardized methodology or apply an internal model. An internal model can be based on different risk measures, with each risk measure having its strengths and weaknesses. Consequently, the volume of risk calculated using a specific risk measure will vary among risk measures. Basel II regulation assumes VaR methodology for capital requirements calculations for the market risks to which commercial banks are exposed. There are two commonly used methods for VaR calculation – historical simulation and the variance-covariance method. Each has its strengths and weaknesses. The goal of this paper is to present the methodology of volatility and time weighted historical simulation as an internal model for market risk measurement in Slovenian commercial banks. The methodology is based on historical simulation and tries to remove the disadvantages of this method with GJR GARCH volatility modelling and the time weighting of returns. © 2007, Versita. All rights reserved.}}, 
pages = {89--94}, 
number = {2}, 
volume = {2}
}
@article{10.1007/s10479-020-03541-8, 
year = {2021}, 
title = {{Set optimization of set-valued risk measures}}, 
author = {Mastrogiacomo, Elisa and Rocca, Matteo}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-020-03541-8}, 
abstract = {{A new approach to optimizing or hedging a portfolio of financial instruments to reduce risk is presented. Central to this approach are concepts and tools of set-optimization theory. It focuses on the problem of minimizing set-valued risk measures applied to portfolios. We present sufficient conditions for the existence of solutions of a set-valued risk minimization problem under some semi-continuity assumption. The methodology is applied to the optimization of set-valued Value at Risk and Average Value at Risk. Two examples at the end illustrate various features of the theoretical construction, among them the geometry of the image sets. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {291--314}, 
number = {1-2}, 
volume = {296}
}
@article{10.1109/pess.2001.970058, 
year = {2001}, 
title = {{Tools for generation risk management}}, 
author = {Shahidehpour, M.}, 
journal = {2001 Power Engineering Society Summer Meeting. Conference Proceedings (Cat. No.01CH37262)}, 
issn = {21608563}, 
doi = {10.1109/pess.2001.970058}, 
abstract = {{The first part of the presentation is on a comprehensive price-based unit commitment (PBUC) formulation. It is perceived that GENCOs would use PBUC to schedule their generating units on short-term, and determine optimal bids for energy and ancillary services in order to maximize their profits. PBUC is a mixed integer program, which considers the characteristics of individual units, and is solved based on the augmented Lagrangian relaxation. The input to PBUC will also include market price forecasts. The output of PBUC will identify the on/off schedules of available generating units within the scheduling horizon. The PBUC solution will be for multiple products (energy and ancillary services portfolio) and will consider bidding in multiple markets. The second part of the presentation encompasses generation asset valuation and value-at-risk (VaR) for measuring the risk of committing the generating assets. The asset valuation problem is formulated as a multi-stage problem and VaR is used as an index for measuring risk. The presumption in this study is that there are hourly markets for both electricity and fuel with fluctuating prices. A Monte Carlo simulation is used for modeling different pricing and generation scenarios and calculate VaR for minimizing the financial risk of GENCOs for supplying the generation to the market.}}, 
pages = {414--416}, 
number = {SUMMER}, 
volume = {1}
}
@article{10.24425/124504, 
year = {2018}, 
title = {{Investment risk in the energy sector on the example of a biogas power plant}}, 
author = {}, 
issn = {14296675}, 
doi = {10.24425/124504}, 
abstract = {{The aim of the article is to present the issue of risk and related management methods, with a particular emphasis on the conditions of investment in energy infrastructure. The work consists of two main parts; the first one is the theoretical analysis of the issue, while the second discusses the application of analysis methods on the example of the investment in an agricultural biogas plant. The article presents the definitions related to the investment risk and its management, with a particular emphasis on the distinction between the risk and uncertainty. In addition, the main risk groups of the energy sector were subjected to an analysis. Then, the basic systematics and the division into particular risk groups were presented and the impact of the diversification of investments in the portfolio on the general level of risk was determined. The sources of uncertainty were discussed with particular attention to the categories of energy investments. The next part of the article presents risk mitigation methods that are part of the integrated risk management process and describes the basic methods supporting the quantification of the risk level and its effects - including the Monte Carlo (MC), Value at risk (VaR), and other methods. Finally, the paper presents the possible application of the methods presented in the theoretical part. The investment in agricultural biogas plant, due to the predictable operation accompanied by an extremely complicated and long-term investment process, was the subject of the analysis. An example of "large drawing analysis" was presented, followed by a Monte Carlo simulation and a VaR value determination. The presented study allows for determining the risk in the case of deviation of financial flows from the assumed values in particular periods and helps in determining the effects of such deviations. The conducted analysis indicates a low investment risk and suggests the ease of similar calculations for other investments. © 2018 Polityka Energetyczna. All rights reserved.}}, 
number = {4}, 
volume = {21}
}
@article{10.1177/1938965510395746, 
year = {2011}, 
title = {{Estimating cashflow-at-risk (CFaR): A comparables approach for restaurant firms}}, 
author = {Jang, SooCheong (Shawn) and Park, Kwangmin and Lee, Ji-eun}, 
journal = {Cornell Hospitality Quarterly}, 
issn = {19389655}, 
doi = {10.1177/1938965510395746}, 
abstract = {{Of the many risks faced by restaurants, a shortage of cashflow is among the most damaging. This study introduces an effective tool to estimate the cashflow risk of restaurants, so that firms can be ready to address the potential risks. Using a comparables approach to Cashflow-at-Risk (CFaR), the study analyzes cashflow data from publicly listed U.S. restaurant firms from 1988 to 2007. The study found that estimated cashflow shortfalls were larger for small and medium-size firms than for large firms. Also, full-service restaurants' cashflow shortfalls were larger than those of limited-service restaurants, suggesting relative cashflow risk positions for different restaurant segments. This study provides nonfinancial firms such as restaurant firms with a practical tool to estimate cashflow risk, which could mitigate the probability of financial distress and improve the financial health of firms. © The Author(s) 2011.}}, 
pages = {232--240}, 
number = {3}, 
volume = {52}
}
@article{10.1109/iwisa.2010.5473619, 
year = {2010}, 
title = {{Relationship between volatility of Shibor rates and IPOs of big and medium sized enterprises - An empiricial study using Chinese data}}, 
author = {Dong, Lifeng and Zheng, Xiutian}, 
journal = {2010 2nd International Workshop on Intelligent Systems and Applications}, 
issn = {NA}, 
doi = {10.1109/iwisa.2010.5473619}, 
abstract = {{Using a daily sample of the 1-week Shanghai interbank offered rate, we research the volatility of Chinese spot rate using EGARCH model, and then calculate its value at risk by VaR-EGARCH model. We also examine the effect of IPOs of big and medium enterprises in Chinese stock market on Shibor rates. The results show that big and medium sized IPOs have a significant impact on Shibor rates. The huge IPOs resulted in abnormal volatility of Shibor rates, and caused great risk in 2007 and 2008. The reform of IPOs system in 2009 has reduced the effect of huge IPOs on Shibor rates a lot. ©2010 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/iscc-c.2013.59, 
year = {2014}, 
title = {{Commercial bank stress tests based on credit risk}}, 
author = {Wang, Weiqing and Zhang, Xue and Liu, Xiangdong}, 
journal = {2013 International Conference on Information Science and Cloud Computing Companion}, 
issn = {NA}, 
doi = {10.1109/iscc-c.2013.59}, 
abstract = {{Based on the History-Based Stressed PD model which is derived from Merton theory and IRB model which is derived from Basel New Capital Accord, this paper selects six commercial banks to conduct the empirical research of credit risk stress testing. The result indicates that the value-at-risk calculated by IRM model is much higher than History-Based Stressed PD model, because the former is completely based on the theoretical model while the latter takes into consideration of the historical and realistic significance. In practice, this paper suggests to comprehensively consider the measuring results of two models to formulate risk control measures. © 2013 IEEE.}}, 
pages = {508--514}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10287-019-00350-8, 
year = {2020}, 
title = {{The Skew Normal multivariate risk measurement framework}}, 
author = {Bernardi, Mauro and Cerqueti, Roy and Palestini, Arsen}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-019-00350-8}, 
abstract = {{In this paper, we consider a random vector X= (X1, X2) following a multivariate Skew Normal distribution and we provide an explicit formula for the expected value of X conditioned to the event X≤ X¯ , with X¯ ∈ R2. Such a conditional expectation has an intuitive interpretation in the context of risk measures. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {105--119}, 
number = {1}, 
volume = {17}
}
@article{10.1080/09603107.2011.624081, 
year = {2012}, 
title = {{An analysis of the extreme returns distribution: The case of the Istanbul Stock Exchange}}, 
author = {Goncu, A and Akgul, A Karaman and Imamoğlu, O and Tiryakioğlu, M and Tiryakioğlu, M}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2011.624081}, 
abstract = {{The assumption of normality of asset returns is widely used in financial modelling, financial regulation on risks and capital and Value-at-Risk (VaR) modelling. As observed during times of stock market crashes or financial stress, extreme returns cannot be adequately modelled using the Gaussian distribution. In this study, we use the Extreme Value Theory (EVT) to model the extreme return behaviour of the Istanbul Stock Exchange (ISE), Turkey. Three different distributions are used, namely Gumbel, Fréchet and Weibull, for modelling extreme returns over different investment horizons. The goodness-of-fit for these distributions is verified by the Anderson-Darling goodness-of-fit test. VaR is computed with the proposed distributions and backtesting results indicate that the EVT provides superior risk management in all the sub-intervals considered compared to the VaR estimation under the assumption of a normal distribution. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {723--732}, 
number = {9}, 
volume = {22}
}
@article{10.1016/j.ejor.2013.07.008, 
year = {2014}, 
title = {{A semiparametric Bayesian approach to the analysis of financial time series with applications to value at risk estimation}}, 
author = {Ausín, M. Concepción and Galeano, Pedro and Ghosh, Pulak}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2013.07.008}, 
abstract = {{GARCH models are commonly used for describing, estimating and predicting the dynamics of financial returns. Here, we relax the usual parametric distributional assumptions of GARCH models and develop a Bayesian semiparametric approach based on modeling the innovations using the class of scale mixtures of Gaussian distributions with a Dirichlet process prior on the mixing distribution. The proposed specification allows for greater flexibility in capturing the usual patterns observed in financial returns. It is also shown how to undertake Bayesian prediction of the Value at Risk (VaR). The performance of the proposed semiparametric method is illustrated using simulated and real data from the Hang Seng Index (HSI) and Bombay Stock Exchange index (BSE30). © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {350--358}, 
number = {2}, 
volume = {232}
}
@article{10.1109/bife.2010.94, 
year = {2010}, 
title = {{Value-at-risk estimation of crude oil price via morphological component analysis}}, 
author = {He, Kaijian and Lai, Kin Keung and Yen, Jerome}, 
journal = {2010 Third International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2010.94}, 
abstract = {{With the increasing level of volatility in the crude oil market, the transient data feature becomes more prevalent in the market and is no longer ignorable during the risk measurement process. Since using a set of bases available there are multiple representations for these transient data features, the sparsity measure based Morphological Component Analysis (MCF) model is proposed in this paper to find the optimal combinations of representations for them. Therefore, this paper proposes a MCF based hybrid methodology for analyzing and forecasting the risk evolution in the crude oil market. The underlying transient data with distinct behaviors are extracted and analyzed using MCF model. The proposed algorithm incorporates these transient data features to adjust for estimates from traditional approach based on normal market condition during its risk measurement process. The reliability and stability of Value at Risk (VaR) estimated improve as a result of finer modeling procedure in the multi frequency and time domain while maintaining competent accuracy level, as supported by empirical studies in the representative West Taxes Intermediate (WTI) crude oil market. ©2010 IEEE.}}, 
pages = {381--385}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.intfin.2007.07.001, 
year = {2008}, 
title = {{Volatility forecasting: Intra-day versus inter-day models}}, 
author = {Angelidis, Timotheos and Degiannakis, Stavros}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2007.07.001}, 
abstract = {{Volatility prediction is the key variable in forecasting the prices of options, value-at-risk and, in general, the risk that investors face. By estimating not only inter-day volatility models that capture the main characteristics of asset returns, but also intra-day models, we were able to investigate their forecasting performance for three European equity indices. A consistent relation is shown between the examined models and the specific purpose of volatility forecasts. Although researchers cannot apply one model for all forecasting purposes, evidence in favor of models that are based on inter-day datasets when their criteria based on daily frequency, such as value-at-risk and forecasts of option prices, are provided. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {449--465}, 
number = {5}, 
volume = {18}
}
@article{10.1007/s00180-021-01078-3, 
year = {2021}, 
title = {{Testing a parameter restriction on the boundary for the g-and-h distribution: a simulated approach}}, 
author = {Bee, Marco and Hambuckers, Julien and Santi, Flavio and Trapin, Luca}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-021-01078-3}, 
abstract = {{We develop a likelihood-ratio test for discriminating between the g-and-h and the g distribution, which is a special case of the former obtained when the parameter h is equal to zero. The g distribution is a shifted lognormal, and is therefore suitable for modeling economic and financial quantities. The g-and-h is a more flexible distribution, capable of fitting highly skewed and/or leptokurtic data, but is computationally much more demanding. Accordingly, in practical applications the test is a valuable tool for resolving the tractability-flexibility trade-off between the two distributions. Since the classical result for the asymptotic distribution of the test is not valid in this setup, we derive the null distribution via simulation. Further Monte Carlo experiments allow us to estimate the power function and to perform a comparison with a similar test proposed by Xu and Genton (Comput Stat Data Anal 91:78–91, 2015). Finally, the practical relevance of the test is illustrated by two risk management applications dealing with operational and actuarial losses. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.}}, 
pages = {2177--2200}, 
number = {3}, 
volume = {36}
}
@article{10.1109/icmt.2011.06002623, 
year = {2011}, 
title = {{An improved historical simulation method to estimate the amount of refined oil retail value at risk VaR}}, 
author = {}, 
issn = {NA}, 
doi = {10.1109/icmt.2011.06002623}, 
abstract = {{Value at risk, an effective measurement of financial risk, can be used to forecast the risk associated with amount of refined oil. In this paper, an improved Historical Simulation Approach, HSGF is proposed, which is based on a former approach, HSAF. By comparing it with the HS,HSAF and HSGF approach, this paper give evidence to show that HSGF has a more effective forecasting power in the field of amount of refined oil risk management. © 2011 IEEE.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1287/opre.1090.0712, 
year = {2010}, 
title = {{From CVaR to uncertainty set: Implications in joint chance-constrained optimization}}, 
author = {Chen, Wenqing and Sim, Melvyn and Sun, Jie and Teo, Chung-Piaw}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1090.0712}, 
abstract = {{We review and develop different tractable approximations to individual chance-constrained problems in robust optimization on a variety of uncertainty sets and show their interesting connections with bounds on the conditional-value-at-risk (CVaR) measure. We extend the idea to joint chance-constrained problems and provide a new formulation that improves upon the standard approach. Our approach builds on a classical worst-case bound for order statistics problems and is applicable even if the constraints are correlated. We provide an application of the model on a network resource allocation problem with uncertain demand. ©2010 INFORMS.}}, 
pages = {470--485}, 
number = {2}, 
volume = {58}
}
@article{10.1109/icicisys.2009.5357767, 
year = {2009}, 
title = {{A mean-VaR analysis of arbitrage arbitrage portfolios}}, 
author = {Fang, Shuhong}, 
journal = {2009 IEEE International Conference on Intelligent Computing and Intelligent Systems}, 
issn = {NA}, 
doi = {10.1109/icicisys.2009.5357767}, 
abstract = {{Based on the definition of arbitrage portfolio and its return introduced in Fang (2006), the mean-VaR analysis for arbitrage portfolios is presented. The calculation of the mean-VaR arbitrage frontier is discussed which is related to the mean-variance arbitrage frontier. Moreover a practical example is presented. ©2009 IEEE.}}, 
pages = {704--708}, 
number = {NA}, 
volume = {1}
}
@article{10.1080/14697681003685597, 
year = {2010}, 
title = {{Robustness and sensitivity analysis of risk measurement procedures}}, 
author = {Cont, Rama and Deguest, Romain and Scandolo, Giacomo}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697681003685597}, 
abstract = {{Measuring the risk of a financial portfolio involves two steps: estimating the loss distribution of the portfolio from available observations and computing a 'risk measure' that summarizes the risk of the portfolio. We define the notion of 'risk measurement procedure', which includes both of these steps, and introduce a rigorous framework for studying the robustness of risk measurement procedures and their sensitivity to changes in the data set. Our results point to a conflict between the subadditivity and robustness of risk measurement procedures and show that the same risk measure may exhibit quite different sensitivities depending on the estimation procedure used. Our results illustrate, in particular, that using recently proposed risk measures such as CVaR/expected shortfall leads to a less robust risk measurement procedure than historical Value-at-Risk. We also propose alternative risk measurement procedures that possess the robustness property. © 2010 Taylor \& Francis.}}, 
pages = {593--606}, 
number = {6}, 
volume = {10}
}
@article{10.23919/cisti.2017.7975680, 
year = {2017}, 
title = {{Information system for the quantification of financial risk [Sistema de Información para la cuantificación de riesgos financieros]}}, 
author = {Arias-Serna, Maria Andrea and Caro-Lopera, Francisco José and Echeverri-Arias, Jaime Alberto and Castañeda-Palacio, Diego Alejandro and Murillo-Gómez, Juan Guillermo}, 
journal = {2017 12th Iberian Conference on Information Systems and Technologies (CISTI)}, 
issn = {21660727}, 
doi = {10.23919/cisti.2017.7975680}, 
abstract = {{The quantification of financial risk such as liquidity risk and others is one of the most frequent concern in the bank and corporative sector, in this sense, the liquidity risk materialization causes big monetary lost when corporations are incapable on give appropriate fulfillment of obligations due to lack of liquid resources. On the other hand, when operational risk is present, there are large losses due to fails on the procedures that adversely affect the functioning of the organization. With the goal of systematize the risk quantification it has implement the Information System Financial Risk Management, which was constructed like a suite of software compound by two applications that facilities the quantification of liquidity risk and operational risk. Nowadays the Information System is used by corporations in Colombian financial sector, who by means of use of tools has been reached the fulfillment the results, avoiding the materialization of negative events. © 2017 AISTI.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/9781118650318.ch10, 
year = {2016}, 
title = {{Measures of Financial Risk}}, 
author = {Novak, S.Y.}, 
issn = {NA}, 
doi = {10.1002/9781118650318.ch10}, 
abstract = {{Accurate evaluation of risk is fundamental to the financial well-being of financial institutions as well as individual investors. This chapter provides an overview of modern approaches to financial risk measurement, aiming to encourage interdisciplinary research on the topic of dynamic measures of risk. It briefly overviews the popular measures of risk, including value at risk (VaR) and a related measure called conditional VaR (CVaR) or Expected Shortfall. The chapter discusses the strengths and weaknesses of the traditional approach to risk measurement. After discussing the strengths and weaknesses of the traditional approach to risk measurement, the chapter overviews the basic tools of the Technical Analysis approach. The properties of a dynamic risk measure are presented. Finally, the chapter concentrates on a number of open questions concerning computational and statistical issues related to dynamic risk measurement. The traditional approach appears static from a short-term investor point of view. © 2017 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {215--237}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/jerer-01-2017-0007, 
year = {2018}, 
title = {{An empirical-deductive model for the assessment of the mortgage lending value of properties as securities for credit exposures}}, 
author = {Tajani, Francesco and Morano, Pierluigi}, 
journal = {Journal of European Real Estate Research}, 
issn = {17539269}, 
doi = {10.1108/jerer-01-2017-0007}, 
abstract = {{Purpose: This study aims to propose and test an innovative methodology for assessing mortgage lending value. The method tries to improve and rationalize, within the canonical and derivative approach that is generally used by the sector operators, the appraisal of the percentage reduction to be applied to the market value. Design/methodology/approach: Considering that the European Mortgage Federation and the Basel Committee highlight the importance of information about the risks of properties to be loaned on, the value at risk approach has been developed so as to assess the mortgage lending value as a technique of risk analysis. With reference to the Italian context, the method elaborates the historical analysis of the property values in 93 major Italian cities for the residential and commercial intended uses in a significant period (1967-2015) and allows to determine the reduction coefficients of the market value as a function of the central, semi-central and peripheral locations of the property. Findings: The results include the reduction coefficients of the market value for the derivative appraisal of the mortgage lending value. The coefficients obtained satisfy the need for a rational assessment of the property risk and the appropriate spatial contextualization of the risk components related to the local demand and supply, thus eliminating any inconsistency and danger of determining the mortgage lending value using a simple and lump-sum percentage deduction of the market value. Originality/value: The global economic crisis in the past decade, triggered by the 2007 US Subprime mortgage crisis and consequent collapse of property values, has highlighted the need for high level professional skills in the appraisal of properties as securities for credit exposures. The method proposed for the assessment of the mortgage lending value allows to overcome the uncertainties underlying the determination of an independent value through indirect methods (income approach, cost approach) and rationalize the appraisal of the risk in the traditional derivative approach through a flexible procedure, with it being possible to adapt it to any territorial context, as well as any intended use. © 2018, Emerald Publishing Limited.}}, 
pages = {44--70}, 
number = {1}, 
volume = {11}
}
@article{10.1016/j.asoc.2017.09.025, 
year = {2018}, 
title = {{Multi-objective heuristic algorithms for practical portfolio optimization and rebalancing with transaction cost}}, 
author = {Meghwani, Suraj S. and Thakur, Manoj}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2017.09.025}, 
abstract = {{Portfolio optimization is the process of allocating capital among a universe of assets to achieve better risk–return trade-off. Due to the dynamic nature of financial markets, the portfolio needs to be rebalanced to retain the desired risk–return characteristics. The process of rebalancing requires buying or selling of assets that incur transaction costs. This study proposes a tri-objective portfolio optimization model with risk, return and transaction cost as the objectives. Various practical constraints like cardinality, self-financing, quantity, pre-assignment and cost related constraints are included in the proposed model. Three popular risk measures namely variance, Value-At-Risk (VaR) and Conditional Value-At-Risk (CVaR) are studied in the proposed work. The emphasis of the study is on handling equality constraints like self-financing constraint and the constraints arising from the inclusion of transaction cost models using multi-objective evolutionary algorithms (MOEAs). A novel repair algorithm is proposed that can effectively handle equality constraints without any requirement of any constraint handling technique. The proposed repair algorithm is suitable for a larger class of separable transaction cost model. The theoretical proof is given to ensure the validity of our claim. To verify the effectiveness of the proposed approach three algorithms from different multi-objective evolutionary frameworks are adapted and compared. In empirical study, we discuss the performances of algorithms over both in-sample and out-sample data. © 2017 Elsevier B.V.}}, 
pages = {865--894}, 
number = {NA}, 
volume = {67}
}
@article{10.1016/j.insmatheco.2009.09.002, 
year = {2009}, 
title = {{Robust and efficient fitting of the generalized Pareto distribution with actuarial applications in view}}, 
author = {Brazauskas, Vytaras and Kleefeld, Andreas}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2009.09.002}, 
abstract = {{Due to advances in extreme value theory, the generalized Pareto distribution (GPD) emerged as a natural family for modeling exceedances over a high threshold. Its importance in applications (e.g., insurance, finance, economics, engineering and numerous other fields) can hardly be overstated and is widely documented. However, despite the sound theoretical basis and wide applicability, fitting of this distribution in practice is not a trivial exercise. Traditional methods such as maximum likelihood and method-of-moments are undefined in some regions of the parameter space. Alternative approaches exist but they lack either robustness (e.g., probability-weighted moments) or efficiency (e.g., method-of-medians), or present significant numerical problems (e.g., minimum-divergence procedures). In this article, we propose a computationally tractable method for fitting the GPD, which is applicable for all parameter values and offers competitive trade-offs between robustness and efficiency. The method is based on 'trimmed moments'. Large-sample properties of the new estimators are provided, and their small-sample behavior under several scenarios of data contamination is investigated through simulations. We also study the effect of our methodology on actuarial applications. In particular, using the new approach, we fit the GPD to the Danish insurance data and apply the fitted model to a few risk measurement and ratemaking exercises. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {424--435}, 
number = {3}, 
volume = {45}
}
@article{10.1002/asmb.2115, 
year = {2015}, 
title = {{Comparison of two algorithms for solving a two-stage bilinear stochastic programming problem with quantile criterion}}, 
author = {Kibzun, Andrey}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2115}, 
abstract = {{The paper is devoted to solving the two-stage problem of stochastic programming with quantile criterion. It is assumed that the loss function is bilinear in random parameters and strategies, and the random vector has a normal distribution. Two algorithms are suggested to solve the problem, and they are compared. The first algorithm is based on the reduction of the original stochastic problem to a mixed integer linear programming problem. The second algorithm is based on the reduction of the problem to a sequence of convex programming problems. Performance characteristics of both the algorithms are illustrated by an example. A modification of both the algorithms is suggested to reduce the computing time. The new algorithm uses the solution obtained by the second algorithm as a starting point for the first algorithm. © 2015 John Wiley \& Sons, Ltd.}}, 
pages = {862--874}, 
number = {6}, 
volume = {31}
}
@article{10.1007/978-3-642-38279-6_10, 
year = {2013}, 
title = {{Estimating risk with sarmanov copula and nonparametric marginal distributions}}, 
author = {Bahraoui, Zuhair and Bolancé, Catalina and Alemany, Ramon}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-642-38279-6\_10}, 
abstract = {{We show that Sarmanov copula and kernel estimation can be mixed to estimate the risk of an economic loss. We use a bivariate sample from a real data base. We show that the estimation of the dependence parameter of the copula using double transformed kernel estimation to estimate marginal cumulative distribution functions provides balanced risk estimates. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {91--98}, 
number = {NA}, 
volume = {145}
}
@article{10.1080/03610926.2020.1754857, 
year = {2020}, 
title = {{New stochastic comparisons based on tail value at risk measures}}, 
author = {Belzunce, Félix and Franco-Pereira, Alba M. and Mulero, Julio}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2020.1754857}, 
abstract = {{In this article we provide a new criterion for the comparison of claims, when we have conditional claims arising in stop loss contracts or contracts with franchise deductible. These stochastic comparisons are made on the basis of the Tail Value at Risk (also known as conditional tail expectation), just for a fixed level and beyond. In particular, we explain the interest of comparing these quantities, study some preservation properties and, in addition, we provide sufficient conditions for its study. Finally we illustrate its usefulness with some examples. © 2020, © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--22}, 
number = {NA}, 
volume = {NA}
}
@article{10.5539/ass.v11n24p315, 
year = {2015}, 
title = {{Analysis of the development of financial risk management of the enterprise in Kazakhstan}}, 
author = {Aliyeva, B M and Zholamanova, M T and Zhorabayeva, Zh K}, 
journal = {Asian Social Science}, 
issn = {19112017}, 
doi = {10.5539/ass.v11n24p315}, 
abstract = {{In this scientific article based on the study of problems of effective management of financial risks of Kazakhstani companies and “KazMunaiGas” JSC were identified the main risks, which typical for Kazakhstan’s financial market at the present stage of its development, was justified the feasibility of using indicators to measure the level of financial risks on the basis of the comparative analysis based on the VaR methodology, as well as the expediency of the use of futures and options contracts to hedge the financial risks of the companies. © 2015, Canadian Center of Science and Education. All rights reserved.}}, 
number = {24}, 
volume = {11}
}
@article{10.1007/s13385-013-0080-x, 
year = {2013}, 
title = {{A compound renewal model for medical malpractice insurance}}, 
author = {Léveillé, Ghislain and Hamel, Emmanuel}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-013-0080-x}, 
abstract = {{A renewal model for the aggregate discounted payments and expenses assumed by the insurer is proposed for the “medical malpractice” insurance, where real interest rates could be stochastic and the dependencies between the expenses, the payments and the delays of payment are examined through the theory of copulas. As a first approach to this problem, we present formulas for the first two raw moments and the first joint moment of this aggregate risk process. Examples are given for Erlang claims interoccurence times and delays of payment, Pareto payments and expenses, and the influence of the dependency is illustrated by the Joe copula. Finally the distribution, VaR and TVaR of our risk process are also considered through simulations. © 2013, DAV / DGVFM.}}, 
pages = {471--490}, 
number = {2}, 
volume = {3}
}
@article{10.1016/j.econmod.2014.07.032, 
year = {2014}, 
title = {{A two-regime threshold model with conditional skewed Student t distributions for stock returns}}, 
author = {Massacci, Daniele}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2014.07.032}, 
abstract = {{This paper proposes a two-regime threshold model for the conditional distribution of stock returns in which returns follow a distinct skewed Student t distribution within each regime: the model allows capturing time variation in the conditional distribution of returns, as well as higher order moments. An application of the model to daily U.S. stock returns illustrates the advantages of the proposed model in comparison to alternative specifications: the model performs well in terms of in-sample fit; it more accurately estimates the conditional volatility; and it produces useful risk assessment as measured by the term structure of value at risk. © 2014 Elsevier B.V.}}, 
pages = {9--20}, 
number = {NA}, 
volume = {43}
}
@article{10.1016/j.eneco.2011.10.001, 
year = {2012}, 
title = {{Modelling energy spot prices: Empirical evidence from NYMEX}}, 
author = {Nomikos, Nikos and Andriosopoulos, Kostas}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2011.10.001}, 
abstract = {{This paper investigates the behaviour of spot prices in eight energy markets that trade futures contracts on NYMEX. We consider two types of models, a mean-reverting model, and a spike model with mean reversion that incorporates two different speeds of mean reversion; one for the fast mean-reverting behaviour of prices after a jump occurs, and another for the slower mean reversion rate of the diffusive part of the model. We also extend these models to incorporate time-varying volatility in their specification, modelled as a GARCH and an EGARCH process. We compare the relative goodness of fit of the different modelling variations both in sample, using Monte Carlo simulations, as well as out-of-sample, in a Value-at-Risk (VaR) setting. Our results indicate the presence of a "leverage effect" for WTI, Heating Oil and Heating Oil-WTI crack spread, whereas for the remaining energy markets we find the presence of an "inverse leverage" effect. Also, the addition of the EGARCH specification for the volatility process improves both the in-sample fit as well as the out-of-sample VaR performance for most energy markets that we examine. © 2011 Elsevier B.V.}}, 
pages = {1153--1169}, 
number = {4}, 
volume = {34}
}
@article{10.1287/opre.2016.1539, 
year = {2016}, 
title = {{On the measurement of economic tail risk}}, 
author = {Kou, Steven and Peng, Xianhua}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.2016.1539}, 
abstract = {{This paper attempts to provide a decision-theoretic foundation for the measurement of economic tail risk, which is not only closely related to utility theory but also relevant to statistical model uncertainty. The main result is that the only risk measures that satisfy a set of economic axioms for the Choquet expected utility and the statistical property of general elicitability (i.e., there exists an objective function such that minimizing the expected objective function yields the risk measure) are the mean functional and value-at-risk (VaR), in particular the median shortfall, which is the median of tail loss distribution and is also the VaR at a higher confidence level. We also discuss various approaches of backtesting and their relations to elicitability and co-elicitability; in particular, we show that the co-elicitability of VaR and expected shortfall does not lead to a reliable backtesting method for expected shortfall and there have been only indirect backtesting methods for expected shortfall. Furthermore, we extend the result to address model uncertainty by incorporating multiple scenarios. As an application, we argue that median shortfall is a better alternative than expected shortfall for setting capital requirements in Basel Accords. © 2016 INFORMS.}}, 
pages = {1056--1072}, 
number = {5}, 
volume = {64}
}
@article{10.1016/j.ejor.2019.07.011, 
year = {2020}, 
title = {{Estimating Value-at-Risk and Expected Shortfall using the intraday low and range data}}, 
author = {Meng, Xiaochun and Taylor, James W.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2019.07.011}, 
abstract = {{Value-at-Risk (VaR) is a popular measure of market risk. To convey information regarding potential exceedances beyond the VaR, Expected Shortfall (ES) has become the risk measure for trading book bank regulation. However, the estimation of VaR and ES is challenging, as it requires the estimation of the tail behaviour of daily returns. In this paper, we take advantage of recent research that develops joint scoring functions for VaR and ES. Using these functions, we present a novel approach to estimating the two risk measures based on intraday data. We focus on the intraday range, which is the difference between the highest and lowest intraday log prices. In contrast to intraday observations, the intraday low and high are widely available for many financial assets. To alleviate the challenge of modelling extreme risk measures, we propose the use of the intraday low series. We draw on a theoretical result for Brownian motion to show that a quantile of the daily returns can be estimated as the product of a constant term and a less extreme quantile of the intraday low returns, which we define as the difference between the lowest log price of the day and the log closing price of the previous day. In view of this, we use estimates of the VaR and ES of the intraday low returns to estimate the VaR and ES of the daily returns. We provide empirical support for the new proposals using data for five stock indices and five individual stocks. © 2019 Elsevier B.V.}}, 
pages = {191--202}, 
number = {1}, 
volume = {280}
}
@article{10.1109/appeec.2011.5747720, 
year = {2011}, 
title = {{The empirical study of VaR model in the margin of Chinese stock index futures}}, 
author = {Lu, Qian and Yu, Mei}, 
journal = {2011 Asia-Pacific Power and Energy Engineering Conference}, 
issn = {21574839}, 
doi = {10.1109/appeec.2011.5747720}, 
abstract = {{In this paper, we study the margin of Chinese Stock Index Futures using GARCH-VaR model and Monte Carlo simulation respectively. The result of the empirical study shows that the model of GARCH-VaR is more precise in describing the proper margin level. Furthermore, the average margin level of the long is higher than the short, which means the long is exposed to more risks than the short Moreover, we find that the level of the margin is lower than current domestic stock index futures security level at both 99\% and 95\% confidence level. © 2011 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s12190-017-1096-1, 
year = {2018}, 
title = {{Optimal quota-share and stop-loss reinsurance from the perspectives of insurer and reinsurer}}, 
author = {Liu, Hongli and Fang, Ying}, 
journal = {Journal of Applied Mathematics and Computing}, 
issn = {15985865}, 
doi = {10.1007/s12190-017-1096-1}, 
abstract = {{Reinsurance plays a vital role in the insurance activities. The insurer and the reinsurer, which have conflicting interests, compose the two parties of a reinsurance contract. In this paper, we extend the results achieved by Tan et al. (N Am Actuar J 13(4):459–482, 2009) to the case in which the perspectives of both the insurer and the reinsurer are considered. We study the optimal quota-share and stop-loss reinsurance models by minimizing the convex combination of the VaR risk measures of the insurer’s cost and the reinsurer’s cost. Furthermore, as many as 16 reinsurance premium principles are investigated. The results show that optimal quota-share and stop-loss reinsurance may or may not exist depending on the chosen principles. Moreover, we establish the sufficient and necessary conditions for the existence of the nontrivial optimal reinsurance. © 2017, Korean Society for Computational and Applied Mathematics.}}, 
pages = {85--104}, 
number = {1-2}, 
volume = {57}
}
@article{10.1057/palgrave.gpp.2510071, 
year = {2006}, 
title = {{Stochastic modelling - Boon or bane for insurance industry capital regulation?}}, 
author = {Bäte, Oliver and Plato, Philipp von and Thallinger, Günther}, 
journal = {The Geneva Papers on Risk and Insurance - Issues and Practice}, 
issn = {10185895}, 
doi = {10.1057/palgrave.gpp.2510071}, 
abstract = {{Regulation of the financial industry should pursue three key objectives: consumer protection, market stability, and competitive efficiency. This article discusses core elements of a capital regime that could be used to develop regulation that meets these objectives while fostering an industry-wide enhancement of risk management. The authors argue that a pre-commitment approach can have considerable advantages over regulation based on (stochastic) risk models, as the latter can have adverse effects, especially on market stability and competitive efficiency, while consumer protection would have to be supplemented by additional requirements (such as scenario tests) in any case. Academic studies on capital regulation based on stochastic models have focused more on banking and less on insurance, while work by insurance practitioners has concentrated on the implications for management. The authors, therefore, wish to contribute to a more fundamental discussion of the design of capital and risk management regulation of the insurance industry. © 2006 The International Association for the Study of Insurance Economics.}}, 
pages = {57--82}, 
number = {1}, 
volume = {31}
}
@article{10.1504/ijmor.2011.040028, 
year = {2011}, 
title = {{Sensitivity analysis of portfolio properties with budget constraints}}, 
author = {Borgonovo, Emanuele and Percoco, Marco}, 
journal = {International Journal of Mathematics in Operational Research}, 
issn = {17575850}, 
doi = {10.1504/ijmor.2011.040028}, 
abstract = {{We develop a framework for understanding how variations of portfolio properties are apportioned to changes in portfolio composition as trading is performed in the presence of budget constraints. Our approach is based on the concept of constrained derivative. It allows one to obtain the simultaneous sensitivity of portfolio properties w.r.t. assets groups for any choice of pivotal asset. Analytical expressions for the sensitivity of portfolio returns, variance, GARCH volatility and Value at Risk (VaR) are derived. A numerical exemplification is proposed with reference to the 30 assets of the Dow Jones Index. Copyright © 2011 Inderscience Enterprises Ltd.}}, 
pages = {295}, 
number = {3}, 
volume = {3}
}
@article{10.1016/j.cya.2015.09.008, 
year = {2016}, 
title = {{Estimating market risk metrics using gaussian mixtures [Estimación de métricas de riesgo de mercado usando mixturas gaussianas]}}, 
author = {Contreras, Jorge Rosales}, 
journal = {Contaduría y Administración}, 
issn = {01861042}, 
doi = {10.1016/j.cya.2015.09.008}, 
abstract = {{The most commonly used financial models for the estimation of market risk either assume that asset returns follow a Normal distribution or are based on the empirical distribution. More often than not, the normality assumption is taken for granted. However, it is not realistic due to skewness and excess kurtosis observed in the actual behavior of asset returns. In this work we show evidence that finite Gaussian mixtures are an efficient model for the distribution of asset returns. We study the model and obtain expressions to estimate the usual market risk metrics. We illustrate its application by estimating risk figures for a portfolio of Mexican assets using the proposed model and comparing them against values produced with the most widely used models. © 2015 Universidad Nacional Autónoma de México, Facultad de Contaduría y Administración.}}, 
pages = {202--219}, 
number = {1}, 
volume = {61}
}
@article{10.1063/1.4882525, 
year = {2014}, 
title = {{Empirical application of normal mixture GARCH and value-at-risk estimation}}, 
author = {Kamaruzzaman, Zetty Ain and Isa, Zaidi}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.4882525}, 
abstract = {{Normal mixture (NM) GARCH model can capture time variation in both conditional skewness and kurtosis. In this paper, we present the general framework of Normal mixture GARCH (1,1). An empirical application is presented using Malaysia weekly stock market returns. This paper provides evidence that, for modeling stock market returns, two-component Normal mixture GARCH (1,1) model perform better than Normal, symmetric and skewed Student's t-GARCH models. This model can quantify the volatility corresponding to stable and crash market circumstances. We also consider Value-at-Risk (VaR) estimation for Normal mixture GARCH model. © 2014 AIP Publishing LLC.}}, 
pages = {453--459}, 
number = {NA}, 
volume = {1602}
}
@article{10.1080/03610926.2014.938829, 
year = {2016}, 
title = {{The use of flexible quantile-based measures in risk assessment}}, 
author = {Belles-Sampera, Jaume and Guillén, Montserrat and Santolino, Miguel}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2014.938829}, 
abstract = {{A new family of distortion risk measures-GlueVaR-is proposed in Belles-Sampera et al. (2014) to procure a risk assessment lying between those provided by common quantile-based risk measures. GlueVaR measures may be expressed as a combination of these standard risk measures. We show here that this relationship may be used to obtain approximations of GlueVaR measures for general skewed distribution functions using the Cornish-Fisher expansion. A subfamily of GlueVaR measures satisfies the tail-subadditivity property. An example of risk measurement based on real insurance claim data is presented, where implications of tail-subadditivity in the aggregation of risks are illustrated. © 2016 Taylor and Francis Group, LLC.}}, 
pages = {1670--1681}, 
number = {6}, 
volume = {45}
}
@article{10.1016/j.jbankfin.2012.10.009, 
year = {2013}, 
title = {{A leverage ratio rule for capital adequacy}}, 
author = {Jarrow, Robert}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.10.009}, 
abstract = {{This paper studies the economic foundations for maximum leverage ratio capital adequacy rules. The paper makes three contributions to the literature. First, we show how to determine the maximum leverage ratio such that the probability of insolvency is less than some predetermined quantity. Two, we show that a leverage ratio rule controls for the same risks as does a Value-at-Risk (VaR) capital adequacy rule. Third, we argue that leverage ratio rules are better than VaR rules because they are more intuitive and easier to compare across firms. © 2012 Elsevier B.V.}}, 
pages = {973--976}, 
number = {3}, 
volume = {37}
}
@article{10.1016/j.ecosta.2021.03.006, 
year = {2021}, 
title = {{Bayesian estimation of realized GARCH-type models with application to financial tail risk management}}, 
author = {Chen, Cathy W.S. and Watanabe, Toshiaki and Lin, Edward M.H.}, 
journal = {Econometrics and Statistics}, 
issn = {24523062}, 
doi = {10.1016/j.ecosta.2021.03.006}, 
abstract = {{Advances in the various realized GARCH models have proven effective in taking account of the bias in realized volatility (RV) introduced by microstructure noise and non-trading hours. They have been extended into nonlinear or long-memory patterns, including the realized exponential GARCH (EGARCH), realized heterogeneous autoregressive GARCH (HAR-GARCH), and realized threshold GARCH (TGARCH) models. These models with skew Student's t-distribution are applied to quantile forecasts such as Value-at-Risk and expected shortfall of financial returns as well as volatility forecasting. Parameter estimation and quantile forecasting are built on Bayesian Markov chain Monte Carlo sampling methods. Backtesting measures are presented for both Value-at-Risk and expected shortfall forecasts and employ two loss functions to assess volatility forecasts. Results taken from the S\&P500 in the U.S. market with approximately 5-year out-of-sample periods covering the COVID-19 pandemic period are reported as follows: (1) The realized HAR-GARCH model performs best in respect of violation rates and expected shortfall at the 1\% and 5\% significance levels. (2) The realized EGARCH model performs best with regard to volatility forecasts. © 2021 EcoSta Econometrics and Statistics}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1145/324138.324241, 
year = {1999}, 
title = {{Stratification issues in estimating value-at-risk}}, 
author = {Glasserman, Paul and Heidelberger, Philip and Shahabuddin, Perwez}, 
journal = {Proceedings of the 31st conference on Winter simulation Simulation---a bridge to the future - WSC '99}, 
issn = {02750708}, 
doi = {10.1145/324138.324241}, 
abstract = {{This paper considers efficient estimation of value-at-risk, which is an important problem in risk management. The value-at-risk is an extreme quantile of the distribution of the loss in portfolio value during a holding period. An effective importance sampling technique is described for this problem. The importance sampling can be further improved by combining it with stratified sampling. In this setting, an effective stratification variable is the likelihood ratio itself. The paper examines issues associated with the allocation of samples to the strata, and compares the effectiveness of the combination of importance sampling and stratified sampling to that of stratified sampling alone.}}, 
pages = {351--358}, 
number = {NA}, 
volume = {1}
}
@article{10.7232/iems.2017.16.3.400, 
year = {2017}, 
title = {{A monte-carlo-based latent factor modeling approach with time-varying volatility for value-at-risk estimation: Case of the tunisian foreign exchange market}}, 
author = {Saidane, Mohamed}, 
journal = {Industrial Engineering \& Management Systems}, 
issn = {15987248}, 
doi = {10.7232/iems.2017.16.3.400}, 
abstract = {{The normal probability distribution assumption, to model price changes in Finance, belongs to the largest imperfections in the Value-at-Risk (VaR) estimation. In fact, the financial returns are rather distributed leptokurtic than normally and the empirical distributions are often skewed. In these cases, the normal distribution assumption results in over or underestimation of VaR especially when the quantiles are very high/low. Therefore, it is necessary to put emphasis on respecting the leptokurtic and skewed return distribution. In this paper, we propose a new approach for portfolio VaR estimation, which combines the standard latent factor model with the generalized quadratic autoregressive conditionally heteroskedastic model (GQARCH). This new “hybrid” specification provides an alternative, compact, model to handle co-movements, heteroskedasticity and intra-frame correlations in financial data. For maximum likelihood estimation we have used an iterative approach based on an extended version of the Kalman filter algorithm combined with the Expectation Maximization (EM) algorithm. Using a set of historical data, from the Tunisian foreign exchange market, the model parameters are estimated. Then, the fitted model combined with a modified Monte-Carlo simulation algorithm was used to predict the VaR of the Tunisian public debt portfolio. Through a Backtesting analysis, we found that this new specification produces far more accurate forecasts for the VaR compared to the mixture of factor analyzers and other competing approaches. © 2017 KIIE.}}, 
pages = {400--414}, 
number = {3}, 
volume = {16}
}
@article{10.1108/00214660780001211, 
year = {2007}, 
title = {{Cooperative risk management, rationale, and effectiveness: The case of dairy cooperatives}}, 
author = {Manfredo, Mark R. and Richards, Timothy J.}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00214660780001211}, 
abstract = {{Numerical simulation of several typical risk management strategies using pro forma financial statements from representative U.S. dairy cooperatives shows that combinations of forwards, swaps, and cash marketing strategies for output (cheese), along with various forward contracts offered to cooperative members to manage the variability of milk revenues, have the potential to improve cooperative‐, and ultimately member‐level risk‐return performance. Because most cooperatives have limited access to equity capital, effective use of available risk management tools can increase cooperative value by increasing debt capacity, avoiding bankruptcy costs, and preventing the distortion of capital budgeting decisions. Moreover, the offering of risk management tools to individual members as a service may prove valuable in the retention of these members in the cooperative. © 2007, Emerald Group Publishing Limited. All Right Reserved.}}, 
pages = {311--339}, 
number = {2}, 
volume = {67}
}
@article{10.1016/j.energy.2016.05.029, 
year = {2016}, 
title = {{Multi-objective unit commitment with wind penetration and emission concerns under stochastic and fuzzy uncertainties}}, 
author = {Wang, Bo and Wang, Shuming and Zhou, Xianzhong and Watada, Junzo}, 
journal = {Energy}, 
issn = {03605442}, 
doi = {10.1016/j.energy.2016.05.029}, 
abstract = {{Recent years have witnessed the ever increasing renewable penetration in power generation systems, which entails modern unit commitment problems with modelling and computation burdens. This study aims to simulate the impacts of manifold uncertainties on system operation with emission concerns. First, probability theory and fuzzy set theory are applied to jointly represent the uncertainties such as wind generation, load fluctuation and unit outage that interleaved in unit commitment problems. Second, a Value-at-Risk-based multi-objective approach is developed as a bridge of existing stochastic and robust unit commitment optimizations, which not only captures the inherent conflict between operation cost and supply reliability, but also provides easy-to-adjust robustness against worst-case scenarios. Third, a multi-objective algorithm that integrates fuzzy simulation and particle swarm optimization is developed to achieve approximate Pareto-optimal solutions. The research effectiveness is exemplified by two case studies: The comparison between test systems with and without generation uncertainty demonstrates that this study is practicable and can suggest operational insights of generation mix systems. The sensitivity analysis on Value-at-Risk proves that our method can achieve adequate tradeoff between performance optimality and robustness, thus help system operators in making informed decisions. Finally, the model and algorithm comparisons also justify the superiority of this research. © 2016 Elsevier Ltd.}}, 
pages = {18--31}, 
number = {NA}, 
volume = {111}
}
@article{10.1109/iccae.2010.5451427, 
year = {2010}, 
title = {{A risk management framework for electricity market planning}}, 
author = {Song, Jian and Dong, Zhaoyang}, 
journal = {2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)}, 
issn = {NA}, 
doi = {10.1109/iccae.2010.5451427}, 
abstract = {{As the power industry entered into the deregulated era, the risk management has become an essential tool to help power companies manage the risk. This thesis will investigate into this industry and propose a risk management framework for the market. ©2010 IEEE.}}, 
pages = {341--345}, 
number = {NA}, 
volume = {3}
}
@article{10.1016/j.insmatheco.2018.04.003, 
year = {2018}, 
title = {{Which eligible assets are compatible with comonotonic capital requirements?}}, 
author = {Koch-Medina, Pablo and Munari, Cosimo and Svindland, Gregor}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2018.04.003}, 
abstract = {{Within the context of capital adequacy, we study comonotonicity of risk measures in terms of the primitives of the theory: acceptance sets and eligible, or reference, assets. We show that comonotonicity cannot be characterized by the properties of the acceptance set alone and heavily depends on the choice of the eligible asset. In fact, in many important cases, comonotonicity is only compatible with risk-free eligible assets. The incompatibility with risky eligible assets is systematic whenever the acceptability criterion is based on Value-at-Risk or any convex distortion risk measure such as Expected Shortfall. These findings qualify and arguably call for a critical appraisal of the meaning and the role of comonotonicity within a capital adequacy context. © 2018 Elsevier B.V.}}, 
pages = {18--26}, 
number = {NA}, 
volume = {81}
}
@article{10.1007/s11408-016-0281-9, 
year = {2017}, 
title = {{How does the underlying affect the risk-return profiles of structured products?}}, 
author = {Cao, Ji}, 
journal = {Financial Markets and Portfolio Management}, 
issn = {19344554}, 
doi = {10.1007/s11408-016-0281-9}, 
abstract = {{Regulators of some of the major markets have adopted value at risk (VaR) as the risk measure for structured products. Under the mean-VaR framework, this paper discusses the impact of the underlying’s distribution on structured products. We expand the expected return and the VaR of a structured product with its underlying’s moments (mean, variance, skewness, and kurtosis), so that the impact of the moments can be investigated simultaneously. Results are tested by Monte Carlo and historical simulations. The findings show that for the majority of structured products, underlyings with large positive skewness are preferred. The preferences for the variance and the kurtosis of the underlying are both ambiguous. © 2017, Swiss Society for Financial Market Research.}}, 
pages = {27--47}, 
number = {1}, 
volume = {31}
}
@article{10.1016/j.jeconom.2019.07.002, 
year = {2019}, 
title = {{Regime switching dynamic correlations for asymmetric and fat-tailed conditional returns}}, 
author = {Paolella, Marc S. and Polak, Paweł and Walker, Patrick S.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2019.07.002}, 
abstract = {{A non-Gaussian multivariate regime switching dynamic correlation model for financial asset returns is proposed. It incorporates the multivariate generalized hyperbolic law for the conditional distribution of returns. All model parameters are estimated consistently using a new two-stage expectation–maximization algorithm that also allows for incorporation of shrinkage estimation via quasi-Bayesian priors. It is shown that use of Markov switching correlation dynamics not only leads to highly accurate risk forecasts, but also potentially reduces the regulatory capital requirements during periods of distress. In terms of portfolio performance, the new regime switching model delivers consistently higher Sharpe ratios and smaller losses than the equally weighted portfolio and all competing models. Finally, the regime forecasts are employed in a new dynamic risk control strategy that avoids most losses during the financial crisis and vastly improves risk-adjusted returns. © 2019 Elsevier B.V.}}, 
pages = {493--515}, 
number = {2}, 
volume = {213}
}
@article{10.1016/j.irfa.2012.02.003, 
year = {2012}, 
title = {{Econometric modeling and value-at-risk using the Pearson type-IV distribution}}, 
author = {Stavroyiannis, S. and Makris, I. and Nikolaidis, V. and Zarangas, L.}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2012.02.003}, 
abstract = {{The recent financial crisis of 2007-2009 has challenged the requirements of Basel II agreement on capital adequacy as well as, the appropriateness of value-at-risk (VaR) measurement for properly "back-tested" and "stress-tested" models. This paper reconsiders the use of VaR as a measure for potential risk of economic losses in financial markets. We incorporate a GARCH model where the innovation process follows the Pearson-IV distribution, and the results are compared with the skewed Student-t distribution, in the sense of Fernandez and Steel. As case studies we consider the major historical indices of daily returns, DJIA, NASDAQ Composite, FTSE100, CAC40, DAX, and S\&P500. VaR and backtesting are performed by the success-failure ratio, the Kupiec LR test, the Christoffersen independence and conditional coverage tests, the expected shortfall with ESF1 and ESF2 measures, and the dynamic quantile test of Engle and Manganelli. The main findings indicate that the Pearson type-IV distribution gives better results, compared with the skewed student distribution, especially at the high confidence levels, providing a very good candidate as an alternative distributional scheme. © 2012 Elsevier Inc.}}, 
pages = {10--17}, 
number = {NA}, 
volume = {22}
}
@article{10.1109/access.2019.2959789, 
year = {2019}, 
title = {{Mixture of Activation Functions with Extended Min-Max Normalization for Forex Market Prediction}}, 
author = {Munkhdalai, Lkhagvadorj and Munkhdalai, Tsendsuren and Park, Kwang Ho and Lee, Heon Gyu and Li, Meijing and Ryu, Keun Ho}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2019.2959789}, 
abstract = {{An accurate exchange rate forecasting and its decision-making to buy or sell are critical issues in the Forex market. Short-Term currency rate forecasting is a challenging task due to its inherent characteristics, which include high volatility, trend, noise, and market shocks. We propose a novel deep learning architecture consisting of an adaptive activation function selection mechanism to achieve higher predictive accuracy. The proposed architecture is composed of seven neural networks that have different activation functions as well as softmax layer and multiplication layer with a skip connection, which are used to generate the dynamic importance weights that decide which activation function is preferred. In addition, we introduce an extended Min-Max smoothing technique to further normalize financial time series that have non-stationary properties. In our experimental evaluation, the results showed that our proposed model not only outperforms deep neural network baselines but also other classic machine learning approaches. The extended Min-Max smoothing technique is step towards forecasting non-stationary financial time series with deep neural networks. © 2013 IEEE.}}, 
pages = {183680--183691}, 
number = {NA}, 
volume = {7}
}
@article{10.3233/978-1-61499-264-6-225, 
year = {2013}, 
title = {{Fuzzy portfolio selection based on index tracking and value-at-risk}}, 
author = {}, 
issn = {09226389}, 
doi = {10.3233/978-1-61499-264-6-225}, 
abstract = {{In this paper, a new fuzzy portfolio selection model based on index tracking is proposed. The index tracking is to make the portfolio of stocks perform as close to the benchmark as possible. Therefore, it can be taken as a risk controller that ensures the reliability of a portfolio. In addition, fuzzy Value-at-risk (VaR) which describes the greatest loss of an investment at a specified confidence level is employed here as another risk measurement. To solve the proposed models in some special situations, we derived several crisp equivalent forms of the PSM. In general cases, we design a fuzzy simulation-based particle swarm optimization (PSO) algorithm to find optimal results. Finally, a numerical example is provided to illustrate the proposed model and the hybrid PSO algorithm. We engage in discussions about the experimental results, and compare our models with conventional approaches which do not apply index tracking. © 2013 The authors and IOS Press.}}, 
number = {NA}, 
volume = {255}
}
@article{10.1007/s10693-021-00366-9, 
year = {2021}, 
title = {{Exploring the Systemic Risk of Domestic Banks with ΔCoVaR and Elastic-Net}}, 
author = {Bianchi, Michele Leonardo and Sorrentino, Alberto Maria}, 
journal = {Journal of Financial Services Research}, 
issn = {09208550}, 
doi = {10.1007/s10693-021-00366-9}, 
abstract = {{We analyze the systemic risk of Italian banks with the ΔCoVaR from a bivariate normal GARCH model. The results show that it is a good measure of systemic risk and is applicable to the ranking of Italian other systemically important institutions. Using an elastic-net approach, we identify the balance sheet and market variables that explain the ΔCoVaR of Italian banks. The analysis confirms that these variables are key determinants of systemic importance and highlights how higher capitalization is beneficial to tackling systemic risk. And, we detect a connection between ΔCoVaR and some variables for trading and investment banking. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1--15}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/17499518.2014.966116, 
year = {2014}, 
title = {{Analyzing reproduction of correlations in Monte Carlo simulations: application to mine project valuation}}, 
author = {Torikian, Hrair and Kumral, Mustafa}, 
journal = {Georisk: Assessment and Management of Risk for Engineered Systems and Geohazards}, 
issn = {17499518}, 
doi = {10.1080/17499518.2014.966116}, 
abstract = {{Monte Carlo (MC) simulations are extensively used to assess risk in mining ventures; however, the correlation between the inputs used to build the models is often overlooked. We observed how value-at-risk (VaR) of a mining venture was affected by running MC simulations, using two different input correlation methods: Spearman's rank correlation and copulas using Kendall's tau. The goal was to compare different correlation approaches on risk analysis associated with uncertain parameters of mining ventures and uncover which one would yield the most accurate result. Three case studies were carried out to compare correlation structures. Modelling the input variable correlations was better achieved using copulas since they were able to capture a wider range of correlations that did not make any linearity assumptions. In the case study based on MC simulations, the impact of the input correlation choice on the VaR was rather severe with an approximate 9\% difference between the results obtained with Spearman's correlations and the Normal copula correlations. © 2014, © 2014 Taylor \& Francis.}}, 
pages = {235--249}, 
number = {4}, 
volume = {8}
}
@article{10.1080/14697688.2013.819113, 
year = {2014}, 
title = {{Estimation of tail-related value-at-risk measures: Range-based extreme value approach}}, 
author = {Chou, Heng-Chih and Wang, David K.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2013.819113}, 
abstract = {{This study proposes a new approach for estimating value-at-risk (VaR). This approach combines quasi-maximum-likelihood fitting of asymmetric conditional autoregressive range (ACARR) models to estimate the current volatility and classical extreme value theory (EVT) to estimate the tail of the innovation distribution of the ACARR model. The proposed approach reflects two well-known phenomena found in most financial time series: stochastic volatility and the fat-tailedness of conditional distributions. This approach presents two main advantages over the McNeil and Frey approach. First, the ACARR model in this approach is an asymmetric model that treats the upward and downward movements of the asset price asymmetrically, whereas the generalized autoregressive conditional heteroskedasticity model in the McNeil and Frey approach is a symmetric model that ignores the asymmetric structure of the asset price. Second, the proposed method uses classical EVT to estimate the tail of the distribution of the residuals to avoid the threshold issue in the modern EVT model. Since the McNeil and Frey approach uses modern EVT, it may estimate the tail of the innovation distribution poorly. Back testing of historical time series data shows that our approach gives better VaR estimates than the McNeil and Frey approach. © 2014 Copyright Taylor and Francis Group, LLC.}}, 
pages = {293--304}, 
number = {2}, 
volume = {14}
}
@article{10.1002/for.1237, 
year = {2012}, 
title = {{Bayesian forecasting for financial risk management, pre and post the global financial crisis}}, 
author = {Chen, Cathy W.S. and Gerlach, Richard and Lin, Edward M. H. and Lee, W. C. W.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1237}, 
abstract = {{Value-at-risk (VaR) forecasting via a computational Bayesian framework is considered. A range of parametric models is compared, including standard, threshold nonlinear and Markov switching generalized autoregressive conditional heteroskedasticity (GARCH) specifications, plus standard and nonlinear stochastic volatility models, most considering four error probability distributions: Gaussian, Student-t, skewed-t and generalized error distribution. Adaptive Markov chain Monte Carlo methods are employed in estimation and forecasting. A portfolio of four Asia-Pacific stock markets is considered. Two forecasting periods are evaluated in light of the recent global financial crisis. Results reveal that: (i) GARCH models outperformed stochastic volatility models in almost all cases; (ii) asymmetric volatility models were clearly favoured pre crisis, while at the 1\% level during and post crisis, for a 1-day horizon, models with skewed-t errors ranked best, while integrated GARCH models were favoured at the 5\% level; (iii) all models forecast VaR less accurately and anti-conservatively post crisis. Copyright © 2011 John Wiley \& Sons, Ltd. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {661--687}, 
number = {8}, 
volume = {31}
}
@article{10.1016/j.accfor.2003.10.002, 
year = {2004}, 
title = {{The disclosure of risk in financial statements}}, 
author = {Cabedo, J.David and Tirado, José Miguel}, 
journal = {Accounting Forum}, 
issn = {01559982}, 
doi = {10.1016/j.accfor.2003.10.002}, 
abstract = {{The accounting information currently issued by firms is not wholly adequate when used for decision making purposes, and within that process, for forecasting, for which additional information on risks is required. Therefore a reform of the current framework becomes necessary. Within this reform an adequate scheme and typology for the risks facing firms must be established and a set of specific risk quantification models must be designed. This paper focuses on both issues, showing all the risks that can affect firms and proposing a quantification model for each one. © 2004 Elsevier Ltd. All rights reserved.}}, 
pages = {181--200}, 
number = {2}, 
volume = {28}
}
@article{10.1016/s0167-6687(01)00082-8, 
year = {2001}, 
title = {{Pensionmetrics: Stochastic pension plan design and value-at-risk during the accumulation phase}}, 
author = {Blake, David and Cairns, Andrew J.G. and Dowd, Kevin}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/s0167-6687(01)00082-8}, 
abstract = {{We estimate values-at-risk (VaR) in the accumulation phase of defined-contribution pension plans. We examine a range of asset-return models (including stationary moments, regime-switching and fundamentals models) and a range of asset-allocation strategies (both static and with simple dynamic forms, such as lifestyle, threshold and constant proportion portfolio insurance). We draw four conclusions from our investigations. First, we find that defined-contribution (DC) plans can be extremely risky relative to a defined-benefit (DB) benchmark (far more so than most pension plan professionals would be likely to admit). Second, we find that the VaR estimates are very sensitive to the choice of asset-allocation strategy. The VaR estimates are also sensitive, but to a lesser extent, to both the asset-returns model used and its parameterisation. The choice of asset-returns model is found to be the least significant of the three. Third, a static asset-allocation strategy with a high equity weighting delivers substantially better results than any of the dynamic strategies investigated over the long term (40 years) of the sample policy. This is important given that lifestyle strategies are the cornerstone of many DC plans. Fourth, conservative bond-based asset-allocation strategies require substantially higher contribution rates than more risky equity-based strategies if the same retirement pension is to be achieved. © 2001 Elsevier Science B.V.}}, 
pages = {187--215}, 
number = {2}, 
volume = {29}
}
@article{10.1002/ijfe.2006, 
year = {2021}, 
title = {{A study on volatility spurious almost integration effect: A threshold realized GARCH approach}}, 
author = {Xu, Dinghai}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.2006}, 
abstract = {{This paper investigates the “spurious almost integration” effect of volatility under a threshold GARCH structure with realized volatility measures. To closely examine the effect, the realized persistence of volatility is proposed to be used as a threshold trigger for volatility regimes. Under the threshold framework, general closed-form solutions of moment conditions are derived, which provide a convenient way to theoretically examine the “spurious almost integration” effect and its associated impacts. We find that introducing the volatility persistence-driven threshold can capture regime-specific characteristics well. It performs better than the traditional GARCH-type models in terms of both in-sample fitting and out-of-sample forecasting. Based on our Monte Carlo and empirical results, in general we find that overlooking the relatively low-persistence regime(s) could lead to some misleading conclusions. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {4104--4126}, 
number = {3}, 
volume = {26}
}
@article{10.1108/jfrc-08-2015-0043, 
year = {2016}, 
title = {{Overview of the Greek value at risk (VaR) legislation framework: Deficiencies, proposals for future revision and a new suggested method}}, 
author = {Vasileiou, Evangelos}, 
journal = {Journal of Financial Regulation and Compliance}, 
issn = {13581988}, 
doi = {10.1108/jfrc-08-2015-0043}, 
abstract = {{Purpose: The purpose of this paper is to present the Greek value at risk (VaR) legislation framework and to highlight some of its major deficiencies, using not only theoretical scenarios but also empirical evidence. Moreover, this paper does not only highlight the VaR legislation’s framework deficiencies but also suggests legal interventions for its revision and a new-alternative, flexible and simple-to-be-applied filtered estimation method which improves the VaR evaluations. Design/methodology/approach: The Greek legislation framework suggests that for the daily VaR to be estimated, a minimum data set of the previous year (250 observations) at the 99 per cent confidence level should be considered. This approach may lead to inaccurate VaR estimations, for example, when after a long-term growth period, there is a sudden recession period, because the data input is not representative to the current financial environment. Taking into serious consideration that high volatility periods are linked to a financial crisis, it is assumed that volatility could be an indicator for the financial environment representation. The conventional historical VaR back-tested results suggest that the specific methodology should be revised, especially during the high volatility period. For the newly suggested filtered VaR, the data sample is divided into several regimes depending on the volatility range. The filtered VaR estimation process applies the conventional historical methodology but uses different historical data input depending on the current volatility. This new approach improves the VaR estimation by reducing the VaR daily violations. Findings: The findings regarding the current legislation framework suggest that the financial analysts in Greece have a motivation to adopt a relative VaR approach for risk asset class portfolios (e.g. Greek domestic equity mutual funds), which enables them to bear increased risk without presenting it to the investors. For lower risk portfolios, the absolute VaR may be useful for increased risk bearing strategies. The stricter VaR approaches are preferred to be adopted because stricter VaR estimations are linked to a reduced number of violations. The filtered volatility approach improves the VaR estimations (fewer violations are relative to the conventional approach). Research limitations/implications: This methodology is designed to be applied for the VaR estimation, but it could be partly applied in other fields of the financial analysis study. Practical implications: The suggested methodology could present efficient VaR estimation without using sophisticated procedures or expensive VaR systems. Therefore, it could be easily applied by the risk analysts. Moreover, the overview of the Greek legislation’s framework could be useful not only for the Greek regulators but also for the authorities in countries with a similar regulation. Originality/value: The newly proposed methodology is so accurate and simple to apply that it could have far-reaching impact on practitioners. Finally, this is the first paper that examines the Greek VaR legislation framework in detail. © 2016, © Emerald Group Publishing Limited.}}, 
pages = {213--226}, 
number = {2}, 
volume = {24}
}
@article{10.1080/16258312.2021.1989266, 
year = {2021}, 
title = {{Proactive disruption impact assessment in manufacturing supply networks}}, 
author = {Wiedenmann, Marc and Größler, Andreas}, 
journal = {Supply Chain Forum: An International Journal}, 
issn = {16258312}, 
doi = {10.1080/16258312.2021.1989266}, 
abstract = {{Companies are increasingly exposed to the risk profile of their supply network and managing this risk is gaining in importance in a tightly interconnected global and competitive business environment. More frequent and more severe supply disruptions have led to a growing interest in assessing their potential consequences. Therefore, in this paper four supply disruption indicators are derived to proactively assess the potential financial impact of supply disruptions. The stochastic nature of potential supply disruptions is modelled by adopting a Monte-Carlo and Value-at-Risk simulation approach: Quantitative, objective data and qualitative, subjective estimates by managers are combined to improve proactive risk-related decision-making at a company level. The developed methodology for deriving the supply disruption indicators is applied to a case study in a German manufacturing company in order to quantitatively assess differences in the disruption risk structures of various supply relations. The results of the study show that a Value-at-Risk approach based on Monte-Carlo simulation can be effectively adopted to assist supply managers in proactively identifying suitable risk mitigation strategies. © 2021 Kedge Business School.}}, 
pages = {1--13}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/a13050116, 
year = {2020}, 
title = {{The expected utility insurance premium principle with fourth-order statistics: Does it make a difference?}}, 
author = {Mazzoccoli, Alessandro and Naldi, Maurizio}, 
journal = {Algorithms}, 
issn = {19994893}, 
doi = {10.3390/a13050116}, 
abstract = {{The expected utility principle is often used to compute the insurance premium through a second-order approximation of the expected value of the utility of losses. We investigate the impact of using a more accurate approximation based on the fourth-order statistics of the expected loss and derive the premium under this expectedly more accurate approximation. The comparison between the two approximation levels shows that the second-order-based premium is always lower (i.e., an underestimate of the correct one) for the commonest loss distributions encountered in insurance. The comparison is also carried out for real cases, considering the loss parameters values estimated in the literature. The increased risk of the insurer is assessed through the Value-at-Risk. © 2020 by the authors.}}, 
pages = {116}, 
number = {5}, 
volume = {13}
}
@article{10.1007/s00500-020-04975-9, 
year = {2020}, 
title = {{The mean chance conditional value at risk under interval type-2 intuitionistic fuzzy random environment}}, 
author = {Taghikhani, Sepideh and Baroughi, Fahimeh and Alizadeh, Behrooz}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-020-04975-9}, 
abstract = {{The interval type-2 intuitionistic fuzzy random variable is an extension of the intuitionistic fuzzy random variable such that it can be a effective tool to determine some high-uncertainty phenomena. In this paper, the interval type-2 intuitionistic fuzzy random variable is introduced for the first time, and then, a scalar expected value operator of interval type-2 intuitionistic fuzzy random variable is proposed. Moreover, the new concepts of mean chance value at risk and mean chance conditional value at risk are discussed for the interval type-2 intuitionistic fuzzy random variables which have application in uncertain optimization, like fuzzy inverse location problems. Finally, it is proven that mean chance value at risk and mean chance conditional value at risk fulfill the convex risk metric properties. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {9361--9373}, 
number = {13}, 
volume = {24}
}
@article{10.5267/j.uscm.2016.10.001, 
year = {2017}, 
title = {{An efficient risk based multi objective project selection approach considering environmental issues}}, 
author = {Manavizadeh, Neda and Malek, Shadab and Vosoughi-Kia, Reza and Farrokhi-Asl, Hamed}, 
journal = {Uncertain Supply Chain Management}, 
issn = {22916822}, 
doi = {10.5267/j.uscm.2016.10.001}, 
abstract = {{There are many researches on project selection field, but few of them have considered environmental criteria in their studies. Moreover, there are many articles in evaluating risk but there is no article that considers value at risk concept to evaluate the amount of risk in multi project selection. We propose a multi objective mathematical model to address a situation in which several projects are candidate to be invested completely or partially. Three objective functions are considered in this paper. The first objective maximizes sum of the net present value of pure cash flow obtained from selected projects. In this objective, we consider the factor of time and its impact on value of money. In addition, we use the concept of value at risk (VAR) as the present value for the first time in project selection problems. Although green projects are more interesting, there are few articles, which address environmental issues. Hence, we consider the objective, which are related to environmental issues as the final criterion. Multi-Objective Differential Evolution algorithm (MODE) algorithm is applied to solve a problem, which is known as robust and efficient algorithm. Then computational results are compared with solutions obtained by NSGA-II algorithm which is well-known algorithm in this field with respect to seven comparison metrics. © 2017 Growing Science Ltd. All rights reserved.}}, 
pages = {143--158}, 
number = {2}, 
volume = {5}
}
@article{10.1002/for.2758, 
year = {2021}, 
title = {{Assessing liquidity-adjusted risk forecasts}}, 
author = {Berger, Theo and Uffmann, Christina}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2758}, 
abstract = {{In this paper, we provide a thorough study on the relevance of liquidity-adjusted value-at-risk (LVaR) and expected shortfall (LES) forecasts. We measure additional liquidity of an asset via the difference between its respective bid and ask prices and we assess the non-normality of bid–ask spreads, especially in turbulent market times. The empirical assessment comprises German stocks in both calm and turmoil market times, and our results provide evidence that liquidity risk turns out to be crucial for the quality of regulatory risk assessment in turmoil market times. We find that a Cornish–Fisher approximation describes a sensible choice for LVaR forecasts whereas an extreme value approach results in adequate LES forecasts. © 2021 The Authors. Journal of Forecasting published by John Wiley \& Sons Ltd.}}, 
pages = {1179--1189}, 
number = {7}, 
volume = {40}
}
@article{10.1016/j.econmod.2016.09.025, 
year = {2017}, 
title = {{How does issuing contingent convertible bonds improve bank's solvency? A Value-at-Risk and Expected Shortfall approach}}, 
author = {Jaworski, Piotr and Liberadzki, Kamil and Liberadzki, Marcin}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2016.09.025}, 
abstract = {{This paper examines how issuing an innovative financial instrument called contingent convertible bond (CoCo) may enhance bank's solvency in comparison to issuing a conventional bond. CoCos convert automatically into common equity or have a principal write-down when bank's regulatory capital fails to meet a predetermined level. They have been invented and put into legislation with an objective to absorb losses thus preventing institutions from bankruptcy. From the standpoint of an issuer CoCos bring about two counter effects regarding his solvency: on one hand they recapitalize a bank approaching insolvency on the other hand CoCos pay much higher coupon comparing to conventional bonds. In our model a bank has two funding alternatives: either to issue CoCos or conventional bonds. We measure issuer's default risk using the concept of Value-at-Risk (VaR) and Expected Shortfall (ES). We conclude that CoCos have the potential to strengthen the resilience of the issuer on the condition that the probability of conversion triggering is higher than the VaR's significance level. Our findings can be helpful to the policymakers and banks to better understand the impact of CoCos on issuer's solvency. © 2016 Elsevier Ltd}}, 
pages = {162--168}, 
number = {NA}, 
volume = {60}
}
@article{10.1002/asmb.660, 
year = {2007}, 
title = {{Extreme value analysis within a parametric outlier detection framework}}, 
author = {Cabras, S. and Morales, J.}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.660}, 
abstract = {{Threshold selection is a key aspect in extreme values analysis, especially when the sample size is small. The main idea underpinning this work is that extreme observations are assumed to be outliers of a specified parametric model. We propose a threshold selection method based on outlier detection using a suitable measure of surprise. Copyright © 2006 John Wiley \& Sons, Ltd.}}, 
pages = {157--164}, 
number = {2}, 
volume = {23}
}
@article{10.7819/rbgn.v16i51.1802, 
year = {2014}, 
title = {{Multivariate models to forecast portfolio value at risk: From the dot-com crisis to the global financial crisis [Modelos multivariados na previsão do valor em risco de carteiras de investimento: Da crise das empresas tecnológicas à crise financeira global]}}, 
author = {Gabriel, Vítor Manuel de Sousa}, 
journal = {Revista Brasileira de Gestão de Negócios}, 
issn = {18064892}, 
doi = {10.7819/rbgn.v16i51.1802}, 
abstract = {{This study analyzed market risk of an international investment portfolio by means of a new methodological proposal based on Value-at-Risk, using the covariance matrix of multivariate GARCH-type models and the extreme value theory to realize if an international diversification strategy minimizes market risk, and to determine if the VaR methodology adequately captures market risk, by applying Backtesting tests. To this end, we considered twelve international stock indexes, accounting for about 62\% of the world stock market capitalization, and chose the period from the Dot-Com crisis to the current global financial crisis. Results show that the proposed methodology is a good alternative to accommodate the high market turbulence and can be considered as an adequate portfolio risk management instrument. © FECAP.}}, 
pages = {299--318}, 
number = {51}, 
volume = {16}
}
@article{10.1109/fuzz-ieee.2017.8015420, 
year = {2017}, 
title = {{Optimization of dynamic maximum for value-at-risks with fuzziness in asset management}}, 
author = {Yoshida, Yuji}, 
journal = {2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
issn = {10987584}, 
doi = {10.1109/fuzz-ieee.2017.8015420}, 
abstract = {{A dynamic portfolio allocation is discussed in asset management with fuzziness. By perception-based extension for fuzzy random variables, a dynamic portfolio model for value-at-risks of fuzzy random variables is introduced. By dynamic programming and mathematical programming, this paper derives analytical solutions for the optimization problem. A numerical example is given to demonstrate the results. © 2017 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0378-4371(02)01822-8, 
year = {2003}, 
title = {{Functional correlation approach to operational risk in banking organizations}}, 
author = {Kühn, Reimer and Neu, Peter}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/s0378-4371(02)01822-8}, 
eprint = {cond-mat/0204368}, 
abstract = {{A Value-at-Risk-based model is proposed to compute the adequate equity capital necessary to cover potential losses due to operational risks, such as human and system process failures, in banking organizations. Exploring the analogy to a lattice gas model from physics, correlations between sequential failures are modeled by as functionally defined, heterogeneous couplings between mutually supportive processes. In contrast to traditional risk models for market and credit risk, where correlations are described as equal-time-correlations by a covariance matrix, the dynamics of the model shows collective phenomena such as bursts and avalanches of process failures. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {650--666}, 
number = {NA}, 
volume = {322}
}
@article{10.1016/j.cor.2013.09.015, 
year = {2014}, 
title = {{Generalized route planning model for hazardous material transportation with VaR and equity considerations}}, 
author = {Kang, Yingying and Batta, Rajan and Kwon, Changhyun}, 
journal = {Computers \& Operations Research}, 
issn = {03050548}, 
doi = {10.1016/j.cor.2013.09.015}, 
abstract = {{Recently, the Value-at-Risk (VaR) framework was introduced for the routing problem of a single hazmat trip. In this paper, we extend the VaR framework in two important ways. First, we show how to apply the VaR concept to a more realistic multi-trip multi-hazmat type framework, which determines routes that minimize the global VaR value while satisfying equity constraints. Second, we show how to embed the algorithm for the single hazmat trip problem into a Lagrangian relaxation framework to obtain an efficient solution method for this general case. We test our computational experience based on a real-life hazmat routing scenario in the Albany district of New York State. Our results indicate that one can achieve a high degree of risk dispersion while controlling the VaR value within the desired confidence level. © 2013 Elsevier Ltd. All rights reserved.}}, 
pages = {237--247}, 
number = {NA}, 
volume = {43}
}
@article{10.1016/b978-0-12-813830-4.00003-4, 
year = {2018}, 
title = {{Modeling risks in supply chains}}, 
author = {Sheffi, Yossi}, 
journal = {Finance and Risk Management for International Logistics and the Supply Chain}, 
issn = {NA}, 
doi = {10.1016/b978-0-12-813830-4.00003-4}, 
pmcid = {PMC7148736}, 
abstract = {{This chapter discusses ways to identify, categorize, and assess risks in supply chains. It pays particular attention to the often hidden risks that can lurk deep within global supply chains. Using case studies, the chapter shows how companies assess and prioritize supplier risks, and it explains the underlying factors that give rise to deep-chain risks. The sweep of examples in the chapter shows that companies can model the likelihood and impacts of a wide range of risks to their facilities, suppliers, and logistics infrastructure. © 2018 Elsevier Inc. All rights reserved.}}, 
pages = {55--84}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2014.11.016, 
year = {2015}, 
title = {{Is volatility clustering of asset returns asymmetric?}}, 
author = {Ning, Cathy and Xu, Dinghai and Wirjanto, Tony S.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2014.11.016}, 
abstract = {{Volatility clustering is a well-known stylized feature of financial asset returns. This paper investigates asymmetric pattern in volatility clustering by employing a univariate copula approach of Chen and Fan (2006). Using daily realized kernel volatilities constructed from high frequency data from stock and foreign exchange markets, we find evidence that volatility clustering is highly nonlinear and strongly asymmetric in that clusters of high volatility occur more often than clusters of low volatility. To the best of our knowledge, this paper is the first one to address and uncover this phenomenon. In particular, the asymmetry in volatility clustering is found to be more pronounced in the stock markets than in the foreign exchange markets. Further, the volatility clusters are shown to remain persistent for over a month and asymmetric across different time periods. Our findings have important implications for risk management. A simulation study indicates that models which accommodate asymmetric volatility clustering can significantly improve the out-of-sample forecasts of Value-at-Risk. © 2014 Elsevier B.V.}}, 
pages = {62--76}, 
number = {NA}, 
volume = {52}
}
@article{10.1080/1351847x.2017.1366350, 
year = {2018}, 
title = {{Measuring systemic risk in the European banking sector: a copula CoVaR approach}}, 
author = {Karimalis, Emmanouil N. and Nomikos, Nikos K.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2017.1366350}, 
abstract = {{We propose a new methodology based on copula functions to estimate CoVaR, the Value-at-Risk (VaR) of the financial system conditional on an institution being under financial distress. Our Copula CoVaR approach provides simple, closed-form expressions for various definitions of CoVaR for a broad range of copula families and allows the CoVaR of an institution to have time-varying exposure to its VaR. We extend this approach to estimate other ‘co-risk’ measures such as Conditional Expected Shortfall (CoES). We focus on a portfolio of large European banks and examine the existence of common market factors triggering systemic risk episodes. Further, we analyse the extent to which bank-specific characteristics such as size, leverage, and equity beta are associated with institutions' contribution to systemic risk and highlight the importance of liquidity risk at the outset of the financial crisis in summer 2007. Finally, we investigate the link between macroeconomy and systemic risk and find that changes in major macroeconomic variables can contribute significantly to systemic risk. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--38}, 
number = {11}, 
volume = {24}
}
@article{10.1016/j.irfa.2011.05.007, 
year = {2011}, 
title = {{Market risk model selection and medium-term risk with limited data: Application to ocean tanker freight markets}}, 
author = {Kavussanos, Manolis G. and Dimitrakopoulos, Dimitris N.}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2011.05.007}, 
abstract = {{The estimation of medium-term market risk dictated by limited data availability, is a challenging issue of concern amongst academics and practitioners. This paper addresses the issue by exploiting the concepts of volatility and quantile scaling in order to determine the best method for extrapolating medium-term risk forecasts from high frequency data. Additionally, market risk model selection is investigated for a new dataset on ocean tanker freight rates, which refer to the income of the capital good - tanker vessels. Certain idiosyncrasies inherent in the very competitive shipping freight rate markets, such as excessive volatility, cyclicality of returns and the medium-term investment horizons - found in few other markets - make these issues challenging. Findings indicate that medium-term risk exposures can be estimated accurately by using an empirical scaling law which outperforms the conventional scaling laws of the square and tail index root of time. Regarding the market risk model selection for short-term investment horizons, findings contradict most studies on conventional financial assets: interestingly, freight rate market risk quantification favors simpler specifications, such as the GARCH and the historical simulation models. © 2011 Elsevier Inc.}}, 
pages = {258--268}, 
number = {5}, 
volume = {20}
}
@article{10.1016/j.jempfin.2009.09.002, 
year = {2010}, 
title = {{Risk management and dynamic portfolio selection with stable Paretian distributions}}, 
author = {Ortobelli, Sergio and Rachev, Svetlozar T. and Fabozzi, Frank J.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2009.09.002}, 
abstract = {{This paper assesses stable Paretian models in portfolio theory and risk management. We describe an investor's optimal choices under the assumption of non-Gaussian distributed equity returns in the domain of attraction of a stable law. In particular, we examine dynamic portfolio strategies with and without transaction costs in order to compare the forecasting power of discrete-time optimal allocations obtained under different stable Paretian distributional assumptions. We also consider a conditional extension of the stable Paretian approach and compare the model with others that consider different distributional assumptions. Finally, we empirically evaluate the forecasting power of the model for predicting the value at risk of a heavy-tailed return series. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {195--211}, 
number = {2}, 
volume = {17}
}
@article{10.1002/for.934, 
year = {2004}, 
title = {{Value at risk from econometric models and implied from currency options}}, 
author = {Chong, James}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.934}, 
abstract = {{This paper compares daily exchange rate value at risk estimates derived from econometric models with those implied by the prices of traded options. Univariate and multivariate GARCH models are employed in parallel with the simple historical and exponentially weighted moving average methods. Overall, we find that during periods of stability, the implied model tends to overestimate value at risk, hence over-allocating capital. However, during turbulent periods, it is less responsive than the GARCH-type models, resulting in an under-allocation of capital and a greater number of failures. Hence our main conclusion, which has important implications for risk management, is that market expectations of future volatility and correlation, as determined from the prices of traded options, may not be optimal tools for determining value at risk. Therefore, alternative models for estimating volatility should be sought. Copyright © 2004 John Wiley \& Sons, Ltd.}}, 
pages = {603--620}, 
number = {8}, 
volume = {23}
}
@article{10.1016/j.econmod.2012.11.049, 
year = {2013}, 
title = {{Testing for Granger causality in distribution tails: An application to oil markets integration}}, 
author = {Candelon, Bertrand and Joëts, Marc and Tokpavi, Sessi}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2012.11.049}, 
abstract = {{This paper proposes an original procedure which allows for testing of Granger-causality for multiple risk levels across tail distributions, hence extending the procedure proposed by Hong et al. (2009). Asymptotic and finite sample properties of the test are considered. This new Granger-causality framework is applied for a set of regional oil markets series. It helps to tackle two main questions 1) Whether oil markets are more or less integrated during periods of extreme energetic prices movements and 2) Whether price-setter markets change during such periods. Our findings indicate that the integration level between crude oil markets tends to decrease during extreme periods and that price-setter markets also change. Such results have policy implication and stress the importance of an active energetic policy during episode of extreme movements. © 2012 Elsevier B.V.}}, 
pages = {276--285}, 
number = {1}, 
volume = {31}
}
@article{10.1186/s13660-015-0709-1, 
year = {2015}, 
title = {{An explicit version of the Chebyshev-Markov-Stieltjes inequalities and its applications}}, 
author = {Hürlimann, Werner}, 
journal = {Journal of Inequalities and Applications}, 
issn = {10255834}, 
doi = {10.1186/s13660-015-0709-1}, 
abstract = {{Given is the Borel probability space on the set of real numbers. The algebraic-analytical structure of the set of all finite atomic random variables on it with a given even number of moments is determined. It is used to derive an explicit version of the Chebyshev-Markov-Stieltjes inequalities suitable for computation. These inequalities are based on the theory of orthogonal polynomials, linear algebra, and the polynomial majorant/minorant method. The result is used to derive generalized Laguerre-Samuelson bounds for finite real sequences and generalized Chebyshev-Markov value-at-risk bounds. A financial market case study illustrates how the upper value-at-risk bounds work in the real world. © 2015, Hürlimann.}}, 
pages = {192}, 
number = {1}, 
volume = {2015}
}
@article{10.1007/978-3-319-26502-5_12, 
year = {2015}, 
title = {{IncidentResponseSim: An agent-based simulation tool for risk management of online Fraud}}, 
author = {Gorton, Dan}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-26502-5\_12}, 
abstract = {{IncidentResponseSim is a multi-agent-based simulation tool supporting risk management of online financial services, by performing a risk assessment of the quality of current countermeasures, in the light of the current and emerging threat environment. In this article, we present a set of simulations using incident response trees in combination with a quantitative model for estimating the direct economic consequences. The simulations generate expected fraud, and conditional fraud value at risk, given a specific fraud scenario. Additionally, we present how different trojan strategies result in different conditional fraud value at risk, given the underlying distribution of wealth in the online channel, and different levels of daily transaction limits. Furthermore, we show how these measures can be used together with return on security investment calculations to support decisions about future security investments. © Springer International Publishing Switzerland 2015.}}, 
pages = {172--187}, 
number = {NA}, 
volume = {9417}
}
@article{10.1016/j.csda.2011.09.022, 
year = {2012}, 
title = {{Bayesian adaptive bandwidth kernel density estimation of irregular multivariate distributions}}, 
author = {Hu, Shuowen and Poskitt, D.S. and Zhang, Xibin}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2011.09.022}, 
abstract = {{In this paper, we propose a new methodology for multivariate kernel density estimation in which data are categorized into low- and high-density regions as an underlying mechanism for assigning adaptive bandwidths. We derive the posterior density of the bandwidth parameters via the KullbackLeibler divergence criterion and use a Markov chain Monte Carlo (MCMC) sampling algorithm to estimate the adaptive bandwidths. The resulting estimator is referred to as the tail-adaptive density estimator. Monte Carlo simulation results show that the tail-adaptive density estimator outperforms the global-bandwidth density estimators implemented using different global bandwidth selection rules. The inferential potential of the tail-adaptive density estimator is demonstrated by employing the estimator to estimate the bivariate density of daily index returns observed from the USA and Australian stock markets. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {732--740}, 
number = {3}, 
volume = {56}
}
@article{10.1007/s10287-021-00412-w, 
year = {2021}, 
title = {{Forecasting Value-at-Risk in turbulent stock markets via the local regularity of the price process}}, 
author = {Frezza, Massimiliano and Bianchi, Sergio and Pianese, Augusto}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-021-00412-w}, 
abstract = {{A new computational approach based on the pointwise regularity exponent of the price time series is proposed to estimate Value at Risk. The forecasts obtained are compared with those of two largely used methodologies: the variance-covariance method and the exponentially weighted moving average method. Our findings show that in two very turbulent periods of financial markets the forecasts obtained using our algorithm decidedly outperform the two benchmarks, providing more accurate estimates in terms of both unconditional coverage and independence and magnitude of losses. © 2021, The Author(s).}}, 
pages = {1--34}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eneco.2020.104757, 
year = {2020}, 
title = {{Forecasting crude oil and refined products volatilities and correlations: New evidence from fractionally integrated multivariate GARCH models}}, 
author = {Marchese, Malvina and Kyriakou, Ioannis and Tamvakis, Michael and Iorio, Francesca Di}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2020.104757}, 
abstract = {{The relationship between the prices of crude oil and its refined products is at the heart of the oil industry. Crude oil and refined products volatilities and correlations have been modelled extensively using short-memory multivariate GARCH models. This paper investigates the potential benefits from using fractionally integrated multivariate GARCH models from a forecasting and a risk management perspective. Several models for the spot returns on three major oil-related markets are compared. In-sample results show significant evidence of long-memory decay and leverage effects in volatilities and of time-varying autocorrelations. The forecasting performance of the models is assessed by means of three approaches: the Superior Predictive Ability test, the Model Confidence Set and the Value-at-Risk. The results indicate that the multivariate models incorporating long-memory outperform the short-memory benchmarks in forecasting the conditional covariance matrix and associated risk magnitudes. The paper makes an innovative contribution to the analysis of the relationship between crude oil and its refined products providing refiners, physical oil traders, non-commercial oil traders and other energy markets agents with significant insights for hedging and risk management operations. © 2020 Elsevier B.V.}}, 
pages = {104757}, 
number = {NA}, 
volume = {88}
}
@article{10.1007/978-3-319-47172-3_8, 
year = {2017}, 
title = {{Computation of operational value at risk using the severity distribution model based on bayesian method with gibbs sampler}}, 
author = {Mwamba, John Weirstrass Muteba}, 
journal = {Contributions to Management Science}, 
issn = {14311941}, 
doi = {10.1007/978-3-319-47172-3\_8}, 
abstract = {{Under Basel III the minimum capital requirement due to operational risk is computed as the 99th quantile of the annual total loss distribution. This annual loss distribution is a result of the convolution between the loss frequency and the loss severity distributions. The estimation of parameters of these two distributions i.e. frequency and severity distributions is not only essential but crucial to obtaining reliable estimates of operational risk measures. In practical applications, Poisson and lognormal distributions are used to fit these two distributions respective. The maximum likelihood method, the method of moments as well as the probability-weighted moments used to obtain the parameters of these distributions can sometimes produce nonsensical estimates due to estimation risk and sample bias. This paper proposes a different calibration of the frequency and the severity distributions based on Bayesian method with Gibbs sampler. Further to that, the paper models the severity distribution by making use of the lognormal and the generalised Pareto distribution simultaneously. Simulated results suggest that computed operational value at risk estimates based of this new method are unbiased with minimum variance. © Springer International Publishing AG 2017.}}, 
pages = {103--121}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10287-009-0107-6, 
year = {2011}, 
title = {{Multiobjective optimization using differential evolution for real-world portfolio optimization}}, 
author = {Krink, Thiemo and Paterlini, Sandra}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-009-0107-6}, 
abstract = {{Portfolio optimization is an important aspect of decision-support in investment management. Realistic portfolio optimization, in contrast to simplistic mean-variance optimization, is a challenging problem, because it requires to determine a set of optimal solutions with respect to multiple objectives, where the objective functions are often multimodal and non-smooth. Moreover, the objectives are subject to various constraints of which many are typically non-linear and discontinuous. Conventional optimization methods, such as quadratic programming, cannot cope with these realistic problem properties. A valuable alternative are stochastic search heuristics, such as simulated annealing or evolutionary algorithms. We propose a new multiobjective evolutionary algorithm for portfolio optimization, which we call DEMPO-Differential Evolution for Multiobjective Portfolio Optimization. In our experimentation, we compare DEMPO with quadratic programming and another well-known evolutionary algorithm for multiobjective optimization called NSGA-II. The main advantage of DEMPO is its ability to tackle a portfolio optimization task without simplifications, while obtaining very satisfying results in reasonable runtime. © 2009 Springer-Verlag.}}, 
pages = {157--179}, 
number = {1-2}, 
volume = {8}
}
@article{10.1287/mnsc.1120.1615, 
year = {2013}, 
title = {{Worst-case value at risk of nonlinear portfolios}}, 
author = {Zymler, Steve and Kuhn, Daniel and Rustem, Berç}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1120.1615}, 
abstract = {{Portfolio optimization problems involving value at risk (VaR) are often computationally intractable and require complete information about the return distribution of the portfolio constituents, which is rarely available in practice. These difficulties are compounded when the portfolio contains derivatives. We develop two tractable conservative approximations for the VaR of a derivative portfolio by evaluating the worst-case VaR over all return distributions of the derivative underliers with given first- and second-order moments. The derivative returns are modelled as convex piecewise linear or-by using a delta-gamma approximation-as (possibly nonconvex) quadratic functions of the returns of the derivative underliers. These models lead to new worst-case polyhedral VaR (WPVaR) and worst-case quadratic VaR (WQVaR) approximations, respectively. WPVaR serves as a VaR approximation for portfolios containing long positions in European options expiring at the end of the investment horizon, whereas WQVaR is suitable for portfolios containing long and/or short positions in European and/or exotic options expiring beyond the investment horizon. We prove that-unlike VaR that may discourage diversification-WPVaR and WQVaR are in fact coherent risk measures. We also reveal connections to robust portfolio optimization. © 2013 INFORMS.}}, 
pages = {172--188}, 
number = {1}, 
volume = {59}
}
@article{10.1007/s11147-013-9088-2, 
year = {2013}, 
title = {{Capital adequacy rules, catastrophic firm failure, and systemic risk}}, 
author = {Jarrow, Robert}, 
journal = {Review of Derivatives Research}, 
issn = {13806645}, 
doi = {10.1007/s11147-013-9088-2}, 
abstract = {{This paper studies capital adequacy rules based on Value-at-Risk (VaR), leverage ratios, and stress testing. VaR is the basis of Basel II, and all three approaches are proposed in Basel III. This paper makes three contributions to the literature. First, we prove that these three rules provide an incentive to increase the probability of catastrophic financial institution failure. Collectively, these rules provide an incentive to increase (not decrease) systemic risk. Second, we argue that an unintended consequence of the Basel II VaR capital adequacy rules was the 2007 credit crisis. Third, we argue that to reduce systemic risk, a new capital adequacy rule is needed. One that is based on a risk measure related to the conditional expected loss given insolvency. © 2013 Springer Science+Business Media New York.}}, 
pages = {219--231}, 
number = {3}, 
volume = {16}
}
@article{10.1007/978-94-017-9558-6_9, 
year = {2015}, 
title = {{Computing value at risk in OpenCL on the graphics processing unit}}, 
author = {Zhang, Nan and Man, Ka Lok and Xie, Dejun}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-94-017-9558-6\_9}, 
abstract = {{We present our work on computing the value at risk (VaR) of a large hypothetical portfolio in the OpenCL programming model on an AMD FirePro V7900 graphics processing unit (GPU). In the computation of the VaR we follow the delta-gamma Monte Carlo approach. The value change of the portfolio within a short time period is approximated by the sum of a linear delta component and a non-linear gamma component. To approximate the distribution of the value change of the portfolio we generate a large number scenarios. From each scenario a loss or gain of the portfolio is calculated by the delta-gamma approximation. All these potential losses and gains are then sorted, from which an appropriate percentile is chosen as the VaR. We implemented this algorithm in OpenCL. The details are discussed and the experimental results are reported. © Springer Science+Business Media Dordrecht 2015.}}, 
pages = {71--78}, 
number = {NA}, 
volume = {329}
}
@article{10.1504/gber.2013.050664, 
year = {2013}, 
title = {{Value-at-risk for the long and short trading position with the Pearson type-IV distribution}}, 
author = {Stavroyiannis, Stavros and Makris, Ilias and Nikolaidis, Vasilis and Zarangas, Leonidas}, 
journal = {Global Business and Economics Review}, 
issn = {10974954}, 
doi = {10.1504/gber.2013.050664}, 
abstract = {{We examine the value-at-risk where the volatility and returns are modelled via a typical GARCH(1,1) model and the innovations process is the Pearson type-IV distribution. As case studies, we examine the NASDAQ and FTSE100 indices from 12-Dec-1984 to 21-Dec-2000. The model is fitted to the data via maximisation of the logarithm of the maximum likelihood estimator. In sample backtesting is performed by the success-failure ratio, the Kupiec p-test, the Christoffersen tests, the expected shortfall, and the DQ test of Engle and Manganelli. The results indicate that the Pearson type-IV distribution gives better results compared with the skewed student distribution. Copyright © 2013 Inderscience Enterprises Ltd.}}, 
pages = {14}, 
number = {1}, 
volume = {15}
}
@article{10.19030/jabr.v34i2.10121, 
year = {2018}, 
title = {{Are value at risk and maximum drawdown different from volatility in stock market?}}, 
author = {Kim, Soo-Hyun}, 
journal = {Journal of Applied Business Research (JABR)}, 
issn = {08927626}, 
doi = {10.19030/jabr.v34i2.10121}, 
abstract = {{Measuring risk is the key component in many asset pricing models. Although volatility is the most widely used measure for the risk, Value at Risk (VaR) and Maximum drawdown (MDD) are also considered as alternative risk measure. This article questions whether VaR and MDD contain additional information to volatility in equity market. The empirical analysis is conducted using the stocks listed in Korean stock market. By constructing portfolios in accordance with three risk measures, cross-sectional predictability is tested. The primary findings are as follow; (1) the return patterns are bell shaped in all measures and (2) VaR and MDD do not capture additional risk factors after conditioning volatility. © 2018, CIBER Institute. All rights reserved.}}, 
pages = {217--222}, 
number = {2}, 
volume = {34}
}
@article{10.1088/1742-6596/1918/4/042023, 
year = {2021}, 
title = {{Risk analysis of five stocks indexed by LQ45 using credible value at risk and credible expected tail loss}}, 
author = {Sulistianingsih, E and Rosadi, D and Abdurakhman}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1918/4/042023}, 
abstract = {{Value at Risk (VaR) and Expected Tail Loss (ETL) are two risk measures that are used frequently to measure the investment risk. Even though VaR can estimate maximum loss when the investor holds a single asset in a particular period and interval confidence, the investor frequently develops a portfolio of assets. This condition can create shared risk among assets in the portfolio so that there will be a chance of an asset for getting loss caused by the other assets developing the portfolio. On the other hand, there is a fact that VaR cannot provide loss information at the tail loss part so that we also need ETL that can overcome this problem. Because of that reason, this paper uses Credible Value at Risk (CredVaR) and Credible Expected Tail Loss (CredETL), which are formulated based on the Buhlman credibility concept. Both methods can estimate an investment risk that can overcome the shortcoming of VaR and ETL that do not consider the risk among assets inside the portfolio. The application of both methods was utilized to evaluate the individual risk of each asset in a portfolio comprised of five stocks in the LQ-45 Index (period of February 2019 until July 2019). The data divided into ten periods of risk analysis comprises of ten-year daily data of each stock from June 2009 to May 2019. According to the result of the analysis, it can be concluded that both methods are powerful in measuring the risk. © Published under licence by IOP Publishing Ltd.}}, 
pages = {042023}, 
number = {4}, 
volume = {1918}
}
@article{10.1177/0972150917713046, 
year = {2017}, 
title = {{A Study of Risk Spillover in the Crude Oil and the Natural Gas Markets}}, 
author = {Kumar, Dilip}, 
journal = {Global Business Review}, 
issn = {09721509}, 
doi = {10.1177/0972150917713046}, 
abstract = {{This article examines the upside and downside risk spillover effects among crude oil (WTI and Brent) and Henry Hub natural gas markets. We consider value-at-risk (VaR) as a measure of risk and model both upside and downside 95 per cent, 99 per cent and 99.5 per cent VaR using various VaR approaches. The VaR models are evaluated using Christoffersen’s (1998) conditional coverage test and Lopez’s loss function approach to select the best-performing VaR model. Finally, we apply Hong, Liu and Wang’s (2009) approach to examine the upside and the downside risk spillover among crude oil and Henry Hub natural gas markets. We find significant two-way as well as one-way upside and downside risk spillover between WTI and Brent crude oil. Our results provide weak evidence of upside risk spillover from natural gas market to crude oil markets for 99.5 per cent VaR. © 2017, © 2017 International Management Institute, New Delhi.}}, 
pages = {1465--1477}, 
number = {6}, 
volume = {18}
}
@article{10.1007/978-3-030-14969-7_7, 
year = {2019}, 
title = {{The future risk of internet companies: Is there a medium-term convergence?}}, 
author = {Cavaeiro, Arnon Nilson and Moralles, Herick Fernando and Silveira, Naijela Janaina Costa and Ferraz, Diogo and Rebelatto, Daisy Aparecida do Nascimento}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-030-14969-7\_7}, 
abstract = {{After the fact of the Internet Bubble, the technology companies, especially the Internet companies, were characterized as a sector of greater risk when compared to the other consolidated sectors. Thus, the present study aims to analyze whether the market risk of companies in the internet sector is still higher than companies in consolidated sectors. For this comparison, the Value-at-Risk (VaR) risk management method was used, which summarizes, in a single number, the worst expected return within certain confidence intervals and time. This methodology was applied to two groups: internet companies, traded in NADASQ, and companies in consolidated sectors, such as consumer goods, manufacturing, financial services, among others, traded on the NYSE. Samples are divided between 2000–2007 and 2008–2014 periods to compare behavior over time. The final result suggests that Internet companies still had a higher market risk than firms in consolidated sectors, but this risk decreased substantially between the periods studied. © Springer Nature Switzerland AG 2019.}}, 
pages = {75--86}, 
number = {NA}, 
volume = {280}
}
@article{10.1080/13657305.2020.1765896, 
year = {2020}, 
title = {{Modeling the return distribution of salmon farming companies: A quantile regression approach}}, 
author = {Steen, Marie and Jacobsen, Fredrik}, 
journal = {Aquaculture Economics \& Management}, 
issn = {13657305}, 
doi = {10.1080/13657305.2020.1765896}, 
abstract = {{As the companies have grown larger, the salmon farming industry has received increased attention from investors, financial analysts and other representatives of the financial community. Still, little is known about the risk and return characteristics of salmon farming companies’ shares. This paper approaches this topic by applying quantile regression to investigate the relationship between risk factors and monthly stock price returns over the entire return distribution at both industry and firm level. The results show that the overall market return, changes in the salmon price and the lagged returns for the major company in the industry have a positive impact on company stock price returns. The risk factor sensitivities are quite stable across quantiles at industry-level, there are substantial differences at the firm level, but formal testing cannot reject the hypothesis that the quantiles are equal. This implies that the relationship between risk factors and stock returns may vary under different return levels reflected in the salmon price cycles. We show how the results can be implemented and applied in a Value-at-Risk analysis. © 2020, © 2020 The Author(s). Published with license by Taylor \& Francis Group, LLC.}}, 
pages = {1--28}, 
number = {3}, 
volume = {24}
}
@article{10.1109/isam.2009.5376919, 
year = {2009}, 
title = {{Optimal outsourcing for intellectual property protection and production cost minimization}}, 
author = {Kim, John P and Hamza, Karim and Saitou, Kazuhiro}, 
journal = {2009 IEEE International Symposium on Assembly and Manufacturing}, 
issn = {NA}, 
doi = {10.1109/isam.2009.5376919}, 
abstract = {{This paper presents a methodology for optimal outsourcing of products. Outsourcing of products can have the advantage of reducing the production cost, but often causes a risk that important technology may leak and get used by competitors. To help reduce the risk of intellectual property (IP) leakage, a model proposed in this paper assumes that it is possible to separate some of the important geometrical features on some of the product parts that are outsourced, and then manufacture them in-house. The model estimates the fraction of IP-value that is subject to risk of leakage based on patent claims and how they relate to the outsourced parts and/or features. Production cost is modelled by assuming a base cost for manufacturing parts in-house, and then a discount rate is applied if the decision to outsource is made. Separation of geometrical features from manufactured parts introduces additional cost, which is modelled as an overhead if the decision to separate features is made. The outsourcing management process is then viewed as a two-objective problem, with the objectives being the minimization of both the fraction of IP-value at risk of leakage, as well as the production cost. A case study of an auto-slide-hinge mechanism is presented, in which the two-objective optimization problem is transformed into a single-objective constrained problem. Genetic algorithm is then applied iteratively on the problem in order generate the Pareto-plot that visualizes the trade-offs between the two objectives. ©2009 IEEE.}}, 
pages = {124--129}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/1351847x.2011.574977, 
year = {2012}, 
title = {{Hedging effectiveness under conditions of asymmetry}}, 
author = {Cotter, John and Hanly, Jim}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2011.574977}, 
abstract = {{We examine whether hedging effectiveness is affected by asymmetry in the return distribution by applying tail-specific metrics, for example, value at risk, to compare the hedging effectiveness of short and long hedgers. Comparisons are applied to a number of hedging strategies including OLS and both symmetric and asymmetric generalised autoregressive conditional heteroskedastic models. We apply our analysis to a dataset consisting of S\&P500 index cash and futures containing symmetric and asymmetric return distributions chosen ex post. Our findings show that asymmetry reduces out-of-sample hedging performance and that significant differences occur in hedging performance between short and long hedgers. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {135--147}, 
number = {2}, 
volume = {18}
}
@article{10.1063/1.5139121, 
year = {2019}, 
title = {{Modeling indemnity of revenue-based crop insurance in Indonesia using time-varying copula models}}, 
author = {Ahdika, Atina and Rosadi, Dedi and Effendie, Adhitya Ronnie and Gunardi}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.5139121}, 
abstract = {{Crop insurance is a potential business line to develop, especially in countries with agricultural bases. In such type of insurance, a loss can be defined as a lack of revenue resulted from the crop yield or the price. The purpose of this paper is to model the variability of revenue-based agricultural losses through the implementation of time-varying copula towards crop yield and price, and to estimate the indemnity of the revenue-based crop insurance. The analysis employs both static and time-varying Normal and Archimedean copula to model the structure of dependency between crop yield and price. Each marginal variable is modeled using ARIMA model. A simple algorithm is proposed to estimate revenue-based losses and indemnity of the revenue-based crop insurance. © 2019 Author(s).}}, 
pages = {030001}, 
number = {NA}, 
volume = {2192}
}
@article{10.1016/j.ijforecast.2011.10.002, 
year = {2012}, 
title = {{Ranking the predictive performances of value-at-risk estimation methods}}, 
author = {Şener, Emrah and Baronyan, Sayad and Mengütürk, Levent Ali}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2011.10.002}, 
abstract = {{We introduce a ranking model and a complementary predictive ability test statistic to investigate the forecasting performances of different Value at Risk (VaR) methods, without specifying a fixed benchmark method. The period including the recent credit crisis offers a unique laboratory for the analysis of the relative successes of different VaR methods when used in both emerging and developed markets. The proposed ranking model aims to form a unified framework which penalizes not only the magnitudes of errors between realized and predicted losses, but also the autocorrelation between the errors. The model also penalizes excessive capital allocations. In this respect, the ranking model seeks for VaR methods which can capture the delicate balance between the minimum governmental regulations for financial sustainability, and cost-efficient risk management for economic vitality. As a complimentary statistical tool for the ranking model, we introduce a predictive ability test which does not require the selection of a benchmark method. This statistic, which compares all methods simultaneously, is an alternative to existing predictive ability tests, which compare forecasting methods two at a time. We test and rank twelve different popular VaR methods on the equity indices of eleven emerging and seven developed markets. According to the ranking model and the predictive ability test, our empirical findings suggest that asymmetric methods, such as CAViaR Asymmetric and EGARCH, generate the best performing VaR forecasts. This indicates that the performance of VaR methods does not depend entirely on whether they are parametric, non-parametric, semi-parametric or hybrid; but rather on whether they can model the asymmetry of the underlying data effectively or not. © 2012 International Institute of Forecasters.}}, 
pages = {849--873}, 
number = {4}, 
volume = {28}
}
@article{10.1007/s13385-017-0163-1, 
year = {2018}, 
title = {{On Pareto-optimal reinsurance with constraints under distortion risk measures}}, 
author = {Jiang, Wenjun and Hong, Hanping and Ren, Jiandong}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-017-0163-1}, 
abstract = {{This paper studies the Pareto-optimal reinsurance policies, where both the insurer’s and the reinsurer’s risks and returns are considered. We assume that the risks of the insurer and the reinsurer, as well as the reinsurance premium, are determined by some distortion risk measures with different distortion operators. Under the constraint that a reinsurance policy is feasible only if the resulting risk of each party is below some pre-determined values, we derive explicit expressions for the optimal reinsurance polices. Methodologically, we show that the generalized Neyman-Pearson method, the Lagrange multiplier method, and the dynamic control methods can be utilized to solve our problem. Special cases when both parties’ risks are measured by Value-at-Risk (VaR) and Tail Value-at-Risk (TVaR) are studied in great details. Numerical examples are provided to illustrate practical implications of the results. © 2017, EAJ Association.}}, 
pages = {215--243}, 
number = {1}, 
volume = {8}
}
@article{10.1142/s0219024905003104, 
year = {2005}, 
title = {{Value-at-Risk and expected shortfall for linear portfolios with elliptically distributed risk factors}}, 
author = {KAMDEM, JULES SADEFO}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024905003104}, 
abstract = {{In this paper, we generalize the parametric Δ-VaR method from portfolios with normally distributed risk factors to portfolios with elliptically distributed ones. We treat both the expected shortfall and the Value-at-Risk of such portfolios. Special attention is given to the particular case of a multivariate t-distribution. © World Scientific Publishing Company.}}, 
pages = {537--551}, 
number = {5}, 
volume = {8}
}
@article{10.5220/0008318704590464, 
year = {2019}, 
title = {{Implementing value-at-risk and expected shortfall for real time risk monitoring}}, 
author = {Ristau, Petra}, 
journal = {Proceedings of the 8th International Conference on Data Science, Technology and Applications}, 
issn = {NA}, 
doi = {10.5220/0008318704590464}, 
abstract = {{Regulatory standards require financial service providers and banks to calculate certain risk figures, such as Value at Risk (VaR) and Expected Shortfall (ES). If properly calculated, their formulas are based on a Monte-Carlo simulation, which is computationally complex. This paper describes architecture and development considerations of a use case building a demonstrator for a big data analytics cloud platform developed in the project CloudDBAppliance (CDBA). The chosen approach will allow for real time risk monitoring using cloud computing and a fast analytical processing platform and data base. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved}}, 
pages = {459--464}, 
number = {NA}, 
volume = {NA}
}
@article{10.1214/16-aos1439, 
year = {2016}, 
title = {{Higher order elicitability and osband's principle}}, 
author = {Fissler, Tobias and Ziegel, Johanna F.}, 
journal = {The Annals of Statistics}, 
issn = {00905364}, 
doi = {10.1214/16-aos1439}, 
eprint = {1503.08123}, 
abstract = {{A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability formulti-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance, this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the a œcorrecta quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications. © Institute of Mathematical Statistics, 2016.}}, 
pages = {1680--1707}, 
number = {4}, 
volume = {44}
}
@article{10.1108/00214740180001115, 
year = {2001}, 
title = {{A comparison of criteria for evaluating risk management strategies}}, 
author = {Gloy, Brent A. and Baker, Timothy G.}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00214740180001115}, 
abstract = {{Several criteria that produce rankings of risk management strategies are evaluated. The criteria considered are expected return, value at risk, the Sharpe ratio, the necessary condition for first-degree stochastic dominance with a risk-free asset, and the necessary condition for second-degree stochastic dominance with a risk-free asset. The criteria performed relatively well in that the most desirable strategy under each criterion was always at least a member of the second-degree stochastic dominance efficient set. There was also a relatively high degree of consistency between the highest ranked strategies under the various criteria. The effectiveness of the criteria increases as decision makers are assumed to be more risk averse and have greater access to financial leverage. © 2001, MCB UP Limited}}, 
pages = {38--56}, 
number = {1}, 
volume = {61}
}
@article{10.1007/978-3-319-73150-6_70, 
year = {2018}, 
title = {{Pricing assets with higher co-moments and value-at-risk by quantile regression approach: Evidence from Vietnam stock market}}, 
author = {Huynh, Toan Luu Duc and Nguyen, Sang Phu and Duong, Duy}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-319-73150-6\_70}, 
abstract = {{This paper examines the role of higher co-moments of the shape of return distribution in capturing secondary data for 274 non-financial firms listed in the Vietnam Index, considered as one of the emerging stock markets, during the period from July 2006 to June 2016. We employ Fama-French model combined with higher co-moments, particularly co-skewness and co-kurtosis, and value-at-risk (VaR) to explain the return-generating process. Quantile regression is also used in descending order with the two methods of equally weighted and value-weighted portfolios. The findings show that investors could maximize their portfolio return by holding more stocks with the positive co-skewness and restricting the large co-kurtosis ones. It implies that in addition to co-momentum effects other determinants such as size, value and maximal value of losses also have a strong influence on stock return. © 2018, Springer International Publishing AG.}}, 
pages = {953--986}, 
number = {NA}, 
volume = {760}
}
@article{10.1016/j.enpol.2005.07.004, 
year = {2006}, 
title = {{Energy risk management and value at risk modeling}}, 
author = {Sadeghi, Mehdi and Shavvalpour, Saeed}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2005.07.004}, 
abstract = {{The value of energy trades can change over time with market conditions and underlying price variables. The rise of competition and deregulation in energy markets has led to relatively free energy markets that are characterized by high price shifts. Within oil markets the volatile oil price environment after OPEC agreements in the 1970s requires a risk quantification." Value-at-risk" has become an essential tool for this end when quantifying market risk. There are various methods for calculating value-at-risk. The methods we introduced in this paper are Historical Simulation ARMA Forecasting and Variance-Covariance based on GARCH modeling approaches. The results show that among various approaches the HSAF methodology presents more efficient results, so that if the level of confidence is 99\%, the value-at-risk calculated through HSAF methodology is greater than actual price changes in almost 97.6 percent of the forecasting period. © 2005 Elsevier Ltd. All rights reserved.}}, 
pages = {3367--3373}, 
number = {18}, 
volume = {34}
}
@article{10.1063/1.5139125, 
year = {2019}, 
title = {{Analysis of risk measures of reinsurance designs under exponential loss distribution using value at risk and ruin probability}}, 
author = {Saputra, Kie Van Ivanky and Winoto, Daniel and Joanna}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.5139125}, 
abstract = {{In this paper, we investigate the optimal conditons for reinsurance models by minimizing the risk exposure of an insurer under the criteria of value at risk (VaR) and ruin probability. We assume that the reinsurance premium is evaluated according to the expected value principle. We also consider two reinsurance models which are the quota-share and the stop-loss reinsurance models. Under the assumption that the loss is exponentially distributed, we develop conditions for optimality of such reinsurance designs. © 2019 Author(s).}}, 
pages = {030005}, 
number = {NA}, 
volume = {2192}
}
@article{10.1080/17509653.2006.10671002, 
year = {2006}, 
title = {{Some remarks on value-at-risk optimization}}, 
author = {Henrion, René}, 
journal = {International Journal of Management Science and Engineering Management}, 
issn = {17509653}, 
doi = {10.1080/17509653.2006.10671002}, 
abstract = {{We discuss on observations related to value-at-risk optimization. Firstly, we consider a portfolio problem under an infinite number of value-at-risk inequality constraints (modeling first order stochastic dominance). The random data are assumed to be normally distributed. Although this problem is necessarily non-convex, an explicit solution can be derived. Secondly, we provide a (negative) result on quantitative stability of the value-at-risk under variation of the random variable. Although reduced Lipschitz properties (in the sense of calmness) may hold true at continuously distributed random variables under suitable conditions, the result shows that no full Lipschitz property (more generally: Holder property at any rate) can hold in the neighbourhood of an arbitrary continuously distributed random variable. Even worse, this observation holds true with respect to any probability metric weaker than that of total variation. © 2006 Taylor \& Francis Group, LLC.}}, 
pages = {111--118}, 
number = {2}, 
volume = {1}
}
@article{10.1016/j.frl.2004.03.002, 
year = {2004}, 
title = {{Scale-consistent Value-at-Risk}}, 
author = {Lehnert, Thorsten and Wolff, Christian C.P.}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2004.03.002}, 
abstract = {{Returns in financial assets show consistent excess kurtosis and skewness, indicating the presence of large fluctuations not predicted by Gaussian models. In this paper we propose a generalization to the popular RiskMetrics approach to Value-at-Risk. In order to model scale-consistent Value-at-Risk (VaR), we propose a model with a time varying scale parameter and error terms that are truncated Lévy distributed. Lévy flights include a method for scaling up from a single-day volatility to a multi-day volatility. We use this rule to approximate future volatility and estimate Value-at-Risk several days ahead, and compare it to the popular approach, which is a special case of our method. Back-testing results suggest that the inclusion of more sophisticated tail properties and the data-driven scaling rule improves the performance of the VaR model significantly, for short and long time horizons. Our approach is easier to implement and is less time and computer intensive compared to Monte Carlo simulation methods. © 2004 Elsevier Inc. All rights reserved.}}, 
pages = {127--134}, 
number = {2}, 
volume = {1}
}
@article{10.1063/1.4801257, 
year = {2013}, 
title = {{Estimation of value at risk and conditional value at risk using normal mixture distributions model}}, 
author = {Kamaruzzaman, Zetty Ain and Isa, Zaidi}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.4801257}, 
abstract = {{Normal mixture distributions model has been successfully applied in financial time series analysis. In this paper, we estimate the return distribution, value at risk (VaR) and conditional value at risk (CVaR) for monthly and weekly rates of returns for FTSE Bursa Malaysia Kuala Lumpur Composite Index (FBMKLCI) from July 1990 until July 2010 using the two component univariate normal mixture distributions model. First, we present the application of normal mixture distributions model in empirical finance where we fit our real data. Second, we present the application of normal mixture distributions model in risk analysis where we apply the normal mixture distributions model to evaluate the value at risk (VaR) and conditional value at risk (CVaR) with model validation for both risk measures. The empirical results provide evidence that using the two components normal mixture distributions model can fit the data well and can perform better in estimating value at risk (VaR) and conditional value at risk (CVaR) where it can capture the stylized facts of non-normality and leptokurtosis in returns distribution. © 2013 AIP Publishing LLC.}}, 
pages = {1123--1131}, 
number = {NA}, 
volume = {1522}
}
@article{10.2753/ree1540-496x490306, 
year = {2013}, 
title = {{Downside risk in emerging markets}}, 
author = {Atilgan, Yigit and Demirtas, K. Ozgur}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.2753/ree1540-496x490306}, 
abstract = {{This paper investigates the relation between downside risk and expected returns on the aggregate stock market in an international context. Nonparametric and parametric value at risk are used as measures of downside risk to determine the existence of a risk-return trade-off. For emerging markets, fixed effects panel data regressions provide evidence for a significantly positive relationship between monthly expected market returns and downside risk. This result is robust after controlling for aggregate dividend yield and price-to-fundamental ratios. The relationship between expected returns and downside risk is weaker for developed markets and vanishes when control variables are included in the specification. © 2013 M.E. Sharpe, Inc. All rights reserved.}}, 
pages = {65--83}, 
number = {3}, 
volume = {49}
}
@article{10.1016/j.eneco.2015.06.010, 
year = {2015}, 
title = {{Value-at-Risk estimation of energy commodities: A long-memory GARCH-EVT approach}}, 
author = {Youssef, Manel and Belkacem, Lotfi and Mokni, Khaled}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2015.06.010}, 
abstract = {{In this paper, we evaluate Value-at-Risk (VaR) and expected shortfall (ES) for crude oil and gasoline market. We adopt three long-memory-models including, FIGARCH, HYGARCH and FIAPARCH to forecast energy commodity volatility by capturing some volatility stylized fact such as long-range memory, heteroscedasticity, asymmetry and fat-tails. Then we consider extreme value theory which concentrates on the tail distribution rather than the entire distribution. EVT is considered as a potential framework for the separate treatment of tails of distributions which allows for asymmetry. Our results show that the FIAPARCH model with extreme value theory performs better in predicting the one-day-ahead VaR. Using the fitted long-memory GARCH-model and a simulation approach to estimate VaR for horizons over than one day, backtesting results show that our approach still performs for lower estimation frequencies. Overall, our findings confirm that taking into account long-range memory, asymmetry and fat tails in the behavior of energy commodity prices returns combined with filtering process such as EVT are important in improving risk management assessments and hedging strategies in the high volatile energy market. © 2015 Elsevier B.V.}}, 
pages = {99--110}, 
number = {NA}, 
volume = {51}
}
@article{10.3390/econometrics5020021, 
year = {2017}, 
title = {{Bayesian inference for latent factor copulas and application to financial risk forecasting}}, 
author = {Schamberger, Benedikt and Gruber, Lutz F. and Czado, Claudia}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics5020021}, 
abstract = {{Factor modeling is a popular strategy to induce sparsity in multivariate models as they scale to higher dimensions. We develop Bayesian inference for a recently proposed latent factor copula model, which utilizes a pair copula construction to couple the variables with the latent factor. We use adaptive rejection Metropolis sampling (ARMS) within Gibbs sampling for posterior simulation: Gibbs sampling enables application to Bayesian problems, while ARMS is an adaptive strategy that replaces traditional Metropolis-Hastings updates, which typically require careful tuning. Our simulation study shows favorable performance of our proposed approach both in terms of sampling efficiency and accuracy. We provide an extensive application example using historical data on European financial stocks that forecasts portfolio Value at Risk (VaR) and Expected Shortfall (ES). © 2017 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {21}, 
number = {2}, 
volume = {5}
}
@article{10.1080/02102412.2003.10779482, 
year = {2003}, 
title = {{Value at risk of non-normal portfolios}}, 
author = {Perote, Javier}, 
journal = {Spanish Journal of Finance and Accounting / Revista Española de Financiación y Contabilidad}, 
issn = {02102412}, 
doi = {10.1080/02102412.2003.10779482}, 
abstract = {{This paper sheds light on the evaluation of portfolio risk when portfolio variables are not normal, as is usually the case with financial variables. The methodology proposed in these cases is based on the assumption of a more general distribution capable of incorporating the behaviour of such variables, especially at the tails: the so called Edgeworth-Sargan distribution. This density is preferable over other distributions, such as the Student's t, when fitting high frequency financial variables, because of its flexibility for improving data fits by adding more parameters in a natural way. F urthermore, this distribution is easy to generalise to a multivariate context and, therefore, correlation coefficients among variables can be estimated efficiently. This article, therefore, provides new insights into VaR methodology by estimating the joint density of portfolio variables, and simultaneously calculating the right critical values of the underlying portfolio density. The empirical examples include the estimation and evaluation of different portfolios composed of stock indices for major financial markets.}}, 
pages = {290--310}, 
number = {115 SPEC. ISSUE}, 
volume = {32}
}
@article{10.1080/10170660809509070, 
year = {2008}, 
title = {{Interval estimation of value-at-risk for Taiwan weighted stock index based on extreme value theory}}, 
author = {Chou, Jian-Hsin and Yu, Hong-Fwu and Chen, Zhen-Yu}, 
journal = {Journal of the Chinese Institute of Industrial Engineers}, 
issn = {10170669}, 
doi = {10.1080/10170660809509070}, 
abstract = {{Value-at-Risk (VaR) is a tool widely used by financial institutions to report and measure market risk. There have been a great number of studies in estimating VaR. Most of which are focused on the point estimation of VaR. However, in estimating a quantity, a point estimate can be misleading, because it may or may not be close to the quantity being estimated. So we cannot know the accuracy of estimating the quantity. Confidence interval (CI) is one of the most useful manners of quantifying uncertain due to “sampling error”. Besides, the mathematics of interval estimation and hypotheses testing are closely re-lated. Hence, in practical situations, the interval estimation is more preferred than the point estimation. This paper is aimed at applying the extreme value theory (EVT) to evaluate CIs of the VaR of Taiwan weighted stock index. To assess the efficiency of the proposed method in this paper, comparisons with the methods studied in Chang et al. [9] are also made. The empirical results show that the widths of the CIs obtained by the EVT model are narrower than those obtained by Chang et al. [9]. This indicates that the EVT model is more efficient in estimating the VaR of Taiwan weighted stock index than those in Chang et al. [9]. © 2008, Taylor \& Francis Group, LLC.}}, 
pages = {31--42}, 
number = {1}, 
volume = {25}
}
@article{10.1093/imaman/dpv016, 
year = {2016}, 
title = {{Value-at-Risk for fixed-income portfolios: A Kalman filtering approach}}, 
author = {Date, P and Bustreo, R}, 
journal = {IMA Journal of Management Mathematics}, 
issn = {1471678X}, 
doi = {10.1093/imaman/dpv016}, 
abstract = {{We propose a way of measuring the risk of a sovereign debt portfolio by using a simple two-factor short rate model. The model is calibrated from data and then the changes in the bond prices are simulated by using a Kalman filter. The bond prices being simulated remain arbitrage-free, in contrast with principal component analysis-based strategies for simulation and risk measurement of debt portfolios. In liquid sovereign debt markets, a risk measurement methodology which allows the future bond price scenarios to be arbitrage-free may be seen as a potentially more realistic way of measuring the debt portfolio risk due to interest rate fluctuations. We demonstrate the performance of this methodology with calibration and backtesting, both on simulated data as well as on a real portfolio of US government bonds. © 2015 The Authors.}}, 
pages = {557--573}, 
number = {4}, 
volume = {27}
}
@article{10.1007/s10986-018-9408-1, 
year = {2018}, 
title = {{Expectation of the truncated randomly weighted sums with dominatedly varying summands}}, 
author = {Jaunė, Eglė and Ragulina, Olena and Šiaulys, Jonas}, 
journal = {Lithuanian Mathematical Journal}, 
issn = {03631672}, 
doi = {10.1007/s10986-018-9408-1}, 
abstract = {{We consider the asymptotic behavior of the values P(S \&gt; x), E(S1\{S\&gt;x\}), and E(S | S \&gt; x). Here S = θ1X1 + θ2X2 + · · · + θnXn is a randomly weighted sum of the basic random variables X1,X2,.. , Xn, which follow some special dependence structure, and \{θ1, θ2,.. , θn\} is a collection of nonnegative and arbitrarily dependent random weights; the collections \{X1,X2,..,Xn\} and \{θ1, θ2,.. , θn\} are supposed to be independent. We derive asymptotic formulas in the case where the number of summands n is fixed and the distributions of the basic random variables are dominatedly varying.We apply them to some values related to the risk measures of certain weighted sums. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {421--440}, 
number = {4}, 
volume = {58}
}
@article{10.3934/math.2020208, 
year = {2020}, 
title = {{Optimal reinsurance for both an insurer and a reinsurer under general premium principles}}, 
author = {Fang, Ying and Cheng, Guo and Qu, Zhongfeng}, 
journal = {AIMS Mathematics}, 
issn = {24736988}, 
doi = {10.3934/math.2020208}, 
abstract = {{A reinsurance contract should consider the conflicting interests of the insurer and the reinsurer. An optimal reinsurance contract for one party may not be optimal for another party and it might be unacceptable for another party. Therefore, in this paper, we study the optimal reinsurance models from the perspective of both the insurer and the reinsurer by minimizing their total costs under the criteria of loss function which is defined by the joint value-at-risk, assuming that the reinsurance premium principles satisfy risk loading and stop-loss ordering preserving. We derive the optimal reinsurance policies over three ceded loss function sets, the change-loss reinsurance is optimal among the class of increasing convex ceded loss functions; when the constraints on both ceded and retained loss functions are relaxed to increasing functions, the layer reinsurance is shown to be optimal; the quota-share reinsurance with a limit is always optimal when the ceded loss functions are in the class of increasing concave functions. We further use the expectation premium principle and Dutch premium principle to illustrate the application of our results by deriving the optimal parameters. © 2020 the Author(s), licensee AIMS Press.}}, 
pages = {3231--3255}, 
number = {4}, 
volume = {5}
}
@article{10.1016/j.spl.2015.02.004, 
year = {2015}, 
title = {{Elicitable distortion risk measures: A concise proof}}, 
author = {Wang, Ruodu and Ziegel, Johanna F.}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2015.02.004}, 
abstract = {{Elicitability has recently been discussed as a desirable property for risk measures. Kou and Peng (2014) showed that an elicitable distortion risk measure is either a Value-at-Risk or the mean. We give a concise alternative proof of this result, and discuss the conflict between comonotonic additivity and elicitability. © 2015 Elsevier B.V..}}, 
pages = {172--175}, 
number = {NA}, 
volume = {100}
}
@article{10.1080/03461238.2019.1626762, 
year = {2019}, 
title = {{On additivity of tail comonotonic risks}}, 
author = {Cheung, Ka Chun and Ling, Hok Kan and Tang, Qihe and Yam, Sheung Chi Phillip and Yuen, Fei Lung}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2019.1626762}, 
abstract = {{As perceived from daily experience together with numerous empirical studies, the multivariate risks demonstrate a strong coherence in the extremal dependence structure especially over the course of financial turmoil or industrial accidents and outbreaks. Under this motivating paradigm, we show the universal asymptotic additivity under upper tail comonotonicity, as the probability level approaching to 1, for Value-at-Risk and Conditional Tail Expectation for a portfolio of fixed number of risks, in which each marginal risk could be any one having a finite endpoint or belonging to one of the three max domains of attraction. Our obtained results do not require the tail equivalence assumption as needed in the existing literature. This resolves a lasting problem in quantitative risk management and covers most distributions commonly encountered in practice. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--30}, 
number = {10}, 
volume = {2019}
}
@article{10.21314/jcf.2016.313, 
year = {2016}, 
title = {{Extended saddlepoint methods for credit risk measurement}}, 
author = {García-Céspedes, Rubén and Moreno, Manuel}, 
journal = {The Journal of Computational Finance}, 
issn = {14601559}, 
doi = {10.21314/jcf.2016.313}, 
abstract = {{We propose a new method that extends the saddlepoint approximation to allocate credit risk. This method applies a Taylor expansion to the inverse Laplace transform around an arbitrary point to characterize the loss distribution of a portfolio. It is based on Hermite polynomials. From a computational point of view, our method is less demanding than other approximate methods. We also extend the current saddlepoint methods to deal with random recoveries and market valuation. Considering a portfolio that includes Spanish financial institutions, we show that these extensions can characterize the risk of the portfolio very well. The risk allocation method generates more accurate results than other approximate methods, with few calculations for default mode models and pure macroeconomy driven recoveries. We also find that modeling mixed idiosyncratic and macroeconomic random recoveries does not generate much greater risk than a pure macroeconomic random recoveries model. Finally, the results for the market valuation approximation are also very accurate, but this method requires a higher number of calculations. Other methods, such as the Monte Carlo importance sampling one, may be more suitable. © 2016 Incisive Risk Information (IP) Limited.}}, 
pages = {1--37}, 
number = {2}, 
volume = {20}
}
@article{10.1002/er.5846, 
year = {2021}, 
title = {{GGlueVaR-based participation of electric vehicles in automatic demand response for two-stage scheduling}}, 
author = {Gao, Jianwei and Yang, Yu and Ma, Zeyang and Gao, Fangjie}, 
journal = {International Journal of Energy Research}, 
issn = {0363907X}, 
doi = {10.1002/er.5846}, 
abstract = {{Due to the uncertainty of the external situation and the varied ability of electric vehicle (EV) owners to understand and process information, the demand response optimization method is not timely and flexible enough. This article puts forward a two-stage electric vehicle automatic demand response (ADR) optimization method based on generalized Glue value-at-risk (GGlueVaR) to solve existing problems. First, a two-stage electric vehicle ADR optimization method is proposed considering both the EV owner's benefit and network load fluctuation. In the process of ADR, different risk preferences of electric vehicle owners affect the EV owner participation in ADR. Second, the GGlueVaR-based EV owner willingness decision model is adopted to measure an EV group's risk attitude. Finally, a case study is provided to verify the effectiveness of the proposed method. Results show that the proposed model reduced the average charging cost of EV owners by 45\% and increased the profit resulting from DR by 91\% compared with the price-based demand response model. Therefore, the proposed model is more efficient than disorder charging model. The method is timelier and more flexible compared with other prior demand response optimization methods. © 2020 John Wiley \& Sons Ltd}}, 
pages = {1128--1141}, 
number = {1}, 
volume = {45}
}
@article{10.1007/s00291-006-0050-7, 
year = {2007}, 
title = {{Computational aspects of integrated market and credit portfolio models}}, 
author = {Grundke, Peter}, 
journal = {OR Spectrum}, 
issn = {01716468}, 
doi = {10.1007/s00291-006-0050-7}, 
abstract = {{In this paper, it is analyzed whether a Fourier-based approach can be an efficient tool for calculating risk measures in the context of a credit portfolio model with integrated market risk factors. For this purpose, this technique is applied to a version of the well-known credit portfolio model CreditMetrics, extended by correlated interest rate and credit spread risk. While Fourier-based methods are reported to be superior to full Monte Carlo simulations for default mode models, this result cannot be confirmed for the integrated market and credit portfolio model used here. The application of standard importance sampling techniques for improving the performance of the Fourier-based approach is problematic, too. However, combining the full Monte Carlo simulation with an importance sampling technique indeed yields superior results, even for the integrated market and credit portfolio model. © 2006 Springer.}}, 
pages = {259--294}, 
number = {2}, 
volume = {29}
}
@article{10.1007/s00500-020-04751-9, 
year = {2020}, 
title = {{Uncertain random portfolio selection based on risk curve}}, 
author = {Mehralizade, Rouhollah and Amini, Mohammad and Gildeh, Bahram Sadeghpour and Ahmadzade, Hamed}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-020-04751-9}, 
abstract = {{This paper discusses the uncertain random portfolio selection problem when there are some existing risky securities which have enough historical data and some newly listed ones with insufficient data in the portfolio. So far, in the field of uncertain random portfolio selection, variance, skewness, and value-at-risk have been proposed as the risk criterion. This paper gives a new risk criterion for uncertain random portfolio selection and proposes a new type of mean-risk model based on this criterion to optimization. And in the end, a numerical example is presented for the sake of illustration. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {13331--13345}, 
number = {17}, 
volume = {24}
}
@article{10.1109/ccdc.2017.7979099, 
year = {2017}, 
title = {{Newsvendor problem with VaR and CVaR criteria under random demand and supply}}, 
author = {Wang, Rui and Son, Shiji and Li, Liuxi and Hu, Changzhang and Yue, Fan and Zhao, Han}, 
journal = {2017 29th Chinese Control And Decision Conference (CCDC)}, 
issn = {NA}, 
doi = {10.1109/ccdc.2017.7979099}, 
abstract = {{This paper studies the effect of random supply on the inventory decision of a newsvendor. For risk-neutral newsvendor, we find that the optimal order quantity is affected by random yield and the combination of random yield and capacity, but not affected by capacity uncertainty. For risk-averse newsvendor, we consider the newsvendor problem with a Value-at-Risk (VaR) constraint and the newsvendor model with Conditional Value-at-Risk (CVaR) measure. Under the VaR constraint, we find that random supply may lead to an increase or decrease in the optimal order quantity. However, under the CVaR criterion, random supply decreases the order quantity. © 2017 IEEE.}}, 
pages = {3428--3433}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-319-05014-0_2, 
year = {2014}, 
title = {{Evaluation of volatility forecasts in a VaR framework}}, 
author = {Amendola, Alessandra and Candila, Vincenzo}, 
issn = {NA}, 
doi = {10.1007/978-3-319-05014-0\_2}, 
abstract = {{Many methods can be considered to select which volatility model has a better forecast accuracy. In this work a loss function approach in a Value at Risk (VaR) framework is chosen. By using high-frequency data it is possible to achieve a consistent estimate of the VaR bootstrapping the intraday increments of an asset. The VaR estimate is used to find a threshold discriminating low from high loss function values. The analysis concerns the high-frequency data of a stock listed on the New York Stock Exchange. © 2014 Springer International Publishing Switzerland. All rights are reserved.}}, 
pages = {7--10}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/14697688.2019.1614652, 
year = {2019}, 
title = {{Extreme downside risk and market turbulence}}, 
author = {Harris, Richard D. F. and Nguyen, Linh H. and Stoja, Evarist}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2019.1614652}, 
abstract = {{We investigate the dynamics of the relationship between returns and extreme downside risk in different states of the market by combining the framework of Bali et al. [Is there an intertemporal relation between downside risk and expected returns? Journal of Financial and Quantitative Analysis, 2009, 44, 883–909] with a Markov switching mechanism. We show that the risk-return relationship identified by Bali et al. (2009) is highly significant in the low volatility state but disappears during periods of market turbulence. This is puzzling since it is during such periods that downside risk should be most prominent. We show that the absence of the risk-return relationship in the high volatility state is due to leverage and volatility feedback effects arising from increased persistence in volatility. To better filter out these effects, we propose a simple modification that yields a positive tail risk-return relationship in all states of market volatility. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--18}, 
number = {11}, 
volume = {19}
}
@article{10.1108/sef-09-2016-0236, 
year = {2018}, 
title = {{Quantile forecasts using the Realized GARCH-EVT approach}}, 
author = {Paul, Samit and Sharma, Prateek}, 
journal = {Studies in Economics and Finance}, 
issn = {10867376}, 
doi = {10.1108/sef-09-2016-0236}, 
abstract = {{Purpose: This study aims to implement a novel approach of using the Realized generalized autoregressive conditional heteroskedasticity (GARCH) model within the conditional extreme value theory (EVT) framework to generate quantile forecasts. The Realized GARCH-EVT models are estimated with different realized volatility measures. The forecasting ability of the Realized GARCH-EVT models is compared with that of the standard GARCH-EVT models. Design/methodology/approach: One-step-ahead forecasts of Value-at-Risk (VaR) and expected shortfall (ES) for five European stock indices, using different two-stage GARCH-EVT models, are generated. The forecasting ability of the standard GARCH-EVT model and the asymmetric exponential GARCH (EGARCH)-EVT model is compared with that of the Realized GARCH-EVT model. Additionally, five realized volatility measures are used to test whether the choice of realized volatility measure affects the forecasting performance of the Realized GARCH-EVT model. Findings: In terms of the out-of-sample comparisons, the Realized GARCH-EVT models generally outperform the standard GARCH-EVT and EGARCH-EVT models. However, the choice of the realized estimator does not affect the forecasting ability of the Realized GARCH-EVT model. Originality/value: It is one of the earliest implementations of the two-stage Realized GARCH-EVT model for generating quantile forecasts. To the best of the authors’ knowledge, this is the first study that compares the performance of different realized estimators within Realized GARCH-EVT framework. In the context of high-frequency data-based forecasting studies, a sample period of around 11 years is reasonably large. More importantly, the data set has a cross-sectional dimension with multiple European stock indices, whereas most of the earlier studies are based on the US market. © 2018, Emerald Publishing Limited.}}, 
pages = {481--504}, 
number = {4}, 
volume = {35}
}
@article{10.1108/15265941111176154, 
year = {2011}, 
title = {{Constructing stress tests}}, 
author = {Abbink, John B.}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/15265941111176154}, 
abstract = {{Purpose – There is limited discussion in the literature of the problems associated with constructing stress tests. The Credit Crunch has revealed that attention simply to haircuts to asset values and resulting margin calls is insufficient. The purpose of this paper is to explore additional avenues for stress testing. Design/methodology/approach – The paper is largely discursive. Findings – Stress tests must look into the debt position of the firm, as well as its position and credit exposures. Not only the volume of debt but its maturity structure, callability and the indentures attached to it are extremely important. Research limitations/implications – The paper is geared more toward management and practitioners than to academic researchers. Implications for the analysis of corporate strategy are significant. Social implications – Stress testing is essential to the confident continuance of firms. Originality/value – So much of the work in this area is proprietary and so little has been published on it. © 2011, © Emerald Group Publishing Limited.}}, 
pages = {421--434}, 
number = {5}, 
volume = {12}
}
@article{10.1007/s11749-019-00627-9, 
year = {2019}, 
title = {{Asymptotics for the linear kernel quantile estimator}}, 
author = {Wang, Xuejun and Wu, Yi and Yu, Wei and Yang, Wenzhi and Hu, Shuhe}, 
journal = {TEST}, 
issn = {11330686}, 
doi = {10.1007/s11749-019-00627-9}, 
abstract = {{The method of linear kernel quantile estimator was proposed by Parzen (J Am Stat Assoc 74:105–121, 1979), which is a reasonable estimator for Value-at-risk (VaR). In this paper, we mainly investigate the asymptotic properties for linear kernel quantile estimator of VaR based on φ-mixing samples. At first, the Bahadur representation for sample quantiles under φ-mixing sequence is established. By using the Bahadur representation for sample quantiles, we further obtain the Bahadur representation for linear kernel quantile estimator of VaR in sense of almost surely convergence with the rate O(n- 1 / 2log - αn) for some α\&gt; 0. In addition, the strong consistency for the linear kernel quantile estimator of VaR with the convergence rate O(n- 1 / 2(log log n) 1 / 2) is established, and the asymptotic normality for linear kernel quantile estimator of VaR based on φ-mixing samples is obtained. Finally, a simulation study and a real data analysis are undertaken to assess the finite sample performance of the results that we established. © 2019, Sociedad de Estadística e Investigación Operativa.}}, 
pages = {1144--1174}, 
number = {4}, 
volume = {28}
}
@article{10.1016/j.compind.2018.08.002, 
year = {2018}, 
title = {{Future developments in cyber risk assessment for the internet of things}}, 
author = {Radanliev, Petar and Roure, David Charles De and Nicolescu, Razvan and Huth, Michael and Montalvo, Rafael Mantilla and Cannady, Stacy and Burnap, Peter}, 
journal = {Computers in Industry}, 
issn = {01663615}, 
doi = {10.1016/j.compind.2018.08.002}, 
eprint = {1809.05229}, 
abstract = {{This article is focused on the economic impact assessment of Internet of Things (IoT) and its associated cyber risks vectors and vertices – a reinterpretation of IoT verticals. We adapt to IoT both the Cyber Value at Risk model, a well-established model for measuring the maximum possible loss over a given time period, and the MicroMort model, a widely used model for predicting uncertainty through units of mortality risk. The resulting new IoT MicroMort for calculating IoT risk is tested and validated with real data from the BullGuard's IoT Scanner (over 310,000 scans) and the Garner report on IoT connected devices. Two calculations are developed, the current state of IoT cyber risk and the future forecasts of IoT cyber risk. Our work therefore advances the efforts of integrating cyber risk impact assessments and offer a better understanding of economic impact assessment for IoT cyber risk. © 2018 The Authors}}, 
pages = {14--22}, 
number = {NA}, 
volume = {102}
}
@article{10.1080/03610926.2014.964808, 
year = {2018}, 
title = {{Shuffle of min’s random variable approximations of bivariate copulas’ realization}}, 
author = {Zheng, Yanting and Yang, Jingping and Huang, Jianhua Z.}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2014.964808}, 
abstract = {{The comonotonicity and countermonotonicity provide intuitive upper and lower dependence relationship between random variables. This paper constructs the shuffle of min’s random variable approximations for a given Uniform [0, 1] random vector. We find the two optimal orders under which the shuffle of min’s random variable approximations obtained are shown to be extensions of comonotonicity and countermonotonicity. We also provide the rate of convergence of these random vectors approximations and apply them to compute value-at-risk. © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {2337--2350}, 
number = {10}, 
volume = {47}
}
@article{10.3969/j.issn.1000-565x.2012.06.022, 
year = {2012}, 
title = {{Calculation of construction period value-at-risk of foundation pit engineering for subway station}}, 
author = {}, 
issn = {1000565X}, 
doi = {10.3969/j.issn.1000-565x.2012.06.022}, 
abstract = {{This paper deals with the calculation methods and the corresponding steps of the value-at-risk due to the construction period change, based on the total probability and the Monte Carlo thoughts. In the investigation, first, the risk probability is calculated for the project without anomalous change during the construction, with anomalous change but without remedial measures, and with anomalous change as well as remedial measures. Based on the probability calculation in the three above-mentioned cases, a calculation formula of full-probability method of abnormal project time is established. Then, by using the Monte Carlo thoughts, the Monte Carlo method calculation steps of the value-at-risk are presented based on Matlab, and the construction period change is described with the Beta distribution function. Finally, with an excavation engineering of a subway station as an example, the value-at-risk during the excavation period and the supporting period is calculated. The results indicate that the full-probability method and the Monte Carlo method are both effective.}}, 
number = {6}, 
volume = {40}
}
@article{10.1109/tencon.2018.8650551, 
year = {2019}, 
title = {{Value-at-Risk and Expected Shortfall in Technology Hardware and Equipment Industry Using Fuzzy Model}}, 
author = {Amante, Mayca Joy M. and Calinisan, John Paulo A. and Co, Vicente L. and Quevedo, Venusmar C. and Quevedo, Francispito P.}, 
journal = {TENCON 2018 - 2018 IEEE Region 10 Conference}, 
issn = {21593442}, 
doi = {10.1109/tencon.2018.8650551}, 
abstract = {{The fuzzy logic systems assist in simplifying large-scale risk management frameworks. For risks that do not have a proper quantitative probability model, a fuzzy logic system can aid model the cause-and-effect relationships, assess the degree of risk exposure and rank the key risks in a reliable way, considering both the available data and experts' opinions. For companies with broad risk exposure, diversified business and operations in multiple geographic regions, the long list of risks that need to be monitored makes in-depth risk analysis excessive, especially when there are entangled relationships among risk factors. Such an analysis could be costly and extremely tedious without the use of a fuzzy logic system. In addition, fuzzy logic systems include rules that clearly explain the linkage, dependence and relationships among modeled factors. It is helpful for identifying risk mitigation solutions. Resources can then be used to mitigate the risks with the highest level of exposure and relatively low hedging cost. This paper studied the application of Fuzzy Logic System Approach in determining the company's value-at-risk and also to assess risk analysis. This was applied to Technology Hardware and Equipment Industry in the Philippine. It was explored in areas where fuzzy logic models may be applied to improve risk assessment and risk decision-making. It discussed the methodology, framework and process of using fuzzy logic systems for risk management. © 2018 IEEE.}}, 
pages = {2539--2544}, 
number = {NA}, 
volume = {2018-October}
}
@article{10.1080/00949655.2010.547196, 
year = {2012}, 
title = {{A computational study of a quasi-PORT methodology for VaR based on second-order reduced-bias estimation}}, 
author = {Figueiredo, Fernanda and Gomes, M. Ivette and Henriques-Rodrigues, Lígia and Miranda, M. Cristina}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2010.547196}, 
abstract = {{In this paper, we deal with the estimation, under a semi-parametric framework, of the Value-at-Risk (VaR) at a level p, the size of the loss occurred with a small probability p. Under such a context, the classical VaR estimators are the Weissman-Hill estimators, based on any intermediate number k of top-order statistics. But these VaR estimators do not enjoy the adequate linear property of quantiles, contrarily to the PORT VaR estimators, which depend on an extra tuning parameter q, with 0 ≤ q < 1. We shall here consider 'quasi-PORT' reduced-bias VaR estimators, for which such a linear property is obtained approximately. They are based on a partially shifted version of a minimum-variance reduced-bias (MVRB) estimator of the extreme value index (EVI), the primary parameter in Statistics of Extremes. Due to the stability on k of the MVRB EVI and associated VaR estimates, we propose the use of a heuristic stability criterion for the choice of k and q, providing applications of the methodology to simulated data and to log-returns of financial stocks. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {587--602}, 
number = {4}, 
volume = {82}
}
@article{10.1016/j.qref.2012.04.006, 
year = {2012}, 
title = {{Parametric Value-at-Risk analysis: Evidence from stock indices}}, 
author = {Mabrouk, Samir and Saadi, Samir}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2012.04.006}, 
abstract = {{We evaluate the performance of several volatility models in estimating one-day-ahead Value-at-Risk (VaR) of seven stock market indices using a number of distributional assumptions. Because all returns series exhibit volatility clustering and long range memory, we examine GARCH-type models including fractionary integrated models under normal, Student- t and skewed Student- t distributions. Consistent with the idea that the accuracy of VaR estimates is sensitive to the adequacy of the volatility model used, we find that AR (1)-FIAPARCH (1,. d,1) model, under a skewed Student- t distribution, outperforms all the models that we have considered including widely used ones such as GARCH (1,1) or HYGARCH (1,. d,1). The superior performance of the skewed Student- t FIAPARCH model holds for all stock market indices, and for both long and short trading positions. Our findings can be explained by the fact that the skewed Student- t FIAPARCH model can jointly accounts for the salient features of financial time series: fat tails, asymmetry, volatility clustering and long memory. In the same vein, because it fails to account for most of these stylized facts, the RiskMetrics model provides the least accurate VaR estimation. Our results corroborate the calls for the use of more realistic assumptions in financial modeling. © 2012 The Board of Trustees of the University of Illinois.}}, 
pages = {305--321}, 
number = {3}, 
volume = {52}
}
@article{10.1287/opre.2019.1888, 
year = {2020}, 
title = {{Submodularity in conic quadratic mixed 0-1 optimization}}, 
author = {Atamtürk, Alper and Gómez, Andrés}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.2019.1888}, 
abstract = {{We describe strong convex valid inequalities for conic quadratic mixed 0-1 optimization. These inequalities can be utilized for solving numerous practical nonlinear discrete optimization problems from value-at-risk minimization to queueing system design, from robust interdiction to assortment optimization through appropriate conic quadratic mixed 0-1 relaxations. The inequalities exploit the submodularity of the binary restrictions and are based on the polymatroid inequalities over binaries for the diagonal case. We prove that the convex inequalities completely describe the convex hull of a single conic quadratic constraint as well as the rotated cone constraint over binary variables and unbounded continuous variables. We then generalize and strengthen the inequalities by incorporating additional constraints of the optimization problem. Computational experiments on mean-risk optimization with correlations, assortment optimization, and robust conic quadratic optimization indicate that the new inequalities strengthen the convex relaxations substantially and lead to significant performance improvements. © 2020 INFORMS.}}, 
number = {2}, 
volume = {68}
}
@article{10.1016/j.jeconom.2015.02.032, 
year = {2015}, 
title = {{A stochastic dominance approach to financial risk management strategies}}, 
author = {Chang, Chia-Lin and Jiménez-Martín, Juan-Ángel and Maasoumi, Esfandiar and Pérez-Amaral, Teodosio}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2015.02.032}, 
abstract = {{The Basel III Accord requires that banks and other Authorized Deposit-taking Institutions (ADIs) communicate their daily risk forecasts to the appropriate monetary authorities at the beginning of each trading day, using one of a range of alternative risk models to forecast Value-at-Risk (VaR). The risk estimates from these models are used to determine the daily capital charges (DCC) and associated capital costs of ADIs, depending in part on the number of previous violations, whereby realized losses exceed the estimated VaR. In this paper we define risk management in terms of choosing sensibly from a variety of risk models and discuss the optimal selection of the risk models. Previous approaches to model selection for predicting VaR proposed combining alternative risk models and ranking such models on the basis of average DCC, or other quantiles of its distribution. These methods are based on the first moment, or specific quantiles of the DCC distribution, and supported by restrictive evaluation functions. In this paper, we consider robust uniform rankings of models over large classes of loss functions that may reflect different weights and concerns over different intervals of the distribution of losses and DCC. The uniform rankings are based on recently developed statistical tests of stochastic dominance (SD). The SD tests are illustrated using the prices and returns of VIX futures. The empirical findings show that the tests of SD can rank different pairs of models to a statistical degree of confidence, and that the alternative (recentered) SD tests are in general agreement. © 2015 Elsevier B.V.}}, 
pages = {472--485}, 
number = {2}, 
volume = {187}
}
@article{10.1145/2830556.2830559, 
year = {2015}, 
title = {{Optimization strategies for portable code for monte carlo-based value-at-risk systems}}, 
author = {Varela, Javier Alejandro and Kestel, Claus and Schryver, Christian De and Wehn, Norbert and Desmettre, Sascha and Korn, Ralf}, 
journal = {Proceedings of the 8th Workshop on High Performance Computational Finance}, 
issn = {NA}, 
doi = {10.1145/2830556.2830559}, 
abstract = {{Value-at-risk (VaR) computations are one important basic element of risk analysis and management applications. On the one hand, risk management systems need to be exible and maintainable, but on the other hand they require a very high computational power. In general, accelerators provide high speedups, but come with a limited exibility. In this work, we investigate two approaches towards portable and fast code for VaR computations on heterogeneous platforms: operator tuning and the use of OpenCL. We show that operator tuning can save up one third of run time on CPU-based systems in the calibration step. For OpenCL, we present a detailed analysis of run time on CPU, GPU, and Xeon Phi, and evaluate its portability. We also find that the same code runs up to 12x faster in a VaR setting with an accelerator card being present, without any code changes required.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/globalsip.2013.6737103, 
year = {2013}, 
title = {{Beyond PCA for modeling financial time-series}}, 
author = {Malioutov, Dmitry}, 
journal = {2013 IEEE Global Conference on Signal and Information Processing}, 
issn = {NA}, 
doi = {10.1109/globalsip.2013.6737103}, 
abstract = {{Statistical factor models based on principal component analysis (PCA) have been widely used to reduce the dimensionality of financial time-series. We investigate the sensitivity of PCA to peculiarities of financial data, such as heavy tails and asymmetry and suggest to use alternatives to PCA. We investigate a recent reformulation of principal components as a search for projections which allows to go beyond the squared-error in the objective. We suggest to use a robust formulation for PCA and also a version of PCA with conditional value at risk (cVaR) as the error metric to drive the low-rank approximation. cVaR has received considerable attention in risk management as a coherent replacement of Value at Risk. We describe a convex formulation for both robust PCA and cVaR-PCA and apply them on an computational example with US equities. © 2013 IEEE.}}, 
pages = {1140--1140}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/15567240802053756, 
year = {2010}, 
title = {{Catastrophic event modeling in the gulf of mexico-II. industry exposure and value at risk}}, 
author = {Kaiser, M. J. and Pulsipher, A. G. and Darr, J. and Singhal, A. and Foster, T. and Vojjala, R.}, 
journal = {Energy Sources, Part B: Economics, Planning, and Policy}, 
issn = {15567249}, 
doi = {10.1080/15567240802053756}, 
abstract = {{The offshore insurance industry has traditionally been defined by a "trader" mentality in which companies have been willing to hold risks without attempting to model or further quantify their risk through technical appraisal. The 2005 hurricane season in the Gulf of Mexico changed this perspective, and underwriters began to control their exposure to contingent losses through policy sub-limits and other restrictions on coverage. The purpose of the final part of this two-part article is to quantify the losses to property damage in the Gulf of Mexico for historic weather events using the Risk Management Solutions Offshore Platform Model®. Value at Risk measures indicate that the financial exposure of Gulf assets is on the order of \$60-70 billion in platform and rig exposures not accounting for pipelines and wells. Probabilistic scenarios from storm simulations indicate a 100-year loss estimated at \$5.7 billion and a 250-year loss estimated at \$7.5 billion for physical damage to platforms and rigs not including business interruption and operator extra expense losses. Copyright © Taylor \&amp; Francis Group, LLC.}}, 
pages = {147--154}, 
number = {2}, 
volume = {5}
}
@article{10.1002/etep.2049, 
year = {2015}, 
title = {{Transmission maintenance scheduling strategy considering potential fault risk balance}}, 
author = {Pan, Zhi‐jun and Zhang, Yan}, 
journal = {International Transactions on Electrical Energy Systems}, 
issn = {20507038}, 
doi = {10.1002/etep.2049}, 
abstract = {{In the period of transmission maintenance, the phenomenon of power interruption or even progressing to cascading failure occurs commonly just originating from one or some elements out of service. Therefore, reasonable transmission maintenance scheduling is important to ensure the safe and stable operation of power supply system. In this paper, a transmission maintenance scheduling strategy considering potential fault risk balance is proposed. Combined with the theory of value at risk and the character of potential fault, the potential fault risk index is defined to represent the possibility and risk level of the whole system power failure under the condition of the certain element failing. Meanwhile, compared with traditional transmission maintenance scheduling model, the balance index and normalized weight factor shown as the system's risk difference of each maintenance period is considered into the objective function to achieve risk sharing and real operation reliability in the whole schedule. In addition, the random potential fault simulation is applied to solve calculation problem when making the schedule. Numerical results of IEEE-118 bus system show the validity of the methodology proposed. © 2015 John Wiley \& Sons, Ltd.}}, 
pages = {3523--3537}, 
number = {12}, 
volume = {25}
}
@article{10.1080/09603107.2012.709601, 
year = {2013}, 
title = {{Evaluating forecast performances of the quantile autoregression models in the present global crisis in international equity markets}}, 
author = {Xu, Qing and Childs, Terry}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2012.709601}, 
abstract = {{In this research, we compare the one-step-ahead out-of-sample forecast performances of the linear Quantile Autoregression (QAR) model as well as the latest sophisticated nonlinear copula-based QAR models for four daily equity index returns during the current financial tumultuous period. In addition, two Conditional Autoregressive Value-at-Risk (CAViaR) models proposed by Engle and Manganelli (2004) are also considered. In order to obtain the robust evaluation results, we estimate the time-varying parameters via two forecasting schemes (recursive and rolling) and examine the accuracy of the Value-at-Risk (VaR) forecast by three different test procedures. Our main findings are that the CAViaR models provide good forecast performance in most cases and they are superior to both linear and nonlinear copula-based QAR models. © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {105--117}, 
number = {2}, 
volume = {23}
}
@article{10.1504/ijbcrm.2018.094163, 
year = {2018}, 
title = {{Expert’s opinion impact on financial risk management}}, 
author = {Masci, Martín Ezequiel and Casparri, María Teresa and Fronti, Javier Ignacio García}, 
journal = {International Journal of Business Continuity and Risk Management}, 
issn = {17582164}, 
doi = {10.1504/ijbcrm.2018.094163}, 
abstract = {{The present work proposes to incorporate expert’s opinions into the traditional calculation of financial risk management models. These traditional quantitative models ignore qualitative knowledge available (subjective and mathematically imprecise). To do this, this paper articulates value-at-risk (VaR) practioner’s perspective with experts’ opinion. This work focuses on the techniques associated with the mathematical approach as opposed to the behavioural one and within this approach, contrasting those axiomatic with the Bayesian. The authors argue that expert’s judgement is relevant to market risk management. In particular, financial risks are especially sensitive to decision makers’ opinions. Three strategies are proposed. Firstly, Bayesian techniques, secondly, fuzzy logic with VaR approximation through triangular fuzzy numbers (TFN) and thirdly, expert discussion through Delphi dynamics are considered. © 2018 Inderscience Enterprises Ltd.}}, 
pages = {249}, 
number = {3}, 
volume = {8}
}
@article{10.1016/j.jbankfin.2012.06.015, 
year = {2013}, 
title = {{Aggregation of exponential smoothing processes with an application to portfolio risk evaluation}}, 
author = {Sbrana, Giacomo and Silvestrini, Andrea}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.06.015}, 
abstract = {{In this paper we propose a unified framework to analyse contemporaneous and temporal aggregation of a widely employed class of integrated moving average (IMA) models. We obtain a closed-form representation for the parameters of the contemporaneously and temporally aggregated process as a function of the parameters of the original one. These results are useful due to the close analogy between the integrated GARCH (1,. 1) model for conditional volatility and the IMA (1,. 1) model for squared returns, which share the same autocorrelation function. In this framework, we present an application dealing with Value-at-Risk (VaR) prediction at different sampling frequencies for an equally weighted portfolio composed of multiple indices. We apply the aggregation results by inferring the aggregate parameter in the portfolio volatility equation from the estimated vector IMA (1,. 1) model of squared returns. Empirical results show that VaR predictions delivered using this suggested approach are at least as accurate as those obtained by applying standard univariate methodologies, such as RiskMetrics. © 2012 Elsevier B.V.}}, 
pages = {1437--1450}, 
number = {5}, 
volume = {37}
}
@article{10.7500/aeps20180612006, 
year = {2019}, 
title = {{vine-Copula Based Dynamic Risk VaR Assessment Method for Operation Profit and Loss of Generation Companies [基于vine-Copula的发电商运营损益动态风险VaR评估方法]}}, 
author = {}, 
issn = {10001026}, 
doi = {10.7500/aeps20180612006}, 
abstract = {{In the power market environment, the fluctuation of load and electricity price, the random output of renewable power supply, the change of network topology, the adjustment of the system operation mode and a large number of uncertainties add risks for the operating income of generation companies (GenCos). In order to help GenCos accurately evaluate their own risks and formulate appropriate bidding strategies and generating unit plans, a vine-Copula theory based value at risk (VaR) evaluation model is presented for the profit and loss of GenCos. Firstly, based on the AC power flow, the optimal power flow model for day-ahead dynamic economic dispatch of power grid is established to calculate the locational marginal price. Then, the pair-Copula theory is introduced to construct the high dimensional vine-Copula dependent structure of the profit and loss functions of GenCos. Finally, the VaRs of operating combined risk of GenCos under different confidence levels are calculated based on the definition of VaR. The example of IEEE 39-bus system is taken to verify the effectiveness of the proposed model and method. © 2019 Automation of Electric Power Systems Press.}}, 
number = {5}, 
volume = {43}
}
@article{10.1016/j.insmatheco.2011.06.001, 
year = {2011}, 
title = {{Worst case risk measurement: Back to the future?}}, 
author = {Goovaerts, Marc J. and Kaas, Rob and Laeven, Roger J.A.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2011.06.001}, 
abstract = {{This paper studies the problem of finding best-possible upper bounds on a rich class of risk measures, expressible as integrals with respect to measures, under incomplete probabilistic information. Both univariate and multivariate risk measurement problems are considered. The extremal probability distributions, generating the worst case scenarios, are also identified. The problem of worst case risk measurement has been studied extensively by Etienne De Vijlder and his co-authors, within the framework of finite-dimensional convex analysis. This paper revisits and extends some of their results. © 2011 Elsevier B.V.}}, 
pages = {380--392}, 
number = {3}, 
volume = {49}
}
@article{10.1108/s1569-3759(2012)0000094013, 
year = {2012}, 
title = {{Re-evaluating hedging performance for asymmetry: The case of crude oil}}, 
author = {Cotter, John and Hanly, Jim}, 
journal = {Contemporary Studies in Economic and Financial Analysis}, 
issn = {15693759}, 
doi = {10.1108/s1569-3759(2012)0000094013}, 
abstract = {{We examine whether the hedging effectiveness of crude oil futures is affected by asymmetry in the return distribution by applying tail-specific metrics to compare the hedging effectiveness of both short and long hedgers. The hedging effectiveness metrics we use are based on lower partial moments (LPM), value at risk (VaR) and conditional value at risk (CVaR). Comparisons are applied to a number of hedging strategies including ordinary least square (OLS), and both symmetric and asymmetric GARCH models. We find that OLS provides consistently better performance across different measures of hedging effectiveness as compared with GARCH models, irrespective of the characteristics of the underlying distribution. Copyright © 2012 by Emerald Group Publishing Limited.}}, 
pages = {259--280}, 
number = {NA}, 
volume = {94}
}
@article{10.1111/j.1468-036x.2010.00540.x, 
year = {2012}, 
title = {{Risk-Adjusted Measures of Value Creation in Financial Institutions}}, 
author = {Milne, Alistair and Onorato, Mario}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/j.1468-036x.2010.00540.x}, 
abstract = {{Abstract Many financial institutions assess portfolio decisions using RAROC, the ratio of expected return to risk (or 'economic') capital. We use asset pricing theory to determine the appropriate hurdle rate, finding that this varies with the skewness of asset returns. We quantify this discrepancy under a range of assumptions showing that the RAROC hurdle rate differs substantially, being higher by a factor of five or more for equity which has a right skew compared to debt which has a pronounced left skew, and also between different qualities of debt exposure. We discuss implications for both financial institution risk management and supervision. © 2011 Blackwell Publishing Ltd.}}, 
pages = {578--601}, 
number = {4}, 
volume = {18}
}
@article{10.1007/s001860200181, 
year = {2002}, 
title = {{Stable modeling in energy risk management}}, 
author = {Khindanova, Irina and Atakhanova, Zauresh}, 
journal = {Mathematical Methods of Operations Research}, 
issn = {14322994}, 
doi = {10.1007/s001860200181}, 
abstract = {{High price volatility in energy markets compels the companies to adopt and implement policies for measurement and management of the energy risk. A popular measure of risk exposure is the Value at Risk (VAR). Traditional methods of estimation of VaR used by major energy companies fail to capture the heavy tails and asymmetry of energy returns distributions. We suggest the use of stable distributions for modeling energy return distributions. The results of our study demonstrate that stable modeling captures asymmetry and heavy-tails of returns, and, therefore, provides more accurate estimates of energy VaR.}}, 
pages = {225--245}, 
number = {2}, 
volume = {55}
}
@article{10.1007/s00180-010-0194-4, 
year = {2010}, 
title = {{A comparison of bootstrap and Monte-Carlo testing approaches to value-at-risk diagnosis}}, 
author = {Herwartz, Helmut and Waichman, Israel}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-010-0194-4}, 
abstract = {{In this note we investigate a particular resampling scheme and Monte Carlo testing to determine critical values for two test statistics typically used for diagnosing value-at-risk models. In cases of small nominal coverage subjected to testing, the dynamic quantile test and a corresponding logit based likelihood ratio test suffer from poor convergence to the asymptotic limit distribution. In terms of empirical size both resampling and Monte Carlo approaches offer most accurate test features with the Monte Carlo technique achieving power gains if a misspecified value-at-risk model is subjected to testing. © Springer-Verlag 2010.}}, 
pages = {725--732}, 
number = {4}, 
volume = {25}
}
@article{10.1016/j.ejor.2009.11.020, 
year = {2010}, 
title = {{List pricing versus dynamic pricing: Impact on the revenue risk}}, 
author = {Koenig, Matthias and Meissner, Joern}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2009.11.020}, 
abstract = {{We consider the problem of a firm selling multiple products that consume a single resource over a finite time period. The amount of the resource is exogenously fixed. We analyze the difference between a dynamic pricing policy and a list-price capacity control policy. The dynamic pricing policy adjusts prices steadily resolving the underlying problem every time step, whereas the list pricing policy sets static prices once but controls the capacity by allowing or preventing product sales. As steady price changes are often costly or unachievable in practice, we investigate the question of how much riskier it is to apply a list pricing policy rather than a dynamic pricing policy. We conduct several numerical experiments and compare expected revenue, standard deviation, and conditional-value-at-risk between the pricing policies. The differences between the policies show that list pricing can be a useful strategy when dynamic pricing is costly or impractical. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {505--512}, 
number = {3}, 
volume = {204}
}
@article{10.1007/s00245-013-9199-z, 
year = {2013}, 
title = {{Nonlinear diffusions and stable-like processes with coefficients depending on the median or VaR}}, 
author = {Kolokoltsov, Vassili N.}, 
journal = {Applied Mathematics \& Optimization}, 
issn = {00954616}, 
doi = {10.1007/s00245-013-9199-z}, 
abstract = {{The paper is devoted to the well-posedness for nonlinear McKean-Vlasov type diffusions with coefficients depending on the median or, more generally, on the α-quantile of the underlying distribution. The median is not a continuous function on the space of probability measures equipped with the weak convergence. This is one reason why well-posedness of the SDE considered in the paper does not follow by standard arguments. © 2013 Springer Science+Business Media New York.}}, 
pages = {85--98}, 
number = {1}, 
volume = {68}
}
@article{10.3390/math8010114, 
year = {2020}, 
title = {{Value at risk estimation using the garch-evt approach with optimal tail selection}}, 
author = {Echaust, Krzysztof and Just, Małgorzata}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math8010114}, 
abstract = {{A conditional Extreme Value Theory (GARCH-EVT) approach is a two-stage hybrid method that combines a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) filter with the Extreme Value Theory (EVT). The approach requires pre-specification of a threshold separating distribution tails from its middle part. The appropriate choice of a threshold level is a demanding task. In this paper we use four different optimal tail selection algorithms, i.e., the path stability method, the automated Eye-Ball method, the minimization of asymptotic mean squared error method and the distance metric method with a mean absolute penalty function, to estimate out-of-sample Value at Risk (VaR) forecasts and compare them to the fixed threshold approach. Unlike other studies, we update the optimal fraction of the tail for each rolling window of the returns. The research objective is to verify to what extent optimization procedures can improve VaR estimates compared to the fixed threshold approach. Results are presented for a long and a short position applying 10 world stock indices in the period from 2000 to June 2019. Although each approach generates different threshold levels, the GARCH-EVT model produces similar Value at Risk estimates. Therefore, no improvement of VaR accuracy may be observed relative to the conservative approach taking the 95th quantile of returns as a threshold. © 2020 by the authors.}}, 
pages = {114}, 
number = {1}, 
volume = {8}
}
@article{10.1007/s10342-009-0280-8, 
year = {2010}, 
title = {{Producing softwood of different quality: Does this provide risk compensation?}}, 
author = {Beinhofer, Bernhard}, 
journal = {European Journal of Forest Research}, 
issn = {16124669}, 
doi = {10.1007/s10342-009-0280-8}, 
abstract = {{This study analysed whether the production of high-quality assortments, saw timber and pulpwood in Norway spruce (Picea abies) and Scots pine (Pinus sylvestris) stands leads to effects of risk compensation. Mixtures of conventionally treated pruned and non-pruned stands, as well as mixtures of stands which had been optimized concerning the occurrence of assortments and financial criteria were considered. The price simulation was done by bootstrapping to avoid the assumption of a certain distribution of timber prices, and to preserve the correlations of the timber prices from different assortments. The financial valuation was carried out with annuities within a Monte Carlo simulation with 10,000 repetitions. To quantify the effect of natural hazards, the calculations were repeated with and without this factor. Minimum risk portfolios were determined and optimization performed using both a utility function as well as the value-at-risk approach. The mixtures with minimum risk were dominated by pine stands, as they are less affected by natural hazards and prices for pine timber have a lower volatility, although pine timber prices are generally lower compared to that of spruce timber. These pine dominated portfolios showed risk reductions up to 92\% compared to the riskiest single stand, but the annuities were reduced even more. In contrast to this, the portfolios optimized with the utility function and the value-at-risk approach consisted of spruce stands. These spruce portfolios showed an efficient risk reduction of up to 60\%. Additionally, higher annuities and larger diversification effects occurred in the portfolios containing the optimized stands. Integrating the risk of natural hazards, the annuities decreased, as did the correlation of the annuities of the different stands, while risk increased. Altogether these effects led to higher relative risk reductions when forming optimal portfolios. These results indicate that producing different assortments of spruce and pine leads to risk compensation, especially when considering portfolios consisting of optimized stands. © 2009 Springer-Verlag.}}, 
pages = {921--934}, 
number = {5}, 
volume = {129}
}
@article{10.17654/ms102081789, 
year = {2017}, 
title = {{Estimation of the value-at-risk using the copula-garch approach: Application in Indonesian market}}, 
author = {Budiarti, Retno and Wigena, Aji Hamim and Purnaba, I Gusti Putu and Achsani, Noer Azam}, 
journal = {Far East Journal of Mathematical Sciences (FJMS)}, 
issn = {09720871}, 
doi = {10.17654/ms102081789}, 
abstract = {{Value-at-risk (VaR) is one of the mostly used risk measures in the risk management. This paper suggests a method for measuring the risk of portfolio composed of assets with volatility clustering return series. In order to obtain good estimates for VaR, the model tries to capture as realistically as possible the data for each return series and the dependence structure that exists at the portfolio level. For this purpose, the individual return series are modelled using GARCH methods with parametric innovations and the dependence structure is modelled using copula. This method is applied to the portfolio of IDX composite index and the exchange rate for Indonesian Rupiah to United States Dollar. The result shows that the GARCH-Student-t copula is the best approach to estimate the VaR portfolio value. © 2017 Pushpa Publishing House, Allahabad, India.}}, 
pages = {1789--1807}, 
number = {8}, 
volume = {102}
}
@article{10.1080/03610918.2018.1535069, 
year = {2020}, 
title = {{A new approach to Value-at-Risk: GARCH-TSLx model with inference}}, 
author = {Altun, Emrah}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2018.1535069}, 
abstract = {{In this paper, two-sided Lomax (TSLx) distribution is proposed. The usefulness of proposed distribution is demonstrated in forecasting Value-at-Risk by applying the TSLx distribution to generalized autoregressive conditional heteroscedasticity (GARCH) models. The real data application on Nasdaq-100 index is given to illustrate the performance of GARCH model specified under TSLx innovation distribution against to normal, Student-t and generalized error distributions in terms of the accuracy of VaR forecasts. The backtesting results reveal that the GARCH models specified under TSLx innovation distribution generates the more realistic VaR forecasts than other competitive models for all confidence levels. © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--18}, 
number = {12}, 
volume = {49}
}
@article{10.1109/cec.2012.6252907, 
year = {2012}, 
title = {{Evolving constrained mean-VaR efficient frontiers}}, 
author = {Jevne, Håken K and Haddow, Pauline C and Gaivoronski, Alexei A}, 
journal = {2012 IEEE Congress on Evolutionary Computation}, 
issn = {NA}, 
doi = {10.1109/cec.2012.6252907}, 
abstract = {{Value-at-Risk - an industry standard risk measure; may be applied to assess and optimize a portfolio of assets. However, traditional optimization software does not provide for value-at-risk optimization. One solution is to apply evolutionary techniques to search for optimal solutions. However, to increase the realism in evolutionary solutions, it is important to consider the inclusion of realistic real-world constraints and to further consider the effect of the initialization scheme on the results achievable. Two techniques are investigated in this work. The key technique is multi-objective differential evolution (MODE) which is applied together with an adapted initialization scheme to search for VaR-optimal portfolios in the presence of real-world constraints. Further, NSGA-II - a more established multi-objective optimization technique; is implemented and extended with real world constraints and a refined initialization scheme, so as to compare the benefits of the MODE technique in the light of a refined NSGA-II technique and highlight the benefits of such refinements on NSGA-II itself. © 2012 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ijar.2007.06.009, 
year = {2008}, 
title = {{Financial risk measurement with imprecise probabilities}}, 
author = {Vicig, Paolo}, 
journal = {International Journal of Approximate Reasoning}, 
issn = {0888613X}, 
doi = {10.1016/j.ijar.2007.06.009}, 
abstract = {{Although financial risk measurement is a largely investigated research area, its relationship with imprecise probabilities has been mostly overlooked. However, risk measures can be viewed as instances of upper (or lower) previsions, thus letting us apply the theory of imprecise previsions to them. After a presentation of some well known risk measures, including Value-at-Risk or VaR, coherent and convex risk measures, we show how their definitions can be generalized and discuss their consistency properties. Thus, for instance, VaR may or may not avoid sure loss, and conditions for this can be derived. This analysis also makes us consider a very large class of imprecise previsions, which we termed convex previsions, generalizing convex risk measures. Shortfall-based measures and Dutch risk measures are also investigated. Further, conditional risks can be measured by introducing conditional convex previsions. Finally, we analyze the role in risk measurement of some important notions in the theory of imprecise probabilities, like the natural extension or the envelope theorems. © 2007 Elsevier Inc. All rights reserved.}}, 
pages = {159--174}, 
number = {1}, 
volume = {49}
}
@article{10.1109/bife.2012.76, 
year = {2012}, 
title = {{Steel products' pledging rate based on value-at-risk model}}, 
author = {Yang, Haoxiong and Fang, Yanan and Zhou, Jingjie and Zhou, Yongsheng}, 
journal = {2012 Fifth International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2012.76}, 
abstract = {{The iron and steel products logistics finance developed with high speed and the pledging rate is a major problem of iron and steel logistics finance within this industry. This paper introduces the Value-at-Risk model (VaR model) to calculate the steel products' pledging rate, and based on empirical research and analysis this paper explored how to provide accurate and significantly pledge to improve the efficiency of the financing with VaR model based on the premise of risk control. It can carry out the logistics for the iron and steel enterprises to provide financial services. © 2012 IEEE.}}, 
pages = {332--336}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.csda.2014.04.016, 
year = {2014}, 
title = {{A sampling algorithm for bandwidth estimation in a nonparametric regression model with a flexible error density}}, 
author = {Zhang, Xibin and King, Maxwell L. and Shang, Han Lin}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2014.04.016}, 
abstract = {{The unknown error density of a nonparametric regression model is approximated by a mixture of Gaussian densities with means being the individual error realizations and variance a constant parameter. Such a mixture density has the form of a kernel density estimator of error realizations. An approximate likelihood and posterior for bandwidth parameters in the kernel-form error density and the Nadaraya-Watson regression estimator are derived, and a sampling algorithm is developed. A simulation study shows that when the true error density is non-Gaussian, the kernel-form error density is often favored against its parametric counterparts including the correct error density assumption. The proposed approach is demonstrated through a nonparametric regression model of the Australian All Ordinaries daily return on the overnight FTSE and S\&P 500 returns. With the estimated bandwidths, the one-day-ahead posterior predictive density of the All Ordinaries return is derived, and a distribution-free value-at-risk is obtained. The proposed algorithm is also applied to a nonparametric regression model involved in state-price density estimation based on S\&P 500 options data. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {218--234}, 
number = {NA}, 
volume = {78}
}
@article{10.18267/j.pep.451, 
year = {2013}, 
title = {{Estimating correlated jumps and stochastic volatilities}}, 
author = {Witzany, Jiří}, 
journal = {Prague Economic Papers}, 
issn = {12100455}, 
doi = {10.18267/j.pep.451}, 
abstract = {{We formulate a bivariate stochastic volatility jump-diffusion model with correlated jumps and volatilities. An MCMC Metropolis-Hastings sampling algorithm is proposed to estimate the model's parameters and latent state variables (jumps and stochastic volatilities) given observed returns. The methodology is successfully tested on several artificially generated bivariate time series and then on the two most important Czech domestic financial market time series of the FX (CZK/EUR) and stock (PX index) returns. Four bivariate models with and without jumps and/or stochastic volatility are compared using the deviance information criterion (DIC) confirming importance of incorporation of jumps and stochastic volatility into the model.}}, 
pages = {251--283}, 
number = {2}, 
volume = {NA}
}
@article{10.1016/s0165-1889(03)00056-3, 
year = {2004}, 
title = {{Capital growth with security}}, 
author = {MacLean, Leonard C. and Sanegre, Rafael and Zhao, Yonggan and Ziemba, William T.}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/s0165-1889(03)00056-3}, 
abstract = {{This paper discusses the allocation of capital over time with several risky assets. The capital growth log utility approach is used with conditions requiring that specific goals are achieved with high probability. The stochastic optimization model uses a disjunctive form for the probabilistic constraints, which identifies an outer problem of choosing an optimal set of scenarios, and an inner (conditional) problem of finding the optimal investment decisions for a given scenarios set. The multiperiod inner problem is composed of a sequence of conditional one period problems. The theory is illustrated for the dynamic allocation of wealth in stocks, bonds and cash equivalents. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {937--954}, 
number = {5}, 
volume = {28}
}
@article{10.1590/s0104-530x2012000400009, 
year = {2012}, 
title = {{Value-at-Risk for Ibovespa: An analysis using long memory models [Value-at-Risk da Carteira do Ibovespa: Uma análise com o uso de modelos de memória longa]}}, 
author = {Gaio, Luiz Eduardo and Júnior, Tabajara Pimenta}, 
journal = {Gestão \&amp; Produção}, 
issn = {0104530X}, 
doi = {10.1590/s0104-530x2012000400009}, 
abstract = {{This study proposes a comparative analysis of the use of ten volatility models to calculate the Value-at-Risk (VaR) for Ibovespa considering the presence of long memory in time series for daily returns. Data from January 02, 2000 to January 02, 2008 were used. The results showed that the long memory models, in especial the FICARCH (1,d1) model, are better to calculate the Value-at-Risk if compared to traditional models, such as the RiskMetrics model.}}, 
pages = {779--792}, 
number = {4}, 
volume = {19}
}
@article{10.1007/s10107-014-0801-1, 
year = {2014}, 
title = {{Random variables, monotone relations, and convex analysis}}, 
author = {Rockafellar, R. T. and Royset, J. O.}, 
journal = {Mathematical Programming}, 
issn = {00255610}, 
doi = {10.1007/s10107-014-0801-1}, 
abstract = {{Random variables can be described by their cumulative distribution functions, a class of nondecreasing functions on the real line. Those functions can in turn be identified, after the possible vertical gaps in their graphs are filled in, with maximal monotone relations. Such relations are known to be the subdifferentials of convex functions. Analysis of these connections yields new insights. The generalized inversion operation between distribution functions and quantile functions corresponds to graphical inversion of monotone relations. In subdifferential terms, it corresponds to passing to conjugate convex functions under the Legendre–Fenchel transform. Among other things, this shows that convergence in distribution for sequences of random variables is equivalent to graphical convergence of the monotone relations and epigraphical convergence of the associated convex functions. Measures of risk that employ quantiles (VaR) and superquantiles (CVaR), either individually or in mixtures, are illuminated in this way. Formulas for their calculation are seen from a perspective that reveals how they were discovered. The approach leads further to developments in which the superquantiles for a given distribution are interpreted as the quantiles for an overlying “superdistribution.” In this way a generalization of Koenker–Basset error is derived which lays a foundation for superquantile regression as a higher-order extension of quantile regression. © 2014, Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society (outside the USA).}}, 
pages = {297--331}, 
number = {1-2}, 
volume = {148}
}
@article{10.1016/j.cam.2006.05.016, 
year = {2007}, 
title = {{Portfolio value at risk based on independent component analysis}}, 
author = {Chen, Ying and Härdle, Wolfgang and Spokoiny, Vladimir}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2006.05.016}, 
abstract = {{Risk management technology applied to high-dimensional portfolios needs simple and fast methods for calculation of value at risk (VaR). The multivariate normal framework provides a simple off-the-shelf methodology but lacks the heavy-tailed distributional properties that are observed in data. A principle component-based method (tied closely to the elliptical structure of the distribution) is therefore expected to be unsatisfactory. Here, we propose and analyze a technology that is based on independent component analysis (ICA). We study the proposed ICVaR methodology in an extensive simulation study and apply it to a high-dimensional portfolio situation. Our analysis yields very accurate VaRs. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {594--607}, 
number = {1}, 
volume = {205}
}
@article{10.1007/978-3-319-49046-5_53, 
year = {2016}, 
title = {{Does Asian credit default swap index improve portfolio performance?}}, 
author = {Khiewngamdee, Chatchai and Yamaka, Woraphon and Sriboonchitta, Songsak}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-49046-5\_53}, 
abstract = {{This study aims to find whether adding Asian Credit Default Swap (CDS) index will improve the portfolio performance. We introduce a new approach namely Markov switching copula approach to estimate the dependence between asset returns and evaluated the risks of the portfolio by using Value at Risk and Expected Shortfall. The empirical results show that the risk level in high dependence regime is less than in low dependence regime. We also find that including Asian CDS index in portfolio clearly increases the portfolio return. © Springer International Publishing AG 2016.}}, 
pages = {624--636}, 
number = {NA}, 
volume = {9978 LNAI}
}
@article{10.18267/j.polek.1134, 
year = {2017}, 
title = {{Examination of market risk estimation models via DEA approach modelling [Posouzení modelů odhadu tržního rizika s využitím dea přístupu]}}, 
author = {Kresta, Aleš and Tichý, Tomáš and Toloo, Mehdi}, 
journal = {Politická ekonomie}, 
issn = {00323233}, 
doi = {10.18267/j.polek.1134}, 
abstract = {{Measuring and managing of financial risks is an essential part of the management of financial institutions. The appropriate risk management should lead to an efficient allocation of available funds. Approaches based on Value at Risk measure have been used as a means for measuring market risk since the late 20th century, although regulators newly suggest to apply more complex method of Expected Shortfall. While evaluating models for market risk estimation based on Value at Risk is relatively simple and involves so-called backtesting procedure, in the case of Expected Shortfall we cannot apply similar procedure. In this article we therefore focus on an alternative method for comprehensive evaluation of VaR models at various significance levels by means of data envelopment analysis (DEA). This approach should lead to the adoption of the model which is also suitable in terms of the Expected Shortfall criterion. Based on the illustrative results from the US stock market we conclude that NIG model and historical simulation should be preferred to normal distribution and GARCH model. We can also recommend to estimate the parameters from the period slightly shorter than two years.}}, 
pages = {161--178}, 
number = {2}, 
volume = {65}
}
@article{10.3923/itj.2010.1104.1114, 
year = {2010}, 
title = {{An earned-value approach to assess and monitor software project uncertainty: A case study in software test execution}}, 
author = {Zhu, Xiaochun and Zhou, Bo}, 
journal = {Information Technology Journal}, 
issn = {18125638}, 
doi = {10.3923/itj.2010.1104.1114}, 
abstract = {{In this study, we proposed an earned-value approach to assess and monitor software uncertainty. It combines the value-at-risk method in financial field and the earned-value-feedback process of earned value project management and proposes a framework which contains value-at-uncertainty system, consumed-value-feedback system and experienced base to support in-process uncertainty measurement and contingency buffer management. Value-at-uncertainty system is used to assess the uncertainty in progress of a project and consumed-value-feedback system is applied to provide the decision support of how to deal with the buffer at that specific time. A case study in software test execution of 24 projects is conducted to evaluate the approach. The study shows how our approach works to measure the uncertainty and manage the buffer size in different project shapes. With the approach, both the accuracy and the effectiveness of uncertainty assessment can be improved along with the test execution progress. © 2010 Asian Network for Scientific Information.}}, 
pages = {1104--1114}, 
number = {6}, 
volume = {9}
}
@article{10.21314/jor.2019.411, 
year = {2019}, 
title = {{Counterparty risk: Credit valuation adjustment variability and value-at-risk}}, 
author = {Breton, Michele and Marzouk, Oussama}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2019.411}, 
abstract = {{The third installment of the Basel Accords advocates a capital charge against credit valuation adjustment (CVA) variability. We propose an efficient numerical approach that allows us to compute risk measures for the CVA process by assessing the distribution of the CVA at a given horizon. This approach relies on a recursive formulation of the CVA, yielding the adjustment as a function of both the time to maturity and the value of the risk factors. Numerical experiments are presented to illustrate the impact of various parameters and assumptions on the CVA distribution. More specifically, we investigate the impact of the constant exposure approximation and show that this assumption significantly affects the tail of the distribution of CVA movements. We also find that distortions between physical and risk-neutral probability measures have practically no impact on the dispersion of the CVA distribution. Finally, we analyze the effect of wrong-way risk and of early exercise opportunities on the evaluation of risk measures. © 2019 Infopro Digital Risk (IP) Limited.}}, 
pages = {1--28}, 
number = {5}, 
volume = {21}
}
@article{10.1007/s10589-008-9178-5, 
year = {2010}, 
title = {{α-Conservative approximation for probabilistically constrained convex programs}}, 
author = {Takano, Yuichi and Gotoh, Jun-ya}, 
journal = {Computational Optimization and Applications}, 
issn = {09266003}, 
doi = {10.1007/s10589-008-9178-5}, 
abstract = {{In this paper, we address an approximate solution of a probabilistically constrained convex program (PCCP), where a convex objective function is minimized over solutions satisfying, with a given probability, convex constraints that are parameterized by random variables. In order to approach to a solution, we set forth a conservative approximation problem by introducing a parameter α which indicates an approximate accuracy, and formulate it as a D.C. optimization problem. As an example of the PCCP, the Value-at-Risk (VaR) minimization is considered under the assumption that the support of the probability of the associated random loss is given by a finitely large number of scenarios. It is advantageous in solving the D.C. optimization that the numbers of variables and constraints are independent of the number of scenarios, and a simplicial branch-and-bound algorithm is posed to find a solution of the D.C. optimization. Numerical experiments demonstrate the following: (i) by adjusting a parameter α, the proposed problem can achieve a smaller VaR than the other convex approximation approaches; (ii) when the number of scenarios is large, a typical 0-1 mixed integer formulation for the VaR minimization cannot be solved in a reasonable time and the improvement of the incumbent values is slow, whereas the proposed method can achieve a good solution at an early stage of the algorithm. © 2008 Springer Science+Business Media, LLC.}}, 
pages = {113--133}, 
number = {1}, 
volume = {46}
}
@article{10.1007/s11156-021-00967-4, 
year = {2021}, 
title = {{Who’s behind the wheel? The role of social and media news in driving the stock–bond correlation}}, 
author = {Alomari, Mohammad and rababa’a, Abdel Razzaq Al and El-Nader, Ghaith and Alkhataybeh, Ahmad}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-021-00967-4}, 
abstract = {{This study investigates the impact of both social and news sentiments indices on the dynamic stock–bond correlation across wavelet-based time-scales over the period 1998–2016. Our results show that the news sentiments namely unemployment, tsunami and sanctions exhibit significant effects during expansion at the shortest time-scale of [2–4] days. These predictors remain significant with reverse signs during recession on the long investment horizon. Yet, the predictability of social media sentiments differs from that of news sentiments with the pattern of reversal in sign also presents for some proxies including windstorm and investment flows. Statistically, our further analysis confirmed the predictability of the sentiments out-of-sample. Excluding the news and social media sentiment effects has also resulted in minimizing the value-at-risk of the (40/60) stock/bond portfolios the most at the investment horizon of [32–64] days during recessions. Our results remain the same after performing some robustness checks. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {959--1007}, 
number = {3}, 
volume = {57}
}
@article{10.1016/j.scitotenv.2015.04.035, 
year = {2016}, 
title = {{'Weather Value at Risk': A uniform approach to describe and compare sectoral income risks from climate change}}, 
author = {Prettenthaler, Franz and Köberl, Judith and Bird, David Neil}, 
journal = {Science of The Total Environment}, 
issn = {00489697}, 
doi = {10.1016/j.scitotenv.2015.04.035}, 
pmid = {25929802}, 
abstract = {{We extend the concept of 'Weather Value at Risk' - initially introduced to measure the economic risks resulting from current weather fluctuations - to describe and compare sectoral income risks from climate change. This is illustrated using the examples of wheat cultivation and summer tourism in (parts of) Sardinia. Based on climate scenario data from four different regional climate models we study the change in the risk of weather-related income losses between some reference (1971-2000) and some future (2041-2070) period. Results from both examples suggest an increase in weather-related risks of income losses due to climate change, which is somewhat more pronounced for summer tourism. Nevertheless, income from wheat cultivation is at much higher risk of weather-related losses than income from summer tourism, both under reference and future climatic conditions. A weather-induced loss of at least 5\% - compared to the income associated with average reference weather conditions - shows a 40\% (80\%) probability of occurrence in the case of wheat cultivation, but only a 0.4\% (16\%) probability of occurrence in the case of summer tourism, given reference (future) climatic conditions. Whereas in the agricultural example increases in the weather-related income risks mainly result from an overall decrease in average wheat yields, the heightened risk in the tourism example stems mostly from a change in the weather-induced variability of tourism incomes. With the extended 'Weather Value at Risk' concept being able to capture both, impacts from changes in the mean and the variability of the climate, it is a powerful tool for presenting and disseminating the results of climate change impact assessments. Due to its flexibility, the concept can be applied to any economic sector and therefore provides a valuable tool for cross-sectoral comparisons of climate change impacts, but also for the assessment of the costs and benefits of adaptation measures. © 2015 Elsevier B.V.}}, 
pages = {1010--1018}, 
number = {NA}, 
volume = {543}
}
@article{10.1016/j.jbankfin.2009.07.003, 
year = {2010}, 
title = {{Diversification and Value-at-Risk}}, 
author = {Pérignon, Christophe and Smith, Daniel R.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2009.07.003}, 
abstract = {{A pervasive and puzzling feature of banks' Value-at-Risk (VaR) is its abnormally high level, which leads to excessive regulatory capital. A possible explanation for the tendency of commercial banks to overstate their VaR is that they incompletely account for the diversification effect among broad risk categories (e.g., equity, interest rate, commodity, credit spread, and foreign exchange). By underestimating the diversification effect, bank's proprietary VaR models produce overly prudent market risk assessments. In this paper, we examine empirically the validity of this hypothesis using actual VaR data from major US commercial banks. In contrast to the VaR diversification hypothesis, we find that US banks show no sign of systematic underestimation of the diversification effect. In particular, diversification effects used by banks is very close to (and quite often larger than) our empirical diversification estimates. A direct implication of this finding is that individual VaRs for each broad risk category, just like aggregate VaRs, are biased risk assessments. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {55--66}, 
number = {1}, 
volume = {34}
}
@article{10.1109/cso.2009.264, 
year = {2009}, 
title = {{Empirical evaluation of the hedge risk of stock index futures based on hushen 300 simulation}}, 
author = {Sun, Yu}, 
journal = {2009 International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2009.264}, 
abstract = {{Hedging with stock index futures brings the hedge risk due to the existence of the basis. In order to measure the hedge risk dynamically, the hedge risk of stock index futures is defined as the VaR (Value at Risk) of the hedge return and measured by the variance-covariance method based on time series analysis method. The Normal distribution, Student-t distribution and GED (Generalize Error Distribution) are utilized to show the fat-tail and heteroscedasticity features of the hedge return. The empirical VaR evaluation of the hedge risk of hedging Hushen 300 Index with Hushen 300 Simulating Stock Index Future shows that the VaR model based on t-GARCH(1,1) gives the most accurate evaluation. Thus the problem how to measure the hedge risk dynamically and accurately is solved. © 2009 IEEE.}}, 
pages = {623--625}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.jfs.2016.02.002, 
year = {2016}, 
title = {{Model risk of risk models}}, 
author = {Danielsson, Jon and James, Kevin R. and Valenzuela, Marcela and Zer, Ilknur}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2016.02.002}, 
abstract = {{This paper evaluates the model risk of models used for forecasting systemic and market risk. Model risk, which is the potential for different models to provide inconsistent outcomes, is shown to be increasing with market uncertainty. During calm periods, the underlying risk forecast models produce similar risk readings; hence, model risk is typically negligible. However, the disagreement between the various candidate models increases significantly during market distress, further frustrating the reliability of risk readings. Finally, particular conclusions on the underlying reasons for the high model risk and the implications for practitioners and policy makers are discussed. © 2016 Elsevier B.V.}}, 
pages = {79--91}, 
number = {NA}, 
volume = {23}
}
@article{10.1007/s10687-007-0050-2, 
year = {2008}, 
title = {{Serial dependence in ARCH-models as measured by tail dependence coefficients}}, 
author = {Brummelhuis, Raymond}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-007-0050-2}, 
abstract = {{Serial dependence in non-linear time series cannot always be reliably quantified using linear autocorrelation. We do a detailed study of serial dependence in an ARCH(1) process from the point of view of the lower tail dependence coefficient and certain generalisations thereof. Our results are relevant for estimating probabilities of consecutive value-at-risk violations in GARCH models. © 2007 Springer Science+Business Media, LLC.}}, 
pages = {167--201}, 
number = {2}, 
volume = {11}
}
@article{10.1080/03610918.2011.560731, 
year = {2011}, 
title = {{Univariate and multivariate value-at-risk: Application and implication in energy markets}}, 
author = {Cheong, Chin Wen}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2011.560731}, 
abstract = {{This study investigated the cross-markets price changes, volatility, and shock transmission mechanism among gasoline, crude oil, and diesel spot markets. An asymmetric time-varying volatility model is used to reveal the hidden dynamic shock transmission mechanism among the markets. An iterative optimization Newton-Raphson algorithm is used in the nonlinear estimation procedures by updating the outer product of the gradient vector. The estimated results are used in quantifying the cross-market risk, optimal portfolio holding, and hedging among the energy markets. Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {957--977}, 
number = {7}, 
volume = {40}
}
@article{10.1109/tpwrs.2015.2463725, 
year = {2016}, 
title = {{Two-Stage Multi-Objective Unit Commitment Optimization under Hybrid Uncertainties}}, 
author = {Wang, Bo and Wang, Shuming and Zhou, Xian-zhong and Watada, Junzo}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2015.2463725}, 
abstract = {{Unit commitment, as one of the most important control processes in power systems, has been studied extensively in the past decades. Usually, the goal of unit commitment is to reduce as much production cost as possible while guaranteeing the power supply operated with a high reliability. However, system operators encounter increasing difficulties to achieve an optimal scheduling due to the challenges in coping with uncertainties that exist in both supply and demand sides. This study develops a day-ahead two-stage multi-objective unit commitment model which optimizes both the supply reliability and the total cost with environmental concerns of thermal generation systems. To tackle the manifold uncertainties of unit commitment in a more comprehensive and realistic manner, stochastic and fuzzy set theories are utilized simultaneously, and a unified reliability measurement is then introduced to evaluate the system reliability under the uncertainties of both sudden unit outage and unforeseen load fluctuation. In addition, a cumulative probabilistic method is proposed to address the spinning reserve optimization during the scheduling. To solve this complicated model, a multi-objective particle swarm optimization algorithm is developed. Finally, a series of experiments were performed to demonstrate the effectiveness of this research; we also justify its feasibility on test systems with generation uncertainty. © 2015 IEEE.}}, 
pages = {2266--2277}, 
number = {3}, 
volume = {31}
}
@article{10.1016/j.jclepro.2018.10.196, 
year = {2019}, 
title = {{A screening methodology for building multiple energy retrofit measures package considering economic and risk aspects}}, 
author = {Zheng, Donglin and Yu, Lijun and Wang, Lizhen and Tao, Jiangang}, 
journal = {Journal of Cleaner Production}, 
issn = {09596526}, 
doi = {10.1016/j.jclepro.2018.10.196}, 
abstract = {{In recent years, more and more existing building has an ever increasing requirement of retrofit, which has been developed from individual measure to combination package of multiple measures. It has become the challenging task for screening building multiple energy retrofit measures package (BMERMP). In this paper, a stepwise screening methodology for BMERMP is proposed considering economic and risk aspects, which integrates Life-Cycle Cost (LCC) and Monte Carlo (MC) analysis. First of all, this paper puts forward a calculation reference table for energy saving retrofit potential. In addition, this paper also uses unit energy saving incremental cost (UESIC) to screen technology and suggests economic internal rate of return（EIRR）as cost-benefit analysis index of BMERMP. Furthermore, the authors propose the concept of Value-at-Risk (VaR) as an effective measure to control the risk. The VaR for EIRR is determined by risk variation and can be obtained through MC simulation. Meanwhile, the study takes a commercial building for example, this building comprises 19 measures in initial BMERMP by energy diagnosis and the VaR of initial BMERMP is 97.6\%, which has a high risk. According to the ranking of UESIC value, when 4 high cost technologies are reduced, the VaR can be reduced to 0.07\%, then the economy of BMERMP is feasible and the risk is controllable. This paper provides a new perspective for the decision maker to select BMERMP. © 2018 Elsevier Ltd}}, 
pages = {1587--1602}, 
number = {NA}, 
volume = {208}
}
@article{10.1142/9789811222634_0024, 
year = {2020}, 
title = {{Econometric Tools for Stress Testing Using Time Heterogeneity and Maximum Entropy}}, 
author = {Guerard, John B and Ziemba, William T and Ziemba, William T and Lleo, S and Zhitlukhin, M}, 
journal = {World Scientific Handbook in Financial Economics Series}, 
issn = {20101732}, 
doi = {10.1142/9789811222634\_0024}, 
abstract = {{Stress testing of economic policies, regulations, financial risk evaluations, and statistical inference have lately become a "requirement."This chapter highlights extant econometric tools for stress testing with an emphasis on: (a) maximum entropy bootstraps, recently implemented in a new version of the R software package called "meboot,"and (b) creation of stress scenarios by considering time heterogeneous nonstationary time series. We use published simulation designs of other authors to report the superiority of meboot over the moving block bootstrap (mbb) and the blocking external bootstrap (BEB) in the context of many types of time-heterogeneity. For illustration, we apply meboot tools to stress test inference regarding Granger-causality between asset prices and world savings rates, and also to the "Value at Risk"used in finance. We indicate potential uses in stress testing of banks. © 2020 World Scientific Publishing Co. Pte. Ltd.}}, 
pages = {635--659}, 
number = {NA}, 
volume = {9}
}
@article{10.5267/j.ijiec.2012.10.002, 
year = {2013}, 
title = {{Determination of the optimal investment portfolio using CAPM in Tehran Stock Exchange industries: A VAR-Multivariate GARCH approach}}, 
author = {Hosseini, Seyed Ahmad and Moradifard, Ahmad and Sabzzadeh, Kobra}, 
journal = {International Journal of Industrial Engineering Computations}, 
issn = {19232926}, 
doi = {10.5267/j.ijiec.2012.10.002}, 
abstract = {{This study determines the optimal investment portfolio in Tehran Stock Exchange (TSE) industries. For this purpose, a conditional capital asset pricing model (CAPM) with time-varying covariance, according to a Multivariate GARCH approach has been formulated. According to this conditional CAPM, the conditional variance-covariance matrix and mean of returns are calculated for some industries. By using the Mean-Value at Risk portfolio selection model, the optimum proportion is detected. Results showed that the Pharmaceutical Industry, Financial Group and Cement Industry have the most quotas in portfolio since they maintain the minimum variance and maximum return among all other industries. © 2012 Growing Science Ltd.}}, 
pages = {155--164}, 
number = {1}, 
volume = {4}
}
@article{10.1109/ams.2009.133, 
year = {2009}, 
title = {{Dynamically weighted continuous ant colony optimization for biobjective portfolio selection using value-at-risk}}, 
author = {Khalidji, Mojtaba and Zeiaee, Mohammad and Taei, Ali and Jahed-Motlagh, Mohammad Reza and Khaloozadeh, Hamid}, 
journal = {2009 Third Asia International Conference on Modelling \& Simulation}, 
issn = {NA}, 
doi = {10.1109/ams.2009.133}, 
abstract = {{An adaptation of Ant Colony for Continuous Domains (ACOR) to bi-objective optimization problems is proposed and used to solve the optimal portfolio selection problem in Markowitz's risk-return framework. The utilized risk measure is Value-at-Risk (VaR). In adapting ACOR to bi-objective optimization, a dynamically weighted aggregation of objective values by a normalized Tchebychev norm is used to obtain a set of non-dominated Pareto optimal solutions to the problem. The proposed method (DW-ACOR) is tested on a set of past return data of 12 assets on Tehran Stock Exchange (TSE). Historical Simulation (HS) is utilized to obtain an estimate of the VaR. In order to compare the performance of DW-ACOR with a successful multi-objective evolutionary algorithm (MOEA), NSGA-II is also used to solve the same portfolio selection problem. A comparison of the obtained results, shows that the proposed method offers high quality solutions and a wide range of risk-return trade-offs. While NSGA-II obtains a set of somewhat more widely spread solutions, the quality of the solutions obtained by DW-ACOR is higher as they are closer to the true Pareto front of the problem. © 2009 IEEE.}}, 
pages = {230--235}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/02286203.2016.1199749, 
year = {2016}, 
title = {{Auction and contracting mechanisms for channel coordination with exponential demand functions}}, 
author = {Guo, Wei and Ma, Cheng}, 
journal = {International Journal of Modelling and Simulation}, 
issn = {02286203}, 
doi = {10.1080/02286203.2016.1199749}, 
abstract = {{This paper considers a supply chain system with two competing suppliers providing identical products or service to a retailer incorporating risk attitudes. The risk attitude is quantified using the value-at-risk measure and the retailer faces a stochastic exponential price-dependent demand function. Several simple situation and auction mechanisms are developed and compared. The results show that, for auction and two-part contracting mechanisms, the retailer and the low-cost supplier are cooperating together to maximize the total profit of the supply chain with complete information and asymmetric information; however, in this simple situation without complicated auction, the channel-coordinated system optimums cannot be achieved. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {87--96}, 
number = {3}, 
volume = {36}
}
@article{10.1016/j.ecosta.2019.08.002, 
year = {2020}, 
title = {{Asymmetric stochastic volatility models: Properties and particle filter-based simulated maximum likelihood estimation}}, 
author = {Mao, Xiuping and Czellar, Veronika and Ruiz, Esther and Veiga, Helena}, 
journal = {Econometrics and Statistics}, 
issn = {24523062}, 
doi = {10.1016/j.ecosta.2019.08.002}, 
abstract = {{The statistical properties of a general family of asymmetric stochastic volatility (A-SV) models which capture the leverage effect in financial returns are derived providing analytical expressions of moments and autocorrelations of power-transformed absolute returns. The parameters of the A-SV model are estimated by a particle filter-based simulated maximum likelihood estimator and Monte Carlo simulations are carried out to validate it. It is shown empirically that standard SV models may significantly underestimate the value-at-risk of weekly S\&P 500 returns at dates following negative returns and overestimate it after positive returns. By contrast, the general specification proposed provide reliable forecasts at all dates. Furthermore, based on daily S\&P 500 returns, it is shown that the most adequate specification of the asymmetry can change over time. © 2019 EcoSta Econometrics and Statistics}}, 
pages = {84--105}, 
number = {NA}, 
volume = {13}
}
@article{10.1515/snde-2014-0044, 
year = {2015}, 
title = {{A triple-threshold leverage stochastic volatility model}}, 
author = {Wu, Xin-Yu and Zhou, Hai-Lin}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2014-0044}, 
abstract = {{In this paper we introduce a triple-threshold leverage stochastic volatility (TTLSV) model for financial return time series. The main feature of the model is to allow asymmetries in the leverage effect as well as mean and volatility. In the model the asymmetric effect is modeled by a threshold nonlinear structure that the two regimes are determined by the sign of the past returns. The model parameters are estimated using maximum likelihood (ML) method based on the efficient importance sampling (EIS) technique. Monte Carlo simulations are presented to examine the accuracy and finite sample properties of the proposed methodology. The EIS-based ML (EIS-ML) method shows good performance according to the Monte Carlo results. The proposed model and methodology are applied to two stock market indices for China. Strong evidence of the mean and volatility asymmetries is detected in Chinese stock market. Moreover, asymmetries in the volatility persistence and leverage effect are also discovered. The log-likelihood and Akaike information criterion (AIC) suggest evidence in favor of the proposed model. In addition, model diagnostics suggest that the proposed model performs relatively well in capturing the key features of the data. Finally, we compare models in a Value at Risk (VaR) study. The results show that the proposed model can yield more accurate VaR estimates than the alternatives. © 2015 by De Gruyter.}}, 
pages = {483--500}, 
number = {4}, 
volume = {19}
}
@article{10.1016/s0167-6687(99)00062-1, 
year = {2000}, 
title = {{Equity allocation and portfolio selection in insurance}}, 
author = {Taflin, Erik}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/s0167-6687(99)00062-1}, 
abstract = {{A discrete time probabilistic model, for optimal equity allocation and portfolio selection, is formulated so as to apply to (at least) reinsurance. In the context of a company with several portfolios (or subsidiaries), representing both liabilities and assets, it is proved that the model has solutions respecting constraints on ROEs, ruin probabilities and market shares currently in practical use. Solutions define global and optimal risk management strategies of the company. Mathematical existence results and tools, such as the inversion of the linear part of the Euler-Lagrange equations, developed in a preceding paper in the context of a simplified model are essential for the mathematical and numerical construction of solutions of the model. © 2000 Elsevier Science B.V.}}, 
pages = {65--81}, 
number = {1}, 
volume = {27}
}
@article{10.1016/j.irfa.2011.02.009, 
year = {2011}, 
title = {{Value-at-risk for long and short trading positions: Evidence from developed and emerging equity markets}}, 
author = {Diamandis, Panayiotis F. and Drakos, Anastassios A. and Kouretas, Georgios P. and Zarangas, Leonidas}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2011.02.009}, 
abstract = {{The financial crisis of 2007-2009 has questioned the provisions of Basel II agreement on capital adequacy requirements and the appropriateness of VaR measurement. This paper reconsiders the use of Value-at-risk as a measure for potential risk of economic losses in financial markets by estimating VaR for daily stock returns with the application of various parametric univariate models that belong to the class of ARCH models which are based on the skewed Student distribution. We used daily data for three groups of stock market indices, namely Developed, Southeast Asia and Latin America. The data covered the period 1987-2009. We conducted our analysis with the adoption of the methodology suggested by Giot and Laurent (2003). Therefore, we estimated an APARCH model based on the skewed Student distribution to fully take into account the fat left and right tails of the returns distribution. The main finding of our analysis is that the skewed Student APARCH improves considerably the forecasts of one-day-ahead VaR for long and short trading positions. Additionally, we evaluate the performance of each model with the calculation of Kupiec's (1995) Likelihood Ratio test on the empirical failure test. Moreover, for the case of the skewed Student APARCH model we computed the expected shortfall and the average multiple of tail event to risk measure. These two measures helped us to further assess the information we obtained from the estimation of the empirical failure rates. © 2011 Elsevier Inc.}}, 
pages = {165--176}, 
number = {3}, 
volume = {20}
}
@article{10.1016/j.csda.2010.06.018, 
year = {2012}, 
title = {{Bayesian Value-at-Risk and expected shortfall forecasting via the asymmetric Laplace distribution}}, 
author = {Chen, Qian and Gerlach, Richard and Lu, Zudi}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2010.06.018}, 
abstract = {{A parametric approach to estimating and forecasting Value-at-Risk (VaR) and expected shortfall (ES) for a heteroscedastic financial return series is proposed. The well-known GJR-GARCH form models the volatility process, capturing the leverage effect. To capture potential skewness and heavy tails, the model assumes an asymmetric Laplace form as the conditional distribution of the series. Furthermore, dynamics in higher moments are modeled by allowing the shape parameter in this distribution to be time-varying. Estimation is via an adaptive Markov chain Monte Carlo (MCMC) sampling scheme, employing the Metropolis-Hastings (MH) algorithm with a mixture of Gaussian proposal distributions. A simulation study highlights accurate estimation and improved inference compared to a single-Gaussian-proposal MH method. The model is illustrated by applying it to four international stock market indices and two exchange rates, generating one-step-ahead forecasts of VaR and ES. Standard and non-standard tests are applied to these forecasts, and the finding is that the proposed model performs favourably compared to some popular competitors: in particular it is the only conservative model of risk over the period studied, which includes the recent global financial crisis. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {3498--3516}, 
number = {11}, 
volume = {56}
}
@article{10.1016/j.csda.2012.03.016, 
year = {2012}, 
title = {{Predicting extreme value at risk: Nonparametric quantile regression with refinements from extreme value theory}}, 
author = {Schaumburg, Julia}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2012.03.016}, 
abstract = {{A framework is introduced allowing us to apply nonparametric quantile regression to Value at Risk (VaR) prediction at any probability level of interest. A monotonized double kernel local linear estimator is used to estimate moderate (1\%) conditional quantiles of index return distributions. For extreme (0.1\%) quantiles, nonparametric quantile regression is combined with extreme value theory. The abilities of the proposed estimators to capture market risk are investigated in a VaR prediction study with empirical and simulated data. Possibly due to its flexibility, the out-of-sample forecasting performance of the new model turns out to be superior to competing models. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {4081--4096}, 
number = {12}, 
volume = {56}
}
@article{10.1109/iciii.2009.329, 
year = {2009}, 
title = {{Measurement of HIS stock index futures market risk based on value-at-risk}}, 
author = {Dan, Yan and Zhiyong, Gong}, 
journal = {2009 International Conference on Information Management, Innovation Management and Industrial Engineering}, 
issn = {NA}, 
doi = {10.1109/iciii.2009.329}, 
abstract = {{This paper examines the forecasting of Value-at-Risk model. We explore and compare two different possible sources of performance improvement: asymmetry in the conditional variance and fat-tailed distributions. The HIS stock index futures are studied using daily data. Our result suggest that for asset returns which exhibit fatter and volatility clustering, like the HIS stock index futures, the VaR values produced by the normal APARCH model are preferred at lower confidence level. © 2009 IEEE.}}, 
pages = {78--81}, 
number = {NA}, 
volume = {3}
}
@article{10.1109/icicisys.2009.5357712, 
year = {2009}, 
title = {{Value at risk estimation based on generalized quantile regression}}, 
author = {Wang, Yongqiao}, 
journal = {2009 IEEE International Conference on Intelligent Computing and Intelligent Systems}, 
issn = {NA}, 
doi = {10.1109/icicisys.2009.5357712}, 
abstract = {{The paper proposes a novel Value-at-Risk measurement method based on kernel quantile regression. The method can build linear quantile regression in a reproduced Hilbert kernel space. It makes no assumption on the dependence between quantile functions and the predictors and achieves nonlinear capabilities. In the experiment on daily returns of crude oil, we compare its capability with other four conventional methods: simple moving average, exponential weighted moving average, GARCH and linear quantile regression. The out-of-sample results clearly show that the new method has superiority over other four methods. ©2009 IEEE.}}, 
pages = {674--678}, 
number = {NA}, 
volume = {1}
}
@article{10.1504/ijram.2009.022198, 
year = {2009}, 
title = {{Autoregressive conditional moments in VaR estimate with Gram-Charlier and Cornish-Fisher expansions}}, 
author = {Russo, Vincenzo}, 
journal = {International Journal of Risk Assessment and Management}, 
issn = {14668297}, 
doi = {10.1504/ijram.2009.022198}, 
abstract = {{This paper proposes a model to compute value at risk by means of Gram-Charlier and Cornish-Fisher expansions. In this model VaR is a function of conditional mean, volatility, skewness and kurtosis where all the moments are computed via a time-varying dynamic. An appropriate form of Gram-Charlier density is used to estimate the parameters of the equations proposed for the first four autoregressive conditional moments. Since skewness and kurtosis appear directly as parameters in the functional form of the density, it is possible to estimate simply the third and fourth moments with the maximum likelihood method. By interpreting the VaR as the quantile of future asset values conditional on current information, a Cornish-Fisher expansion is used to compute VaR as a function of the first four conditional moments that appear directly in the VaR formula. The main goal of the present analysis is to confirm some stylised facts of financial data such as volatility clustering, asymmetry and fat-tails. An evaluation of the predictive performance of four conditional moments in VaR computation context is provided in the last part of the paper. Copyright © 2009 Inderscience Enterprises Ltd.}}, 
pages = {67}, 
number = {1-2}, 
volume = {11}
}
@article{10.1108/ijoem-02-2020-0169, 
year = {2020}, 
title = {{The dependence structure and portfolio risk of Malaysia's foreign exchange rates: the Bayesian GARCH–EVT–copula model}}, 
author = {Yeap, Xiu Wei and Lean, Hooi Hooi and Sampid, Marius Galabe and Hasim, Haslifah Mohamad}, 
journal = {International Journal of Emerging Markets}, 
issn = {17468809}, 
doi = {10.1108/ijoem-02-2020-0169}, 
abstract = {{Purpose: This paper investigates the dependence structure and market risk of the currency exchange rate portfolio from the Malaysian ringgit perspective. Design/methodology/approach: The marginal return of the five major exchange rates series, i.e. United States dollar (USD), Japanese yen (JPY), Singapore dollar (SGD), Thai baht (THB) and Chinese Yuan Renminbi (CNY) are modelled by the Bayesian generalized autoregressive conditional heteroskedasticity (GARCH) (1,1) model with Student's t innovations. In addition, five different copulas, such as Gumbel, Clayton, Frank, Gaussian and Student's t, are applied for modelling the joint distribution for examining the dependence structure of the five currencies. Moreover, the portfolio risk is measured by Value at Risk (VaR) that considers the extreme events through the extreme value theory (EVT). Findings: The finding shows that Gumbel and Student's t are the best-fitted Archimedean and elliptical copulas, for the five currencies. The dependence structure is asymmetric and heavy tailed. Research limitations/implications: The findings of this paper have important implications for diversification decision and hedging problems for investors who involving in foreign currencies. The authors found that the portfolio is diversified with the consideration of extreme events. Therefore, investors who are holding an individual currency with VaR higher than the portfolio may consider adding other currencies used in this paper for hedging. Originality/value: This is the first paper estimating VaR of a currency exchange rate portfolio using a combination of Bayesian GARCH model, EVT and copula theory. Moreover, the VaR of the currency exchange rate portfolio can be used as a benchmark of the currency exchange market risk. © 2020, Emerald Publishing Limited.}}, 
pages = {952--974}, 
number = {5}, 
volume = {16}
}
@article{10.23919/eusipco.2017.8081378, 
year = {2017}, 
title = {{A multimodal asymmetric exponential power distribution: Application to risk measurement for financial high-frequency data}}, 
author = {Thibault, Aymeric and Bondon, Pascal}, 
journal = {2017 25th European Signal Processing Conference (EUSIPCO)}, 
issn = {NA}, 
doi = {10.23919/eusipco.2017.8081378}, 
abstract = {{Interest in risk measurement for high-frequency data has increased since the volume of high-frequency trading stepped up over the two last decades. This paper proposes a multimodal extension of the Exponential Power Distribution (EPD), called the Multimodal Asymmetric Exponential Power Distribution (MAEPD). We derive moments and we propose a convenient stochastic representation of the MAEPD. We establish consistency, asymptotic normality and efficiency of the maximum likelihood estimators (MLE). An application to risk measurement for high-frequency data is presented. An autoregressive moving average multiplicative component generalized autoregressive conditional heteroskedastic (ARMA-mcsGARCH) model is fitted to Financial Times Stock Exchange (FTSE) 100 intraday returns. Performances for Value-at-Risk (VaR) and Expected Shortfall (ES) estimation are evaluated. We show that the MAEPD outperforms commonly used distributions in risk measurement. © 2017 EURASIP.}}, 
pages = {1100--1104}, 
number = {NA}, 
volume = {2017-January}
}
@article{10.1016/j.insmatheco.2007.01.006, 
year = {2008}, 
title = {{On the distribution tail of an integrated risk model: A numerical approach}}, 
author = {Brokate, M. and Klüppelberg, C. and Kostadinova, R. and Maller, R. and Seydel, R.C.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2007.01.006}, 
abstract = {{We consider an insurance risk process with the possibility to invest the capital reserve into a portfolio consisting of a risky asset and a riskless asset. The stock price is modelled by an exponential Lévy process and the riskless interest rate is assumed to be constant. We aim at the risk assessment of the integrated risk process in terms of a high quantile or the far out distribution tail. We indicate an application to an optimal investment strategy of an insurer. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {101--106}, 
number = {1}, 
volume = {42}
}
@article{10.1080/14697688.2013.795675, 
year = {2013}, 
title = {{Relative forecasting performance of volatility models: Monte Carlo evidence}}, 
author = {Lux, Thomas and Morales-Arias, Leonardo}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2013.795675}, 
abstract = {{A Monte Carlo (MC) experiment is conducted to study the forecasting performance of a variety of volatility models under alternative data-generating processes (DGPs). The models included in the MC study are the (Fractionally Integrated) Generalized Autoregressive Conditional Heteroskedasticity models ((FI)GARCH), the Stochastic Volatility model (SV), the Long Memory Stochastic Volatility model (LMSV) and the Markov-switching Multifractal model (MSM). The MC study enables us to compare the relative forecasting performance of the models accounting for different characterizations of the latent volatility process: specifications that incorporate short/long memory, autoregressive components, stochastic shocks, Markov-switching and multifractality. Forecasts are evaluated by means of mean squared errors (MSE), mean absolute errors (MAE) and value-at-risk (VaR) diagnostics. Furthermore, complementarities between models are explored via forecast combinations. The results show that (i) the MSM model best forecasts volatility under any other alternative characterization of the latent volatility process and (ii) forecast combinations provide systematic improvements upon most single misspecified models, but are typically inferior to the MSM model even if the latter is applied to data governed by other processes. © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1--20}, 
number = {9}, 
volume = {13}
}
@article{10.1016/s0927-5398(00)00011-6, 
year = {2000}, 
title = {{Sensitivity analysis of Values at Risk}}, 
author = {Gourieroux, C. and Laurent, J.P. and Scaillet, O.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(00)00011-6}, 
abstract = {{The aim of this paper is to analyze the sensitivity of Value at Risk (VaR) with respect to portfolio allocation. We derive analytical expressions for the first and second derivatives of the VaR, and explain how they can be used to simplify statistical inference and to perform a local analysis of the VaR. An empirical illustration of such an analysis is given for a portfolio of French stocks. © 2000 Elsevier Science B.V.}}, 
pages = {225--245}, 
number = {3-4}, 
volume = {7}
}
@article{10.1108/s0276-897620200000020002, 
year = {2020}, 
title = {{Multiobjective newsvendor models with cvar for flower industry}}, 
author = {Sawik, Bartosz}, 
journal = {Applications of Management Science}, 
issn = {02768976}, 
doi = {10.1108/s0276-897620200000020002}, 
abstract = {{The newsvendor problem is fundamental to many operations management models. The problem focuses on the trade-off between the gains from satisfying demand and losses from unsold products. The newsvendor model and its extensions have been applied to various areas, such as production plan and supply chain management. This chapter examines the study about newsvendor problem. In this research, there is a review of the contributions for the multiproduct newsvendor problem. It focuses on the current literature concerning the mathematical models and the solution methods for the multiitem newsvendor problems with single or multiple constraints, as well as with the risks. The objective of this research is to go over the newsvendor problem and bring into comparison different newsvendor models applied to the flower industry. A few case studies are described addressing topics related to the newsvendor problem such as discounting and replenishment policies, inventory inaccuracies, or demand estimation. Three newsvendor models are put into practice in the field of flower selling. A full database of the flowers sold by an anonymous retailer is available for the study. Computational experiments for practical example have been conducted with use of the CPLEX solver with AMPL programming language. Models are solved, and an analysis of different circumstances and cases is accomplished. © 2020 Emerald Publishing Limited.}}, 
pages = {3--30}, 
number = {NA}, 
volume = {20}
}
@article{10.1016/j.ribaf.2012.09.001, 
year = {2013}, 
title = {{Modeling CAC40 volatility using ultra-high frequency data}}, 
author = {Degiannakis, Stavros and Floros, Christos}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2012.09.001}, 
abstract = {{Autoregressive fractionally integrated moving average (ARFIMA) and heterogeneous autoregressive (HAR) models are estimated and their ability to predict the one-trading-day-ahead CAC40 realized volatility is investigated. In particular, this paper follows three steps: (i) The optimal sampling frequency for constructing the CAC40 realized volatility is examined based on the volatility signature plot. Moreover, the realized volatility is adjusted to the information that flows into the market when it is closed. (ii) We forecast the one-day-ahead realized volatility using the ARFIMA and the HAR models. (iii) The accuracy of the realized volatility forecasts is investigated under the superior predictive ability framework. According to the predicted mean squared error, a simple ARFIMA model provides accurate one-trading day-ahead forecasts of CAC40 realized volatility. The evaluation of model's predictability illustrates that the ARFIMA(1, d', 0) forecasts of realized volatility (i) are statistically superior compared to its competing models and (ii) provide adequate one-trading-day-ahead Value-at-Risk forecasts. © 2012 Elsevier B.V.}}, 
pages = {68--81}, 
number = {1}, 
volume = {28}
}
@article{10.4028/www.scientific.net/amr.217-218.1293, 
year = {2011}, 
title = {{Intelligent simulation method and its application on risk analysis}}, 
author = {Zheng, Hua and Xie, Li and Zhang, Li Zi}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.217-218.1293}, 
abstract = {{This article describes an intelligent simulation method for measuring price risk, which is still one of the focused and unsolved problems for various risk managements and need to be studied profoundly. To solve this problem, electricity price risk measured in terms of Value at Risk is proposed by intelligent simulation. Here the electricity price data under various market scenarios are produced by intelligent simulation model based on fuzzy neural network (FNN). After that, the quantitative model for price risk analysis is achieved by finding the estimated probability distribution, where price VAR is determined, according to parameter set, i.e. confidence level. The proposed method is more realistic and effective than traditional variance approach, which can assess the potential loss of electricity price over some period of time.}}, 
pages = {1293--1296}, 
number = {NA}, 
volume = {217-218}
}
@article{10.1049/cp.2009.1036, 
year = {2009}, 
title = {{Risk assessment in distribution network investment evaluation: Experience with var methodology}}, 
author = {Messias, R and Carvalho, P and Santos, C}, 
journal = {IET Conference Publications}, 
issn = {NA}, 
doi = {10.1049/cp.2009.1036}, 
abstract = {{This paper addresses the subject matter of the evaluation of distribution network investment plans. Typically the distribution network operators (DNO) evaluate investment plans based on the discounted cash flow (DCF) methodology that establishes the net present value (NPV) and internal rate of return (IRR). In the NPV evaluation, the costs and benefits of a plan have load growth rate and investment costs as primitives. Load growth determines both the evolution of losses and the evolution of network capacity adequacy. Today it is harder to obtain a good forecast on load growth, with the ins an outs of distributed generation, as well a correct estimation of the investment costs, with the increased influence of local authorities to modify planned solutions. In this context, DNOs are increasingly interested in evaluating the risk associated with the variations of both NPV primitives. This paper proposes an evaluation methodology based on the value at risk (VAR) to obtain the worth expected value within a predefined time period and confidence level. The methodology is presented together with an application example that takes into account the DNO's historic error deviations both in load growth and costs.}}, 
pages = {868--868}, 
number = {550 CP}, 
volume = {NA}
}
@article{10.1016/j.trc.2016.10.001, 
year = {2016}, 
title = {{Battery capacity design for electric vehicles considering the diversity of daily vehicles miles traveled}}, 
author = {Li, Zhiheng and Jiang, Shan and Dong, Jing and Wang, Shoufeng and Ming, Zhennan and Li, Li}, 
journal = {Transportation Research Part C: Emerging Technologies}, 
issn = {0968090X}, 
doi = {10.1016/j.trc.2016.10.001}, 
abstract = {{In this paper, we study battery capacity design for battery electric vehicles (BEVs). The core of such design problems is to find a good tradeoff between minimizing the capacity to reduce financial costs of drivers and increasing the capacity to satisfy daily travel demands. The major difficulty of such design problems lies in modeling the diversity of daily travel demands. Based on massive trip records of taxi drivers in Beijing, we find that the daily vehicle miles traveled (DVMT) of a driver (e.g., a taxi driver) may change significantly in different days. This investigation triggers us to propose a mixture distribution model to describe the diversity in DVMT for various driver in different days, rather than the widely employed single distribution model. To demonstrate the merit of this new model, we consider value-at-risk and mean-variance battery capacity design problems for BEV, with respect to conventional single and new mixture distribution models of DVMT. Testing results indicate that the mixture distribution model better leads to better solutions to satisfy various drivers. © 2016 Elsevier Ltd}}, 
pages = {272--282}, 
number = {NA}, 
volume = {72}
}
@article{10.1007/s42519-019-0054-7, 
year = {2019}, 
title = {{Skewed Kotz Distribution with Application to Financial Stock Returns}}, 
author = {Bellahnid, Abdellatif and Sarr, Amadou}, 
journal = {Journal of Statistical Theory and Practice}, 
issn = {15598608}, 
doi = {10.1007/s42519-019-0054-7}, 
abstract = {{This paper introduced a five-parameter skewed Kotz (SK) distribution that may be viewed as a generalized skewed T distribution. Its mathematical properties are investigated, and parameters are estimated using the maximum likelihood method. The usefulness of this new distribution has been illustrated by deriving explicit formulae for the value-at-risk (VaR) and the average value-at-risk (AVaR). The obtained results are clearly generalizations of those that were established earlier by Dokov et al. (J Appl Funct Anal 3(1):189–208, 2008). On the other hand, simulation studies have been conducted and showed the accuracy of the VaR and AVaR computations. Furthermore, an application on financial returns of the Universal Health Services stock provided evidence that the SK distribution better fits the empirical distribution than both normal and skewed T distributions. The empirical study revealed the suitability of the SK distribution, specially for modelling data that fall within a small range, with a high excess kurtosis. © 2019, Grace Scientific Publishing.}}, 
pages = {53}, 
number = {4}, 
volume = {13}
}
@article{10.1080/23737484.2020.1805647, 
year = {2021}, 
title = {{Tail risk analysis, evolving efficiency and relative predictability of the African stock markets}}, 
author = {Okorie, Idika E. and Nadarajah, Saralees and Ohakwe, Johnson and Onyemachi, Chris U.}, 
journal = {Communications in Statistics: Case Studies, Data Analysis and Applications}, 
issn = {23737484}, 
doi = {10.1080/23737484.2020.1805647}, 
abstract = {{In this article, we use extreme value analysis to investigate the tail risk behavior of 12 emerging African stock markets using three developed stock markets as benchmarks. Our results indicate that many of the African stock markets like South Africa, Zambia, Côte d’Ivoire, Egypt, Namibia, and Uganda are riskier and can yield higher expected returns than the major developed stock markets in Asia, Europe, and America. But, the expected returns for Kenya can sometimes be equivalent to those of the developed stock markets. The stock markets belonging to Botswana, Mauritius, Morocco, Nigeria, Kenya, and Tunisia are mainly characterized by lower expected returns and lower loss compared to the major developed stock markets. We implemented the generalized spectral test with a rolling window to investigate the adaptive predictability of the African stock markets. Our results confirmed that the South African stock market is the least predictable stock market in Africa while the Nigerian stock market is the most predictable stock market in Africa. © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--21}, 
number = {1}, 
volume = {7}
}
@article{10.1007/978-1-84628-935-4_78, 
year = {2007}, 
title = {{Machine life cycle cost estimation via Monte-Carlo simulation}}, 
author = {Fleischer, Jürgen and Wawerla, Marc and Niggeschmidt, Stephan}, 
issn = {NA}, 
doi = {10.1007/978-1-84628-935-4\_78}, 
abstract = {{Recently an increasing number of customers demands more extensive warranties from the machine building industry. In order to control and maintain the arising costs from the seller's point of view, the paper in hand presents a generic and comprehensive approach to estimate the distribution function of machine warranty cost. Based on the estimation of the failure rate distribution certain life cycle cost elements are quantified either deterministically or stochastically depending on their characteristic. The Monte-Carlo simulation is used for the flexible consideration of the entire system and the estimation of risk figures such as the Value-at-Risk.}}, 
pages = {449--453}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/1351847x.2020.1828961, 
year = {2020}, 
title = {{Quantifying systemic risk with factor copulas}}, 
author = {Chen, Cathy Yi-Hsuan and Nasekin, Sergey}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2020.1828961}, 
abstract = {{We propose a tail dependence based network approach to study systemic risk in a network of systemically important financial institutions (SIFIs). We utilize a flexible factor copula based method which allows to measure the level of extreme risk in a portfolio when dependence is driven by one or several factors. We identify the most ‘connected’ SIFIs based on an eigenvector centrality approach applied to copula-implied dependence structures as ‘central’ SIFIs. We then demonstrate that the level of systemic risk implied by such SIFIs chosen as conditioning factors in the factor copula setup exceeds that which is implied by non-central SIFIs in terms of portfolio Value-at-Risk and the portfolio return under stress. This study contributes to quantification and ranking of the systemic importance of SIFIs which is important for setting adequate capital requirements in particular and stability of financial markets in general. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--22}, 
number = {18}, 
volume = {26}
}
@article{10.1080/03461238.2019.1636859, 
year = {2020}, 
title = {{Optimal asset allocation for participating contracts under the VaR and PI constraint}}, 
author = {Dong, Yinghui and Wu, Sang and Lv, Wenxin and Wang, Guojing}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2019.1636859}, 
abstract = {{Participating contracts provide a maturity guarantee for the policyholder. However, the terminal payoff to the policyholder should be related to financial risks of participating insurance contracts. We investigate an optimal investment problem under a joint value-at-risk and portfolio insurance constraint faced by the insurer who offers participating contracts. The insurer aims to maximize the expected utility of the terminal payoff to the insurer. We adopt a concavification technique and a Lagrange dual method to solve the problem and derive the representations of the optimal wealth process and trading strategies. We also carry out some numerical analysis to show how the joint value-at-risk and the portfolio insurance constraint impacts the optimal terminal wealth. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--26}, 
number = {2}, 
volume = {2020}
}
@article{10.1109/grc.2013.6740377, 
year = {2013}, 
title = {{The YTM-based stock portfolio mining approach by genetic algorithm}}, 
author = {Chen, Chun-Hao and Hsieh, Ching-Yu and Lee, Yeong-Chyi}, 
journal = {2013 IEEE International Conference on Granular Computing (GrC)}, 
issn = {NA}, 
doi = {10.1109/grc.2013.6740377}, 
abstract = {{This study proposes a yield-to-maturity (YTM)-based genetic portfolio selection model with user defined constraints, namely YTMGPSM. A set of real numbers are encoded into a chromosome to form a possible portfolio, which presents whether buy or not buy and purchased units of assets. The fitness value of a chromosome is evaluated by return on investment, value at risk and suitability of the respective portfolio. The suitability of a chromosome consists of portfolio penalty and investment capital penalty that are used to reflect the satisfactions of user predefined maximum investment and maximum number of companies, respectively. Experiments on real dataset are made to show the merits of the proposed approach. © 2013 IEEE.}}, 
pages = {38--42}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s00780-006-0018-0, 
year = {2006}, 
title = {{Asymptotic behaviour of mean-quantile efficient portfolios}}, 
author = {Dmitrašinović-Vidović, Gordana and Ware, Antony}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-006-0018-0}, 
abstract = {{In this paper we investigate portfolio optimization in the Black-Scholes continuous-time setting under quantile based risk measures: value at risk, capital at risk and relative value at risk. We show that the optimization results are consistent with Merton's two-fund separation theorem, i.e., that every optimal strategy is a weighted average of the bond and Merton's portfolio. We present optimization results for constrained portfolios with respect to these risk measures, showing for instance that under value at risk, in better markets and during longer time horizons, it is optimal to invest less into the risky assets. © Springer-Verlag 2006.}}, 
pages = {529--551}, 
number = {4}, 
volume = {10}
}
@article{10.5755/j01.ee.30.4.20838, 
year = {2019}, 
title = {{Investment environment problem analysis and evaluation: An ex post empirical analysis and performance implications}}, 
author = {Djakovic, Vladimir Djuro and Andjelic, Goran B. and Petkovic, Aleksandar D.}, 
journal = {Engineering Economics}, 
issn = {13922785}, 
doi = {10.5755/j01.ee.30.4.20838}, 
abstract = {{The research subject is the investment environment problem analysis and the evaluation of the developing countries, namely, the Republic of Serbia, Croatia, Slovenia, and Hungary. The research problem is to determine performance and adequacy of risk estimation models with special attention to the investment environment specificities of the markets in the developing countries. The analysis was carried out by testing and implementation of the Value-at-Risk models, i.e. the historical simulation (HS VaR), the delta-normal VaR (D VaR) and the extreme value theory model (EVT), with the confidence level of 95 \% for 100, 200 and 300 days, in the period from 2012 to 2016. The research objective is to test the validity of VaR models and performance evaluation regarding determination of the maximum possible loss. The basic hypothesis of the research is that there is a relation between the successful application of the historical simulation (HS VaR), the delta-normal VaR (D VaR) and the extreme value theory model (EVT) and the conditions and opportunities of the investment environment of the developing countries. The research results provide concrete knowledge of the conditions and circumstances of the investment environment in the observed markets, with a simultaneous performance assessment of the tested VaR models. The main result of the study is that regarding investment activities in the markets of developing countries and number of failures of various VaR models, the investment policymakers cannot rely on the analysis of historical trends and on one of the basic postulates of portfolio analysis ‘History Repeats Itself’. Recommendation for further research and for the local societies benefit is to emphasize the necessity of stable investment environment, thus enabling adequate capital allocation and risk estimation, while using the wide variety of approaches to Value-at-Risk modeling, especially for longer-horizon risk prediction. © 2019, Kauno Technologijos Universitetas. All rights reserved.}}, 
pages = {422--433}, 
number = {4}, 
volume = {30}
}
@article{10.1057/s41283-018-0046-z, 
year = {2019}, 
title = {{Testing expected shortfall: an application to emerging market stock indices}}, 
author = {Cardona, Emilio and Mora-Valencia, Andrés and Velásquez-Gaviria, Daniel}, 
journal = {Risk Management}, 
issn = {14603799}, 
doi = {10.1057/s41283-018-0046-z}, 
abstract = {{In a recent paper, Acerbi and Székely (Risk Magazine, 76–81, 2014) presented three methods to test expected shortfall, and this is the first empirical application of that paper on emerging markets. We employ daily stock index returns from the Morgan Stanley Capital International Inc. Emerging Markets Index covering the 2000–2015 period, extending Acerbi and Székely (Risk Magazine, 76–81, 2014) results to derive the significance thresholds for the Student’s skewed-t distribution using two testing methods. We find that the thresholds for the Z1 Test and Z2 Test for skewed-t distribution are similar to the values obtained by Acerbi and Székely for Student’s t distribution. Therefore, the Z1 and Z2 thresholds are invariant to the skewed-t-shaped parameter values found in the emerging market stock indices. Empirical results show outperformance of Student’s skewed-t and Student’s t distributions over Gaussian distribution. © 2018, Springer Nature Limited.}}, 
pages = {153--182}, 
number = {3}, 
volume = {21}
}
@article{10.1111/1468-036x.00182, 
year = {2002}, 
title = {{The effect of var based risk management on asset prices and the volatility smile}}, 
author = {Berkelaar, Arjan and Cumperayot, Phornchanok and Kouwenberg, Roy}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/1468-036x.00182}, 
abstract = {{Value-at-risk (VaR) has become the standard criterion for assessing risk in the financial industry. Given the widespread usage of VaR, it becomes increasingly important to study the effects of VaR based risk management on the prices of stocks and options. We solve a continuous-time asset pricing model, based on Lucas (1978) and Basak and Shapiro (2001), to investigate these effects. We find that the presence of risk managers tends to reduce market volatility, as intended. However, in some cases VaR risk management undesirably raises the probability of extreme losses. Finally, we demonstrate that option prices in an economy with VaR risk managers display a volatility smile. © 2002 Blackwell Publishers Ltd.}}, 
pages = {139--164}, 
number = {2}, 
volume = {8}
}
@article{10.21314/jrmv.2015.142, 
year = {2015}, 
title = {{Downside risk measure performance in the presence of breaks in volatility}}, 
author = {Rohde, Johannes}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2015.142}, 
abstract = {{The accurate evaluation of a risk measure employed by a financial institution is of high importance in view of that institution's minimum capital requirement. Having a sensitive reaction to breaks in the volatility of the profit-and-loss process is a desirable property of the underlying measure. This paper proposes a loss function-based framework for the comparative measurement of the sensitivity of quantile downside risk measures to breaks in volatility or distribution. We do this by extending the model comparison approach introduced by Lopez in 1998.Value-at-risk and expected shortfall (ES) are contrasted over realistic evaluation horizons within a broad simulation study, in which numerous settings involving volatility breaks of different intensities and several data-generating processes are checked by employing a magnitude-type loss function. As a result, it can generally be noted that ES appears to be the superior measure in terms of its ability to identify breaks in the volatility. In addition, an empirical study, in which data from six stock market indexes are used, demonstrates the applicability of this procedure and reconfirms the findings from the simulation study. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {31--68}, 
number = {4}, 
volume = {9}
}
@article{10.1007/s10614-014-9438-7, 
year = {2015}, 
title = {{Measuring Risk in Fixed Income Portfolios using Yield Curve Models}}, 
author = {Caldeira, João F. and Moura, Guilherme V. and Santos, André A. P.}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-014-9438-7}, 
abstract = {{We propose a novel approach to measure risk in fixed income portfolios in terms of value-at-risk (VaR). We obtain closed-form expressions for the vector of expected bond returns and for its covariance matrix based on a general class of dynamic factor models, including the dynamic versions of the Nelson-Siegel and Svensson models, to compute the parametric VaR of a portfolio composed of fixed income securities. The proposed approach provides additional modeling flexibility as it can accommodate alternative specifications of the yield curve as well as alternative specifications of the conditional heteroskedasticity in bond returns. An empirical application involving a data set with 15 fixed income securities with different maturities indicate that the proposed approach delivers accurate VaR estimates. © 2014, Springer Science+Business Media New York.}}, 
pages = {65--82}, 
number = {1}, 
volume = {46}
}
@article{10.1080/03610926.2012.716136, 
year = {2014}, 
title = {{Optimal reinsurance under VaR and CTE risk measures when ceded loss function is concave}}, 
author = {Lu, Zhi-Yi and Liu, Le-Ping and Shen, Qing-Jie and Meng, Li-Li}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2012.716136}, 
abstract = {{Cai et al. (2008) explored the optimal reinsurance designs among the class of increasing convex reinsurance treaties under VaR and CTE risk measures. However, reinsurance contracts always involve a limit on the ceded loss function in practice, and thus it may not be enough to confine the analysis to the class of convex functions only. The object of this article is to present an optimal reinsurance policy under VaR and CTE optimization criteria when the ceded loss function is in a class of increasing concave functions and the reinsurance premium is determined by the expected value principle. The outcomes reveal that the optimal form and amount of reinsurance depend on the confidence level p for the risk measure and the safety loading θ for the reinsurance premium. It is shown that under the VaR optimization criterion, the quota-share reinsurance with a policy limit is optima, while the full reinsurance with a policy limit is optima under CTE optimization criterion. Some illustrative examples are provided. © 2014 Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {3223--3247}, 
number = {15}, 
volume = {43}
}
@article{10.1007/s40745-020-00294-w, 
year = {2020}, 
title = {{An Application of Extreme Value Theory for Measuring Financial Risk in BRICS Economies}}, 
author = {Afuecheta, Emmanuel and Utazi, Chigozie and Ranganai, Edmore and Nnanatu, Chibuzor}, 
journal = {Annals of Data Science}, 
issn = {21985804}, 
doi = {10.1007/s40745-020-00294-w}, 
abstract = {{Characterization and quantification of the tail behaviour of rare events is an important issue in financial risk management. In this paper, the extreme behaviour of stock market returns from BRICS over the period 1995–2015 is described using five parametric distributions based on extreme value theory, including two mixture distributions based on the student’s t distribution. The distributions are fitted to the data using the method of maximum likelihood. The generalized extreme value (GEV) distribution is found to give the best fit. Based on the GEV distribution, estimates of value at risk, VaR p(X) and expected shortfall, ES p(X) from the five countries are computed. In addition, the correlation structure and tail dependence of these markets are characterized using several copula models. The Gumbel copula gives the best fit with evidence of significant relationships for all the pairs of the markets. To account for the possibility that due to sampling variability, a different model might be selected as the preferred model in a new sample from the same population, a short bootstrapping exercise was performed. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {1--40}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/14697680903369492, 
year = {2011}, 
title = {{Efficient and accurate quadratic approximation methods for pricing Asian strike options}}, 
author = {Chang, Chuang-Chang and Tsao, Chueh-Yung}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680903369492}, 
abstract = {{This study is on valuing Asian strike options and presents efficient and accurate quadratic approximation methods that work extremely well, both with regard to the volatility of a wide range of underlying assets, and longer average time windows. We demonstrate that most of the well-known quadratic approximation methods used in the literature for pricing Asian strike options are special cases of our model, with the numerical results demonstrating that our method significantly outperforms the other quadratic approximation methods examined here. Using our method for the calculation of hundreds of Asian strike options, the pricing errors (in terms of the root mean square errors) are reasonably small. Compared with the Monte Carlo benchmark method, our method is shown to be rapid and accurate. We further extend our method to the valuing of quanto forward-starting Asian strike options, with the pricing accuracy of these options being largely the same as the pricing of plain vanilla Asian strike options. © 2011 Taylor \& Francis.}}, 
pages = {729--748}, 
number = {5}, 
volume = {11}
}
@article{10.1109/ccdc.2019.8833273, 
year = {2019}, 
title = {{Portfolio Optimization Using Period Value at Risk Based on Historical Simulation Method}}, 
author = {An, Ruizhi and Wang, Dazhi and Huang, Min and Xu, Chunhui}, 
journal = {2019 Chinese Control And Decision Conference (CCDC)}, 
issn = {NA}, 
doi = {10.1109/ccdc.2019.8833273}, 
abstract = {{We consider market risk over a period of time that is more applicable in real world investment scenarios. This paper aims at solving technical issues related to the application of period value at risk (PVaR) in investment decision, a recently developed tool for measuring market risk over a period of time. An investment decision model with the objective of minimizing PVaR as the indicator of market risk and the constraint of satisfying investors' expected return is established. We consider portfolio selection problems with PVaR that is calculated applying historical simulation method. The PVaR minimization model becomes a complicated nonlinear programming and we suggested to solve the model by changing it to an equivalent mixed programming model. According to numerical analysis, we conclude that PVaR can provide a more conservative investment strategy for risk aversion investors compared with value at risk (VaR). Finally, we conduct a concise analysis of PVaR that is solved using historical simulation and Monte Carlo simulation. © 2019 IEEE.}}, 
pages = {324--328}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/jiabr-02-2018-0019, 
year = {2020}, 
title = {{Comparative analysis between global sukuk and bond indices: value-at-risk approach}}, 
author = {Bhuiyan, Rubaiyat Ahsan and Puspa, Maya and Saiti, Buerhan and Ghani, Gairuzazmi Mat}, 
journal = {Journal of Islamic Accounting and Business Research}, 
issn = {17590817}, 
doi = {10.1108/jiabr-02-2018-0019}, 
abstract = {{Purpose: Sukuk is an innovative financial instrument with a flexible structure based on Islamic financial contracts, unlike a bond which is based on the structure of a loan imposed with interest. With the notion that sukuk differs considerably from the conventional bonds in terms of risks related to investment, this study aims to examine whether the sukuk market is different from conventional bond markets based on the value-at-risk (VaR) approach. Design/methodology/approach: The VaR of a portfolio consists of sukuk and bond indices and is undertaken to determine whether there is any reduction in the VaR amount through the inclusion of the sukuk index in the portfolio. The analysis is undertaken based on the developed and emerging market bond and sukuk indices from January 2010 to December 2015. Findings: This paper examines whether the VaR of sukuk market differs from conventional bond markets by using fundamental techniques. It was observed that the VaR amount of sukuk indices is comparatively much lower than the VaR of bond indices in all the cases. Including the sukuk index with each bond index can reduce the VaR of the portfolio by around 30 to 50 per cent for all the developed and emerging market bond indices. Research limitations/implications: This research is limited to covering six years of data. Nonetheless, it is able to provide findings which are believed to be useful for the market players. Practical implications: This study unveils attractive opportunities in terms of diversification benefits of sukuk indices for international fixed-income portfolios. Originality/value: The VaR method is a useful risk management tool. This study uses this method to emphasise the significant reduction of risks and diversification benefits that sukuk investment could offer by including it in the investment portfolio. © 2020, Emerald Publishing Limited.}}, 
pages = {1245--1256}, 
number = {6}, 
volume = {11}
}
@article{10.1007/s10473-020-0619-2, 
year = {2020}, 
title = {{VaR and CTE Based Optimal Reinsurance from a Reinsurer’s Perspective}}, 
author = {Tan, Tao and Chen, Tao and Wu, Lijun and Sheng, Yuhong and Hu, Yijun}, 
journal = {Acta Mathematica Scientia}, 
issn = {02529602}, 
doi = {10.1007/s10473-020-0619-2}, 
abstract = {{In this article, we study optimal reinsurance design. By employing the increasing convex functions as the admissible ceded loss functions and the distortion premium principle, we study and obtain the optimal reinsurance treaty by minimizing the VaR (value at risk) of the reinsurer’s total risk exposure. When the distortion premium principle is specified to be the expectation premium principle, we also obtain the optimal reinsurance treaty by minimizing the CTE (conditional tail expectation) of the reinsurer’s total risk exposure. The present study can be considered as a complement of that of Cai et al. [5]. © 2020, Wuhan Institute Physics and Mathematics, Chinese Academy of Sciences.}}, 
pages = {1915--1927}, 
number = {6}, 
volume = {40}
}
@article{10.1057/jors.2011.163, 
year = {2012}, 
title = {{Minimization of the k-th maximum and its application on LMS regression and VaR optimization}}, 
author = {Huang, X and Xu, J and Wang, S and Xu, C}, 
journal = {Journal of the Operational Research Society}, 
issn = {01605682}, 
doi = {10.1057/jors.2011.163}, 
abstract = {{Motivated by two important problems, the least median of squares (LMS) regression and value-at-risk (VaR) optimization, this paper considers the problem of minimizing the k-th maximum for linear functions. For this study, a sufficient and necessary condition of local optimality is given. From this condition and other properties, we propose an algorithm that uses linear programming technique. The algorithm is assessed on real data sets and the experiments for LMS regression and VaR optimization both show its effectiveness. © 2012 Operational Research Society Ltd. All rights reserved.}}, 
pages = {1479--1491}, 
number = {11}, 
volume = {63}
}
@article{10.1109/isip.2010.118, 
year = {2010}, 
title = {{A measuring approach of portfolio's VaR based on APGARCH-EWMA model}}, 
author = {Wang, Ping}, 
journal = {2010 Third International Symposium on Information Processing}, 
issn = {NA}, 
doi = {10.1109/isip.2010.118}, 
abstract = {{Value at Risk (VaR) is a commonly statistical tool to measure market risk. In this paper, a mixture method of APGARCH-M model and EWMA algorithm is applied to measure VaR of a portfolio. Empirical study using three stock index of shanghai stock market shows the mixture method is advantageous and accurate to calculate VaR of a portfolio. © 2010 IEEE.}}, 
pages = {6--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1023/a:1022267720606, 
year = {2003}, 
title = {{Computational Tools for the Analysis of Market Risk}}, 
author = {Suárez, Alberto and Carrillo, Santiago}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1023/a:1022267720606}, 
abstract = {{The estimation and management of risk is an important and complex task faced by market regulators and financial institutions. Accurate and reliable quantitative measures of risk are needed to minimize undesirable effects on a given portfolio fromlarge fluctuations in market conditions. To accomplish this, a series of computational tools has beendesigned, implemented, and incorporated into MatRisk, an integratedenvironment for risk assessment developed in MATLAB. Besides standard measures, such as Value at Risk(VaR), the application includes other more sophisticated risk measures that address the inability of VaRproperly to characterize the structure of risk. Conditionalrisk measures can also be estimated for autoregressive models with heteroskedasticity, including some novel mixture models. These tools are illustrated with a comprehensive risk analysis of the Spanish IBEX35 stock index. © 2003 Kluwer Academic Publishers.}}, 
pages = {153--172}, 
number = {1-2}, 
volume = {21}
}
@article{10.1515/mcma-2011-0018, 
year = {2012}, 
title = {{Stochastic approximation with averaging innovation applied to Finance}}, 
author = {Laruelle, Sophie and Pagès, Gilles}, 
journal = {Monte Carlo Methods and Applications}, 
issn = {09299629}, 
doi = {10.1515/mcma-2011-0018}, 
abstract = {{Abstract. The aim of the paper is to establish a convergence theorem for multi-dimensional stochastic approximation when the innovations satisfy some light averaging properties in the presence of a pathwise Lyapunov function. These averaging assumptions allow us to unify apparently remote frameworks where the innovations are simulated (possibly deterministic like in quasi-Monte Carlo simulation) or exogenous (like market data) with ergodic properties. We propose several fields of applications and illustrate our results on five examples mainly motivated by finance. © 2012 de Gruyter.}}, 
pages = {1--51}, 
number = {1}, 
volume = {18}
}
@article{10.2143/ast.40.2.2061133, 
year = {2010}, 
title = {{Evaluating quantile reserve for equity-linked insurance in a stochastic volatility model: Long vs. short memory}}, 
author = {Ho, Hwai-Chung and Yang, Sharon S. and Liu, Fang-I}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.40.2.2061133}, 
abstract = {{This paper evaluates the long-term risk for equity-linked insurance products. We consider a specific type of equity-linked insurance product with guaranteed minimum maturity benef ts (GMMBs), and assume that the underlying equity follows the stochastic volatility model which allows the return's latent volatility component to be short-or long-memory. The explicit form of the quantile reserve or the Value at Risk and its confidence intervals are derived for both the long-memory and short-memory stochastic volatility models. To illustrate the effect of long-memory volatility, we use the S\&P 500 index as an example of linked equity. Simulation studies are performed to examine the accuracy of the quantile reserve and to demonstrate the consequence of low coverage probability if model misspecification takes place. The empirical results show that the confidence interval of quantile reserve could be severely underestimated if the long-memory effect in equity volatility is ignored. © 2010 by Astin Bulletin. All rights reserved.}}, 
pages = {669--698}, 
number = {2}, 
volume = {40}
}
@article{10.1109/cso.2014.91, 
year = {2014}, 
title = {{EMD based value at risk estimate algorithm for electricity markets}}, 
author = {Wang, Hongqian and He, Kaijian and Zou, Yingchao}, 
journal = {2014 Seventh International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2014.91}, 
abstract = {{With the electricity market reform in recent decades, the electricity price volatility brings more uncertainty and greater risks. This paper proposes a novel risk measurement approach Based on the EMD algorithm to estimate Value at Risk (VaR) in the electricity market. The EMD algorithm is used to decompose the time series into several intrinsic mode functions (IMFs) and one residual component. Then the decomposed parts will be calculate with the Exponential Weighted Moving Average (EWMA) model. Empirical studies in the five Australian electricity markets suggest that the proposed algorithm outperforms the benchmark EWMA model, in terms of conventional performance evaluation criteria for the model reliability. © 2014 IEEE.}}, 
pages = {445--449}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jcr.2015.196, 
year = {2015}, 
title = {{Credit risk: Taking fluctuating asset correlations into account}}, 
author = {Schmitt, Thilo A and Schäfer, Rudi and Guhr, Thomas}, 
journal = {The Journal of Credit Risk}, 
issn = {17446619}, 
doi = {10.21314/jcr.2015.196}, 
abstract = {{In structural credit risk models, default events and the ensuing losses are both derived from asset values at maturity. Hence, it is of the utmost importance to choose a distribution for these asset values that is in accordance with empirical data. At the same time, it is desirable to still preserve some analytical tractability. We achieve both goals by putting forward an ensemble approach for asset correlations. Consistent with the data, we view them as fluctuating quantities for which we may choose the average correlation as homogeneous. Thereby, we can reduce the number of parameters to two, the average correlation between assets and the strength of the fluctuations around this average value. Yet, the resulting asset value distribution describes the empirical data well. This allows us to derive the distribution of credit portfolio losses. With Monte Carlo simulations for the value-at-risk and expected tail loss, we validate the assumptions of our approach and demonstrate the necessity of taking fluctuating correlations into account. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {73--94}, 
number = {3}, 
volume = {11}
}
@article{10.1007/s11579-021-00292-3, 
year = {2021}, 
title = {{Safety-first portfolio selection}}, 
author = {Chiu, Wan-Yi}, 
journal = {Mathematics and Financial Economics}, 
issn = {18629679}, 
doi = {10.1007/s11579-021-00292-3}, 
abstract = {{Das, Markowitz, Scheid, and Statman (2010) introduced portfolio optimization with mental accounts (POMA), which connects modern investment theory (MVT) and mean–variance utility (MVU) in a behavioral portfolio-style problem where the investors are concerned about downside risk. Their feasible solution is a system of implicit equations solved by numerical approximations. This article contributes several findings related to the POMA problem. First, with necessary and sufficient conditions measured by the safety-first risk management rule, we derive a closed-form solution with the threshold return relative to the orthogonal portfolio on the mean–variance frontier. Second, we show that POMA feasibility can be substantially improved through stepwise regressions in which sorting assets by the value-at-risk (VaR) constraint is equivalent to sorting the coefficients’ t-statistics. Finally, we implement mean–variance efficiency testing from the VaR perspective. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.}}, 
pages = {657--674}, 
number = {3}, 
volume = {15}
}
@article{10.1016/j.insmatheco.2018.10.003, 
year = {2019}, 
title = {{Delta-hedging longevity risk under the M7–M5 model: The impact of cohort effect uncertainty and population basis risk}}, 
author = {Zhou, Kenneth Q. and Li, Johnny Siu-Hang}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2018.10.003}, 
abstract = {{In a recent project commissioned by the Institute and Faculty of Actuaries and the Life and Longevity Markets Association, a two-population mortality model called the M7–M5 model is developed and recommended as an industry standard for the assessment of population basis risk. In this paper, we contribute a delta hedging strategy for use with the M7–M5 model, taking into account of not only period effect uncertainty but also cohort effect uncertainty and population basis risk. To enhance practicality, the hedging strategy is formulated in both static and dynamic settings, and its effectiveness can be evaluated in terms of either variance or 1-year ahead Value-at-Risk (the latter is highly relevant to solvency capital requirements). Three real data illustrations are constructed to demonstrate (1) the impact of population basis risk and cohort effect uncertainty on hedge effectiveness, (2) the benefit of dynamically adjusting a delta longevity hedge, and (3) the relationship between risk premium and hedge effectiveness. © 2018 Elsevier B.V.}}, 
pages = {1--21}, 
number = {NA}, 
volume = {84}
}
@article{10.1016/j.insmatheco.2012.05.004, 
year = {2012}, 
title = {{The optimal mean-variance investment strategy under value-at-risk constraints}}, 
author = {Ye, Jun and Li, Tiantian}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.05.004}, 
abstract = {{This paper is devoted to study the effects arising from imposing a value-at-risk (VaR) constraint in the mean-variance portfolio selection problem for an insurer who receives a stochastic cash flow which he must then invest in a continuous-time financial market. For simplicity, we assume that there is only one investment opportunity available for the insurer, a risky stock. Using techniques of stochastic linear-quadratic (LQ) control, the optimal mean-variance investment strategy with and without the VaR constraint is derived explicitly in closed forms, based on the solution of the corresponding Hamilton-Jacobi-Bellman (HJB) equation. Furthermore, a numerical example is proposed to show how the addition of the VaR constraint affects the optimal strategy. © 2012 Elsevier B.V.}}, 
pages = {344--351}, 
number = {2}, 
volume = {51}
}
@article{10.1016/s0261-5606(00)00047-4, 
year = {2001}, 
title = {{Forecasting daily exchange rate volatility using intraday returns}}, 
author = {Martens, Martin}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/s0261-5606(00)00047-4}, 
abstract = {{This study investigates whether intraday returns contain important information for forecasting daily volatility. Whereas in the existing literature volatility models for daily returns are improved by including intraday information such as the daily high and low, volume, the number of trades, and intraday returns, here the volatility of intraday returns is explicitly modelled. Daily volatility forecasts are constructed from multiple volatility forecasts for intraday intervals. It is shown for the DEM/USD and the YEN/USD exchange rates that this results in superior forecasts for daily volatility. © 2001 Elsevier Science Ltd.}}, 
pages = {1--23}, 
number = {1}, 
volume = {20}
}
@article{10.1080/03610918.2016.1255972, 
year = {2017}, 
title = {{Riemann manifold Langevin methods on stochastic volatility estimation}}, 
author = {Zevallos, Mauricio and Gasco, Loretta and Ehlers, Ricardo}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2016.1255972}, 
abstract = {{In this article, we perform Bayesian estimation of stochastic volatility models with heavy tail distributions using Metropolis adjusted Langevin (MALA) and Riemman manifold Langevin (MMALA) methods. We provide analytical expressions for the application of these methods, assess the performance of these methodologies in simulated data, and illustrate their use on two financial time series datasets. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {0--0}, 
number = {10}, 
volume = {46}
}
@article{10.1016/j.irfa.2021.101902, 
year = {2021}, 
title = {{Value at risk, mispricing and expected returns}}, 
author = {Yang, Baochen and Ma, Yao}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2021.101902}, 
abstract = {{This study investigates how the relation between value-at-risk (VaR) and expected returns differs under different mispricing statuses. We find that a significantly negative VaR-return relation, defined as the VaR effect, is observed only for overpriced stocks, but not for underpriced stocks. Moreover, VaR has an amplification effect on mispricing, indicating that VaR captures risk that deters arbitrage and thus leads to an increase in mispricing. Our results are robust to alternative VaR definitions, subperiod analysis, different market states, and after controlling for other firm characteristics, well-known risk factors, and those variables that have been shown to have amplifying effects on mispricing. Finally, this study also examines the pricing effect of short sale constraints on the VaR effect under different mispricing statuses. Our findings suggest that the VaR effect observed in overpriced stocks becomes more severe as short sales are more constrained. © 2021}}, 
pages = {101902}, 
number = {NA}, 
volume = {78}
}
@article{10.18267/j.pep.756, 
year = {2020}, 
title = {{Assessing the systemic risk between american and European financial systems}}, 
author = {Orhan, Ayhan and Benli, Vahit Ferhan and Castanho, Rui Alexandre}, 
journal = {Prague Economic Papers}, 
issn = {12100455}, 
doi = {10.18267/j.pep.756}, 
abstract = {{The present study focuses on the analysis of systemic risk in the American and European financial systems for the period from 20 August 2004 to 28 February 2014. The global crisis in 2007 has brought attention to the urgent need to understand the systemic risk issues and the stability of financial systems along with their actors. To assess systemic risk, Adrian and Brunnermeier (2011) advocated the use of conditional value-at-risk (CoVaR) methodology in integrating quantile regression. Instead of the value-at-risk (VaR), which is unable to detect systemic risk, we seek to use the CoVaR methodology to calculate the systemic risk levels of the United States and European markets. In the light of related findings, we conclude that the insurance sector contributes most to the systemic risk in the USA, while in the Eurozone, it is the financial services sector that is highly interconnected with systemic risk. © 2020, Vysoka Skola Ekonomicka. All rights reserved.}}, 
pages = {649--671}, 
number = {6}, 
volume = {29}
}
@article{10.1016/j.insmatheco.2012.07.005, 
year = {2012}, 
title = {{Gram-Charlier densities: Maximum likelihood versus the method of moments}}, 
author = {Brio, Esther B. Del and Perote, Javier}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.07.005}, 
abstract = {{This paper compares two alternative estimation methods for estimating the density underlying financial returns specified in terms of a finite Gram-Charlier (GC) expansion. Maximum likelihood(ML) is the most widely employed method despite the fact that it is only consistent under the Gaussian or the true density, and usually involves convergence problems. Alternatively, the method of moments (MM) is a natural and straightforward procedure, although positivity is only guaranteed in the asymptotic expansion. We show an example for estimating daily returns of the Dow Jones Index with a very long data set, illustrating that both ML and MM yield similar outcomes. Therefore the MM applied to GC densities should be considered as an accurate tool for risk management and forecasting. © 2012 Elsevier B.V.}}, 
pages = {531--537}, 
number = {3}, 
volume = {51}
}
@article{10.1016/j.jbankfin.2018.06.001, 
year = {2018}, 
title = {{Value at risk and expected shortfall based on Gram-Charlier-like expansions}}, 
author = {Zoia, Maria Grazia and Biffi, Paola and Nicolussi, Federica}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2018.06.001}, 
abstract = {{This paper offers a new approach to modeling the distribution of a portfolio composed of either asset returns or insurance losses. To capture the leptokurtosis, which is inherent in most financial series, data are modeled by using Gram-Charlier (GC) expansions. Since we are interested in operating with several series simultaneously, the distribution of the sum of GC random variables is derived. This latter turns out to be a tail-sensitive density, suitable for modeling the distribution of a portfolio return-losses and, accordingly, can be conveniently adopted for computing risk measures such as the value at risk and the expected shortfall as well as some performance measures based on its partial moments. The closed form expressions of these risk measures are derived for cases when the density of a portfolio is the sum of GC expansions, either with the same or different kurtosis. An empirical application of this approach to a portfolio of financial asset indexes provides evidence of the comparative effectiveness of this technique in computing risk measures, both in and out of the sample period. © 2018 Elsevier B.V.}}, 
pages = {92--104}, 
number = {NA}, 
volume = {93}
}
@article{10.1016/j.ejor.2014.07.034, 
year = {2015}, 
title = {{A hybrid stock trading system using genetic network programming and mean conditional value-at-risk}}, 
author = {Chen, Yan and Wang, Xuancheng}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2014.07.034}, 
abstract = {{This paper describes a hybrid stock trading system based on Genetic Network Programming (GNP) and Mean Conditional Value-at-Risk Model (GNP-CVaR). The proposed method, combining the advantages of evolutionary algorithms and statistical model, has provided useful tools to construct portfolios and generate effective stock trading strategies for investors with different risk-attitudes. Simulation results on five stock indices show that model based on GNP and maximum Sharpe Ratio portfolio performs the best in bull market, and that based on GNP and the global minimum risk portfolio performs the best in bear market. The portfolios constructed by Markowitz's mean-variance model performs the same as mean-CVaR model. It is clarified that the proposed system significantly improves the function and efficiency of original GNP, which can help investors make profitable decisions. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {861--871}, 
number = {3}, 
volume = {240}
}
@article{10.1109/cifer.2009.4937505, 
year = {2009}, 
title = {{Accumulator pricing}}, 
author = {Lam, Kin and Yu, Philip L.H. and Xin, Ling}, 
journal = {2009 IEEE Symposium on Computational Intelligence for Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/cifer.2009.4937505}, 
abstract = {{Accumulator is a highly path dependant derivative structure that has been introduced as a retail financial product in recent years and becomes very popular in some Asian cities with its speculative nature. Despite its popularity, its pricing formula is not well known especially when there is a barrier structure. When the barrier in an accumulator contract is applied continuously, this paper obtains exact analytic pricing formulae for immediate settlement and for delay settlement. For discrete barrier, we also obtain analytic formulae which can approximate the fair price of an accumulator under both settlement methods. Through Monte Carlo simulation, we show that the approximation is highly satisfactory. With price formulae in close forms, this paper further explains how to price the product fairly to fit into its zero-cost structure. The analytic formulae also help in computing the Greeks of an accumulator which are documented in this paper. An asymmetry can be observed here that when the buyer is suffering a loss, risk characteristics like delta and vega are substantially larger than when the buyer is enjoying a profit. This means that losing buyers will be more vulnerable to price changes and volatility changes than winning buyers. This is consistent with another observation in the paper that the value at risk for the buyer can be several times larger than that of the seller. © 2009 IEEE.}}, 
pages = {72--79}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-9892.2006.00479.x, 
year = {2006}, 
title = {{Structural laplace transform and compound autoregressive models}}, 
author = {Darolles, Serge and Gourieroux, Christian and Jasiak, Joann}, 
journal = {Journal of Time Series Analysis}, 
issn = {01439782}, 
doi = {10.1111/j.1467-9892.2006.00479.x}, 
abstract = {{This paper presents a new general class of compound autoregressive (Car) models for non-Gaussian time series. The distinctive feature of the class is that Car models are specified by means of the conditional Laplace transforms. This approach allows for simple derivation of the ergodicity conditions and ensures the existence of forecasting distributions in closed form, at any horizon. The last property is of particular interest for applications to finance and economics that investigate the term structure of variables and/or of their nonlinear transforms. The Car class includes a number of time-series models that already exist in the literature, as well as new models introduced in this paper. Their applications are illustrated by examples of portfolio management, term structure and extreme risk analysis. © 2006 Blackwell Publishing Ltd.}}, 
pages = {477--503}, 
number = {4}, 
volume = {27}
}
@article{10.1007/3-540-26993-2_6, 
year = {2005}, 
title = {{Parsimonious value at risk for fixed income portfolios}}, 
author = {Bilson, John F. O.}, 
issn = {NA}, 
doi = {10.1007/3-540-26993-2\_6}, 
abstract = {{The standard approach to the risk analysis of fixed income portfolios involves a mapping of exposures into representative duration buckets. This approach does not provide a transparent description of the portfolio risk in the case of leveraged portfolios, particularly in the case of portfolios whose primary intent is to trade convexity. In this paper, an alternative approach, based upon Level, Slope and Curvature yield curve factors, is described. The alternative approach offers a linear model of non-linear trading strategies. © 2005 Springer Berlin · Heidelberg.}}, 
pages = {125--141}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.csda.2006.01.006, 
year = {2007}, 
title = {{Bayesian estimation of the Gaussian mixture GARCH model}}, 
author = {Ausín, María Concepción and Galeano, Pedro}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2006.01.006}, 
abstract = {{Bayesian inference and prediction for a generalized autoregressive conditional heteroskedastic (GARCH) model where the innovations are assumed to follow a mixture of two Gaussian distributions is performed. The mixture GARCH model can capture the patterns usually exhibited by many financial time series such as volatility clustering, large kurtosis and extreme observations. A Griddy-Gibbs sampler implementation is proposed for parameter estimation and volatility prediction. Bayesian prediction of the Value at Risk is also addressed providing point estimates and predictive intervals. The method is illustrated using the Swiss Market Index. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {2636--2652}, 
number = {5}, 
volume = {51}
}
@article{10.1016/j.renene.2017.11.022, 
year = {2018}, 
title = {{A novel process economics risk model applied to biodiesel production system}}, 
author = {Sajid, Zaman and Khan, Faisal and Zhang, Yan}, 
journal = {Renewable Energy}, 
issn = {09601481}, 
doi = {10.1016/j.renene.2017.11.022}, 
abstract = {{In this paper, the concept of value at risk (VAR) is introduced to study process economics related to biodiesel production and use. Although the VAR concept is actively used in financial engineering for stock investment and trading, it has never been used in process economics. A methodology to develop a VAR model for a biodiesel process facility has been proposed and analysed. The impact of different cost related risk factors is modelled using a stochastic process and interdependence in a Bayesian Network format. The analysis reveals that cost underestimation is the most significant risk factor in biodiesel economics. The VAR model is analysed for 1, 5, and 10 VAR for 5 years of plant operations. Analysing VAR at any point of time (i.e. year 2) shows that with a 1\% chance, 5\% chance and 10\% chance, the maximum loss would be \$6.26, \$9.52 and \$11.34 million respectively (up to year 2). When VAR is considered in the process economics the return period is significantly affected and is increased by 21 months. This study recommends that VAR should be considered as an integral part of process economics, especially for new product or process design. © 2017 Elsevier Ltd}}, 
pages = {615--626}, 
number = {NA}, 
volume = {118}
}
@article{10.1016/s0261-5606(01)00007-9, 
year = {2001}, 
title = {{Jumps and time-varying correlations in daily foreign exchange rates}}, 
author = {Chang, Kook-Hyun and Kim, Myung-Jig}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/s0261-5606(01)00007-9}, 
abstract = {{This paper extends the multivariate latent factor ARCH model approach of Diebold and Nerlove (Journal of Applied Econometrics 4 (1989) 1) as a parsimonious alternative that pays particular attention to time series properties of daily foreign exchange rates such as jumps and to changing volatilities in both the common and country-specific factors. Using seven major daily dollar exchange rates from January 1 1992 to December 31 1996, this paper finds evidence of significant time-varying correlations and the country-specific variances. Consistent with the finding of Alexius and Sellin (1997) (A latent factor model of European exchange rate risk premia. Manuscript, The Economic Research Institute, Stockholm School of Economics), the two factor model appears to be a reasonable description of the major exchange rates. © 2001 Elsevier Science Ltd.}}, 
pages = {611--637}, 
number = {5}, 
volume = {20}
}
@article{10.2753/mis0742-1222250102, 
year = {2008}, 
title = {{Risk management of contract portfolios in IT services: The profit-at-risk approach}}, 
author = {Kauffman, Robert J. and Sougstad, Ryan}, 
journal = {Journal of Management Information Systems}, 
issn = {07421222}, 
doi = {10.2753/mis0742-1222250102}, 
abstract = {{Information technology (IT) services providers are exposed to exogenous risks faced by the industry as a whole, and endogenous risks from their current portfolio of IT contracts. This exposure may lead to cost overruns or legal responsibility for service-level breeches. Providers can leverage information about their risk positions implied by their IT services contract portfolios to gain strategic advantage over their competitors. We build theory in support of a new construct, profit-at-risk, for evaluating the trade-offs between contract profitability and service-level risk, stemming from financial economics theory and models. We simulate an IT services contract portfolio, and show how managers can reduce organizational risk by forgoing profit-maximizing contracts in lieu of more conservative service-level agreements, yet still achieve high returns. Our approach provides decision support for ex ante contract evaluation and negotiation, and a means to conduct ex post efficiency evaluation. It also aligns IT service management with best practices in financial management. © 2008 M.E. Sharpe, Inc.}}, 
pages = {17--48}, 
number = {1}, 
volume = {25}
}
@article{10.1016/j.ijforecast.2019.10.007, 
year = {2020}, 
title = {{Forecasting risk measures using intraday data in a generalized autoregressive score framework}}, 
author = {Lazar, Emese and Xue, Xiaohan}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2019.10.007}, 
abstract = {{A new framework for the joint estimation and forecasting of dynamic value at risk (VaR) and expected shortfall (ES) is proposed by our incorporating intraday information into a generalized autoregressive score (GAS) model introduced by Patton et al., 2019 to estimate risk measures in a quantile regression set-up. We consider four intraday measures: the realized volatility at 5-min and 10-min sampling frequencies, and the overnight return incorporated into these two realized volatilities. In a forecasting study, the set of newly proposed semiparametric models are applied to four international stock market indices (S\&P 500, Dow Jones Industrial Average, Nikkei 225 and FTSE 100) and are compared with a range of parametric, nonparametric and semiparametric models, including historical simulations, generalized autoregressive conditional heteroscedasticity (GARCH) models and the original GAS models. VaR and ES forecasts are backtested individually, and the joint loss function is used for comparisons. Our results show that GAS models, enhanced with the realized volatility measures, outperform the benchmark models consistently across all indices and various probability levels. © 2020 The Author(s)}}, 
pages = {1057--1072}, 
number = {3}, 
volume = {36}
}
@article{10.1007/978-3-642-15390-7_46, 
year = {2010}, 
title = {{Fuzzy power system reliability model based on value-at-risk}}, 
author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y and Weikum, Gerhard and Wang, Bo and Li, You and Watada, Junzo}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-15390-7\_46}, 
abstract = {{Conventional power system optimization problems deal with the power demand and spinning reserve through real values. In this research, we employ fuzzy variables to better characterize these values in uncertain environment. In building the fuzzy power system reliable model, fuzzy Value-at-Risk (VaR) can evaluate the greatest value under given confidence level and is a new technique to measure the constraints and system reliability. The proposed model is a complex nonlinear optimization problem which cannot be solved by simplex algorithm. In this paper, particle swarm optimization (PSO) is used to find optimal solution. The original PSO algorithm is improved to straighten out local convergence problem. Finally, the proposed model and algorithm are exemplified by one numerical example. © 2010 Springer-Verlag.}}, 
pages = {445--453}, 
number = {PART 2}, 
volume = {6277 LNAI}
}
@article{10.1080/1351847x.2014.897960, 
year = {2015}, 
title = {{Non-homogeneous volatility correlations in the bivariate multifractal model}}, 
author = {Liu, Ruipeng and Lux, Thomas}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2014.897960}, 
abstract = {{In this paper, we consider an extension of the recently proposed bivariate Markov-switching multifractal model of Calvet, Fisher, and Thompson [2006. “Volatility Comovement: A Multifrequency Approach.” Journal of Econometrics 131: 179–215]. In particular, we allow correlations between volatility components to be non-homogeneous with two different parameters governing the volatility correlations at high and low frequencies. Specification tests confirm the added explanatory value of this specification. In order to explore its practical performance, we apply the model for computing value-at-risk statistics for different classes of financial assets and compare the results with the baseline, homogeneous bivariate multifractal model and the bivariate DCC-GARCH of Engle [2002. “Dynamic Conditional Correlation: A Simple Class of Multivariate Generalized Autoregressive Conditional Heteroskedasticity Models.” Journal of Business \& Economic Statistics 20 (3): 339–350]. As it turns out, the multifractal model with heterogeneous volatility correlations provides more reliable results than both the homogeneous benchmark and the DCC-GARCH model. © 2014 Taylor \& Francis.}}, 
pages = {971--991}, 
number = {12}, 
volume = {21}
}
@article{10.17159/2222-3436/2015/v18n4a8, 
year = {2015}, 
title = {{Value-at-risk for the USD/ZAR exchange rate: The variance-gamma model}}, 
author = {Establet, Kemda, Lionel and Chun-Kai, Huang, and Knowledge, Chinhamu,}, 
journal = {South African Journal of Economic and Management Sciences}, 
issn = {10158812}, 
doi = {10.17159/2222-3436/2015/v18n4a8}, 
abstract = {{A country’s level of exchange risk is closely linked to its financial stability, on a macro-economic scale. South African exchange rates, in particular, have a significant impact on imports, inflation, consumer prices and monetary policies. Consequently, it is imperative for economists and investors to assess accurately the associated exchange risks. Exchange rates, like most financial time series, are leptokurtic and contradict the classical Gaussian assumption. We therefore introduce subclasses of the generalised hyperbolic distribution as alternative models and contrast these with the normal distribution. We conclude that the variance-gamma model is the most robust for describing the log-returns of daily USD/ZAR exchange rates and their related Value-at-Risk (VaR) estimates. The model selection methodologies utilised in our analyses include the robust Kolmogorov-Smirnov test and the Akaike information criterion. Backtesting on the adequacy of VaR estimates is also performed using the Kupiec likelihood ratio test. © University of Pretoria. All rights reserved.}}, 
pages = {551--556}, 
number = {4}, 
volume = {18}
}
@article{10.1002/ijfe.1594, 
year = {2017}, 
title = {{On equity risk prediction and tail spillovers}}, 
author = {Pouliasis, Panos and Kyriakou, Ioannis and Papapostolou, Nikos}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.1594}, 
abstract = {{This paper studies the impact of modelling time-varying variances of stock returns in terms of risk measurement and extreme risk spillover. Using a general class of regime-dependent models, we find that volatility can be disaggregated into distinct components: a persistent stable process with low sensitivity to shocks and a high volatility process capturing rather short-lived rare events. Out-of-sample forecasts show that, once regime shifts are accounted for, accuracy is improved compared to the standard generalized autoregressive conditional heteroscedasticity or the historical volatility model. Volatility plays an important role in controlling and monitoring financial risks. Therefore, by means of a risk management application, we illustrate the economic value and the practical implications of risk control ability of the models in terms of value at risk. Finally, tests for predictability in co-movements in the tails of stock index returns suggest that large losses are strongly correlated, supporting asymmetric transmission processes for financial contagion in the left tail of return distributions, whereas contagion in reverse direction (gains) is weak. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {379--393}, 
number = {4}, 
volume = {22}
}
@article{10.1007/s00181-020-01882-8, 
year = {2021}, 
title = {{Using the conditional volatility channel to improve the accuracy of aggregate equity return predictions}}, 
author = {Nonejad, Nima}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-020-01882-8}, 
abstract = {{In a recent study, Maheu et al. (Int J Forecast 36: 570–587, 2020) suggest a predictive regression model, where besides the conditional mean, the lagged value of the predictor of interest can also impact the dependent variable through the conditional volatility process. Their out-of-sample study focusing on predicting the conditional distribution of the US real GDP growth rate by conditioning on the price of crude oil finds strong evidence in favor of the suggested specification with respect to density forecast accuracy. In this study, we demonstrate that their framework is also very useful with regard to predicting aggregate equity returns by conditioning on macroeconomic variables. Using the well-known Goyal and Welsh dataset, we show that the suggested framework results in statistically significant more accurate density predictions relative to the stochastic volatility benchmark as well as competitors, where the lagged value of the predictor of interest impacts aggregate equity returns exclusively through the conditional mean process. Evidence of statistical predictability also results in VaR accuracy gains. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {973--1009}, 
number = {2}, 
volume = {61}
}
@article{10.1080/14697680500531676, 
year = {2006}, 
title = {{A new technique for calibrating stochastic volatility models: The Malliavin gradient method}}, 
author = {Ewald, Christian-Oliver and Zhang, Aihua}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680500531676}, 
abstract = {{We discuss the application of gradient methods to calibrate mean reverting stochastic volatility models. For this we use formulas based on Girsanov transformations as well as a modification of the Bismut-EKvorthy formula lo compute the derivatives of certain option prices with respect lo the parameters of the model by applying Monte Carlo methods. The article presents an extension of the ideas lo apply Malliavin calculus methods in the computation of Greek's.}}, 
pages = {147--158}, 
number = {2}, 
volume = {6}
}
@article{10.2478/danb-2020-0015, 
year = {2020}, 
title = {{Volatility modelling and VAR: The case of Bitcoin, ether and ripple}}, 
author = {Ječmínek, Jakub and Kukalová, Gabriela and Moravec, Lukáš}, 
journal = {DANUBE}, 
issn = {18046746}, 
doi = {10.2478/danb-2020-0015}, 
abstract = {{Since Bitcoin introduction in 2008, the cryptocurrency market has grown into hundreds-of-billion-dollar market. The cryptocurrency market is well known as very volatile, mainly for the fact that the cryptocurrencies have not the price to fall back upon and that anybody can join the trading (no license or approval is required). Since empirical literature suggests that GARCH-type models dominate as VaR estimators the overall objective of this paper is to perform comprehensive volatility and VaR estimation for three major digital assets and conclude which method gives the best results in terms of risk management. The methods we used are parametric (GARCH and EWMA model), non-parametric (historical VaR) and Monte Carlo simulation (given by Geometric Brownian Motion). We conclude that the best method for value-at-risk estimation for cryptocurrencies is the Monte Carlo simulation due to the heavy diffusion (stochastic) process and robustness of the results. © 2020 Sciendo. All rights reserved.}}, 
pages = {253--269}, 
number = {3}, 
volume = {11}
}
@article{10.1142/s0219024917500327, 
year = {2017}, 
title = {{THEORETICAL SENSITIVITY ANALYSIS for QUANTITATIVE OPERATIONAL RISK MANAGEMENT}}, 
author = {KATO, TAKASHI}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024917500327}, 
eprint = {1104.0359}, 
abstract = {{We study the asymptotic behavior of the difference between the values at risk (VaR) VaRα(L) and VaRα(L + S) for heavy-tailed random variables L and S with α 1 for application in sensitivity analysis of quantitative operational risk management within the framework of the advanced measurement approach of Basel II (and III). Here, L describes the loss amount of the present risk profile and S describes the loss amount caused by an additional loss factor. We obtain different types of results according to the relative magnitudes of the thicknesses of the tails of L and S. In particular, if the tail of S is sufficiently thinner than that of L, then the difference between prior and posterior risk amounts VaRα(L + S) -VaRα(L) is asymptotically equivalent to the expectation (expected loss) of S. © 2017 World Scientific Publishing Company.}}, 
pages = {1750032}, 
number = {5}, 
volume = {20}
}
@article{10.1016/j.eneco.2009.02.007, 
year = {2009}, 
title = {{Multiple zone power forwards: A value at risk framework}}, 
author = {Demers, Jean-Guy}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2009.02.007}, 
abstract = {{Over the 1990s, deregulated power markets in New-England provided zones with fluctuating spot prices. Such prices have a notoriously high volatility, owing to the difficulty of storing electrical energy and the delays needed to adjust generation levels. In this context, forward contracts have become increasingly popular and understanding their dynamic is a problem facing many market players. This paper proposes a parsimonious parametric model, based on the price series of all n-month forward contracts (n = 1,2,3...), encompassing multiple zones. The model is then used for value at risk forecasts, which are backtested and compared with the ones in use by the risk management unit of an important electricity producer. Extensions to include natural gas and power-relevant oil-based future markets are discussed. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {714--726}, 
number = {5}, 
volume = {31}
}
@article{10.1016/j.iref.2013.12.001, 
year = {2014}, 
title = {{Why does skewness and the fat-tail effect influence value-at-risk estimates? Evidence from alternative capital markets}}, 
author = {Su, Jung-Bin and Lee, Ming-Chih and Chiu, Chien-Liang}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2013.12.001}, 
abstract = {{In this study, the generalized autoregressive conditional heteroskedasticity (GARCH) model involving skewed generalized error distribution (SGED) was used to estimate the corresponding volatility and value-at-risk (VaR) measures for various commodities distributed across four types of commodity markets. The empirical results indicated that the return (volatility) of most of the assets distributed in alternative markets significantly decreased (increased) as a result of the global financial crisis. Conversely, the oil crisis yielded inconsistent results. Regarding the influences of both crises on return and volatility, the global financial crisis was more influential than the oil crisis was. Moreover, regarding confidence levels, the skewness effect existed among VaR estimations for only the long position, whereas the fat-tail effect existed among the VaR estimations for only high confidence levels, irrespective of whether a long or short position was traded. Finally, regarding the popular confidence levels in risk management, the SGED (GED) was the optimal return distribution setting for a long (short) position. © 2013 Elsevier Inc.}}, 
pages = {59--85}, 
number = {NA}, 
volume = {31}
}
@article{10.11118/actaun201563041287, 
year = {2015}, 
title = {{Assessing efficiency of d-vine copula ARMA-GARCH method in value at risk forecasting: Evidence from PSE listed companies}}, 
author = {Klepáč, Václav and Hampel, David}, 
journal = {Acta Universitatis Agriculturae et Silviculturae Mendelianae Brunensis}, 
issn = {12118516}, 
doi = {10.11118/actaun201563041287}, 
abstract = {{The article points out the possibilities of using static D-Vine copula ARMA-GARCH model for estimation of 1 day ahead market Value at Risk. For the illustration we use data of the four companies listed on Prague Stock Exchange in range from 2010 to 2014. Vine copula approach allows us to construct high-dimensional copula from both elliptical and Archimedean bivariate copulas, i.e. multivariate probability distribution, created from process innovations. Due to a deeper shortage of existing domestic results or comparison studies with advanced volatility governed VaR forecasts we backtested D-Vine copula ARMA-GARCH model against the VaR rolling out of sample forecast from October 2012 to April 2014 of chosen benchmark models, e.g. multivariate VAR-GO-GARCH, VAR-DCC-GARCH and univariate ARMA-GARCH type models. Common backtesting via Kupiec and Christoffersen procedures offer generalization that technological superiority of model supports accuracy only in case of an univariate modeling - working with non-basic GARCH models and innovations with leptokurtic distributions. Multivariate VAR governed type models and static Copula Vines performed in stated backtesting comparison worse than selected univariate ARMA-GARCH, i.e. it have overestimated the level of actual market risk, probably due to hardly tractable time-varying dependence structure.}}, 
pages = {1287--1295}, 
number = {4}, 
volume = {63}
}
@article{10.1080/07350015.2000.10524845, 
year = {2000}, 
title = {{A note on optimal estimation from a risk-management perspective under possibly misspecified tail behavior}}, 
author = {Lucas, André}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1080/07350015.2000.10524845}, 
abstract = {{Many financial time series show leptokurtic behavior-that is, fat tails. Such tail behavior is important for risk management. In this article I focus on the calculation of Value-at-Risk (VaR) as a downside-risk measure for optimal asset portfolios. Using a framework centered on the Student-t distribution, I explicitly allow for a discrepancy between the fat-tailedness of the true distribution of asset returns and that of the distribution used by the investment manager. As a result, numbers for the overestimation or underestimation of the true VaR of a given portfolio can be computed. These numbers are used to rank several well-known estimation methods for determining the unknown parameters of the distribution of asset returns. Minimizing the absolute (percentage) mismatch between the nominal and actual or true VaR leads to the choice of a Gaussian maximum quasilikelihood estimator-that is, a least squares type of estimator. The maximum likelihood estimator has less satisfactory behavior. Outlier-robust estimators perform even worse if the required confidence level for the VaR is high. An explanation for these results is provided. © 2000 Taylor \& Francis Group, LLC.}}, 
pages = {31--39}, 
number = {1}, 
volume = {18}
}
@article{10.1016/j.matcom.2021.05.029, 
year = {2021}, 
title = {{Portfolio Value-at-Risk and expected-shortfall using an efficient simulation approach based on Gaussian Mixture Model}}, 
author = {Seyfi, Seyed Mohammad Sina and Sharifi, Azin and Arian, Hamidreza}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2021.05.029}, 
abstract = {{Monte Carlo Approaches for calculating Value-at-Risk (VaR) are powerful tools widely used by financial risk managers across the globe. However, they are time consuming and sometimes inaccurate. In this paper, a fast and accurate Monte Carlo algorithm for calculating VaR and ES based on Gaussian Mixture Models is introduced. Gaussian Mixture Models are able to cluster input data with respect to market's conditions and therefore no correlation matrices are needed for risk computation. Sampling from each cluster with respect to their weights and then calculating the volatility-adjusted stock returns leads to possible scenarios for prices of assets. Our results on a sample of US stocks show that the Gmm-based VaR model is computationally efficient and accurate. From a managerial perspective, our model can efficiently mimic the turbulent behavior of the market. As a result, our VaR measures before, during and after crisis periods realistically reflect the highly non-normal behavior and non-linear correlation structure of the market. © 2021 International Association for Mathematics and Computers in Simulation (IMACS)}}, 
pages = {1056--1079}, 
number = {NA}, 
volume = {190}
}
@article{10.1016/j.eswa.2014.05.017, 
year = {2014}, 
title = {{An application of capital allocation principles to operational risk and the cost of fraud}}, 
author = {Urbina, Jilber and Guillén, Montserrat}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2014.05.017}, 
abstract = {{The costs of operational risk refer to the capital needed to cover the losses generated by a firm's ordinary activities. In this paper several capital allocation principles are examined to demonstrate how such principles can be used to distribute aggregated capital across the various constituents that generate operational risk. Proportional allocation, for example, allows a cost per unit to be calculated. An application to fraud risk in the banking sector is presented and correlation scenarios between business lines are compared. © 2014 Elsevier Ltd. All rights reserved.}}, 
pages = {7023--7031}, 
number = {16}, 
volume = {41}
}
@article{10.1016/j.insmatheco.2005.11.003, 
year = {2006}, 
title = {{Mortality-dependent financial risk measures}}, 
author = {Dowd, Kevin and Cairns, Andrew J.G. and Blake, David}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2005.11.003}, 
abstract = {{This paper uses a recently developed two-factor stochastic mortality model to estimate financial risk measures for four illustrative types of mortality-dependent financial position: investments in zero-coupon longevity bonds; investments in longevity bonds that pay annual survivor-dependent coupons; and two examples of an insurer's annuity book that are each hedged by a longevity bond, one based on the annuity book and hedge having the same reference cohort, and the other not. The risk measures estimated are the value-at-risk, the expected shortfall and a spectral risk measure based on an exponential risk-aversion function. Results are reported on a model calibrated on data provided by the UK Government Actuary's Department, both with and without underlying parameter uncertainty. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {427--440}, 
number = {3}, 
volume = {38}
}
@article{10.1186/s40854-021-00297-3, 
year = {2021}, 
title = {{On the factors of Bitcoin’s value at risk}}, 
author = {Kwon, Ji Ho}, 
journal = {Financial Innovation}, 
issn = {21994730}, 
doi = {10.1186/s40854-021-00297-3}, 
abstract = {{This study investigates the factors of Bitcoin’s tail risk, quantified by Value at Risk (VaR). Extending the conditional autoregressive VaR model proposed by Engle and Manganelli (2004), I examine 30 potential drivers of Bitcoin’s 5\% and 1\% VaR. For the 5\% VaR, quantity variables, such as Bitcoin trading volume and monetary policy rate, were positively significant, but these effects were attenuated when new samples were added. The 5\% VaR responds positively to the Internet search index and negatively to the fluctuation of returns on commodity variables and the Chinese stock market index. For the 1\% VaR, variables related to the macroeconomy play a key role. The consumer sentiment index exerts a strong positive effect on the 1\% VaR. I also find that the 1\% VaR has positive relationships with the US economic policy uncertainty index and the fluctuation of returns on the corporate bond index. © 2021, The Author(s).}}, 
pages = {87}, 
number = {1}, 
volume = {7}
}
@article{10.1080/14697680600580912, 
year = {2006}, 
title = {{The generalized value at risk admissible set: Constraint consistency and portfolio outcomes}}, 
author = {Bowden, Roger J.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680600580912}, 
abstract = {{Generalized value at risk (GVaR) adds a conditional value at risk or censored mean lower bound to the standard value at risk and considers portfolio optimization problems in the presence of both constraints. For normal distributions the censored mean is synonymous with the statistical hazard function, but this is not true for fat-tailed distributions. The latter turn out to imply much tighter bounds for the admissible portfolio sel and indeed for the logistic, an upper bound for the portfolio variance that yields a simple portfolio choice rule. The choice theory in GVaR is in general not consistent with classic Von Neumann-Morgenslern utility functions for money. A re-specification is suggested to make it so that gives a clearer picture of the economic role of the respective constraints. This can be used analytically to explore the choice of portfolio hedges.}}, 
pages = {159--171}, 
number = {2}, 
volume = {6}
}
@article{10.1287/mnsc.49.2.230.12743, 
year = {2003}, 
title = {{Probabilistic error bounded for simulation quantile estimators}}, 
author = {Jin, Xing and Fu, Michael C and Xiong, Xiaoping}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.49.2.230.12743}, 
abstract = {{Quantile estimation has become increasingly important, particularly in the financial industry, where value at risk (VaR) has emerged as a standard measurement tool for controlling portfolio risk. In this paper, we analyze the probability that a simulation-based quantile estimator fails to lie in a prespecified neighborhood of the true quantile. First, we show that this error probability converges to zero exponentially fast with sample size for negatively dependent sampling. Then we consider stratified quantile estimators and show that the error probability for these estimators can be guaranteed to be 0 with sufficiently large, but finite, sample size. These estimators, however, require sample sizes that grow exponentially in the problem dimension. Numerical experiments on a simple VaR example illustrate the potential for variance reduction.}}, 
pages = {230--246}, 
number = {2}, 
volume = {49}
}
@article{10.1515/demo-2018-0006, 
year = {2018}, 
title = {{Risk bounds with additional information on functionals of the risk vector}}, 
author = {Rüschendorf, L.}, 
journal = {Dependence Modeling}, 
issn = {23002298}, 
doi = {10.1515/demo-2018-0006}, 
abstract = {{We consider the problem of determining risk bounds for the Value at Risk for risk vectors X where besides the marginal distributions also information on the distribution or on the expectation of some functionals Tj(X), 1 ≤ j ≤ m, is available. In particular this formulation includes the case where information on subgroup sums or maxima or on the correlations or covariances is available. Based on the method of dual bounds we obtain improved risk bounds compared to the marginal case. In general the explicit calculation of the dual bounds poses a challenge. We discuss various forms of relaxation of these bounds which are accessible and in some cases even lead to sharp bounds. © 2018 L. Rüschendorf, published by Sciendo 2018.}}, 
pages = {102--113}, 
number = {1}, 
volume = {6}
}
@article{10.1080/15326349.2014.930603, 
year = {2014}, 
title = {{Queues and risk processes with dependencies}}, 
author = {Badila, E. S. and Boxma, O. J. and Resing, J. A. C.}, 
journal = {Stochastic Models}, 
issn = {15326349}, 
doi = {10.1080/15326349.2014.930603}, 
abstract = {{We study the generalization of the G/G/1 queue obtained by relaxing the assumption of independence between inter-arrival times and service requirements. The analysis is carried out for the class of multivariate matrix exponential distributions introduced in Ref.[13]. In this setting, we obtain the steady-state waiting time distribution, and we show that the classical relation between the steady-state waiting time and workload distributions remains valid when the independence assumption is relaxed. We also prove duality results with the ruin functions in an ordinary and a delayed ruin process. These extend several known dualities between queueing and risk models in the independent case. Finally, we show that there exist stochastic order relations between the waiting times under various instances of correlation. © 2014 Taylor \&amp; Francis Group, LLC.}}, 
pages = {390--419}, 
number = {3}, 
volume = {30}
}
@article{10.3390/risks9040070, 
year = {2021}, 
title = {{An optimal tail selection in risk measurement}}, 
author = {Just, Małgorzata and Echaust, Krzysztof}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks9040070}, 
abstract = {{The appropriate choice of a threshold level, which separates the tails of the probability distribution of a random variable from its middle part, is considered to be a very complex and challenging task. This paper provides an empirical study on various methods of the optimal tail selection in risk measurement. The results indicate which method may be useful in practice for investors and financial and regulatory institutions. Some methods that perform well in simulation studies, based on theoretical distributions, may not perform well when real data are in use. We analyze twelve methods with different parameters for forty-eight world indices using returns from the period of 2000-Q1 2020 and four sub-periods. The research objective is to compare the methods and to identify those which can be recognized as useful in risk measurement. The results suggest that only four tail selection methods, i.e., the Path Stability algorithm, the minimization of the Asymptotic Mean Squared Error approach, the automated Eyeball method with carefully selected tuning parameters and the Hall single bootstrap procedure may be useful in practical applications. © 2021 by the authors.}}, 
pages = {70}, 
number = {4}, 
volume = {9}
}
@article{10.3390/risks2030260, 
year = {2014}, 
title = {{The impact of systemic risk on the diversification benefits of a risk portfolio}}, 
author = {Busse, Marc and Dacorogna, Michel and Kratz, Marie}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks2030260}, 
abstract = {{Risk diversification is the basis of insurance and investment. It is thus crucial to study the effects that could limit it. One of them is the existence of systemic risk that affects all of the policies at the same time. We introduce here a probabilistic approach to examine the consequences of its presence on the risk loading of the premium of a portfolio of insurance policies. This approach could be easily generalized for investment risk. We see that, even with a small probability of occurrence, systemic risk can reduce dramatically the diversification benefits. It is clearly revealed via a non-diversifiable term that appears in the analytical expression of the variance of our models. We propose two ways of introducing it and discuss their advantages and limitations. By using both VaR and TVaR to compute the loading, we see that only the latter captures the full effect of systemic risk when its probability to occur is low. © 2014 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {260--276}, 
number = {3}, 
volume = {2}
}
@article{10.1109/pmaps.2010.5528988, 
year = {2010}, 
title = {{Risk sensitivity of failure rate and maintenance Expenditure}}, 
author = {Schreiner, Andrej and Balzer, Gerd and Precht, Armin}, 
journal = {2010 IEEE 11th International Conference on Probabilistic Methods Applied to Power Systems}, 
issn = {NA}, 
doi = {10.1109/pmaps.2010.5528988}, 
abstract = {{Analysis of risks is an inseparable part of asset management of power systems. The paper presents the reliable method for risk management of power system carried out for an urban 10 kV network located in Germany. The basis of the method is the operational risk calculation applied in financial and insurance industries. There the popular method for risk calculation is the loss distribution approach (LDA). The LDA is based on estimation of frequency and severity distribution of failure events over one year time horizon. Under LDA, the adequate capital hold against operational risk losses is defined as Value at Risk, which is the quantile of the distribution for the next year annual losses. Especially the proposed approach combines the probability and severity of risk contingencies based on reliability indices of the system assets. The risk indices are finally applied for sensitivity analysis of two particular risk factors, failure rate and maintenance expenditure. In course of asset management the proposed approach and the following sensitivity analysis of risk factors yield to comprehensible recommendations for maintenance activities. ©2010 IEEE.}}, 
pages = {137--142}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/03461238.2012.723638, 
year = {2014}, 
title = {{Optimal reinsurance arrangements in the presence of two reinsurers}}, 
author = {Chi, Yichun and Meng, Hui}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2012.723638}, 
abstract = {{In this paper, we investigate the optimal form of reinsurance from the perspective of an insurer when he decides to cede part of the loss to two reinsurers, where the first reinsurer calculates the premium by expected value principle while the premium principle adopted by the second reinsurer satisfies three axioms: distribution invariance, risk loading, and preserving stop-loss order. In order to exclude the moral hazard, a typical reinsurance treaty assumes that both the insurer and reinsurers are obligated to pay more for the larger loss. Under the criterion of minimizing value at risk (VaR) or conditional value at risk (CVaR) of the insurer's total risk exposure, we show that an optimal reinsurance policy is to cede two adjacent layers, where the upper layer is distributed to the first reinsurer. To further illustrate the applicability of our results, we derive explicitly the optimal layer reinsurance by assuming a generalized Wang's premium principle to the second reinsurer. © 2014 Copyright Taylor \& Francis Group, LLC.}}, 
pages = {424--438}, 
number = {5}, 
volume = {NA}
}
@article{10.1109/ciced.2018.8592570, 
year = {2018}, 
title = {{Research on optimized purchasing strategy and risk management in electricity market}}, 
author = {Jie, Bao and Qin, Xie and Zhongrong, Wang}, 
journal = {2018 China International Conference on Electricity Distribution (CICED)}, 
issn = {21617481}, 
doi = {10.1109/ciced.2018.8592570}, 
abstract = {{This paper discussed the optimized purchasing strategy and risk management in different electricity market based on Markowitz portfolio theory and Value at Risk (VaR). The simulation results show that this model is correct and valuable. We provide a method of improving upon the Markowitz portfolio using value at risk as the decision-making criteria for distribution system operator in deregulated electricity market. © 2018 IEEE}}, 
pages = {2714--2717}, 
number = {NA}, 
volume = {NA}
}
@article{10.1541/ieejeiss.134.780, 
year = {2014}, 
title = {{A fuzzy multi-objective portfolio selection model with piecewise linear transaction costs}}, 
author = {Li, You and Wang, Bo and Watada, Junzo}, 
journal = {IEEJ Transactions on Electronics, Information and Systems}, 
issn = {03854221}, 
doi = {10.1541/ieejeiss.134.780}, 
abstract = {{In this paper, we studied a multi-objective portfolio selection problem with piecewise linear transaction costs in a fuzzy environment. Transaction costs are expenses incurred when buying or selling securities and they are a burden on investors who frequently make trades to balance their portfolio. To better evaluate portfolio performance with transaction costs, this paper extends a previous study to a new model called fuzzy multi-objective portfolio selection model with piecewise linear transaction costs. A fuzzy simulation-based particle swarm optimization algorithm is designed to solve the model considering investors' individual risk attitude. In addition, several numerical examples are provided to illustrate the effectiveness of this model and algorithm. A conclusion is drawn at the end of this paper. © 2014 The Institute of Electrical Engineers of Japan.}}, 
pages = {780--787}, 
number = {6}, 
volume = {134}
}
@article{10.1111/jtsa.12450, 
year = {2019}, 
title = {{Extending the Limits of Backtesting via the ‘Vanishing p’-Approach}}, 
author = {Hoga, Yannick}, 
journal = {Journal of Time Series Analysis}, 
issn = {01439782}, 
doi = {10.1111/jtsa.12450}, 
abstract = {{We derive backtests of value-at-risk and expected shortfall forecasts for levels that vanish as a function of the sample size n. In the standard case, the level of the forecasts is assumed to be fixed, leading to χ 2-limiting distributions of the Portmanteau-type backtests. We show that for levels vanishing at the order of n −1/2, Poisson-type limits arise instead. These mimic key features of the test statistics, such as discreteness. Simulations demonstrate that for forecast levels and sample sizes of practical interest, using the Poisson-type limits leads to much improved size vis-à-vis the standard χ 2-limits. © 2019 John Wiley \& Sons Ltd}}, 
pages = {858--866}, 
number = {5}, 
volume = {40}
}
@article{10.1007/978-3-319-18239-1_2, 
year = {2014}, 
title = {{Using value-at-Risk (VaR) to measure market risk of the equity inventory of a market maker}}, 
author = {Kuketayev, Argyn and Beatty, James}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-18239-1\_2}, 
abstract = {{We propose a simple approach to using value-at-risk (VaR) to measure market risk within the equity inventory of a market making entity, a task which presents several challenges specific to the market making function. Market makers constantly stand ready to buy and sell shares to market participants. In doing so, they inevitably maintain the inventory of shares, a portfolio of sorts, subject to market risk. VaR is a standard tool for measuring the market risk in investment portfolios, so a variety of calculation techniques have been developed over the years. However, the application of VaR to market making inventories requires a few adjustments, for unlike the typical investment portfolio these inventories change rapidly, as if they were rebalanced intra-day. Moreover, the number of unique tickers in the inventory for a given day may routinely list thousands of securities. As a result, at any moment in an inventory there could be hundreds of items with missing historical price data, which makes challenging the application of even the simplest VaR methods. The approach proposed in this paper deals with the rapidly rebalancing portfolio and missing data issues inherent in market making equity portfolios by rescaling portfolio weight to allow for the application of well-known VaR techniques to very large inventories. © Springer International Publishing Switzerland 2015.}}, 
pages = {15--26}, 
number = {NA}, 
volume = {135}
}
@article{10.1016/j.ijforecast.2017.01.006, 
year = {2017}, 
title = {{Dependence in credit default swap and equity markets: Dynamic copula with Markov-switching}}, 
author = {Fei, Fei and Fuertes, Ana-Maria and Kalotychou, Elena}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2017.01.006}, 
abstract = {{Theoretical credit risk models à la Merton (1974) predict a non-linear negative link between the default likelihood and asset value of a firm. This motivates us to propose a flexible empirical Markov-switching bivariate copula that allows for distinct time-varying dependence between credit default swap (CDS) spreads and equity prices in “crisis” and “tranquil” periods. The model identifies high-dependence regimes that coincide with the recent credit crunch and the European sovereign debt crises, and is supported by in-sample goodness-of-fit criteria relative to nested copula models that impose within-regime constant dependence or no regime-switching. Value-at-Risk forecasts that aim to set day-ahead trading limits for the hedging of CDS-equity portfolios reveal the economic relevance of the model from the viewpoints of both regulatory and asymmetric piecewise linear loss functions. © 2017 International Institute of Forecasters}}, 
pages = {662--678}, 
number = {3}, 
volume = {33}
}
@article{10.1016/j.ijforecast.2019.10.006, 
year = {2020}, 
title = {{Forecasting value at risk with intra-day return curves}}, 
author = {Rice, Gregory and Wirjanto, Tony and Zhao, Yuqian}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2019.10.006}, 
abstract = {{Methods for incorporating high resolution intra-day asset price data into risk forecasts are being developed at an increasing pace. Existing methods such as those based on realized volatility depend primarily on reducing the observed intra-day price fluctuations to simple scalar summaries. In this study, we propose several methods that incorporate full intra-day price information as functional data objects in order to forecast value at risk (VaR). Our methods are based on the recently proposed functional generalized autoregressive conditionally heteroscedastic (GARCH) models and a new functional linear quantile regression model. In addition to providing daily VaR forecasts, these methods can be used to forecast intra-day VaR curves, which we considered and studied with companion backtests to evaluate the quality of these intra-day risk measures. Using high-frequency trading data from equity and foreign exchange markets, we forecast the one-day-ahead daily and intra-day VaR with the proposed methods and various benchmark models. The empirical results suggested that the functional GARCH models estimated based on the overnight cumulative intra-day return curves exhibited competitive performance with benchmark models for daily risk management, and they produced valid intra-day VaR curves. © 2020 International Institute of Forecasters}}, 
pages = {1023--1038}, 
number = {3}, 
volume = {36}
}
@article{10.1504/ijmef.2008.019219, 
year = {2008}, 
title = {{Risk budgeting and Value-at-Risk}}, 
author = {Pilbeam, Keith and Noronha, Rehan}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2008.019219}, 
abstract = {{Value-at-Risk (VaR) is a popular risk-metric for reporting financial exposure, for evaluating fund/manager performance and for regulatory disclosures. Yet, VaR is not a coherent risk measure because it is not sub-additive. This paper applies the methodology of risk budgeting to determine if VaR qualifies as a coherent risk measure. We show that the tools of risk budgeting allow VaR to be treated as a coherent risk measure, even though it does not restore sub-additivity. The main finding is that the additional analysis provided by risk budgeting means that VaR is a useful tool even if it is not sub-additive. © 2008 Inderscience Enterprises Ltd.}}, 
pages = {149}, 
number = {2}, 
volume = {1}
}
@article{10.1016/j.matcom.2008.04.014, 
year = {2009}, 
title = {{Modelling the financial risk associated with U.S. movie box office earnings}}, 
author = {Bi, Guang and Giles, David E.}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2008.04.014}, 
abstract = {{In this paper we use extreme value theory to model the U.S. movie box office returns, using weekly data for the period January 1982 to September 2006. The Peak over Threshold method is used to fit the Generalized Pareto distribution to the tails of the distributions of both positive weekly returns and negative returns. Tail risk measures such as value at risk and expected shortfall are computed using maximum likelihood methods. These measures can be used as indicators for the film distributors in the preparation of movie prints, or as references for actual or potential investors in the movie industry. © 2008 IMACS.}}, 
pages = {2759--2766}, 
number = {9}, 
volume = {79}
}
@article{10.1016/j.ejor.2017.01.005, 
year = {2017}, 
title = {{Mean-VaR portfolio optimization: A nonparametric approach}}, 
author = {Lwin, Khin T. and Qu, Rong and MacCarthy, Bart L.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2017.01.005}, 
abstract = {{Portfolio optimization involves the optimal assignment of limited capital to different available financial assets to achieve a reasonable trade-off between profit and risk. We consider an alternative Markowitz's mean–variance model in which the variance is replaced with an industry standard risk measure, Value-at-Risk (VaR), in order to better assess market risk exposure associated with financial and commodity asset price fluctuations. Realistic portfolio optimization in the mean-VaR framework is a challenging problem since it leads to a non-convex NP-hard problem which is computationally intractable. In this work, an efficient learning-guided hybrid multi-objective evolutionary algorithm (MODE-GL) is proposed to solve mean-VaR portfolio optimization problems with real-world constraints such as cardinality, quantity, pre-assignment, round-lot and class constraints. A learning-guided solution generation strategy is incorporated into the multi-objective optimization process to promote efficient convergence by guiding the evolutionary search towards promising regions of the search space. The proposed algorithm is compared with the Non-dominated Sorting Genetic Algorithm (NSGA-II) and the Strength Pareto Evolutionary Algorithm (SPEA2). Experimental results using historical daily financial market data from S \& P 100 and S \& P 500 indices are presented. The results show that MODE-GL outperforms two existing techniques for this important class of portfolio investment problems in terms of solution quality and computational time. The results highlight that the proposed algorithm is able to solve the complex portfolio optimization without simplifications while obtaining good solutions in reasonable time and has significant potential for use in practice. © 2017 Elsevier B.V.}}, 
pages = {751--766}, 
number = {2}, 
volume = {260}
}
@article{10.1007/3-540-26993-2_5, 
year = {2005}, 
title = {{Value at rist: Regulatory and other applications, methods, and criticism}}, 
author = {Knobloch, Alois Paul}, 
issn = {NA}, 
doi = {10.1007/3-540-26993-2\_5}, 
abstract = {{This article surveys several applicational as well as theoretical aspects of Value at Risk as a measure of risk. First, we compare different calculation methods with respect to accuracy, implementational issues as well as suitability for resource allocation and optimization. We contribute to capital allocation based on Value at Risk and provide an optimization model. Afterwards, we concentrate on shortcomings of Value at Risk as a measure of risk from a theoretical point of view. The focus is on the relation to decision theory and to coherent measures of risk. Alternatives to Value at Risk such as the lower partial moment one or the tail conditional expectation are included. We give some reasons to prefer the latter as a measure of risk. © 2005 Springer Berlin · Heidelberg.}}, 
pages = {99--124}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/axioms9030098, 
year = {2020}, 
title = {{Value at risk based on fuzzy numbers}}, 
author = {Guerra, Maria Letizia and Sorini, Laerte}, 
journal = {Axioms}, 
issn = {20751680}, 
doi = {10.3390/axioms9030098}, 
abstract = {{Value at Risk (VaR) has become a crucial measure for decision making in risk management over the last thirty years and many estimation methodologies address the finding of the best performing measure at taking into account unremovable uncertainty of real financial markets. One possible and promising way to include uncertainty is to refer to the mathematics of fuzzy numbers and to its rigorous methodologies which offer flexible ways to read and to interpret properties of real data which may arise in many areas. The paper aims to show the effectiveness of two distinguished models to account for uncertainty in VaR computation; initially, following a non parametric approach, we apply the Fuzzy-transform approximation function to smooth data by capturing fundamental patterns before computing VaR. As a second model, we apply the Average Cumulative Function (ACF) to deduce the quantile function at point p as the potential loss VaRp for a fixed time horizon for the 100p\% of the values. In both cases a comparison is conducted with respect to the identification of VaR through historical simulation: twelve years of daily S\&amp;P500 index returns are considered and a back testing procedure is applied to verify the number of bad VaR forecasting in each methodology. Despite the preliminary nature of the research, we point out that VaR estimation, when modelling uncertainty through fuzzy numbers, outperforms the traditional VaR in the sense that it is the closest to the right amount of capital to allocate in order to cover future losses in normal market conditions. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {98}, 
number = {3}, 
volume = {9}
}
@article{10.1007/s12667-019-00339-x, 
year = {2021}, 
title = {{Backtesting energy portfolio with copula dependence structure}}, 
author = {Masala, Giovanni}, 
journal = {Energy Systems}, 
issn = {18683967}, 
doi = {10.1007/s12667-019-00339-x}, 
abstract = {{The energy markets play a crucial role in the economic development of every country. These markets are characterized by high volatilities due to sizeable price fluctuations. The correct development of risk measures to quantify the inherent risk market is then a challenging task for the risk management systems. We consider in this survey a portfolio composed of two energy assets: crude oil and natural gas. We adopt, as a risk measure, the value-at-risk and the expected shortfall. In order to estimate these risk measures efficiently, we model the tails of each marginal with extreme value theory and we adopt a general dependence structure between the two assets given by a t-copula. The performance of the model is then validated with backtesting technique. To this end, we use a database ranging from years 1997 to 2017. We have then highlighted that the backtesting based on the value-at-risk and the Expected Shortfall passed the most common tests. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {393--410}, 
number = {2}, 
volume = {12}
}
@article{10.1016/j.insmatheco.2012.02.006, 
year = {2012}, 
title = {{Are quantile risk measures suitable for risk-transfer decisions?}}, 
author = {Guerra, Manuel and Centeno, M.L.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.02.006}, 
abstract = {{Although controversial from the theoretical point of view, quantile risk measures are widely used by institutions and regulators. In this paper, we use a unified approach to find the optimal treaties for an agent who seeks to minimize one of these measures, assuming premium calculation principles of various types. We show that the use of measures like Value at Risk or Conditional Tail Expectation as optimization criteria for insurance or reinsurance leads to treaties that are not enforceable and/or are clearly bad for the cedent. We argue that this is one further argument against the use of quantile risk measures, at least for the purpose of risk-transfer decisions. © 2012 Elsevier B.V.}}, 
pages = {446--461}, 
number = {3}, 
volume = {50}
}
@article{10.1109/wsc.2012.6465096, 
year = {2012}, 
title = {{Stochastic kriging for conditional value-at-risk and its sensitivities}}, 
author = {Chen, Xi and Nelson, Barry L. and Kim, Kyoung-Kuk}, 
journal = {Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2012.6465096}, 
abstract = {{Measuring risks in asset portfolios has been one of the central topics in the financial industry. Since the introduction of coherent risk measures, studies on risk measurement have flourished and measures beyond value-at-risk, such as expected shortfall, have been adopted by academics and practitioners. However, the complexity of financial products makes it very difficult and time consuming to perform the numerical tasks necessary to compute these risk measures. In this paper, we introduce a stochastic kriging metamodel-based method for efficient estimation of risks and their sensitivities. In particular, this method uses gradient estimators of assets in a portfolio and gives the best linear unbiased predictor of the risk sensitivities with minimum mean squared error. Numerical comparisons of the proposed method with two other stochastic kriging based approaches demonstrate the promising role that the proposed method can play in the estimation of financial risk. © 2012 IEEE.}}, 
pages = {1--12}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jmoneco.2005.10.001, 
year = {2006}, 
title = {{An options-based approach to evaluating the risk of Fannie Mae and Freddie Mac}}, 
author = {Lucas, Deborah and McDonald, Robert L.}, 
journal = {Journal of Monetary Economics}, 
issn = {03043932}, 
doi = {10.1016/j.jmoneco.2005.10.001}, 
abstract = {{Fannie Mae and Freddie Mac assume a significant amount of interest and prepayment risk and all of the credit risk for about half of the \$8 trillion U.S. residential mortgage market. Their hybrid government-private status, and the perception that they are too big to fail, make them a potentially large, but largely unaccounted for, risk to the federal government. Measuring the size and risk of this liability is technically difficult, but important for the debate over the appropriate regulation of these institutions. Here we take an options pricing approach to evaluating these costs and risks. Under the base case assumptions, the estimated value of the guarantees is \$7.9 billion over 10 years, with a combined .5 percent value at risk of \$122 billion. We evaluate the sensitivity of these estimates to various modeling assumptions, and also to the regulatory regime, including forbearance policies and capital requirements. The analysis highlights the benefits, but also the challenges, of taking an options-based approach to evaluating the value of federal credit guarantees. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {155--176}, 
number = {1}, 
volume = {53}
}
@article{10.1108/sef-03-2016-0055, 
year = {2016}, 
title = {{Comparison of methods for estimating the uncertainty of value at risk}}, 
author = {Gamba-Santamaria, Santiago and Jaulin-Mendez, Oscar Fernando and Melo-Velandia, Luis Fernando and Quicazán-Moreno, Carlos Andrés}, 
journal = {Studies in Economics and Finance}, 
issn = {10867376}, 
doi = {10.1108/sef-03-2016-0055}, 
abstract = {{Purpose: Value at risk (VaR) is a market risk measure widely used by risk managers and market regulatory authorities, and various methods are proposed in the literature for its estimation. However, limited studies discuss its distribution or its confidence intervals. The purpose of this paper is to compare different techniques for computing such intervals to identify the scenarios under which such confidence interval techniques perform properly. Design/methodology/approach: The methods that are included in the comparison are based on asymptotic normality, extreme value theory and subsample bootstrap. The evaluation is done by computing the coverage rates for each method through Monte Carlo simulations under certain scenarios. The scenarios consider different persistence degrees in mean and variance, sample sizes, VaR probability levels, confidence levels of the intervals and distributions of the standardized errors. Additionally, an empirical application for the stock market index returns of G7 countries is presented. Findings: The simulation exercises show that the methods that were considered in the study are only valid for high quantiles. In particular, in terms of coverage rates, there is a good performance for VaR(99 per cent) and bad performance for VaR(95 per cent) and VaR(90 per cent). The results are confirmed by an empirical application for the stock market index returns of G7 countries. Practical implications: The findings of the study suggest that the methods that were considered to estimate VaR confidence interval are appropriated when considering high quantiles such as VaR(99 per cent). However, using these methods for smaller quantiles, such as VaR(95 per cent) and VaR(90 per cent), is not recommended. Originality/value: This study is the first one, as far as it is known, to identify the scenarios under which the methods for estimating the VaR confidence intervals perform properly. The findings are supported by simulation and empirical exercises. © 2016, © Emerald Group Publishing Limited.}}, 
pages = {595--624}, 
number = {4}, 
volume = {33}
}
@article{10.1109/naps.2018.8600680, 
year = {2019}, 
title = {{Risk Management for Optimal Wind Power Bidding in an Electricity Market: A Comparative Study}}, 
author = {AlAshery, Mohamed Kareem and Qiao, Wei}, 
journal = {2018 North American Power Symposium (NAPS)}, 
issn = {NA}, 
doi = {10.1109/naps.2018.8600680}, 
abstract = {{Currently, it is common to trade wind energy in competitive electricity markets. However, wind power traders face additional risks beside the common market risks due to wind uncertainty. Hence, risk management is crucial for trading wind power in the pool. In this paper, the risk management of a stochastic optimization-based model without and with different risk measures, including variance, value at risk (VaR), and conditional value at risk (CVaR), as well as the stochastic dominance-based risk management method for trading wind energy in the pool are presented and compared. Case studies are conducted for an 80 MW wind power producer to show and compare the performance of the stochastic optimization-based bidding models without risk management and with different risk management approaches. In this paper, the comparison is based on various parameters of the distributions of the optimal objective function value obtained from different considered cases, rather than just one or two distributions' parameter(s) which is the common in the literature. © 2018 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2013.01.036, 
year = {2013}, 
title = {{Nonlinear portfolio selection using approximate parametric Value-at-Risk}}, 
author = {Cui, Xueting and Zhu, Shushang and Sun, Xiaoling and Li, Duan}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.01.036}, 
abstract = {{As the skewed return distribution is a prominent feature in nonlinear portfolio selection problems which involve derivative assets with nonlinear payoff structures, Value-at-Risk (VaR) is particularly suitable to serve as a risk measure in nonlinear portfolio selection. Unfortunately, the nonlinear portfolio selection formulation using VaR risk measure is in general a computationally intractable optimization problem. We investigate in this paper nonlinear portfolio selection models using approximate parametric Value-at-Risk. More specifically, we use first-order and second-order approximations of VaR for constructing portfolio selection models, and show that the portfolio selection models based on Delta-only, Delta-Gamma-normal and worst-case Delta-Gamma VaR approximations can be reformulated as second-order cone programs, which are polynomially solvable using interior-point methods. Our simulation and empirical results suggest that the model using Delta-Gamma-normal VaR approximation performs the best in terms of a balance between approximation accuracy and computational efficiency. © 2013.}}, 
pages = {2124--2139}, 
number = {6}, 
volume = {37}
}
@article{10.3923/jas.2013.2436.2442, 
year = {2013}, 
title = {{Evaluating volatility of price risk in electricity market based on GARCH and value-at-risk theory}}, 
author = {Deng, Pan}, 
journal = {Journal of Applied Sciences}, 
issn = {18125654}, 
doi = {10.3923/jas.2013.2436.2442}, 
abstract = {{The restructuring and deregulation in electric power industry has created the extreme short-time price volatility and heightened the importance of risk management in competitive electricity markets. Accurately characterizing the electricity price volatilities is the foundation to effectively evaluate the price risk in electricity market. With the system demand for electricity as an exogenous explanatory variable, a model to estimate value-at-risk via., a GARCH specification (GARCH-VaR) is proposed, in which the seasonalities, heteroscedasticities, lepkurtosises, heavy-tails and volatility-clustering can be jointly addressed. The impacts of probability distribution assumption and the time-varying features of parameters of the proposed model on the accuracy of VaR estimation are analyzed for three innovation's distributions: normal, student-t and skewed student-t. The numerical example based on the historical data of the PJM electricity market shows that the GARCH-VaR model with conditional skewed student-t innovations performs better in predicting one-period-ahead VaR but the one with Gaussian distribution underestimates the higher quantiles and the one with student-t distribution overestimates the lower quantiles. These results present several potential implications for electricity markets risk quantifications and hedging strategies. © 2013 Asian Network for Scientific Information.}}, 
pages = {2436--2442}, 
number = {13}, 
volume = {13}
}
@article{10.1016/s0123-5923(13)70018-4, 
year = {2013}, 
title = {{Value-at-risk: Evaluation of the behavior of different methodologies for 5 Latin American countries [Valor em risco: avaliação do desempenho de diferentes metodologias para 5 países latino americanos] [Valor en riesgo: evaluación del desempeño de diferentes metodologías para 5 países latinoamericanos]}}, 
author = {Alonso, Julio César and Chaves, Juan Manuel}, 
journal = {Estudios Gerenciales}, 
issn = {01235923}, 
doi = {10.1016/s0123-5923(13)70018-4}, 
abstract = {{This paper evaluates the performance of 20 different methods (parametric, and semi-parametric, and nonparametric), as well as the historical simulation method, to estimate the next-trading-day value-at risk (VaR) of a representative portfolio for 5 different Latin American countries (Argentina, Brasil, Colombia and Peru). We found that the non-parametric (i.e. historic simulation), and the semi-parametric methods were the best way to estimate the risk among the twenty different methods evaluated for all the countries in the sample. © 2013 Universidad ICESI. Published by Elsevier España. All rights reserved.}}, 
pages = {37--48}, 
number = {126}, 
volume = {29}
}
@article{10.1016/j.procs.2013.05.084, 
year = {2013}, 
title = {{The research of liquidity risk measurements in China stock market}}, 
author = {Li, Lili and Feng, Yuhui}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2013.05.084}, 
abstract = {{In this paper, the theories of L-VaR and La-VaR are used to describe the liquidity of stocks and the liquidity risk of the stock market. In the empirical analysis, six stocks are selected from each industry which makes the total samples. The data from 2002 to 2012 are used to construct these two indicators and the results are compared. Through the comparative study, the conclusion is that despite the difference in characterizations of liquidity risk, the results of two indicators are consistent. To evaluate the liquidity risk of the stock market, various indicators should be comprehensively analyzed in order to reach a more reliable conclusion. © 2013 The Authors. Published by Elsevier B.V.}}, 
pages = {647--655}, 
number = {NA}, 
volume = {17}
}
@article{10.1111/mafi.12028, 
year = {2014}, 
title = {{Risk measures on P(R) and value at risk with probability/loss function}}, 
author = {Frittelli, Marco and Maggis, Marco and Peri, Ilaria}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12028}, 
abstract = {{We propose a generalization of the classical notion of the V at Rλ that takes into account not only the probability of the losses, but the balance between such probability and the amount of the loss. This is obtained by defining a new class of law invariant risk measures based on an appropriate family of acceptance sets. The V at Rλ and other known law invariant risk measures turn out to be special cases of our proposal. We further prove the dual representation of Risk Measures on P(R). © 2014 Wiley Periodicals, Inc.}}, 
pages = {442--463}, 
number = {3}, 
volume = {24}
}
@article{10.1109/cisti.2014.6876976, 
year = {2014}, 
title = {{Value and risk in investment portfolios, critical variables calculated from an intelligent hybrid system based on CBR [Valor y riesgo en portafolios de inversión, variables críticas calculadas con un sistema hibrido inteligente basado en CBR]}}, 
author = {Estrada, Ernesto Camilo Diaz and Toro, Carlos Hernan Fajardo}, 
journal = {2014 9th Iberian Conference on Information Systems and Technologies (CISTI)}, 
issn = {21660727}, 
doi = {10.1109/cisti.2014.6876976}, 
abstract = {{This paper presents the partial results obtained at the doctoral work titled ADAPTIVE HYBRID SYSTEM FOR THE VALUE AT RISK ESTIMATION ON PORTFOLIOS, where different AI techniques are used. This expert system tries to predict the portfolio variability with two o more assets, where the ROI and the risk rate are vital to define the efficient frontier market proposed by Markowitz. © 2014 AISTI.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/00036846.2020.1814946, 
year = {2021}, 
title = {{Modelling asset returns in the presence of price limits with Markov-switching mixture of truncated normal GARCH distribution: evidence from China}}, 
author = {Wang, Donghua and Ding, Jin and Chu, Guoqing and Xu, Dinghai and Wirjanto, Tony S.}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2020.1814946}, 
abstract = {{This article proposes a general framework of a Markov Switching GARCH model with a mixture of truncated Gaussian to model asset returns with price limits in China. Theoretically, while retaining many convenient statistical properties of the Gaussian distribution, the proposed model also assumes a flexible time-varying volatility structure to accommodate the feature of the return data under price restrictions in China, such as the clusters near the bounds (due to the ‘bound effect’). Empirically, we apply the model to eight representative stocks from Shanghai and Shenzhen stock markets in China. Lastly, we find that our proposed model dominates the conventional volatility models in terms of Value-at-Risk measures. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--24}, 
number = {7}, 
volume = {53}
}
@article{10.1155/2021/5580228, 
year = {2021}, 
title = {{A new class of heavy-tailed distributions: Modeling and simulating actuarial measures}}, 
author = {Zhao, Jin and Ahmad, Zubair and Mahmoudi, Eisa and Hafez, E. H. and El-Din, Marwa M. Mohie}, 
journal = {Complexity}, 
issn = {10762787}, 
doi = {10.1155/2021/5580228}, 
abstract = {{Statistical distributions play a prominent role for modeling data in applied fields, particularly in actuarial, financial sciences, and risk management fields. Among the statistical distributions, the heavy-tailed distributions have proven the best choice to use for modeling heavy-tailed financial data. The actuaries are often in search of such types of distributions to provide the best description of the actuarial and financial data. This study presents a new power transformation to introduce a new family of heavy-tailed distributions useful for modeling heavy-tailed financial data. A submodel, namely, heavy-tailed beta-power transformed Weibull model is considered to demonstrate the adequacy of the proposed method. Some actuarial measures such as value at risk, tail value at risk, tail variance, and tail variance premium are calculated. A brief simulation study based on these measures is provided. Finally, an application to the insurance loss dataset is analyzed, which revealed that the proposed distribution is a superior model among the competitors and could potentially be very adequate in describing and modeling actuarial and financial data. Copyright © 2021 Jin Zhao et al.}}, 
pages = {1--18}, 
number = {NA}, 
volume = {2021}
}
@article{10.1109/wsc.2015.7408203, 
year = {2016}, 
title = {{Robust simulation of stochastic systems with input uncertainties modeled by statistical divergences}}, 
author = {Hu, Zhaolin and Hong, L. Jeff}, 
journal = {2015 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2015.7408203}, 
abstract = {{Simulation is often used to study stochastic systems. A key step of this approach is to specify a distribution for the random input. This is called input modeling, which is important and even critical for simulation study. However, specifying a distribution precisely is usually difficult and even impossible in practice. This issue is called input uncertainty in simulation study. In this paper we study input uncertainty when using simulation to estimate important performance measures: expectation, probability, and value-at-risk. We propose a robust simulation (RS) approach, which assumes the real distribution is contained in a certain ambiguity set constructed using statistical divergences, and simulates the maximum and the minimum of the performance measures when the distribution varies in the ambiguity set. We show that the RS approach is computationally tractable and the corresponding results can disclose important information about the systems, which may help decision makers better understand the systems. © 2015 IEEE.}}, 
pages = {643--654}, 
number = {NA}, 
volume = {2016-February}
}
@article{10.1093/jjfinec/nbi005, 
year = {2005}, 
title = {{Evaluating interest rate covariance models within a value-at-risk framework}}, 
author = {Ferreira, M A}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbi005}, 
abstract = {{A key component of managing international interest rate portfolios is forecasts of the covariances between national interest rates and accompanying exchange rates. How should portfolio managers choose among the large number of covariance forecasting models available? We find that covariance matrix forecasts generated by models incorporating interest-rate level volatility effects perform best with respect to statistical loss functions. However, within a value-at-risk (VaR) framework, the relative performance of the covariance matrix forecasts depends greatly on the VaR distributional assumption, and forecasts based just on weighted averages of past observations perform best. In addition, portfolio variance forecasts that ignore the covariance matrix generate the lowest regulatory capital charge, a key economic decision variable for commercial banks. Our results provide empirical support for the commonly used VaR models based on simple covariance matrix forecasts and distributional assumptions. © Oxford University Press 2005; all rights reserved.}}, 
pages = {126--168}, 
number = {1}, 
volume = {3}
}
@article{10.1109/sege.2015.7324617, 
year = {2015}, 
title = {{Financial tools to manage dispatchable Distributed Generation economic risks}}, 
author = {Karimi, S.A. and Rajabi-Ghahnavieh, A. and Azad, H.}, 
journal = {2015 IEEE International Conference on Smart Energy Grid Engineering (SEGE)}, 
issn = {NA}, 
doi = {10.1109/sege.2015.7324617}, 
abstract = {{Distributed Generation (DG) has received increasing attention during the last decade. Advantage and constraints of DG application are well known to both DG owner and electric utility. Various technologies are available for DG units among them gas GenSet is, in particular, more attractive to the investors as the technology provides the control on DG generation. However, there are various financial risks associated with dispatchable DG units that prohibit wide private investment in such technologies. This paper examines the use of financial tools to manage dispatchable DG economic risks. A comprehensive framework has been proposed to consider various economic risks to DG owner. Suitable models have been used to incorporate risk management tools, in particular the financial tools, in the problem. Optimal use of risk management tools has been determined based on conditional value-at-risk (CVaR) approach. The proposed approach has been applied to a test case and the effectiveness of the solution has been verified. © 2015 IEEE.}}, 
pages = {1--9}, 
number = {NA}, 
volume = {NA}
}
@article{10.5709/ce.1897-9254.230, 
year = {2017}, 
title = {{Improving value-at-risk estimation from the normal EGARCH model}}, 
author = {Gorji, Mahsa and Sajjad, Rasoul}, 
journal = {Contemporary Economics}, 
issn = {20840845}, 
doi = {10.5709/ce.1897-9254.230}, 
abstract = {{Returns in financial assets display consistent excess kurtosis and skewness, implying the presence of large fluctuations not forecasted by Gaussian models. This paper applies a resampling method based on the bootstrap and a bias-correction step to improve Value-at-Risk (VaR) forecasting ability of the n-EGARCH (normal EGARCH) model and correct the VaR for both long and short positions. Our aim is to utilize the advantages of this model, but still use the bootstrap resampling method to accurate for the tendency of the model tomiscalculate the VaR. Empirical results indicate that the bias-correction method can improve the n-GARCH and n-EGARCH VaR forecasts so much that the acquired VaR predictions are different from the proposed probability. Additionally, allowing asymmetry in the conditional variance using the EGARCH model with normal distribution instead of GARCH improves the performance of the bias-correction method in forecasting the VaR for almost all considered indices. Moreover, the bias-corrected n-EGARCH model performs better than the simple t-EGARCH model. Thus, it seems that this model can take account of both the asymmetry in the conditional variance and leptokurtosis in returns distribution. However, we find that the superiority of the bias-corrected n-EGARCH model over the t-EGARCH model is not completely confirmed for short positions based on the censored likelihood scoring rule. © 2017, University of Finance and Management in Warsaw. All rights reserved.}}, 
pages = {91--106}, 
number = {1}, 
volume = {11}
}
@article{10.2202/1558-3708.1645, 
year = {2009}, 
title = {{Mixed exponential power asymmetric conditional heteroskedasticity}}, 
author = {Rombouts, Jeroen V. K. and Bouaddi, Mohammed}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.2202/1558-3708.1645}, 
abstract = {{To match the stylized facts of high frequency financial time series precisely and parsimoniously, this paper presents a finite mixture of conditional exponential power distributions where each component exhibits asymmetric conditional heteroskedasticity. We provide weak stationarity conditions and unconditional moments to the fourth order. We apply this new class to Dow Jones index returns. We find that a two-component mixed exponential power distribution dominates mixed normal distributions with more components, and more parameters, both in-sample and out-of-sample. In contrast to mixed normal distributions, all the conditional variance processes become stationary. This happens because the mixed exponential power distribution allows for component-specific shape parameters so that it can better capture the tail behaviour. Therefore, the more general new class has attractive features over mixed normal distributions in our application: less components are necessary and the conditional variances in the components are stationary processes. Results on NASDAQ index returns are similar. Copyright © 2009 The Berkeley Electronic Press. All rights reserved.}}, 
number = {3}, 
volume = {13}
}
@article{10.2308/accr.2002.77.4.911, 
year = {2002}, 
title = {{How informative are value-at-risk disclosures?}}, 
author = {Jorion, Philippe}, 
journal = {The Accounting Review}, 
issn = {00014826}, 
doi = {10.2308/accr.2002.77.4.911}, 
abstract = {{Value at Risk (VAR), a measure of the dollar amount of potential loss from adverse market moves, has become a standard benchmark for measuring financial risk. Spurred by regulators and competitive pressures, more institutions are, reporting VAR numbers in annual and quarterly financial reports. To provide preliminary evidence on the informativeness of these new disclosures, I investigate the relation between the trading VAR disclosed by a small sample of U.S. commercial banks and the subsequent variability of their trading revenues. The empirical results suggest that VAR disclosures are informative in that they predict the variability of trading revenues. Thus, analysts and investors can use VAR disclosures to compare, the risk profiles of banks' trading portfolios.}}, 
pages = {911--931}, 
number = {4}, 
volume = {77}
}
@article{10.1016/j.jhazmat.2004.06.004, 
year = {2004}, 
title = {{Making the business case for process safety using value-at-risk concepts}}, 
author = {Fang, Jayming S. and Ford, David M. and Mannan, M.Sam}, 
journal = {Journal of Hazardous Materials}, 
issn = {03043894}, 
doi = {10.1016/j.jhazmat.2004.06.004}, 
pmid = {15518960}, 
abstract = {{An increasing emphasis on chemical process safety over the last two decades has led to the development and application of powerful risk assessment tools. Hazard analysis and risk evaluation techniques have developed to the point where quantitatively meaningful risks can be calculated for processes and plants. However, the results are typically presented in semi-quantitative "ranked list" or "categorical matrix" formats, which are certainly useful but not optimal for making business decisions. A relatively new technique for performing valuation under uncertainty, value at risk (VaR), has been developed in the financial world. VaR is a method of evaluating the probability of a gain or loss by a complex venture, by examining the stochastic behavior of its components. We believe that combining quantitative risk assessment techniques with VaR concepts will bridge the gap between engineers and scientists who determine process risk and business leaders and policy makers who evaluate, manage, or regulate risk. We present a few basic examples of the application of VaR to hazard analysis in the chemical process industry.}}, 
pages = {17--26}, 
number = {1-3 SPEC. ISS.}, 
volume = {115}
}
@article{10.1080/14697688.2016.1142671, 
year = {2016}, 
title = {{Model risk of contingent claims}}, 
author = {Detering, Nils and Packham, Natalie}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2016.1142671}, 
abstract = {{Paralleling regulatory developments, we devise value-at-risk and expected shortfall type risk measures for the potential losses arising from using misspecified models when pricing and hedging contingent claims. Essentially, P\&L from model risk corresponds to P\&L realized on a perfectly hedged position. Model uncertainty is expressed by a set of pricing models, each of which represents alternative asset price dynamics to the model used for pricing. P\&L from model risk is determined relative to each of these models. Using market data, a unified loss distribution is attained by weighing models according to a likelihood criterion involving both calibration quality and model parsimony. Examples demonstrate the magnitude of model risk and corresponding capital buffers necessary to sufficiently protect trading book positions against unexpected losses from model risk. A further application of the model risk framework demonstrates the calculation of gap risk of a barrier option when employing a semi-static hedging strategy. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--18}, 
number = {9}, 
volume = {16}
}
@article{10.1016/j.insmatheco.2015.12.007, 
year = {2016}, 
title = {{Insights to systematic risk and diversification across a joint probability distribution}}, 
author = {Choo, Weihao and Jong, Piet de}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.12.007}, 
abstract = {{This paper analyses and develops insights to systematic risk and diversification when random, imperfectly dependent, losses are aggregated. Systematic risk and diversification are shown to vary across layers of component losses according to local dependence and volatility structures. Systematic risk is high and diversification is weak overall if high risk layers are heavily dependent on the aggregate loss. This result explains weak diversification observed in financial markets despite weak to moderate correlations overall. A coherent risk setup is assumed in this paper, where risks are measured using distortion and allocated using the Euler principle. © 2016 Elsevier B.V.}}, 
pages = {142--150}, 
number = {NA}, 
volume = {67}
}
@article{10.21314/jor.2015.305, 
year = {2015}, 
title = {{Advanced risk profile analysis of Islamic equity investment: Evidence from the American, Asian and European markets}}, 
author = {Bellalah, Mondher and Chayeh, Zeineb}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.305}, 
abstract = {{As far as asset and liability management is concerned, the advanced analysis of the risk and return profile of Islamic financing and investment products today constitutes one of the greatest challenges for Islamic financial institutions. Given the fact that, by design, Islamic financial institutions should be keeping a handsome portion of their assets in equity investments, this paper brings out the dynamics of a specific risk profile inherent in Islamic equity investment based on investing in the sharia-compliant stock market. Therefore, we investigate three Islamic equity indexes, classified by economic hubs (Dow Jones Europe, Asia/Pacific and United States), against their conventional peers from 2003 to 2009. Our risk profile analysis is based on the estimation of two well-known advanced risk measures, namely value-at-risk (VaR) and expected shortfall (ES). FourVaR methodologies are used: historicalVaR, RiskMetrics, generalized autoregressive conditional heteroscedasticity conditional VaR models and the Cornish-Fisher VaR model. The models’ assessments, based on the Kupiec (1995) test, allow us to better investigate the equity indexes’ daily returns tails through the estimation of the generalized Pareto distributionVaR and ES measures. The end result of our advanced risk profile analysis is that Islamic equity indexes seem to depict a regional, as well as a periodic, character risk profile. Drawing from the specific features of the Dow Jones Islamic indexes and their compounds’ inherent features, this paper argues that sharia screening, which alters the composition of Islamic indexes, contributes to the shaping of a distinguished risk profile. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {73--99}, 
number = {6}, 
volume = {17}
}
@article{10.1111/j.1539-6924.2012.01824.x, 
year = {2012}, 
title = {{Quantile Uncertainty and Value-at-Risk Model Risk}}, 
author = {Alexander, Carol and Sarabia, José María}, 
journal = {Risk Analysis}, 
issn = {02724332}, 
doi = {10.1111/j.1539-6924.2012.01824.x}, 
pmid = {22594633}, 
abstract = {{This article develops a methodology for quantifying model risk in quantile risk estimates. The application of quantile estimates to risk assessment has become common practice in many disciplines, including hydrology, climate change, statistical process control, insurance and actuarial science, and the uncertainty surrounding these estimates has long been recognized. Our work is particularly important in finance, where quantile estimates (called Value-at-Risk) have been the cornerstone of banking risk management since the mid 1980s. A recent amendment to the Basel II Accord recommends additional market risk capital to cover all sources of "model risk" in the estimation of these quantiles. We provide a novel and elegant framework whereby quantile estimates are adjusted for model risk, relative to a benchmark which represents the state of knowledge of the authority that is responsible for model risk. A simulation experiment in which the degree of model risk is controlled illustrates how to quantify Value-at-Risk model risk and compute the required regulatory capital add-on for banks. An empirical example based on real data shows how the methodology can be put into practice, using only two time series (daily Value-at-Risk and daily profit and loss) from a large bank. We conclude with a discussion of potential applications to nonfinancial risks. © 2012 Society for Risk Analysis.}}, 
pages = {1293--1308}, 
number = {8}, 
volume = {32}
}
@article{10.1504/gber.2016.073305, 
year = {2016}, 
title = {{Expected shortfall and tail conditional expectation with the Pearson type IV distribution}}, 
author = {Stavroyiannis, Stavros}, 
journal = {Global Business and Economics Review}, 
issn = {10974954}, 
doi = {10.1504/gber.2016.073305}, 
abstract = {{The Basel Committee of Banking Supervision has used value-at-risk as a measure of market risk in the trading book for two decades in several accords. After the global financial crisis of 2008-2009 value-at-risk was criticised on the grounds that as a measure of risk is not sub-additive, in the sense that it behaves very erratically when banks or regulators try to aggregate compartmentalised risk across all branches of a large diverse bank. Expected shortfall emerged as a natural alternative of value-at-risk fulfilling all four axioms of a coherent risk measure and belongs to the category of spectral risk measures which are not elicitable unless they reduce to minus the expected value. Another critical issue is that the Basel committee indicates that risk managers should conduct the risk measurement tests based on a dataset which is very small for high confidence levels calculations. In this work we calculate and apply the theoretical tail conditional expectation of the standardised Pearson type IV distribution. This is a coherent measure of risk and assuming it describes appropriately the data generation process it can provide the risk manager with reliable results. Copyright © 2016 Inderscience Enterprises Ltd.}}, 
pages = {41}, 
number = {1}, 
volume = {18}
}
@article{10.1016/j.physa.2006.10.004, 
year = {2007}, 
title = {{Portfolio management under sudden changes in volatility and heterogeneous investment horizons}}, 
author = {Fernandez, Viviana and Lucey, Brian M.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2006.10.004}, 
abstract = {{We analyze the implications for portfolio management of accounting for conditional heteroskedasticity and sudden changes in volatility, based on a sample of weekly data of the Dow Jones Country Titans, the CBT-municipal bond, spot and futures prices of commodities for the period 1992-2005. To that end, we first proceed to utilize the ICSS algorithm to detect long-term volatility shifts, and incorporate that information into PGARCH models fitted to the returns series. At the next stage, we simulate returns series and compute a wavelet-based value at risk, which takes into consideration the investor's time horizon. We repeat the same procedure for artificial data generated from semi-parametric estimates of the distribution functions of returns, which account for fat tails. Our estimation results show that neglecting GARCH effects and volatility shifts may lead to an overestimation of financial risk at different time horizons. In addition, we conclude that investors benefit from holding commodities as their low or even negative correlation with stock and bond indices contribute to portfolio diversification. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {612--624}, 
number = {2}, 
volume = {375}
}
@article{10.14716/ijtech.v6i5.1764, 
year = {2015}, 
title = {{Risk impact analysis on the investment of drinking water supply system development using project risk management}}, 
author = {Hidayatno, Akhmad and Moeis, Armand Omar and Sutrisno, Aziiz and Maulidiah, Wulan}, 
journal = {International Journal of Technology}, 
issn = {20869614}, 
doi = {10.14716/ijtech.v6i5.1764}, 
abstract = {{The development of a water supply system requires a high investment cost, and financial, environmental, and institutional aspects need to be considered. As major projects involving many stakeholders, drinking water supply projects become vulnerable to risks. A risk-based analysis is required to reduce the likelihood of failure in both the operational and financial aspects of such projects. This study describes the process of risk management planning for a drinking water supply system construction project in South Bali. The case study is based on the project risk management method with the value at risk to calculate the impact of risks in project investment. The purpose of this study is to obtain a financial risk model that maps potential risk factors and calculates the financial impact of risks on the project. This is used to create alternative strategies to reduce the impact of risks on investment made during the development of the project. The analysis showed that of the three priority risk factors, production capacity has the greatest influence on the net present value of the project. © IJTech 2015.}}, 
pages = {894--904}, 
number = {5}, 
volume = {6}
}
@article{10.1134/s000511791907004x, 
year = {2019}, 
title = {{Risk Management in Hierarchical Games with Random Factors}}, 
author = {Gorelov, M. A.}, 
journal = {Automation and Remote Control}, 
issn = {00051179}, 
doi = {10.1134/s000511791907004x}, 
abstract = {{A game-theoretic model of the Principal-agent type is considered, in which the result of an agent’s activity depends not only on his/her choice but also on some random factor. The Principal is assumed to choose the total probability of all negative events that he/she will exclude from consideration; in the other respects, he/she is cautious. The structure of the Principal’s optimal strategies is found. Two models differing by the Principal’s awareness of the partner’s actions are studied. © 2019, Pleiades Publishing, Ltd.}}, 
pages = {1265--1278}, 
number = {7}, 
volume = {80}
}
@article{10.1080/03610926.2016.1242740, 
year = {2017}, 
title = {{The effect of dependence on distribution of the functions of random variables}}, 
author = {Dolati, Ali and Roozegar, Rasool and Ahmadi, Najmeh and Shishebor, Zohreh}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2016.1242740}, 
abstract = {{In this article, we study the effect of dependence on the distributional properties of functions of two random variables. Expressions for the cumulative distribution functions of the linear combinations, products, and ratios of two dependent random variables in terms of their associated copula are derived. We discuss the effect of dependence on quantities such as the variances of linear combinations of functions, the value-at-risk measure, and the stress–strength parameter. Several examples, a simulation study, and a real data analysis are provided to illustrate the result. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {10704--10717}, 
number = {21}, 
volume = {46}
}
@article{10.1109/icsssm.2011.5959412, 
year = {2011}, 
title = {{Multiple-day VaR calculating rules in financial risk management}}, 
author = {Zhou, Pengpeng and Tian, Maozai}, 
journal = {ICSSSM11}, 
issn = {NA}, 
doi = {10.1109/icsssm.2011.5959412}, 
abstract = {{In financial risk management, the Value-at-Risk (VaR) is a commonly used measure of asset's short period market risk. General external risk indicator for multiple-day VaR, proposed by The Basel Committee on Banking Supervision (1996, 2003), is the square-root-of-time (SQRT) rule which is derived based on the Gaussian return assumption. Danielsson and Vries (1997), Dacorogna, Muller, Pictet and Vries (2001) pointed out the risks and drawbacks by using the SQRT rule and proposed a-root rule which is derived from the heavy-tailed distributions. In this paper, the authors derive and conduct a research on a scaling rule based on some more generalized tail which also has heavier tail than Gaussian. An example by using the property of Normal Inverse Gaussian distribution which is called NIG rule is shown to be a more stable indicator to regulate market risk for external risk regulators. The results of numerical calculation with nancial data are described and a comparative study by using all three rules are included. It is expected the research undertaken on scaling rules and its realization can provide practical reference to nancial risk supervision departments. © 2011 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2008.12.014, 
year = {2009}, 
title = {{The dynamics of the volatility skew: A Kalman filter approach}}, 
author = {Bedendo, Mascia and Hodges, Stewart D.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2008.12.014}, 
abstract = {{Much attention has been devoted to understanding and modeling the dynamics of implied volatility curves and surfaces. This is crucial for both trading, pricing and risk management of option positions. We suggest a simple, yet flexible, model, based on a discrete and linear Kalman filter updating of the volatility skew. From a risk management perspective, we assess whether this model is capable of producing good density forecasts of daily returns on a number of option portfolios. We also compare our model to the sticky-delta and the vega-gamma alternatives. We find that it clearly outperforms both alternatives, given its ability to easily account for movements of different nature in the volatility curve. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {1156--1165}, 
number = {6}, 
volume = {33}
}
@article{10.1109/icee.2010.1070, 
year = {2010}, 
title = {{Portfolio model based on CVaR under friction market}}, 
author = {Gao, Yuelin and Wang, Bo and An, Xiaohui}, 
journal = {2010 International Conference on E-Business and E-Government}, 
issn = {NA}, 
doi = {10.1109/icee.2010.1070}, 
abstract = {{Taking into more account practical situation of Chinese securities market with non-convex non-concave typical transaction cost and revenue and measuring the risk of the securities by the conditional value at risk, we establish a portfolio model in which the expected income of portfolio is taken as objective function and Conditional Value-at-Risk is taken as a constraint. The model is solved by a particle swarm optimization algorithm based on punished function. It is shown by the numerical results of six stocks in Hu and Shen market that the proposed model is rational. © 2010 IEEE.}}, 
pages = {4260--4263}, 
number = {NA}, 
volume = {NA}
}
@article{10.1088/1742-6596/1811/1/012034, 
year = {2021}, 
title = {{Evaluating the stock volatility of service companies in the rise of COVID-19 using ARMAX-GARCHX}}, 
author = {Putera, Muhammad Luthfi Setiarno}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1811/1/012034}, 
abstract = {{This study evaluates the impact of Covid-19 variables which has attracted huge attention from whole countries. The service companies listed in IDX are being the main issue while many are struggling amidst the economy slowdown due to pandemy. By comparing the time series regression (TSR) - GARCHX and ARMAX - GARCHX of stock return of 4 companies listed in service sub-sector since early March 2020, proposed GARCHX models are able to assess the volatility of stock return. As a proxy of the volatility, Value-at-Risk (VAR) is analyzed at 1\% and 5\% quantiles. It is found that ARMAX-GARCHX is more accurate than TSR-GARCHX in evaluating the volatility in lower risk, particularly that of medical service stock. Through ARMAX-GARCHX model, it is indicated that JCI return and lagged Covid-19 daily death cases have significant impact to the stock volatility, particularly that of transportation and medical service companies. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012034}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2006.06.001, 
year = {2007}, 
title = {{An application of comonotonicity and convex ordering to present values with truncated stochastic interest rates}}, 
author = {Koch, Inge and Schepper, Ann De}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2006.06.001}, 
abstract = {{A subject often recurring in recent financial and actuarial research is the investigation of present value functions with stochastic interest rates. Only in the case of uncomplicated payment streams and rather basic interest rate models is an exact analytical result for the distribution function available. In the present contribution, we introduce the concept of truncated stochastic interest rates, useful to adapt general stochastic models to specific financial requirements, and we show how to obtain analytical results for bounds for the present value. We elaborate our method in extension for the Hull and White model and related models. We illustrate the accuracy of the approximations graphically, and we use the bounds to estimate the Value-at-Risk. © 2006 Elsevier Ltd. All rights reserved.}}, 
pages = {386--402}, 
number = {3}, 
volume = {40}
}
@article{10.4067/s0718-52862014000100001, 
year = {2014}, 
title = {{Thinly traded securities and risk management [Activos con baja frecuencia de transacciones y manejo de riesgo]}}, 
author = {Bernales, Alejandro and Beuermann, Diether W and Cortazar, Gonzalo}, 
journal = {Estudios de economía}, 
issn = {03042758}, 
doi = {10.4067/s0718-52862014000100001}, 
abstract = {{Thinly traded securities exist in both emerging and well developed markets. However, plausible estimations of market risk measures for portfolios with infrequently traded securities have not been explored in the literature. We propose a methodology to calculate market risk measures based on the Kalman filter which can be used on incomplete datasets. We implement our approach in a fixed-income portfolio within a thin trading environment. However, a similar approach may be also applied to other markets with thinly traded securities. Our methodology provides reliable market risk measures in portfolios with infrequent trading.}}, 
pages = {5--48}, 
number = {1}, 
volume = {41}
}
@article{10.1007/s11156-011-0256-x, 
year = {2012}, 
title = {{Alternative statistical distributions for estimating value-at-risk: Theory and evidence}}, 
author = {Lee, Cheng-Few and Su, Jung-Bin}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-011-0256-x}, 
abstract = {{A number of applications presume that asset returns are normally distributed, even though they are widely known to be skewed leptokurtic and fat-tailed and excess kurtosis. This leads to the underestimation or overestimation of the true value-at-risk (VaR). This study utilizes a composite trapezoid rule, a numerical integral method, for estimating quantiles on the skewed generalized t distribution (SGT) which permits returns innovation to flexibly treat skewness, leptokurtosis and fat tails. Daily spot prices of the thirteen stock indices in North America, Europe and Asia provide data for examining the one-day-ahead VaR forecasting performance of the GARCH model with normal, student's t and SGT distributions. Empirical results indicate that the SGT provides a good fit to the empirical distribution of the log-returns followed by student's t and normal distributions. Moreover, for all confidence levels, all models tend to underestimate real market risk. Furthermore, the GARCH-based model, with SGT distributional setting, generates the most conservative VaR forecasts followed by student's t and normal distributions for a long position. Consequently, it appears reasonable to conclude that, from the viewpoint of accuracy, the influence of both skewness and fat-tails effects (SGT) is more important than only the effect of fat-tails (student's t) on VaR estimates in stock markets for a long position. © 2011 Springer Science+Business Media, LLC.}}, 
pages = {309--331}, 
number = {3}, 
volume = {39}
}
@article{10.21511/imfi.15(4).2018.02, 
year = {2018}, 
title = {{An evaluation and comparison of value at risk and expected shortfall}}, 
author = {Burdorf, Tom and Vuuren, Gary van}, 
journal = {Investment Management and Financial Innovations}, 
issn = {18104967}, 
doi = {10.21511/imfi.15(4).2018.02}, 
abstract = {{As a risk measure, Value at Risk (VaR) is neither sub-additive nor coherent. These drawbacks have coerced regulatory authorities to introduce and mandate Expected Shortfall (ES) as a mainstream regulatory risk management metric. VaR is, however, still needed to estimate the tail conditional expectation (the ES): the average of losses that are greater than the VaR at a significance level. These two risk measures behave quite differently during growth and recession periods in developed and emerging economies. Using equity portfolios assembled from securities of the banking and retail sectors in the UK and South Africa, historical, variance-covariance and Monte Carlo approaches are used to determine VaR (and hence ES). The results are back-tested and compared, and normality assumptions are tested. Key findings are that the results of the variance covariance and the Monte Carlo approach are more consistent in all environments in comparison to the historical outcomes regardless of the equity portfolio regarded. The industries and periods analyzed influenced the accuracy of the risk measures; the different economies did not. © Tom Burdorf, Gary van Vuuren, 2018}}, 
pages = {17--34}, 
number = {4}, 
volume = {15}
}
@article{10.1109/ias44978.2020.9334895, 
year = {2020}, 
title = {{A Novel Hierarchical Demand Response Strategy for Residential Microgrid with Time-Varying Price}}, 
author = {Zhang, Zhenyuan and Huang, Yuxiang and Huang, Qi and Lee, Wei-Jen}, 
journal = {2020 IEEE Industry Applications Society Annual Meeting}, 
issn = {NA}, 
doi = {10.1109/ias44978.2020.9334895}, 
abstract = {{With the increasing proportion of distributed energy resources (DER) penetrated into the distribution power systems, Microgrids are widely used in the operation of power system. Renewable energy generation is known to be uncertain and volatile. Incentive-Based Demand Response (IBDR) is a good option to reduce the effect of this drawbacks because it has better timeliness and lower cost than other method. In this paper, a versatile hierarchical demand response strategy is proposed. With the determination of hierarchical incentive coefficients and the corresponding response quantities, the rebound effect considered dynamic DR decision-making strategy is established to optimize the total costs of residents. In order to verify the effectiveness of the strategy in cost saving, Monte Carlo algorithm is used for verification, where the optimization rate under the fluctuant price and stochastic power vacancy is calculated. Besides, two types of participating users were considered in the response results, and the confidence intervals for the response quantities under different price were calculated. The result shows the effectiveness of this strategy in reducing cost while the deficiency of DER occurs. © 2020 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-030-18553-4_50, 
year = {2019}, 
title = {{What Value-at-Risk and Expected Shortfall Metrics Tell a Risk Averse Investor in Cryptocurrencies}}, 
author = {Kopytin, I. and Maslennikov, A. and Zhukov, S.}, 
journal = {Smart Innovation, Systems and Technologies}, 
issn = {21903018}, 
doi = {10.1007/978-3-030-18553-4\_50}, 
abstract = {{The article aims to quantify relative attractiveness of investing into cryptocurrencies for a risk averse investor with standard measures of risk. As market capitalization of cryptocurrencies reached a record \$823 billion in January 2018 they could be considered to represent a new investable asset class. Introduction of Bitcoin futures contracts on the Chicago Board Options Exchange (Cboe) and the Chicago Mercantile Exchange (CME) in December 2017 gave large institutional investors such as hedge funds and mutual funds opportunities to enter the cryptocurrencies market in order to diversify investment portfolios and/or to gain exposure to potentially undervalued assets. CFTC’s qualification of cryptocurrencies as commodities makes analysis of market risk associated with them even more necessary. Having compared non-parametric historical one-day Value-at-Risk and Expected Shortfall metrics for 283 cryptocurrencies with that of traditional asset classes (equities, bonds, currencies and commodity futures) we show that the least volatile cryptocurrency Bitcoin is almost twice as risky as the most volatile traditional asset, i.e. natural gas. Risk averse investors can lower market risk of Ethereum and Ripple by investing in portfolio of most valuable digital currencies. By tightness of returns correlations 8 most valuable cryptocurrencies form three clusters. Combining Bitcoin, which with Litecoin comprises a separate cluster, with Dash and Stellar from other clusters could reduce the capitalization-weighted portfolio’s VaR below Bitcoin’s VaR level. However, none of the possible combinations of Bitcoin with cryptocurrencies from two other clusters could reduce ES metric below that of Bitcoin itself. Stellar and Monero have the most prominent footprint in the crypto-currencies space after Bitcoin. Bitcoin – Monero – Stellar capitalization-weighted portfolio is less risky than Bitcoin in terms of VaR. © 2019, Springer Nature Switzerland AG.}}, 
pages = {398--410}, 
number = {NA}, 
volume = {139}
}
@article{10.1080/03461238.2015.1119717, 
year = {2017}, 
title = {{Reduction of Value-at-Risk bounds via independence and variance information}}, 
author = {Puccetti, Giovanni and Rüschendorf, Ludger and Small, Daniel and Vanduffel, Steven}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2015.1119717}, 
abstract = {{We derive lower and upper bounds for the Value-at-Risk of a portfolio of losses when the marginal distributions are known and independence among (some) subgroups of the marginal components is assumed. We provide several actuarial examples showing that the newly proposed bounds strongly improve those available in the literature that are based on the sole knowledge of the marginal distributions. When the variance of the joint portfolio loss is small enough, further improvements can be obtained. © 2015 Taylor \& Francis.}}, 
pages = {1--22}, 
number = {3}, 
volume = {2017}
}
@article{10.1109/icsssm.2005.1500194, 
year = {2005}, 
title = {{Calculation method for portfolio's value at risk based on principal factor analysis}}, 
author = {Li, Sanping and Xu, Chengxian and Xue, Honggang}, 
journal = {Proceedings of ICSSSM '05. 2005 International Conference on Services Systems and Services Management, 2005.}, 
issn = {NA}, 
doi = {10.1109/icsssm.2005.1500194}, 
abstract = {{In this paper, we propose principle factor analysis method to reduce the dimensions of a high dimensional random vector in calculating portfolio's Value at Risk. The theoretical foundation, algorithm and numerical example of the method are given. This method outperforms the principle component analysis method. Especially, the advantages of the method are marked, while the factors F's multicollinearity is serious. © 2005 IEEE.}}, 
pages = {1233--1236}, 
number = {NA}, 
volume = {2}
}
@article{10.1142/s0219024915500545, 
year = {2015}, 
title = {{The Stress-Dependent Random Walk}}, 
author = {GREMM, MARTIN}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024915500545}, 
eprint = {1310.4538}, 
abstract = {{A log-normal random walk with parameters that are functions of market stress naturally accounts for volatility clustering and fat-tailed return distributions. Fitting this model to a stock and a bond index we find no evidence of significant misspecification despite the fact that the model has no adjustable parameters. This model can be interpreted as a stochastic volatility model without latent variables. We obtain a closed-form expression for the Value at Risk (VaR) that accommodates returns of any magnitude and discuss several other applications. © 2015 World Scientific Publishing Company.}}, 
pages = {1550054}, 
number = {8}, 
volume = {18}
}
@article{10.1080/14697688.2017.1298831, 
year = {2017}, 
title = {{Dynamic mean–VaR portfolio selection in continuous time}}, 
author = {Zhou, Ke and Gao, Jiangjun and Li, Duan and Cui, Xiangyu}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2017.1298831}, 
abstract = {{The value-at-risk (VaR) is one of the most well-known downside risk measures due to its intuitive meaning and wide spectra of applications in practice. In this paper, we investigate the dynamic mean–VaR portfolio selection formulation in continuous time, while the majority of the current literature on mean–VaR portfolio selection mainly focuses on its static versions. Our contributions are twofold, in both building up a tractable formulation and deriving the corresponding optimal portfolio policy. By imposing a limit funding level on the terminal wealth, we conquer the ill-posedness exhibited in the original dynamic mean–VaR portfolio formulation. To overcome the difficulties arising from the VaR constraint and no bankruptcy constraint, we have combined the martingale approach with the quantile optimization technique in our solution framework to derive the optimal portfolio policy. In particular, we have characterized the condition for the existence of the Lagrange multiplier. When the opportunity set of the market setting is deterministic, the portfolio policy becomes analytical. Furthermore, the limit funding level not only enables us to solve the dynamic mean–VaR portfolio selection problem, but also offers a flexibility to tame the aggressiveness of the portfolio policy. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--13}, 
number = {10}, 
volume = {17}
}
@article{10.2174/1874110x01408010896, 
year = {2014}, 
title = {{Estimating value-at-risk in electricity market based on grey extreme value theory}}, 
author = {Wang, Ruiqing}, 
journal = {The Open Cybernetics \& Systemics Journal}, 
issn = {1874110X}, 
doi = {10.2174/1874110x01408010896}, 
abstract = {{How to effectively evaluate price of volatility risk is the basis of risk management in electricity market. Electricity price connotes a grey system, due to uncertainty and incomplete information for partial external or inner parameters. A two-stage model for estimating value-at-risk based on grey system and extreme value theory is proposed. Firstly, in order to capture the dependencies, seasonalities and volatility-clustering, a GM(1,2) model is used to filter electricity price series. In this way, an approximately independently and identically distributed residual series with better statistical properties is acquired. Then extreme value theory is adopted to explicitly model the tails of the residuals of GM(1,2) model, and accurate estimates of electricity market value-at-risk can be produced. The empirical analysis based on the historical data of the PJM electricity market shows that the proposed model can be rapidly reflect the most recent and relevant changes of electricity prices and can produce accurate forecasts of value-at-risk at all confidence levels, and the computational cost is far less than the existing two-stage value-at-risk estimating models, further improving the ability of risk management for electricity market participants. © Ruiqing Wang.}}, 
pages = {896--903}, 
number = {NA}, 
volume = {8}
}
@article{10.1016/j.insmatheco.2017.07.003, 
year = {2017}, 
title = {{Optimal insurance design in the presence of exclusion clauses}}, 
author = {Chi, Yichun and Liu, Fangda}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2017.07.003}, 
abstract = {{The present work studies the design of an optimal insurance policy from the perspective of an insured, where the insurable loss is mutually exclusive from another loss that is denied in the insurance coverage. To reduce ex post moral hazard, we assume that both the insured and the insurer would pay more for a larger realization of the insurable loss. When the insurance premium principle preserves the convex order, we show that any admissible insurance contract is suboptimal to a two-layer insurance policy under the criterion of minimizing the insured's total risk exposure quantified by value at risk, tail value at risk or an expectile. The form of optimal insurance can be further simplified to be one-layer by imposing an additional weak condition on the premium principle. Finally, we use Wang's premium principle and the expected value premium principle to illustrate the applicability of our results, and find that optimal insurance solutions are affected not only by the size of the excluded loss but also by the risk measure chosen to quantify the insured's risk exposure. © 2017 Elsevier B.V.}}, 
pages = {185--195}, 
number = {NA}, 
volume = {76}
}
@article{10.1016/s0895-7177(01)00128-5, 
year = {2001}, 
title = {{The impact of stationarity assessment on studies of volatility and value-at-risk}}, 
author = {Leśkow, J.}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/s0895-7177(01)00128-5}, 
abstract = {{Recent research on volatility of asset returns demonstrates that model innovations frequently show unconditional heteroscedasticity. On the other hand, ARMA-GARCH models incorporate the heteroscedasticity only in the conditional distribution of the innovations, assuming the unconditional distributions to be stationary (see, e.g., [1,2]). Given the observed unconditional heteroscedasticity of the return innovations [3], there is a need to overcome this shortcoming of existing models. The purpose of this paper is to introduce a test of stationarity of the innovations and show its impact in the analysis of volatility and value at risk. The methodological results are accompanied with examples and simulations. © 2001 Elsevier Science Ltd. All rights reserved.}}, 
pages = {1213--1222}, 
number = {9-11}, 
volume = {34}
}
@article{10.1007/s40953-020-00197-w, 
year = {2020}, 
title = {{Value-at-Risk in the Presence of Structural Breaks Using Unbiased Extreme Value Volatility Estimator}}, 
author = {Kumar, Dilip}, 
journal = {Journal of Quantitative Economics}, 
issn = {09711554}, 
doi = {10.1007/s40953-020-00197-w}, 
abstract = {{We provide a framework based on the unbiased extreme value volatility estimator to predict long and short position value-at-risk (VaR). The given framework incorporates the impact of asymmetry, structural breaks and fat tails in volatility. We generate forecasts of long and short position VaR for the cases when future structural breaks are known as well as unknown. We evaluate its VaR forecasting performance using various backtesting approaches for both long and short positions and compare the results with that from return based models. Our findings indicate that incorporating the impact of structural breaks in volatility indeed improves the accuracy of VaR forecasts of the proposed framework. © 2020, The Indian Econometric Society.}}, 
pages = {587--610}, 
number = {3}, 
volume = {18}
}
@article{10.1007/s10589-005-2059-2, 
year = {2005}, 
title = {{Treasury management model with foreign exchange exposure}}, 
author = {Volosov, Konstantin and Mitra, Gautam and Spagnolo, Fabio and Lucas, Cormac}, 
journal = {Computational Optimization and Applications}, 
issn = {09266003}, 
doi = {10.1007/s10589-005-2059-2}, 
abstract = {{In this paper we formulate a model for foreign exchange exposure management and (international) cash management taking into consideration random fluctuations of exchange rates. A vector error correction model (VECM) is used to predict the random behaviour of the forward as well as spot rates connecting dollar and sterling. A two-stage stochastic programming (TWOSP) decision model is formulated using these random parameter values. This model computes currency hedging strategies, which provide rolling decisions of how much forward contracts should be bought and how much should be liquidated. The model decisions are investigated through ex post simulation and backtesting in which value at risk (VaR) for alternative decisions are computed. The investigation (a) shows that there is a considerable improvement to "spot only" strategy, (b) provides insight into how these decisions are made and (c) also validates the performance of this model. © 2005 Springer Science + Business Media, Inc.}}, 
pages = {179--207}, 
number = {1-2}, 
volume = {32}
}
@article{10.1016/j.jbankfin.2005.05.024, 
year = {2006}, 
title = {{A credit risk model for large dimensional portfolios with application to economic capital}}, 
author = {Nyström, Kaj and Skoglund, Jimmy}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.05.024}, 
abstract = {{In this paper we develop a multi-period and multi-state portfolio credit risk model which is applicable to large dimensional portfolios like for example retail and mortgage portfolios. The model includes a methodology for estimation and simulation of systematic transition risk through a model for stochastic migration, a methodology for the modelling of recoveries in the case of stochastic collaterals as well as an approach to dimension reduction of the portfolio. One important application of our model is economic capital (EC) and a concept of EC based on the analogy with classical risk theory is introduced and the questions of allocation as well as risk-adjusted pricing based on the allocation of EC are structured and described. The model is illustrated by an extensive numerical example giving a concretization of the model as well as of several of the concepts introduced. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {2163--2197}, 
number = {8}, 
volume = {30}
}
@article{10.1016/j.jempfin.2010.05.004, 
year = {2011}, 
title = {{Skewness and leptokurtosis in GARCH-typed VaR estimation of petroleum and metal asset returns}}, 
author = {Cheng, Wan-Hsiu and Hung, Jui-Cheng}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2010.05.004}, 
abstract = {{This paper utilizes the most flexible skewed generalized t (SGT) distribution for describing petroleum and metal volatilities that are characterized by leptokurtosis and skewness in order to provide better approximations of the reality. The empirical results indicate that the forecasted Value-at-Risk (VaR) obtained using the SGT distribution provides the most accurate out-of-sample forecasts for both the petroleum and metal markets. With regard to the unconditional and conditional coverage tests, the SGT distribution produces the most appropriate VaR estimates in terms of the total number of rejections; this is followed by the nonparametric distribution, generalized error distribution (GED), and finally the normal distribution. Similarly, in the dynamic quantile test, the VaR estimates generated by the SGT and nonparametric distributions perform better than that generated by other distributions. Finally, in the superior predictive test, the SGT distribution has significantly lower capital requirements than the nonparametric distribution for most commodities. © 2010 Elsevier B.V.}}, 
pages = {160--173}, 
number = {1}, 
volume = {18}
}
@article{10.1587/transinf.e97.d.11, 
year = {2014}, 
title = {{Portfolio selection models with technical analysis-based fuzzy birandom variables}}, 
author = {LI, You and WANG, Bo and WATADA, Junzo}, 
journal = {IEICE Transactions on Information and Systems}, 
issn = {09168532}, 
doi = {10.1587/transinf.e97.d.11}, 
abstract = {{Recently, fuzzy set theory has been widely employed in building portfolio selection models where uncertainty plays a role. In these models, future security returns are generally taken for fuzzy variables and mathematical models are then built to maximize the investment profit according to a given risk level or to minimize a risk level based on a fixed profit level. Based on existing works, this paper proposes a portfolio selection model based on fuzzy birandom variables. Two original contributions are provided by the study: First, the concept of technical analysis is combined with fuzzy set theory to use the security returns as fuzzy birandom variables. Second, the fuzzy birandom Value-at-Risk (VaR) is used to build our model, which is called the fuzzy birandom VaR-based portfolio selection model (FBVaR-PSM). The VaR can directly reflect the largest loss of a selected case at a given confidence level and it is more sensitive than other models and more acceptable for general investors than conventional risk measurements. To solve the FBVaR-PSM, in some special cases when the security returns are taken for trapezoidal, triangular or Gaussian fuzzy birandom variables, several crisp equivalent models of the FBVaR-PSM are derived, which can be handled by any linear programming solver. In general, the fuzzy birandom simulation-based particle swarm optimization algorithm (FBS-PSO) is designed to find the approximate optimal solution. To illustrate the proposed model and the behavior of the FBS-PSO, two numerical examples are introduced based on investors' different risk attitudes. Finally, we analyze the experimental results and provide a discussion of some existing approaches. Copyright © 2014 The Institute of Electronics, Inf rmation and Communication Engineers.}}, 
pages = {11--21}, 
number = {1}, 
volume = {E97-D}
}
@article{10.1287/isre.1070.0143, 
year = {2008}, 
title = {{A value-at-risk approach to information security investment}}, 
author = {Wang, Jingguo and Chaudhury, Aby and Rao, H Raghav}, 
journal = {Information Systems Research}, 
issn = {10477047}, 
doi = {10.1287/isre.1070.0143}, 
abstract = {{Information security investment has been getting increasing attention in recent years. Various methods have been proposed to determine the effective level of security investment. However, traditional expected value methods (such as annual loss expectancy) cannot fully characterize the information security risk confronted by organizations, considering some extremal yet perhaps relatively rare cases in which a security failure may be critical and cause high losses. In this research note we introduce the concept of value-at-risk to measure the risk of daily losses an organization faces due to security exploits and use extreme value analysis to quantitatively estimate the value at risk. We collect a set of internal daily activity data from a large financial institution in the northeast United States and then simulate its daily losses with information based on data snapshots and interviews with security managers at the institution. We illustrate our methods using these simulated daily losses. With this approach, decision makers can make a proper investment choice based on their own risk preference instead of pursuing a solution that minimizes only the expected cost. © 2008 INFORMS.}}, 
pages = {106--120}, 
number = {1}, 
volume = {19}
}
@article{10.1016/s0378-4266(01)00226-6, 
year = {2002}, 
title = {{Modeling correlated market and credit risk in fixed income portfolios}}, 
author = {Jr, Theodore M. Barnhill and Maxwell, William F.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(01)00226-6}, 
abstract = {{Current risk assessment methodologies separate the analysis of market and credit risk and thus misestimate security and portfolio risk levels. We propose a new approach that relates financial market volatility to firm specific credit risk and integrates interest rate, interest rate spread, and foreign exchange rate risk into one overall fixed income portfolio risk assessment. Accounting for the correlation between these significant risk factors as well as portfolio diversification results in improved risk measurement and management. The methodology is shown to produce reasonable credit transition probabilities, prices for bonds with credit risk, and portfolio value-at-risk measures. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {347--374}, 
number = {2-3}, 
volume = {26}
}
@article{10.1504/ijbcrm.2015.070352, 
year = {2015}, 
title = {{The whole at a glance: Risk, information and corporate decisions}}, 
author = {Corelli, Angelo}, 
journal = {International Journal of Business Continuity and Risk Management}, 
issn = {17515254}, 
doi = {10.1504/ijbcrm.2015.070352}, 
abstract = {{The paper aims to be a guide for understanding the role of information in decision making and how this influences the decisions regarding financing a corporation. The risk approach is widely used in practice in order to establish precise boundaries in which decision plans can be constrained in order to optimise the outcome of the funding process. That is in the interest of the shareholders but also external investors who might be interested in joining determined projects. Finally, the Appendix describes in details the delta normal approach as a good value at risk methodology to assess risk. Copyright © 2015 Inderscience Enterprises Ltd.}}, 
pages = {48}, 
number = {1}, 
volume = {6}
}
@article{10.1016/j.jimonfin.2019.04.008, 
year = {2019}, 
title = {{Pricing corporate financial distress: Empirical evidence from the French stock market}}, 
author = {Mselmi, Nada and Hamza, Taher and Lahiani, Amine and Shahbaz, Muhammad}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2019.04.008}, 
abstract = {{This study examines whether financial distress, liquidity, and Value-at-Risk are sources of priced systematic risk in the stock returns of the French stock market. In particular, we investigate the explanatory power of the Fama and French (1993)model augmented by and substituted with these three risk factors for distressed and non-distressed firms. For this purpose, we construct nine portfolios composed of non-distressed firms and one portfolio consisting only of distressed firms. We find that for the portfolios of non-distressed firms, the financial distress factor is significantly priced only in the absence of the size and book-to-market factors. Not surprisingly, the financial distress is a systematic risk factor for the portfolio of distressed firms. Our findings also show that liquidity is priced for the portfolios of distressed and non-distressed firms. Furthermore, our empirical results show that only investors in the portfolios of non-distressed firms are rewarded for bearing Value-at-Risk (VaR)risk. Likewise, our findings indicate that the alternative model, constructed by substituting the Fama and French (1993)factors with the financial distress, liquidity and VaR risk factors, underperforms the Fama and French (1993)model, which, in turn, underperforms the considered augmented models. Our results provide insights both for international investors for new opportunities and for financial market supervisory authority. © 2019 Elsevier Ltd}}, 
pages = {13--27}, 
number = {NA}, 
volume = {96}
}
@article{10.1016/j.ijpe.2015.03.027, 
year = {2015}, 
title = {{Value-at-risk optimal policies for revenue management problems}}, 
author = {Koenig, Matthias and Meissner, Joern}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2015.03.027}, 
abstract = {{Consider a single-leg dynamic revenue management problem with fare classes controlled by capacity in a risk-averse setting. The revenue management strategy aims at limiting the down-side risk, and in particular, value-at-risk. A value-at-risk optimised policy offers an advantage when considering applications which do not allow for a large number of reiterations. They allow for specifying a confidence level regarding undesired scenarios. We introduce a computational method for determining policies which optimises the value-at-risk for a given confidence level. This is achieved by computing dynamic programming solutions for a set of target revenue values and combining the solutions in order to attain the requested multi-stage risk-averse policy. We reduce the state space used in the dynamic programming in order to provide a solution which is feasible and has less computational requirements. Numerical examples and comparison with other risk-sensitive approaches are discussed. © 2015 Elsevier B.V. All rights reserved.}}, 
pages = {11--19}, 
number = {NA}, 
volume = {166}
}
@article{10.1186/s13638-018-1322-z, 
year = {2019}, 
title = {{An empirical study on the relationship between China’s financial development and economic growth based on sensor technology}}, 
author = {Wang, Aidong}, 
journal = {EURASIP Journal on Wireless Communications and Networking}, 
issn = {16871472}, 
doi = {10.1186/s13638-018-1322-z}, 
abstract = {{Since the reform and opening up, China’s economic growth and financial development have made remarkable achievements. Based on the macroeconomic data from 1992 to 2012, this paper studies the relationship between China’s financial development and economic growth from the perspective of empirical analysis. Based on the value at risk model, Granger causality test is carried out on the variables of financial development and economic growth to analyze the relationship of interaction. A vector error correction model is established to estimate the parameters of each variable. The interaction between variables is further analyzed. The conclusion is that there is a mutual promotion mechanism between finance and economy in China, and economic growth can significantly promote the level of financial deepening. © 2019, The Author(s).}}, 
pages = {42}, 
number = {1}, 
volume = {2019}
}
@article{10.1080/13504851.2015.1021453, 
year = {2015}, 
title = {{Extreme value analysis of electricity demand in the UK}}, 
author = {Chan, Stephen and Nadarajah, Saralees}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2015.1021453}, 
abstract = {{For the first time, an extreme value analysis of electricity demand in the UK is provided. The analysis is based on the generalized Pareto distribution. Its parameters are allowed to vary linearly and sinusoidally with respect to time to capture patterns in the electricity demand data. The models are shown to give reasonable fits. Some useful predictions are given for the value at risk of the returns of electricity demand. © 2015 Taylor \& Francis.}}, 
pages = {1246--1251}, 
number = {15}, 
volume = {22}
}
@article{10.1007/s00780-005-0165-8, 
year = {2006}, 
title = {{Generalized deviations in risk analysis}}, 
author = {Rockafellar, R. Tyrrell and Uryasev, Stan and Zabarankin, Michael}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-005-0165-8}, 
abstract = {{General deviation measures are introduced and studied systematically for their potential applications to risk management in areas like portfolio optimization and engineering. Such measures include standard deviation as a special case but need not be symmetric with respect to ups and downs. Their properties are explored with a mind to generating a large assortment of examples and assessing which may exhibit superior behavior. Connections are shown with coherent risk measures in the sense of Artzner, Delbaen, Eber and Heath, when those are applied to the difference between a random variable and its expectation, instead of to the random variable itself. However, the correspondence is only one-to-one when both classes are restricted by properties called lower range dominance, on the one hand, and strict expectation boundedness on the other. Dual characterizations in terms of sets called risk envelopes are fully provided. © Springer-Verlag 2006.}}, 
pages = {51--74}, 
number = {1}, 
volume = {10}
}
@article{10.1504/ijbaaf.2016.080481, 
year = {2016}, 
title = {{Do the Troika's financial assistance programs reduce systemic risk? Evidence from Eurozone countries}}, 
author = {Moutsianas, Konstantinos A and Kosmidou, Kyriaki}, 
journal = {International Journal of Banking, Accounting and Finance}, 
issn = {17553830}, 
doi = {10.1504/ijbaaf.2016.080481}, 
abstract = {{The European sovereign debt crisis has forced many countries in the Eurozone to request financial assistance from the European Central Bank (ECB) and the International Monetary Fund (IMF) to prevent the contagion of the crisis to national banking systems. In the framework of providing large-scale financial rescue packages, the Troika, a unique institutional construction that consists of the ECB, the IMF and the European Commission (EU), has developed economic policies in combination with economic adjustment programs that aim to reinforce financial stability and reduce systemic risk. Our research focuses on the estimation of systemic risk in the countries that have implemented the Troika's bailout programs. The main objective is the evaluation of systemic risk in Greece, Ireland, Portugal, Cyprus and Spain 'pre' and 'post' the Troika's financial rescue programs to examine whether the Troika's bailout programs reduce systemic risk and financial fragility. Copyright © 2016 Inderscience Enterprises Ltd.}}, 
pages = {149}, 
number = {2}, 
volume = {7}
}
@article{10.1016/j.jspi.2007.01.008, 
year = {2008}, 
title = {{Statistical estimation errors of VaR under ARCH returns}}, 
author = {Taniai, Hiroyuki and Taniguchi, Masanobu}, 
journal = {Journal of Statistical Planning and Inference}, 
issn = {03783758}, 
doi = {10.1016/j.jspi.2007.01.008}, 
abstract = {{In this paper we discuss some problems of existing methods for calculating the Value-at-Risk (VaR) in ARCH setting. It should be noted that the commonly used approaches often confuse the true innovations with the empirical residuals, i.e., estimation errors for unknown ARCH parameters are ignored. We adjust this by using the asymptotics of the residual empirical process, and propose a feasible VaR which, according to the spirit of VaR, keeps the assets away from a specified risk with high confidence level. Its meaningfulness in comparison with the usual VaR will be illustrated clearly by numerical studies. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {3568--3577}, 
number = {11}, 
volume = {138}
}
@article{10.3969/j.issn.1005-3026.2015.02.030, 
year = {2015}, 
title = {{Control of VaR of rebar in inventory financing}}, 
author = {}, 
issn = {10053026}, 
doi = {10.3969/j.issn.1005-3026.2015.02.030}, 
abstract = {{Aiming at the price risk of inventory financing in steel market, the prices of rebar were selected as the sample data to calculate the risk rate and efficiency loss rate based on empirical values. The results calculated using the empirical values were compared with that using the historical simulation method based on the values at risk(VaR), and it was found that the length of sample period affected the performance of calculation result of historical simulation method. The historical simulation method was suitable for inventory financing in short term. The back testing of the calculation results shows that pledge rate determined by VaR is more effective on the control over the price risk. ©, 2015, Northeastern University. All right reserved.}}, 
number = {2}, 
volume = {36}
}
@article{10.1142/9789812709691_0015, 
year = {2007}, 
title = {{Measuring demographic uncertainty via actuarial indexes}}, 
author = {Skiadas, Christos H and Coppola, Mariarosaria and Lorenzo, Emilia Di and Orlando, Albina and Sibillo, Marilena}, 
journal = {Recent Advances in Stochastic Modeling and Data Analysis}, 
issn = {NA}, 
doi = {10.1142/9789812709691\_0015}, 
abstract = {{Aim of the paper is the analysis of the behaviour of risk filters connected to the demographic risk drivers for a portfolio of life annuities. The model, easily suitable to the rase of pension annuities, involves the evolution in time of the mortality rates, taking into account the randomness of the financial scenario. Within this context, the uncertainty in the choice of the deiriograpllic scenario is measured and the analysis is also supported by the VaR sensitivity to this risk source. © 2007 by World Scientific Publishing Co. Pte. Ltd. All rights reserved.}}, 
pages = {122--129}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-629x.2009.00294.x, 
year = {2009}, 
title = {{Transitional credit modelling and its relationship to market value at risk: An Australian sectoral perspective}}, 
author = {Allen, David E. and Powell, Robert}, 
journal = {Accounting \& Finance}, 
issn = {08105391}, 
doi = {10.1111/j.1467-629x.2009.00294.x}, 
abstract = {{Internal credit risk modelling is important for banks for the calculation of capital adequacy in terms of the Basel Accords, and for the management of sectoral exposure. We examine Credit Value at Risk (VaR), Conditional Credit Value at Risk (Credit CVaR) and the relationship between market and credit risk. Significant association is found between different Credit CVaR methods, and between market and credit risk. Simpler Credit CVaR methods are found to be viable alternatives to more complex methodology. The relationship between market and credit risk is used to develop a new model that allows banks to incorporate industry risk into transition modelling, without macroeconomic analysis. © 2009 AFAANZ.}}, 
pages = {425--444}, 
number = {3}, 
volume = {49}
}
@article{10.1007/s11425-014-4841-z, 
year = {2014}, 
title = {{Tail asymptotic expansions for L-statistics}}, 
author = {Hashorva, Enkelejd and Ling, ChengXiu and Peng, ZuoXiang}, 
journal = {Science China Mathematics}, 
issn = {16747283}, 
doi = {10.1007/s11425-014-4841-z}, 
eprint = {1402.6302}, 
abstract = {{We derive higher-order expansions of L-statistics of independent risks X1, ...,Xn under conditions on the underlying distribution function F. The new results are applied to derive the asymptotic expansions of ratios of two kinds of risk measures, stop-loss premium and excess return on capital, respectively. Several examples and a Monte Carlo simulation study show the efficiency of our novel asymptotic expansions. © 2014 Science China Press and Springer-Verlag Berlin Heidelberg.}}, 
pages = {1993--2012}, 
number = {10}, 
volume = {57}
}
@article{10.11591/ijece.v9i3.pp1561-1568, 
year = {2019}, 
title = {{Risk assessment for ancillary services}}, 
author = {Hadzic, Omer and Bisanovic, Smajo}, 
journal = {International Journal of Electrical and Computer Engineering (IJECE)}, 
issn = {20888708}, 
doi = {10.11591/ijece.v9i3.pp1561-1568}, 
abstract = {{The power trading and ancillary services provision comprise technical and financial risks and therefore require a structured risk management. Focus in this paper is on financial risk management that is important for the system operator faces when providing and using ancillary services for balancing of power system. Risk on ancillary services portfolio is modeled through value at risk and conditional value at risk measures. The application of these risk measures in power system is given in detail to show how to using the risk concept in practice. Conditional value at risk optimization is analysed in the context of portfolio selection and how to apply this optimization for hedging a portfolio consisting of different types of ancillary services. Copyright © 2019 Institute of Advanced Engineering and Science. All rights reserved.}}, 
pages = {1561--1568}, 
number = {3}, 
volume = {9}
}
@article{10.1109/tsp.2017.8075927, 
year = {2017}, 
title = {{Evaluation of customer’s losses and value-at-risk under cloud outages}}, 
author = {Naldi, Maurizio}, 
journal = {2017 40th International Conference on Telecommunications and Signal Processing (TSP)}, 
issn = {NA}, 
doi = {10.1109/tsp.2017.8075927}, 
abstract = {{Cloud services may suffer from outages, which interrupt customers’ activities and result in economic losses. A linear model is proposed here to translate outages into economic losses, accounting for both the frequency and the duration of outages. Two tools, the exceedance loss probability and the Value-at-Risk, are employed to assess the maximum economic exposure of a customer. The use of both tools is demonstrated by applying them to a dataset concerning five major cloud providers. Those tools can be employed to drive the choice of a cloud provider towards the least risky one. © 2017 IEEE.}}, 
pages = {12--15}, 
number = {NA}, 
volume = {2017-January}
}
@article{10.1016/j.automatica.2016.07.032, 
year = {2016}, 
title = {{Optimal portfolios with maximum Value-at-Risk constraint under a hidden Markovian regime-switching model}}, 
author = {Zhu, Dong-Mei and Xie, Yue and Ching, Wai-Ki and Siu, Tak-Kuen}, 
journal = {Automatica}, 
issn = {00051098}, 
doi = {10.1016/j.automatica.2016.07.032}, 
abstract = {{This paper studies an optimal portfolio selection problem in the presence of the Maximum Value-at-Risk (MVaR) constraint in a hidden Markovian regime-switching environment. The price dynamics of n risky assets are governed by a hidden Markovian regime-switching model with a hidden Markov chain whose states represent the states of an economy. We formulate the problem as a constrained utility maximization problem over a finite time horizon and then reduce it to solving a Hamilton–Jacobi–Bellman (HJB) equation using the separation principle. The MVaR constraint for n risky assets plus one riskless asset is derived and the method of Lagrange multiplier is used to deal with the constraint. A numerical algorithm is then adopted to solve the HJB equation. Numerical results are provided to demonstrate the implementation of the algorithm. © 2016 Elsevier Ltd}}, 
pages = {194--205}, 
number = {NA}, 
volume = {74}
}
@article{10.1017/s1446181110000878, 
year = {2010}, 
title = {{Optimal proportional reinsurance under two criteria: Maximizing the expected utility and minimizing the value at risk}}, 
author = {LIANG, ZHIBIN and GUO, JUNYI}, 
journal = {The ANZIAM Journal}, 
issn = {14461811}, 
doi = {10.1017/s1446181110000878}, 
abstract = {{We consider the optimal proportional reinsurance from an insurer's point of view to maximize the expected utility and minimize the value at risk. Under the general premium principle, we prove the existence and uniqueness of the optimal strategies and Pareto optimal solution, and give the relationship between the optimal strategies. Furthermore, we study the optimization problem with the variance premium principle. When the total claim sizes are normally distributed, explicit expressions for the optimal strategies and Pareto optimal solution are obtained. Finally, some numerical examples are presented to show the impact of the major model parameters on the optimal results. © 2010 Australian Mathematical Society.}}, 
pages = {449--463}, 
number = {4}, 
volume = {51}
}
@article{10.1088/1742-6596/1090/1/012026, 
year = {2018}, 
title = {{The application of genetic algorithm optimization on quadratic investment portfolio without a risk-free asset under Value-at-Risk}}, 
author = {Sukono and Supian, S. and Napitupulu, H. and Hidayat, Yuyun and Putra, Adam Sukma}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1090/1/012026}, 
abstract = {{In this paper, we performed the Genetic Algorithm within problems of quadratic investment portfolio without a risk-free asset under Value-at-Risk. The limitation of this study is that the risk of an investment portfolio measured by Value-at-Risk, and each investor has the nature of risk aversion. To solve these problems: First, we established the mean vector and covariance matrix. The second step was to define the vector mean and covariance matrices for the formulation of Value-at-Risk of the investment portfolio. Third, using the mean vector and Value-at-Risk established the model. To complete the optimization problem, we performed the Genetic Algorithm. The results show that the trade-off between risk and expected return does not only depend on the type of investor but also on the size of the investment. The Genetic Algorithm certifies us the robust solution in the optimization problem because of its natural ability to locate the global minimal. Moreover, genetic algorithm can be used as an effective way in numerical completion of the optimization of quadratic investment portfolio. In a realistic investment situation, it has likely more constraints. For example, the restriction on short-selling, is need to be considered. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012026}, 
number = {1}, 
volume = {1090}
}
@article{10.1007/978-3-642-23869-7_39, 
year = {2012}, 
title = {{Allocation of economic capital in banking: A simulation approach}}, 
author = {Burghof, H.-P. and Müller, J.}, 
issn = {NA}, 
doi = {10.1007/978-3-642-23869-7\_39}, 
abstract = {{The approach describes the difficulties implied through consistently equating a bank's allocation of economic capital with an allocation of decision rights in the form of value-at-risk limits. These days, risk measurement through value-at-risk methods is widespread. Using these methods strategically in order to optimize the return to risk ratio actively on an overall bank level is hardly developed. Thereto we model a bank's central planner coping with correlations' uncertainty and learning about the limit addressees' skills. In order to face the underlying mixed integer non linear program the model provides the central planner with a heuristic optimization approach. According to the given information and the assumed rationality of the central planner, resulting limit allocations are optimal in a portfolio theoretical sense. The numerical model generates a data set providing evidence concerning this allocation method's superiority compared to others. © Springer-Verlag Berlin Heidelberg 2012.}}, 
pages = {541--549}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s12532-018-0152-7, 
year = {2019}, 
title = {{Simplex QP-based methods for minimizing a conic quadratic objective over polyhedra}}, 
author = {Atamtürk, Alper and Gómez, Andrés}, 
journal = {Mathematical Programming Computation}, 
issn = {18672949}, 
doi = {10.1007/s12532-018-0152-7}, 
abstract = {{We consider minimizing a conic quadratic objective over a polyhedron. Such problems arise in parametric value-at-risk minimization, portfolio optimization, and robust optimization with ellipsoidal objective uncertainty; and they can be solved by polynomial interior point algorithms for conic quadratic optimization. However, interior point algorithms are not well-suited for branch-and-bound algorithms for the discrete counterparts of these problems due to the lack of effective warm starts necessary for the efficient solution of convex relaxations repeatedly at the nodes of the search tree. In order to overcome this shortcoming, we reformulate the problem using the perspective of the quadratic function. The perspective reformulation lends itself to simple coordinate descent and bisection algorithms utilizing the simplex method for quadratic programming, which makes the solution methods amenable to warm starts and suitable for branch-and-bound algorithms. We test the simplex-based quadratic programming algorithms to solve convex as well as discrete instances and compare them with the state-of-the-art approaches. The computational experiments indicate that the proposed algorithms scale much better than interior point algorithms and return higher precision solutions. In our experiments, for large convex instances, they provide up to 22x speed-up. For smaller discrete instances, the speed-up is about 13x over a barrier-based branch-and-bound algorithm and 6x over the LP-based branch-and-bound algorithm with extended formulations. The software that was reviewed as part of this submission was given the Digital Object identifier https://doi.org/10.5281/zenodo.1489153. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature and The Mathematical Programming Society.}}, 
pages = {311--340}, 
number = {2}, 
volume = {11}
}
@article{10.1007/978-3-319-44914-2_41, 
year = {2016}, 
title = {{Convergence of discrete approximations of stochastic programming problems with probabilistic criteria}}, 
author = {Kibzun, Andrey I. and Ivanov, Sergey V.}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-44914-2\_41}, 
abstract = {{We consider stochastic programming problems with probabilistic and quantile objective functions. The original distribution of the random variable is replaced by a discrete one. We thus consider a sequence of problems with discrete distributions. We suggest conditions, which guarantee that the sequence of optimal strategies converges to an optimal strategy of the original problem. We consider the case of a symmetrical distribution, the case of the loss function increasing in the random variable, and the case of the loss function increasing in the optimization strategy. © Springer International Publishing Switzerland 2016.}}, 
pages = {525--537}, 
number = {NA}, 
volume = {9869 LNCS}
}
@article{10.1515/strm-2015-0010, 
year = {2016}, 
title = {{Time-consistency of risk measures with GARCH volatilities and their estimation}}, 
author = {Klüppelberg, Claudia and Zhang, Jianing}, 
journal = {Statistics \& Risk Modeling}, 
issn = {21931402}, 
doi = {10.1515/strm-2015-0010}, 
abstract = {{In this paper we study time-consistent risk measures for returns that are given by a GARCH(1,1) model. We present a construction of risk measures based on their static counterparts that overcomes the lack of time-consistency. We then study in detail our construction for the risk measures Value-at-Risk (VaR) and Average Value-at-Risk (AVaR). While in the VaR case we can derive an analytical formula for its time-consistent counterpart, in the AVaR case we derive lower and upper bounds to its time-consistent version. Furthermore, we incorporate techniques from extreme value theory (EVT) to allow for a more tail-geared statistical analysis of the corresponding risk measures. We conclude with an application of our results to a data set of stock prices. © 2016 by De Gruyter.}}, 
pages = {103--124}, 
number = {2}, 
volume = {32}
}
@article{10.1016/j.jeconom.2012.08.011, 
year = {2013}, 
title = {{Fat tails, VaR and subadditivity}}, 
author = {Daníelsson, Jón and Jorgensen, Bjørn N. and Samorodnitsky, Gennady and Sarma, Mandira and Vries, Casper G. de}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2012.08.011}, 
abstract = {{Financial institutions rely heavily on Value-at-Risk (VaR) as a risk measure, even though it is not globally subadditive. First, we theoretically show that the VaR portfolio measure is subadditive in the relevant tail region if asset returns are multivariate regularly varying, thus allowing for dependent returns. Second, we note that VaR estimated from historical simulations may lead to violations of subadditivity. This upset of the theoretical VaR subadditivity in the tail arises because the coarseness of the empirical distribution can affect the apparent fatness of the tails. Finally, we document a dramatic reduction in the frequency of subadditivity violations, by using semi-parametric extreme value techniques for VaR estimation instead of historical simulations. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {283--291}, 
number = {2}, 
volume = {172}
}
@article{10.1108/s1569-375920140000096017, 
year = {2014}, 
title = {{Testing for rational speculative bubbles in the Brazilian residential real-estate market}}, 
author = {Oliveira, Marcelo M. de and Almeida, Alexandre C. L.}, 
journal = {Contemporary Studies in Economic and Financial Analysis}, 
issn = {15693759}, 
doi = {10.1108/s1569-375920140000096017}, 
abstract = {{Speculative bubbles have been occurring periodically in local or global real-estate markets and are considered a potential cause of economic crises. In this context, the detection of explosive behaviors in the financial market and the implementation of early warning diagnosis tests are of critical importance. The recent increase in Brazilian housing prices has risen concerns that the Brazilian economy may have a speculative housing bubble. In the present chapter, we employ a recently proposed recursive unit root test in order to identify possible speculative bubbles in data from the Brazilian residential real-estate market. The empirical results show evidence for speculative price bubbles both in Rio de Janeiro and São Paulo, the two main Brazilian cities. Copyright © 2014 by Emerald Group Publishing Limited All rights of reproduction in any form reserved.}}, 
pages = {401--416}, 
number = {NA}, 
volume = {96}
}
@article{10.1016/j.csda.2007.09.025, 
year = {2008}, 
title = {{A Bayesian approach to estimate the marginal loss distributions in operational risk management}}, 
author = {Valle, L. Dalla and Giudici, P.}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2007.09.025}, 
abstract = {{One of the main problems in operational risk management is the lack of loss data, which affects the parameter estimates of the marginal distributions of the losses. The principal reason is that financial institutions only started to collect operational loss data a few years ago, due to the relatively recent definition of this type of risk. Considering this drawback, the employment of Bayesian methods and simulation tools could be a natural solution to the problem. The use of Bayesian methods allows us to integrate the scarce and, sometimes, inaccurate quantitative data collected by the bank with prior information provided by experts. An original proposal is a Bayesian approach for modelling operational risk and for calculating the capital required to cover the estimated risks. Besides this methodological innovation a computational scheme, based on Markov chain Monte Carlo simulations, is required. In particular, the application of the MCMC method to estimate the parameters of the marginals shows advantages in terms of a reduction of capital charge according to different choices of the marginal loss distributions. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {3107--3127}, 
number = {6}, 
volume = {52}
}
@article{10.1186/s13660-015-0618-3, 
year = {2015}, 
title = {{Bidimensional discrete-time risk models based on bivariate claim count time series}}, 
author = {Ma, Dongxing and Wang, Dehui and Cheng, Jianhua}, 
journal = {Journal of Inequalities and Applications}, 
issn = {10255834}, 
doi = {10.1186/s13660-015-0618-3}, 
abstract = {{In this paper, we consider a class of bidimensional discrete-time risk models, which are based on the assumptions that the claim counts obey some specific bivariate integer-valued time series such as bivariate Poisson MA (BPMA) and the bivariate Poisson AR (BPAR) processes. We derive the moment generating functions (m.g.f.’s) for these processes, and we present their explicit expressions for the adjustment coefficient functions. The asymptotic approximations (upper bounds) to three different types of ruin probabilities are discussed, and the marginal value-at-risk (VaR) for each model is obtained. Numerical examples are provided to compute the adjustment coefficients discussed in the paper. © 2015, Ma et al.; licensee Springer.}}, 
pages = {105}, 
number = {1}, 
volume = {2015}
}
@article{10.1016/j.ijforecast.2019.01.003, 
year = {2019}, 
title = {{Forecasting Bitcoin risk measures: A robust approach}}, 
author = {Trucíos, Carlos}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2019.01.003}, 
abstract = {{Over the last few years, Bitcoin and other cryptocurrencies have attracted the interest of many investors, practitioners and researchers. However, little attention has been paid to the predictability of their risk measures. This paper compares the predictability of the one-step-ahead volatility and Value-at-Risk of Bitcoin using several volatility models. We also include procedures that take into account the presence of outliers and estimate the volatility and Value-at-Risk in a robust fashion. Our results show that robust procedures outperform non-robust ones when forecasting the volatility and estimating the Value-at-Risk. These results suggest that the presence of outliers plays an important role in the modelling and forecasting of Bitcoin risk measures. © 2019 International Institute of Forecasters}}, 
pages = {836--847}, 
number = {3}, 
volume = {35}
}
@article{10.1007/s10436-012-0205-2, 
year = {2012}, 
title = {{On the necessity of five risk measures}}, 
author = {Guégan, Dominique and Tarrant, Wayne}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-012-0205-2}, 
abstract = {{The banking systems that deal with risk management depend on underlying risk measures. Following the Basel II accord, there are two separate methods by which banks may determine their capital requirement. The Value at Risk measure plays an important role in computing the capital for both approaches. In this paper we analyze the errors produced by using this measure. We discuss other measures, demonstrating their strengths and shortcomings. We give examples, showing the need for the information from multiple risk measures in order to determine a bank's loss distribution. We conclude by suggesting a regulatory requirement of multiple risk measures being reported by banks, giving specific recommendations. © 2012 Springer-Verlag.}}, 
pages = {533--552}, 
number = {4}, 
volume = {8}
}
@article{10.1080/02664763.2016.1161738, 
year = {2017}, 
title = {{Real-time monitoring of carbon monoxide using value-at-risk measure and control charting}}, 
author = {Bersimis, Sotirios and Degiannakis, Stavros and Georgakellos, Dimitrios}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664763.2016.1161738}, 
abstract = {{One of the most important environmental health issues is air pollution, causing the deterioration of the population's quality of life, principally in cities where the urbanization level seems limitless. Among ambient pollutants, carbon monoxide (CO) is well known for its biological toxicity. Many studies report associations between exposure to CO and excess mortality. In this context, the present work provides an advanced modelling scheme for real-time monitoring of pollution data and especially of carbon monoxide pollution in city level. The real-time monitoring is based on an appropriately adjusted multivariate time series model that is used in finance and gives accurate one-step-ahead forecasts. On the output of the time series, we apply an empirical monitoring scheme that is used for the early detection of abnormal increases of CO levels. The proposed methodology is applied in the city of Athens and as the analysis revealed has a valuable performance. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--20}, 
number = {1}, 
volume = {44}
}
@article{10.1016/j.matcom.2008.11.016, 
year = {2009}, 
title = {{Modelling time-varying higher moments with maximum entropy density}}, 
author = {Chan, Felix}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2008.11.016}, 
abstract = {{Since the introduction of the Autoregressive Conditional Heteroscedasticity (ARCH) model of Engle [R. Engle, Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation, Econometrica 50 (1982) 987-1007], the literature of modelling the conditional second moment has become increasingly popular in the last two decades. Many extensions and alternate models of the original ARCH have been proposed in the literature aiming to capture the dynamics of volatility more accurately. Interestingly, the Quasi Maximum Likelihood Estimator (QMLE) with normal density is typically used to estimate the parameters in these models. As such, the higher moments of the underlying distribution are assumed to be the same as those of the normal distribution. However, various studies reveal that the higher moments, such as skewness and kurtosis of the distribution of financial returns are not likely to be the same as the normal distribution, and in some cases, they are not even constant over time. These have significant implications in risk management, especially in the calculation of Value-at-Risk (VaR) which focuses on the negative quantile of the return distribution. Failed to accurately capture the shape of the negative quantile would produce inaccurate measure of risk, and subsequently lead to misleading decision in risk management. This paper proposes a solution to model the distribution of financial returns more accurately by introducing a general framework to model the distribution of financial returns using maximum entropy density (MED). The main advantage of MED is that it provides a general framework to estimate the distribution function directly based on a given set of data, and it provides a convenient framework to model higher order moments up to any arbitrary finite order k. However this flexibility comes with a high cost in computational time as k increases, therefore this paper proposes an alternative model that would reduce computation time substantially. Moreover, the sensitivity of the parameters in the MED with respect to the dynamic changes of moments is derived analytically. This result is important as it relates the dynamic structure of the moments to the parameters in the MED. The usefulness of this approach will be demonstrated using 5 min intra-daily returns of the Euro/USD exchange rate. © 2008 IMACS.}}, 
pages = {2767--2778}, 
number = {9}, 
volume = {79}
}
@article{10.5705/ss.202019.0003, 
year = {2021}, 
title = {{Quantile estimation of regression models with GARCH-X errors}}, 
author = {Zhu, Qianqian and Li, Guodong and Xiao, Zhijie}, 
journal = {Statistica Sinica}, 
issn = {10170405}, 
doi = {10.5705/ss.202019.0003}, 
abstract = {{Conditional quantile estimations are an essential ingredient in modern risk management, and many other applications, where the conditional heteroscedastic structure is usually assumed to capture the volatility in financial time series. This study examines linear quantile regression models with GARCH-X errors. These models include the most popular generalized autoregressive conditional heteroscedasticity (GARCH) as a special case, and incorporate additional covariates into the conditional variance. Three conditional quantile estimators are proposed, and their asymptotic properties are established under mild conditions. A bootstrap procedure is developed to approximate their asymptotic distributions. The finite-sample performance of the proposed estimators is examined using simulation experiments. An empirical application illustrates the usefulness of the proposed methodology. © 2021 Institute of Statistical Science. All rights reserved.}}, 
number = {3}, 
volume = {31}
}
@article{10.1109/icicic.2009.376, 
year = {2009}, 
title = {{VaR based assets portfolio optimization model with risk measure}}, 
author = {Lin, Pingping and Wang, Qing and Liu, Shu-An}, 
journal = {2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)}, 
issn = {NA}, 
doi = {10.1109/icicic.2009.376}, 
abstract = {{By means of analyzing Yong's minimax portfolio selection model, a novel risk function is introduced with risk measure considering risk and extra return factors. The risk factor can tune the effects of asset yield on the investment decision, and the extra return factor can control the effects of assets portfolio's margin on the investment decision. Furthermore, an assets portfolio optimization model is designed based on the risk function. To solve the model, it is converted to a linear programming model. With actual data of the stock market, computational experiments show that the proposed model is effective and practicable. © 2009 IEEE.}}, 
pages = {1135--1139}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/risks6040132, 
year = {2018}, 
title = {{The asymptotic decision scenarios of an emerging stock exchange market: Extreme value theory and artificial neural network}}, 
author = {Musah, Abdul-Aziz Ibn and Du, Jianguo and Khan, Hira Salah ud din and Akeji, Alhassan Alolo Abdul-Rasheed}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks6040132}, 
abstract = {{In recent times, investing in volatile security increases the risk of losses and reduces gains. Many traders who depend on these risks indulge in multiple volatility procedures to inform their trading strategies. We explore two models to measure the tails behaviour and the period the stock will gain or fall within a five-month trading period. We obtained data from the Ghana stock exchange and applied generalized extreme value distribution validated by backtesting and an artificial neural network for forecasting. The network training produces and manages more than 90\% accuracy respectively for gains and falls for given input-output pairs. Based on this, estimates of extreme value distribution proves that it is formidable. There is a significant development in market prediction in assessing the results of actual and forecast performance. The study reveals that once every five months, at a 5\% confidence level, the market is expected to gain and fall 2.12\% and 2.23\%, respectively. The Ghana stock exchange market showed a maximum monthly stock gain above or below 2.12\% in the fourth and fifth months, whiles maximum monthly stock fell above or below 2.23\% in the third and fourth months. The study reveals that once every five months’ trading period, the stock market will gain and fall by almost an equal percentage, with a significant increase in value-at-risk and expected shortfall at the left tail as the quantiles increases compared to the right tail. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {132}, 
number = {4}, 
volume = {6}
}
@article{10.1109/tsg.2015.2477101, 
year = {2017}, 
title = {{Risk-Averse Optimal Bidding Strategy for Demand-Side Resource Aggregators in Day-Ahead Electricity Markets under Uncertainty}}, 
author = {Xu, Zhiwei and Hu, Zechun and Song, Yonghua and Wang, Jianhui}, 
journal = {IEEE Transactions on Smart Grid}, 
issn = {19493053}, 
doi = {10.1109/tsg.2015.2477101}, 
abstract = {{This paper first presents a generic model to characterize a variety of flexible demand-side resources (e.g., plug-in electric vehicles and distributed generation). Key sources of uncertainty affecting the modeling results are identified and are characterized via multiple stochastic scenarios. We then propose a risk-averse optimal bidding formulation for the resource aggregator at the demand side based on the conditional value-at-risk (VaR) theory. Specifically, this strategy seeks to minimize the expected regret value over a subset of worst-case scenarios whose collective probability is no more than a threshold value. Our approach ensures the robustness of the day-ahead (DA) bidding strategy while considering the uncertainties associated with the renewable generation, real-time price, and electricity demand. We carry out numerical simulations against three benchmark bidding strategies, including a VaR-based approach and a traditional scenario based stochastic programming approach. We find that the proposed strategy outperforms the benchmark strategies in terms of hedging high regret risks, and results in computational efficiency and DA bidding costs that are comparable to the benchmarks. © 2010-2012 IEEE.}}, 
pages = {96--105}, 
number = {1}, 
volume = {8}
}
@article{10.1016/j.najef.2013.02.001, 
year = {2013}, 
title = {{Recent developments in financial economics and econometrics: An overview}}, 
author = {Chang, Chia-Lin and Allen, David and McAleer, Michael}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2013.02.001}, 
abstract = {{Research papers in empirical finance and financial econometrics are among the most widely cited, downloaded and viewed articles in the discipline of Finance. The special issue presents several papers by leading scholars in the field on "Recent Developments in Financial Economics and Econometrics". The breadth of coverage is substantial, and includes original research and comprehensive review papers on theoretical, empirical and numerical topics in Financial Economics and Econometrics by leading researchers in finance, financial economics, financial econometrics and financial statistics. The purpose of this special issue on "Recent Developments in Financial Economics and Econometrics" is to highlight several novel and significant developments in financial economics and financial econometrics, specifically dynamic price integration in the global gold market, a conditional single index model with local covariates for detecting and evaluating active management, whether the Basel Accord has improved risk management during the global financial crisis, the role of banking regulation in an economy under credit risk and liquidity shock, separating information maximum likelihood estimation of the integrated volatility and covariance with micro-market noise, stress testing correlation matrices for risk management, whether bank relationship matters for corporate risk taking, with evidence from listed firms in Taiwan, pricing options on stocks denominated in different currencies, with theory and illustrations, EVT and tail-risk modelling, with evidence from market indices and volatility series, the economics of data using simple model free volatility in a high frequency world, arbitrage-free implied volatility surfaces for options on single stock futures, the non-uniform pricing effect of employee stock options using quantile regression, nonlinear dynamics and recurrence plots for detecting financial crisis, how news sentiment impacts asset volatility, with evidence from long memory and regime-switching approaches, quantitative evaluation of contingent capital and its applications, high quantiles estimation with Quasi-PORT and DPOT, with an application to value-at-risk for financial variables, evaluating inflation targeting based on the distribution of inflation and inflation volatility, the size effects of volatility spillovers for firm performance and exchange rates in tourism, forecasting volatility with the realized range in the presence of noise and non-trading, using CARRX models to study factors affecting the volatilities of Asian equity markets, deciphering the Libor and Euribor spreads during the subprime crisis, information transmission between sovereign debt CDS and other financial factors for Latin America, time-varying mixture GARCH models and asymmetric volatility, and diagnostic checking for non-stationary ARMA models with an application to financial data. © 2013 Elsevier Inc.}}, 
pages = {217--226}, 
number = {NA}, 
volume = {26}
}
@article{10.1016/j.econlet.2016.10.024, 
year = {2016}, 
title = {{Heavy tails and copulas: Limits of diversification revisited}}, 
author = {Ibragimov, Rustam and Prokhorov, Artem}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2016.10.024}, 
abstract = {{We show that diversification does not reduce Value-at-Risk for a large class of dependent heavy tailed risks. The class is characterized by power law marginals with tail exponent no greater than one and by a general dependence structure which includes some of the most commonly used copulas. © 2016 Elsevier B.V.}}, 
pages = {102--107}, 
number = {NA}, 
volume = {149}
}
@article{10.1016/s0304-4076(03)00215-x, 
year = {2004}, 
title = {{Subsampling the distribution of diverging statistics with applications to finance}}, 
author = {Bertail, Patrice and Haefke, Christian and Politis, Dimitris N. and White, Halbert}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/s0304-4076(03)00215-x}, 
abstract = {{In this paper we propose a subsampling estimator for the distribution of statistics diverging at either known or unknown rates when the underlying time series is strictly stationary and strong mixing. Based on our results we provide a detailed discussion of how to estimate extreme order statistics with dependent data and present two applications to assessing financial market risk. Our method performs well in estimating Value at Risk and provides a superior alternative to Hill's estimator in operationalizing Safety First portfolio selection. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {295--326}, 
number = {2}, 
volume = {120}
}
@article{10.1080/03610910802361341, 
year = {2008}, 
title = {{The use of ratios to compare volatilities and VaRs}}, 
author = {Zyl, J. Martin van}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610910802361341}, 
abstract = {{The riskiness of two investments can be compared by looking at the ratio of the respective Value-at-Risk's (VaRs) or the ratio of volatilities. The exact distribution of the ratio of two volatilities calculated from normal observations and an asymptotic confidence interval for the ratio of two VaRs is derived. A simulation study shows good coverage rates for ratios of VaRs calculated from observations from distributions commonly used to model logarithmic returns.}}, 
pages = {2089--2095}, 
number = {10}, 
volume = {37}
}
@article{10.1109/icaee.2010.5557557, 
year = {2010}, 
title = {{A long-term price risk early-warning model of electricity company based on EGARCH and VAR}}, 
author = {XueBin, Zhang}, 
journal = {2010 International Conference on Advances in Energy Engineering}, 
issn = {NA}, 
doi = {10.1109/icaee.2010.5557557}, 
abstract = {{In china's electricity market, the risk of price fluctuation is significant to grid corporations. A historically reality, the electricity price in market is of gathering effects and heteroscedasticity. Furthermore, the key role of grid corporations in electricity market induces that the same scope of rise and down in price have different impacts on them, which called "lever effect". Based on exponential generalized autoregressive conditional heteroscedasticity and Value-at-Risk theory, we define the risk factor of price fluctuation and establish price risk early-warning model. The empirical analysis demonstrates that the model complements the conventional methods and has high precision. The model can supervise the market price and conduct warning signals. So the grid corporations can take timely countermeasures. © 2010 IEEE.}}, 
pages = {295--298}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s00186-008-0244-7, 
year = {2009}, 
title = {{Estimating allocations for Value-at-Risk portfolio optimization}}, 
author = {Charpentier, Arthur and Oulidi, Abder}, 
journal = {Mathematical Methods of Operations Research}, 
issn = {14322994}, 
doi = {10.1007/s00186-008-0244-7}, 
abstract = {{Value-at-Risk, despite being adopted as the standard risk measure in finance, suffers severe objections from a practical point of view, due to a lack of convexity, and since it does not reward diversification (which is an essential feature in portfolio optimization). Furthermore, it is also known as having poor behavior in risk estimation (which has been justified to impose the use of parametric models, but which induces then model errors). The aim of this paper is to chose in favor or against the use of VaR but to add some more information to this discussion, especially from the estimation point of view. Here we propose a simple method not only to estimate the optimal allocation based on a Value-at-Risk minimization constraint, but also to derive-empirical-confidence intervals based on the fact that the underlying distribution is unknown, and can be estimated based on past observations. © 2008 Springer-Verlag.}}, 
pages = {395}, 
number = {3}, 
volume = {69}
}
@article{10.1515/dema-2009-0222, 
year = {2009}, 
title = {{Approximation of market valuations on the set of risk measures}}, 
author = {Tkalinski, Tomasz}, 
journal = {Demonstratio Mathematica}, 
issn = {04201213}, 
doi = {10.1515/dema-2009-0222}, 
abstract = {{In this work we introduce the problem of choice of a risk measure providing best approximation of risk estimates derived from market valuations. We begin with a brief overview of connections between pricing and risk measurement issues which reveal importance of the problem we consider and lead to the mathematical formulation. In the main result under fairly general assumptions we establish the existence of the solution. In the second part we define a problem of finding a risk measure optimal with respect to the capital requirements. We impose additional assumptions, all of which have strong practical justification and in this particular setting we show that a solution exists and is a spectral measure of risk. As an example of application we show that there is some optimal spectral measure of risk for speculative position created in a market model with CIR short rate dynamics. © 2009 Warsaw University. All rights reserved.}}, 
pages = {441--452}, 
number = {2}, 
volume = {42}
}
@article{10.1007/s10479-012-1285-0, 
year = {2014}, 
title = {{Value-at-Risk model for hazardous material transportation}}, 
author = {Kang, Yingying and Batta, Rajan and Kwon, Changhyun}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-012-1285-0}, 
abstract = {{This paper introduces a Value-at-Risk (VaR) model to generate route choices for a hazmat shipment based on a specified risk confidence level. VaR is a threshold value such that the probability of the loss exceeding the VaR value is less than a given probability level. The objective is to determine a route which minimizes the likelihood that the risk will be greater than a set threshold. Several properties of the VaR model are established. An exact solution procedure is proposed and tested to solve the single-trip problem. To test the applicability of the approach, routes obtained from the VaR model are compared with those obtained from other hazmat objectives, on a numerical example as well as a hazmat routing scenario derived from the Albany district of New York State. Depending on the choice of the confidence level, the VaR model gives different paths from which we conclude that the route choice is a function of the level of risk tolerance of the decision-maker. Further refinements of the VaR model are also discussed. © 2013, Springer Science+Business Media New York.}}, 
pages = {361--387}, 
number = {1}, 
volume = {222}
}
@article{10.1108/s1571-0386(2010)0000020014, 
year = {2010}, 
title = {{GARCH models with CPPI application}}, 
author = {Ameur, Hachmi Ben}, 
journal = {International Symposia in Economic Theory and Econometrics}, 
issn = {15710386}, 
doi = {10.1108/s1571-0386(2010)0000020014}, 
abstract = {{Purpose - The aim of this chapter is to examine the constant proportion portfolio insurance (CPPI) method when the multiple is allowed to vary over. Methodology/approach - A quantile approach is introduced under the dependent return hypothesis. We use for example ARCH-type models. Findings - In this framework, we provide explicit values of the multiple as function of the past asset returns and other state variables. We show how the multiple can be chosen to satisfy the guarantee condition, at a given level of probability and for particular market conditions. Originality/value of paper - We show in this chapter that it is possible to choose variable multiples for the CPPI method if quantile hedging is used and in the case of dependent log returns. Upper bounds can be calculated for each level of probability and according to state variables. This new multiple can be determined according to the distributions of the risky asset log return and volatility. Copyright © 2013 by Emerald Group Publishing Limited.}}, 
pages = {187--205}, 
number = {NA}, 
volume = {20}
}
@article{10.2495/cf080191, 
year = {2008}, 
title = {{Value at risk, outliers and chaotic dynamics}}, 
author = {Kyrtsou, C and Terraza, V}, 
journal = {Computational Finance and its Applications III}, 
issn = {17433517}, 
doi = {10.2495/cf080191}, 
abstract = {{Financial returns series typically exhibit excess kurtosis and volatility clustering. The GARCH is often applied to describe these two stylized facts. Nevertheless, all excess kurtosis in stock returns cannot be captured by this model. In presence of such dynamics, a non-linear model in variance cannot take into account all the non-linearity in returns. The aim of this work is the study of relationships between chaotic dynamics, and outliers as well as their impact on Value at Risk. In this paper, our empirical study is based on the daily Nikkei returns series from 06/01/1987 to 14/03/2001. We apply the traditional GARCH and the Mackey-Glass-GARCH (Kyrtsou and Terraza (2003)) both to the initial and the filtered Nikkei returns series without outliers. The performance test of Lopez (1988) for Value at Risk models is then applied to determine the best model. We show that during the period of estimation, the GARCH and the Mackey-Glass-GARCH VaR models with (without) outliers at 95\% overestimate (underestimate) risks. The filtering of outliers lead to underestimation of risk more accentuated during the Asian crisis period confirming the presence of severe distortions in the underlying structure of the series when removing extreme events. During this period, the Mackey-Glass-GARCH is better than the GARCH comparing the hedging rates.}}, 
pages = {197--205}, 
number = {NA}, 
volume = {41}
}
@article{10.1016/j.ijforecast.2015.07.005, 
year = {2016}, 
title = {{Volatility and quantile forecasts by realized stochastic volatility models with generalized hyperbolic distribution}}, 
author = {Takahashi, Makoto and Watanabe, Toshiaki and Omori, Yasuhiro}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2015.07.005}, 
abstract = {{The predictive performance of the realized stochastic volatility model of Takahashi et al. (2009), which incorporates the asymmetric stochastic volatility model with the realized volatility, is investigated. Considering the well-known characteristics of financial returns, namely heavy tails and skewness, the model is extended by employing a wider class distribution, the generalized hyperbolic skew Student's t-distribution, for financial returns. Using the Bayesian estimation scheme via a Markov chain Monte Carlo method, the model enables us to estimate the parameters in the return distribution and in the model jointly. It also makes it possible to forecast the volatility and return quantiles by sampling from their posterior distributions jointly. The model is applied to quantile forecasts of financial returns such as value-at-risk and expected shortfall, as well as to volatility forecasts, and the forecasts are evaluated using a range of tests and performance measures. The empirical results using the US and Japanese stock indices, the Dow Jones Industrial Average and Nikkei 225, show that the extended model improves the volatility and quantile forecasts, especially in some volatile periods. © 2015 International Institute of Forecasters.}}, 
pages = {437--457}, 
number = {2}, 
volume = {32}
}
@article{10.1007/978-3-030-04200-4_33, 
year = {2019}, 
title = {{Value at risk of the stock market in asean-5}}, 
author = {Boonyakunakorn, Petchaluck and Pastpipatkul, Pathairat and Sriboonchitta, Songsak}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-030-04200-4\_33}, 
abstract = {{This paper analyzes the Value at Risk (VaR) of ASEAN-5 stock market indexes by employing Bayesian MSGARCH models. The estimated MSGARCH models with two-regime results show that the two regimes have different unconditional volatility levels and volatility persistence for all ASEAN-5 stock return. This different parameter estimate shows that the volatility process evolution is heterogeneous across the two regimes. Therefore, MSGARCH with two-regime should provide a better result than the standard GRACH model since Markov-switching model can capture characterize the time series behaviors in different regimes. For the estimated VaR results, we found that Philippines stock return presents the highest risk, whereas it provides the highest average yield among ASEAN-5 which is attractive for risk-lover investors. Malaysia is the preferred one for the risk-averse investors since it presents the lowest VaR, but provides a high return. Thailand stock return offers the median risk and median returns among ASEAN-5. Singapore stock return presents a high VaR estimate, but provides the lowest yield, being the most not attractive for investors. © Springer Nature Switzerland AG 2019.}}, 
pages = {452--462}, 
number = {NA}, 
volume = {809}
}
@article{10.1051/matecconf/201821208028, 
year = {2018}, 
title = {{Developing a risk management system and assessing the effectiveness of integrated risk management in the space rocket sector}}, 
author = {Sobol, Alexander and Fadeev, Oleg}, 
journal = {MATEC Web of Conferences}, 
issn = {2261236X}, 
doi = {10.1051/matecconf/201821208028}, 
abstract = {{The article discusses the formation of the system of risk management and evaluation of the effectiveness of integrated project risk management in the space rocket sector. The classification of the integrated project risks for knowledge-based enterprise in the space rocket sector and credit institution was made. The main methods of project risks evaluation, which can be applied in the space rocket sector, are described. The scheme of integrated project risk management was made on the basis of the life cycle approach when implementing projects in the space rocket sector. When implementing project risks in the space rocket sector, an enlarged scheme of the Regulations on Integrated Project Risk Management System, was developed. An issue of integrated risk management in the space rocket sector using a system of measures being named the "Value-at-Risk" (VaR) was reviewed. It was concluded that there was a need for project implementation in the space rocket sector using a complex, effective, and integrated system of risk management. © The Authors, published by EDP Sciences, 2018.}}, 
pages = {08028}, 
number = {NA}, 
volume = {212}
}
@article{10.1093/jjfinec/nbi004, 
year = {2005}, 
title = {{Portfolio diversification effects of downside risk}}, 
author = {Hyung, N}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbi004}, 
abstract = {{Risk managers use portfolios to diversify away the unpriced risk of individual securities. In this article we compare the benefits of portfolio diversification for downside risk in case returns are normally distributed with the case of fat-tailed distributed returns. The downside risk of a security is decomposed into a part which is attributable to the market risk, an idiosyncratic part, and a second independent factor. We show that the fat-tailed-based downside risk, measured as value-at-risk (VaR), should decline more rapidly than the normal-based VaR. This result is confirmed empirically. © Oxford University Press 2005; all rights reserved.}}, 
pages = {107--125}, 
number = {1}, 
volume = {3}
}
@article{10.1016/j.physa.2017.04.036, 
year = {2017}, 
title = {{Fireworks algorithm for mean-VaR/CVaR models}}, 
author = {Zhang, Tingting and Liu, Zhifeng}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2017.04.036}, 
abstract = {{Intelligent algorithms have been widely applied to portfolio optimization problems. In this paper, we introduce a novel intelligent algorithm, named fireworks algorithm, to solve the mean-VaR/CVaR model for the first time. The results show that, compared with the classical genetic algorithm, fireworks algorithm not only improves the optimization accuracy and the optimization speed, but also makes the optimal solution more stable. We repeat our experiments at different confidence levels and different degrees of risk aversion, and the results are robust. It suggests that fireworks algorithm has more advantages than genetic algorithm in solving the portfolio optimization problem, and it is feasible and promising to apply it into this field. © 2017 Elsevier B.V.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {483}
}
@article{10.12011/1000-6788(2017)03-0570-10, 
year = {2017}, 
title = {{Measuring the dependence risk between QFII and HS300 index based on the Copula-ASV-EVT model}}, 
author = {}, 
issn = {10006788}, 
doi = {10.12011/1000-6788(2017)03-0570-10}, 
abstract = {{This paper is concerned with the statistical modeling of the dependence structure of QFII and HS300 index using the theory of Copulas. We select some Copulas and identify the type of dependency to capture nonlinear asymmetric and tail dependence. The EVT (extreme value theory, EVT) model needs to estimate the threshold values in order to exactly fit the margin distribution of Copula functions. Our analysis is based on a semi-parametric extreme value model. EV Copulas, Archimax Copulas and Archimedean Copulas simulate the correlation between QFII index and HS300 index. The empirical analysis indicates that the BB1 Copula has a higher correlation in the lower tail than the upper tail for a variety of parameters used in the Copula function. These findings illustrate that two different return series are more likely to correlate with each other during market downturns than upturns. Moreover, the backtesting results show that Copula-ASV-EVT (asymmetry stochastic volatility, ASV) model is suitable for measurement of tail risk of QFII and HS300 portfolio. In addition, we find the striking evidence of QFII value investment in Chinese A-share stock market for the period 2006-2012. Meanwhile QFII institutional investors gradually increase and the improvement of listed companies' profit sharing system so that there is an opportunity for the rational return of value investment in Chinese stock markets. © 2017, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.}}, 
number = {3}, 
volume = {37}
}
@article{10.1109/icise.2009.731, 
year = {2009}, 
title = {{Measuring market risk of China wheat futures based on AAVS-CAViaR model}}, 
author = {Wang, Xin-yu and Zheng, Chun-yan}, 
journal = {2009 First International Conference on Information Science and Engineering}, 
issn = {NA}, 
doi = {10.1109/icise.2009.731}, 
abstract = {{Considering the asymmetry of return distributions and asymmetric impact of positive and negative returns on the quantiles, the paper puts forth a new conditional autoregressive value-at-risk by regression quantiles model with asymmetric absolute values and slops (AAVS-CAViaR) quantile specification for heavy-tail data applications. An empirical study on the evolution patterns of market risk of wheat futures in Zhengzhou Commodity Exchange is performed. The dynamic quantile test, the regression quantile criteria and back testing results support our new model works well. It is found that the asymmetric impacts of price news on the quantiles of returns exist in Chinese wheat futures market. A rule to select proper VaR predicting model is suggested, and we also find AAVS model performs better than indirect GARCH model for our selected sample.}}, 
pages = {4382--4385}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-842x.2007.00498.x, 
year = {2008}, 
title = {{Heavy-tailed-distributed threshold stochastic volatility models in financial time series}}, 
author = {Chen, Cathy W. S. and Liu, F. C. and So, Mike K. P.}, 
journal = {Australian \& New Zealand Journal of Statistics}, 
issn = {13691473}, 
doi = {10.1111/j.1467-842x.2007.00498.x}, 
abstract = {{Summary To capture mean and variance asymmetries and time-varying volatility in financial time series, we generalize the threshold stochastic volatility (THSV) model and incorporate a heavy-tailed error distribution. Unlike existing stochastic volatility models, this model simultaneously accounts for uncertainty in the unobserved threshold value and in the time-delay parameter. Self-exciting and exogenous threshold variables are considered to investigate the impact of a number of market news variables on volatility changes. Adopting a Bayesian approach, we use Markov chain Monte Carlo methods to estimate all unknown parameters and latent variables. A simulation experiment demonstrates good estimation performance for reasonable sample sizes. In a study of two international financial market indices, we consider two variants of the generalized THSV model, with US market news as the threshold variable. Finally, we compare models using Bayesian forecasting in a value-at-risk (VaR) study. The results show that our proposed model can generate more accurate VaR forecasts than can standard models. © 2008 Australian Statistical Publishing Association Inc.}}, 
pages = {29--51}, 
number = {1}, 
volume = {50}
}
@article{10.1016/j.matcom.2012.05.010, 
year = {2013}, 
title = {{Extreme market risk and extreme value theory}}, 
author = {Singh, Abhay K. and Allen, David E. and Robert, Powell J.}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2012.05.010}, 
abstract = {{The phenomenon of the occurrence of rare yet extreme events, "Black Swans" in Taleb's terminology, seems to be more apparent in financial markets around the globe. This means there is not only a need to design proper risk modelling techniques which can predict the probability of risky events in normal market conditions but also a requirement for tools which can assess the probabilities of rare financial events; like the recent global financial crisis (2007-2008). An obvious candidate, when dealing with extreme financial events and the quantification of extreme market risk is extreme value theory (EVT). This proves to be a natural statistical modelling technique of relevance. Extreme value theory provides well-established statistical models for the computation of extreme risk measures like the return level, value at risk and expected shortfall. In this paper we apply univariate extreme value theory to model extreme market risk for the ASX-All Ordinaries (Australian) index and the S\&P-500 (USA) Index. We demonstrate that EVT can be successfully applied to financial market return series for predicting static VaR, CVaR or expected shortfall (ES) and expected return level and also daily VaR using a GARCH(1,1) and EVT based dynamic approach. © 2012 IMACS. Published by Elsevier B.V. All rights reserved.}}, 
pages = {310--328}, 
number = {NA}, 
volume = {94}
}
@article{10.1111/j.1813-6982.2011.01267.x, 
year = {2011}, 
title = {{Extreme value at risk: A scenario for risk management}}, 
author = {KABUNDI, ALAIN and MUTEBA, JOHN MWAMBA}, 
journal = {South African Journal of Economics}, 
issn = {00382280}, 
doi = {10.1111/j.1813-6982.2011.01267.x}, 
abstract = {{This paper uses the generalised extreme value (GEV) distribution to model the extreme losses that are likely to occur during market crashes, in the case of an investor who has long positions in stocks and currencies. The null hypothesis - which tests for normality of asset returns - is rejected due to asymmetry of these returns. We assume that the asymmetric behaviour and volatility of the returns are captured by the shape and scale parameters, respectively, of a GEV distribution. The data set includes stock indices for the United States, Japan, the United Kingdom, Germany, France and South Africa, and the South African rand exchange rates against the US dollar observed from 3 January 2005 to 30 December 2009. In addition, we divide this sample period into two periods: the pre-crisis period, from 3 January 2005 to 31 December 2007 and the crisis period, from 1 January 2008 to 30 December 2009. We compared the estimates of value at risk (VaR) using an extreme value theory (EVT) model, with the estimates derived from the traditional variance-covariance method and found that during the crisis the 99\% extreme VaR estimates are more reliable as they lie within the Basel II green zone. These results suggest that, at higher quintiles, the VaR estimates based on EVT are reliable and more accurate than estimates from the traditional method. © 2011 Economic Society of South Africa.}}, 
pages = {173--183}, 
number = {2}, 
volume = {79}
}
@article{10.1111/j.1467-9469.2011.00759.x, 
year = {2013}, 
title = {{Quantile Regression Estimator for GARCH Models}}, 
author = {LEE, SANGYEOL and NOH, JUNGSIK}, 
journal = {Scandinavian Journal of Statistics}, 
issn = {03036898}, 
doi = {10.1111/j.1467-9469.2011.00759.x}, 
abstract = {{In this article, we study the quantile regression estimator for GARCH models. We formulate the quantile regression problem by a reparametrization method and verify that the obtained quantile regression estimator is strongly consistent and asymptotically normal under certain regularity conditions. We also present our simulation results and a real data analysis for illustration. © 2012 Board of the Foundation of the Scandinavian Journal of Statistics.}}, 
pages = {2--20}, 
number = {1}, 
volume = {40}
}
@article{10.1016/j.resourpol.2021.102456, 
year = {2022}, 
title = {{GAS and GARCH based value-at-risk modeling of precious metals}}, 
author = {Junior, Peterson Owusu and Tiwari, Aviral Kumar and Tweneboah, George and Asafo-Adjei, Emmanuel}, 
journal = {Resources Policy}, 
issn = {03014207}, 
doi = {10.1016/j.resourpol.2021.102456}, 
abstract = {{We employ 38 VaR model specifications (32 GARCH and - 6 GAS), assuming Gaussian and non-Gaussian distributional innovations. Using the elicitability property of VaR, we further use the Model Confidence Set (MCS) technique, which creates superior set models (SSMs) and ranks them based predictive ability of the VaR forecasts. We employ 4580 daily log-returns of Gold, Palladium, Platinum, and Silver, which span January 01, 2000, to April 04, 2018, which covers turbulent (Eurozone and Global Financial crises periods) and tranquil (post-Global Financial crisis period) market conditions. We find that, for both 1\% and 5\% VaR forecasts, Platinum exhibits a higher level of heterogeneity among models in contrast with Silver, Gold, and Palladium. Hence, Platinum has the smallest number of models in the SSM. Empirically, the homogeneity in the SSM is suggestive of well-diversified portfolios for the respective metals. Except for a few models, both DQ and CC tests support adequate forecast abilities of the respective 1\% and 5\% VaR models. This suggests the strength of the MCS procedure to select superior set models as compared to the initial set of 38 models. Our study is important for internal risk modelling, regulatory oversight and may bolster confidence in global investors concerning investments in precious metals. © 2021 Elsevier Ltd}}, 
pages = {102456}, 
number = {NA}, 
volume = {75}
}
@article{10.2174/1874110x01509011849, 
year = {2015}, 
title = {{Research on financial risk management based on VAR model}}, 
author = {Li, Xingchen}, 
journal = {The Open Cybernetics \& Systemics Journal}, 
issn = {1874110X}, 
doi = {10.2174/1874110x01509011849}, 
abstract = {{VaR is a widely- applied tool in the international financial risk management area, and it is also a new technical standard for measuring financial risk. VAR model was first used to measure market risk. Currently VAR analysis methods are gradually being introduced in all areas of financial risk management. VAR model in financial risk management uses more widely. Especially with the continuous improvement of the VAR model, it can be apply to financial institutions, market risk, credit risk management. And it has a wide range of applications in the liquidity risk management and financial regulation and so on. Based on this, we In-depth discuss financial risk management based on VAR model. © Xingchen Li.}}, 
pages = {1849--1852}, 
number = {NA}, 
volume = {9}
}
@article{10.1016/j.insmatheco.2016.03.001, 
year = {2016}, 
title = {{Optimal reinsurance under VaR and TVaR risk measures in the presence of reinsurer's risk limit}}, 
author = {Lu, ZhiYi and Meng, LiLi and Wang, Yujin and Shen, Qingjie}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2016.03.001}, 
abstract = {{In most studies on optimal reinsurance, little attention has been paid to controlling the reinsurer's risk. However, real-world insurance markets always place a limit on coverage, otherwise the insurer will be subjected to under a heavy financial burden when the insured suffers a large unexpected covered loss. In this paper, we revisit the optimal reinsurance problem under the optimality criteria of VaR and TVaR risk measures when the constraints for the reinsurer's risk exposure are presented. Two types of constraints are considered that have been proposed by Cummins and Mahul (2004) and Zhou et al. (2010), respectively. It is shown that two-layer reinsurance is always the optimal reinsurance policy under both VaR and TVaR risk measures and under both types of constraints. This implies that the two-layer reinsurance policy is more robust. Furthermore, the optimal quantity of ceded risk depends on the confidence level, the safety loading and the tolerance level, as well as on the relation between them. © 2016 Elsevier B.V.}}, 
pages = {92--100}, 
number = {NA}, 
volume = {68}
}
@article{10.21314/jrmv.2017.172, 
year = {2017}, 
title = {{Modeling impacts of stock jumps on real estate investment trust returns with application to value-at-risk}}, 
author = {Chen, Fen-Ying}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2017.172}, 
abstract = {{In contrast to most of the existing literature that has concentrated empirically on the relationship between real estate investment trust (REIT) prices and the stock market, this paper directly models the effects of stock jumps on REIT returns associated with an alternative dynamic process. Three key features of the model are that it decomposes the impacts of stock jumps into two parts (the effect of jumps in the expected returns of REITs, and the influence of jumps in REIT volatility), it can efficiently describe the effect of stock returns on REITs according to Christoffersen's independence test during pre- and postcrisis periods; and the empirical results show that the magnitudes of jumps in expected returns and volatility are sensitive to the value-at-risk of REITs. Therefore, the effects of stock returns on expected returns and volatility of REITs cannot be neglected. © 2017. Incisive Risk Information (IP) Limited. All rights reserved.}}, 
number = {2}, 
volume = {11}
}
@article{10.1108/jrf-07-2020-0161, 
year = {2021}, 
title = {{How dark is the dark side of diversification?}}, 
author = {Cadenas, Pedro E and Gzyl, Henryk and Park, Hyun Woong}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-07-2020-0161}, 
abstract = {{Purpose: This paper aims to illustrate, within the context of a well-known linear diversification model, that risk management as exerted by banks and regulators ultimately depends on how risk is assessed and conceptualized. The two risk metrics used are the probability of bank failure and value at risk (VaR). The paper also extends the results of the model by incorporating an explicit analysis of correlation of the bank's portfolios. Design/methodology/approach: The paper is based on a well-known model of linear diversification of two banking institutions developed by Wagner (2010) in the Journal of Financial Intermediation. The authors added considerations that were unexplored by Wagner and derived the corresponding logical and practical implications. Findings: The authors found that depending on which of the two risk metrics being used, the way diversification is perceived and risk is managed may differ. This situation may very well end-up generating different incentives for banks and regulators. The authors suggest a general rationale for considering how to think about the apparent dilemma and the challenges faced by regulators. The authors also offer an explicit analysis of correlation for the bank's portfolios. Research limitations/implications: The results are dependent on the particular aspects of the model, so the research results may lack generality in other contexts. Practical implications: Despite the limitations already mentioned, the paper illustrates some relevant points within the open debate about risk measurement and diversification. Originality/value: This paper contributes to the open discussion of diversification, risk perception and systemic crisis. © 2021, Emerald Publishing Limited.}}, 
pages = {44--55}, 
number = {1}, 
volume = {22}
}
@article{10.1007/978-3-540-69387-1_56, 
year = {2008}, 
title = {{Estimating real estate value-at-risk using wavelet denoising and time series model}}, 
author = {He, Kaijian and Xie, Chi and Lai, Kin Keung}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-540-69387-1\_56}, 
abstract = {{As the real estate market develops rapidly and is increasingly securitized, it has become an important investment asset in the portfolio design. Thus the measurement of its market risk exposure has attracted attentions from academics and industries due to its peculiar behavior and unique characteristics such as heteroscedasticity and multi scale heterogeneity in its risk and noise evolution etc. This paper proposes the wavelet denoising ARMA-GARCH approach for measuring the market risk level in the real estate sector. The multi scale heterogeneous noise level is determined in the level dependent manner in wavelet analysis. The autocorrelation and heteroscedasticity characteristics for both data and noises are modeled in the ARMA-GARCH framework. Experiment results in Chinese real estate market suggest that the proposed methodology achieves the superior performance by improving the reliability of VaR estimated upon those from traditional ARMA-GARCH approach. © 2008 Springer-Verlag Berlin Heidelberg.}}, 
pages = {494--503}, 
number = {PART 2}, 
volume = {5102 LNCS}
}
@article{10.1016/j.insmatheco.2020.05.013, 
year = {2020}, 
title = {{Range Value-at-Risk bounds for unimodal distributions under partial information}}, 
author = {Bernard, Carole and Kazzi, Rodrigue and Vanduffel, Steven}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.05.013}, 
abstract = {{In this paper, we derive upper and lower bounds on the Range Value-at-Risk of the portfolio loss when we only know its mean, variance, and feature of unimodality. In a first step, we use some classic results on stochastic ordering to reduce this optimization problem to a parametric one, which in a second step can be solved using standard methods. The novel approach we propose makes it possible to obtain analytical results for all probability levels and is moreover amendable to other situations of interest. Specifically, we apply our method to obtain risk bounds in the case of a portfolio loss that is non-negative (as is often the case in practice) and whose variance is possibly infinite. Numerical illustrations show that in various cases of interest we obtain bounds that are of practical importance. © 2020 Elsevier B.V.}}, 
pages = {9--24}, 
number = {NA}, 
volume = {94}
}
@article{10.1142/s0219024906003792, 
year = {2006}, 
title = {{Two-component extreme value distribution for Asia-Pacific stock index returns}}, 
author = {ANÉ, THIERRY}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024906003792}, 
abstract = {{Financial risk management typically deals with low-probability events in the tails of asset return distributions. To better capture the behavior of these tails, several studies have clearly highlighted that one should rely on a methodology that directly focuses on the tails of the distribution rather than getting the tails as an outcome of modelling the entire density function. Traditional Extreme Value Theory (EVT) distributions, however, provide a good fit for the bulk of the extreme data but usually underestimate a small amount of observations considered as "outliers". Since the main objective of risk management analysis is to estimate the size and probability of very large price movements, these "outliers" are by definition the very events we need to investigate. In this paper we suggest the use of a Two-Component Extreme Value (TCEV) distribution where a 'basic distribution' generates ordinary extremes (more frequent and less severe in the mean) while an "outlying distribution" generates rarer but severe extremes. Goodness-of-flt tests show the superiority of this distribution to capture the extremes of eleven MSCI Indices of the Pacific-Basin region relative to traditional EVT densities. Measures of accuracy and efficiency used to assess the performance of VaR forecasts also indicate that the additional flexibility brought by the TCEV model provides strong improvements for risk management. © World Scientific Publishing Company.}}, 
pages = {643--671}, 
number = {5}, 
volume = {9}
}
@article{10.1016/j.irfa.2005.10.003, 
year = {2006}, 
title = {{Modelling the implied volatility surface: Does market efficiency matter?. An application to MIB30 index options}}, 
author = {Cassese, Gianluca and Guidolin, Massimo}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2005.10.003}, 
abstract = {{We analyze the volatility surface vs. moneyness and time-to-expiration implied by MIBO options written on the MIB30, the most important Italian stock index. We specify and fit a number of models of the implied volatility surface and find that it has a rich and interesting structure that strongly departs from a constant volatility, Black-Scholes benchmark. This result is robust to alternative econometric approaches, including generalized least squares approaches that take into account both the panel structure of the data and the likely presence of heteroskedasticity and serial correlation in the random disturbances. Finally we show that the degree of pricing efficiency of this options market can strongly condition the results of the econometric analysis and therefore our understanding of the pricing mechanism underlying observed MIBO option prices. Applications to value-at-risk and portfolio choice calculations illustrate the importance of using arbitrage-free data only. © 2005 Elsevier Inc. All rights reserved.}}, 
pages = {145--178}, 
number = {2}, 
volume = {15}
}
@article{10.1016/j.procs.2014.03.085, 
year = {2014}, 
title = {{An analytic portfolio approach to system of systems evolutions}}, 
author = {Davendralingam, Navindran and DeLaurentis, Daniel}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2014.03.085}, 
abstract = {{The development of large collections of systems or a 'System-of-Systems (SoS)' is challenging due to the large number of systems involved, complex dynamics attributed to interdependencies between systems, and inherent technical and programmatic uncertainties The sheer number of decision variables involved in SoS development exceeds the mental faculties of the SoS practitioner, prompting the need for effective analytical support frameworks Current frameworks and guidelines in addressing SoSE challenges lack analytical means of objective SoS level decision-making Research in this paper adopts computational decision support methods from financial engineering that allows SoS practitioners the means to identify optimal 'portfolios' of systems based on dimensions of capability, cost and operational risk The SoS architecture is represented as a hierarchical collection genenc nodes that interact to provide the overarching SoS level set of capabilities based on an archetypal set of internodal behaviors Our research leverages a Conditional Value-at-Risk (CVaR) perspective to managing risks that can incorporate agent based simulation data in the decision-making process We demonstrate the method using a LCS inspired Naval Warfare Scenario (NWS) as an illustrative case study © 2014 The Authors. Published by Elsevier B.V.}}, 
pages = {711--719}, 
number = {NA}, 
volume = {28}
}
@article{10.1007/s10687-020-00382-3, 
year = {2020}, 
title = {{Risk concentration under second order regular variation}}, 
author = {Das, Bikramjit and Kratz, Marie}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-020-00382-3}, 
eprint = {1704.02609}, 
abstract = {{Measures of risk concentration and their asymptotic behavior for portfolios with heavy-tailed risk factors is of interest in risk management. Second order regular variation is a structural assumption often imposed on such risk factors to study their convergence rates. In this paper, we provide the asymptotic rate of convergence of the measure of risk concentration for a portfolio of heavy-tailed risk factors, when the portfolio admits the so-called second order regular variation property. Moreover, we explore the relationship between multivariate second order regular variation for a vector (e.g., risk factors) and the second order regular variation property for the sum of its components (e.g., the portfolio of risk factors). Results are illustrated with a variety of examples. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {381--410}, 
number = {3}, 
volume = {23}
}
@article{10.1109/icife.2010.5609467, 
year = {2010}, 
title = {{A new approach in multi-objective portfolio optimization using Value-at-Risk based risk measure}}, 
author = {Fulga, Cristinca and Dedu, Silvia}, 
journal = {2010 2nd IEEE International Conference on Information and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/icife.2010.5609467}, 
abstract = {{Mean-risk models have been widely used in solving portfolio selection problems in the last years. Since the mean-variance theory of Markowitz, an enormous amount of papers have been published extending or modifying the basic model in several directions like the simplification of the type and amount of input data, the introduction of alternative measures of risk, the incorporation of additional criteria or constraints. Recently, risk measures concerned with the left tails of distributions that evaluate the extremely unfavourable outcomes are used. The most used risk measure for such purposes is Value-at-Risk (VaR). In this paper we concentrate on the second direction of incorporating of a new risk measure in portfolio modeling. We define a new risk measure, which take into consideration the values exceeding a certain threshold in the extreme tail of the loss distribution, called Limited Value-at-Risk (LVaR). We study the properties of this risk measure. We build a new model for portfolio selection, named mean-LVaR model, in which risk is evaluated using LVaR risk measure. We study the properties of the new mean-risk model and compare it with the classical mean-VaR model. We derive the analytical form of LVaR risk measure in the case of normal distribution. We provide computational results and analyze the implications of using the mean-LVaR risk model in portfolio optimization problem. © 2010 IEEE.}}, 
pages = {765--769}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/1540496x.2014.998557, 
year = {2016}, 
title = {{Volatility Modeling and Value-at-Risk (VaR) Forecasting of Emerging Stock Markets in the Presence of Long Memory, Asymmetry, and Skewed Heavy Tails}}, 
author = {Gencer, Hatice Gaye and Demiralay, Sercan}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2014.998557}, 
abstract = {{In this article, we elaborate some empirical stylized facts of eight emerging stock markets for estimating one-day-and one-week-ahead Value-at-Risk (VaR) in the case of both short-and long-trading positions. We model the emerging equity market returns via APARCH, FIGARCH, and FIAPARCH models under Student-t and skewed Student-t innovations. The FIAPARCH models under skewed Student-t distribution provide the best fit for all the equity market returns. Furthermore, we model the daily and one-week-ahead market risks with the conditional volatilities generated from the FIAPARCH models and document that the skewed Student-t distribution yields the best results in predicting one-day-ahead VaR forecasts for all the stock markets. The results also reveal that the prediction power of the models deteriorate for longer forecasting horizons. © 2016 Taylor \& Francis Group, LLC.}}, 
pages = {639--657}, 
number = {3}, 
volume = {52}
}
@article{10.1080/09692290.2014.957233, 
year = {2015}, 
title = {{Predicting the unpredictable: Value-at-risk, performativity, and the politics of financial uncertainty}}, 
author = {Lockwood, Erin}, 
journal = {Review of International Political Economy}, 
issn = {09692290}, 
doi = {10.1080/09692290.2014.957233}, 
abstract = {{ABSTRACT: Starting from an observation about the high-profile predictive failures of Value-at-Risk (VaR), an internationally instituted financial risk model, this article has attempted to make sense of its continued use by analyzing its productive, rather than predictive, power. This line of inquiry leads me to identify VaR's (counter)performative effects and the way in which it produces banks as authoritative, responsible managers of an uncertain financial future. Viewing financial markets through the lens of Keynesian uncertainty and model performativity helps explain VaR's failures by revealing VaR to be an inherently limited and potentially destabilizing practice. Its use participates in the construction of a financial system that is only temporarily stable and controllable. At the same time, VaR is an important source of authority for banks vis-à-vis regulators and the public because it represents the future as statistically calculable and expert prediction as the optimal, objective mode of preparing for that future. This, in turn, makes less thinkable other responses to uncertainty – ones that might be better suited to contend with the possibility of devastating losses unforeseeable – and perhaps produced – by the widespread use of VaR. © 2014 Taylor \& Francis.}}, 
pages = {719--756}, 
number = {4}, 
volume = {22}
}
@article{10.1017/asb.2017.37, 
year = {2018}, 
title = {{Modelling insurance losses using contaminated generalised beta type-II distribution}}, 
author = {Chan, J.S.K. and Choy, S.T.B. and Makov, U.E. and Landsman, Z.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2017.37}, 
abstract = {{The four-parameter distribution family, the generalised beta type-II (GB2), also known as the transformed beta distribution, has been proposed for modelling insurance losses. As special cases, this family nests many distributions with light and heavy tails, including the lognormal, gamma, Weibull, Burr and generalised gamma distributions. This paper extends the GB2 family to the contaminated GB2 family, which offers many flexible features, including bimodality and a wide range of skewness and kurtosis. Properties of the contaminated distribution are derived and evaluated in a simulation study and the suitability of the contaminated GB2 distribution for actuarial purposes is demonstrated through two real loss data sets. Analysis of tail quantiles for the data suggests large differences in extreme quantile estimates for different loss distribution assumptions, showing that the selection of appropriate distributions has a significant impact for insurance companies. © Copyright Astin Bulletin 2018.}}, 
pages = {871--904}, 
number = {2}, 
volume = {48}
}
@article{10.1080/14697680903512786, 
year = {2012}, 
title = {{Bayesian Value-at-Risk with product partition models}}, 
author = {Bormetti, Giacomo and Giuli, Maria Elena De and Delpini, Danilo and Tarantola, Claudia}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680903512786}, 
abstract = {{In this paper we propose a novel Bayesian methodology for Value-at-Risk computation based on parametric Product Partition Models. Value-at-Risk is a standard tool for measuring and controlling the market risk of an asset or portfolio, and is also required for regulatory purposes. Its popularity is partly due to the fact that it is an easily understood measure of risk. The use of Product Partition Models allows us to remain in a Normal setting even in the presence of outlying points, and to obtain a closed-form expression for Value-at-Risk computation. We present and compare two different scenarios: a product partition structure on the vector of means and a product partition structure on the vector of variances. We apply our methodology to an Italian stock market data set from Mib30. The numerical results clearly show that Product Partition Models can be successfully exploited in order to quantify market risk exposure. The obtained Value-at-Risk estimates are in full agreement with Maximum Likelihood approaches, but our methodology provides richer information about the clustering structure of the data and the presence of outlying points. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {769--780}, 
number = {5}, 
volume = {12}
}
@article{10.1111/j.1467-6419.2009.00588.x, 
year = {2009}, 
title = {{The ten commandments for optimizing value-at-risk and daily capital charges}}, 
author = {McAleer, Michael}, 
journal = {Journal of Economic Surveys}, 
issn = {09500804}, 
doi = {10.1111/j.1467-6419.2009.00588.x}, 
abstract = {{Credit risk is the most important type of risk in terms of monetary value. Another key risk measure is market risk, which is concerned with stocks and bonds, and related financial derivatives, as well as exchange rates and interest rates. This paper is concerned with market risk management and monitoring under the Basel II Accord, and presents Ten Commandments for optimizing value-at-risk (VaR) and daily capital charges, based on choosing wisely from (1) conditional, stochastic and realized volatility; (2) symmetry, asymmetry and leverage; (3) dynamic correlations and dynamic covariances; (4) single index and portfolio models; (5) parametric, semi-parametric and non-parametric models; (6) estimation, simulation and calibration of parameters; (7) assumptions, regularity conditions and statistical properties; (8) accuracy in calculating moments and forecasts; (9) optimizing threshold violations and economic benefits; and (10) optimizing private and public benefits of risk management. For practical purposes, it is found that the Basel II Accord would seem to encourage excessive risk taking at the expense of providing accurate measures and forecasts of risk and VaR. © 2009 Blackwell Publishing Ltd.}}, 
pages = {831--849}, 
number = {5}, 
volume = {23}
}
@article{10.1016/j.csda.2010.07.016, 
year = {2012}, 
title = {{Vine copulas with asymmetric tail dependence and applications to financial return data}}, 
author = {Nikoloulopoulos, Aristidis K. and Joe, Harry and Li, Haijun}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2010.07.016}, 
abstract = {{It has been shown that vine copulas constructed from bivariate t copulas can provide good fits to multivariate financial asset return data. However, there might be stronger tail dependence of returns in the joint lower tail of assets than the upper tail. To this end, vine copula models with appropriate choices of bivariate reflection asymmetric linking copulas will be used to assess such tail asymmetries. Comparisons of various vine copulas are made in terms of likelihood fit and forecasting of extreme quantiles. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {3659--3673}, 
number = {11}, 
volume = {56}
}
@article{10.1198/016214507000001003, 
year = {2008}, 
title = {{Nonparametric risk management with generalized hyperbolic distributions}}, 
author = {Chen, Ying and Härdle, Wolfgang and Jeong, Seok-Oh}, 
journal = {Journal of the American Statistical Association}, 
issn = {01621459}, 
doi = {10.1198/016214507000001003}, 
abstract = {{In this article we propose the generalized hyperbolic adaptive volatility (GHADA) risk management model based on the generalized hyperbolic (GH) distribution and on a nonparametric adaptive methodology. Compared with the normal distribution, the GH distribution has semiheavy tails and represents the financial risk factors more appropriately. Nonparametric adaptive methodology has the desirable property of being able to estimate homogeneous volatility over a short time interval and reflects a sudden change in the volatility process. For the German mark/U.S. dollar exchange rate and German bank portfolio data, the proposed GHADA model provides more accurate Value at risk calculations than the models with assumptions of the normal and t distributions. © 2008 American Statistical Association.}}, 
pages = {910--923}, 
number = {483}, 
volume = {103}
}
@article{10.1007/s13385-013-0068-6, 
year = {2013}, 
title = {{Optimal risk transfers in insurance groups}}, 
author = {Asimit, Alexandru V. and Badescu, Alexandru M. and Tsanakas, Andreas}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-013-0068-6}, 
abstract = {{Optimal risk transfers are derived within an insurance group consisting of two separate legal entities, operating under potentially different regulatory capital requirements and capital costs. Consistent with regulatory practice, capital requirements for each entity are computed by either a value-at-risk or an expected shortfall risk measure. The optimality criterion consists of minimising the risk-adjusted value of the total group liabilities, with valuation carried out using a cost-of-capital approach. The optimisation problems are analytically solved and it is seen that optimal risk transfers often involve the transfer of tail risk (unlimited reinsurance layers) to the more weakly regulated entity. We show that, in the absence of a capital requirement for the credit risk that specifically arises from the risk transfer, optimal risk transfers achieve capital efficiency at the cost of increasing policyholder deficit. However, when credit risk is properly reflected in the capital requirement, incentives for tail-risk transfers vanish and policyholder welfare is restored. © 2013, DAV / DGVFM.}}, 
pages = {159--190}, 
number = {1}, 
volume = {3}
}
@article{10.1007/s00780-017-0328-4, 
year = {2017}, 
title = {{Risk bounds for factor models}}, 
author = {Bernard, Carole and Rüschendorf, Ludger and Vanduffel, Steven and Wang, Ruodu}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-017-0328-4}, 
abstract = {{Recent literature has investigated the risk aggregation of a portfolio X=(Xi)1≤i≤n under the sole assumption that the marginal distributions of the risks Xi are specified, but not their dependence structure. There exists a range of possible values for any risk measure of S=∑i=1nXi, and the dependence uncertainty spread, as measured by the difference between the upper and the lower bound on these values, is typically very wide. Obtaining bounds that are more practically useful requires additional information on dependence. Here, we study a partially specified factor model in which each risk Xi has a known joint distribution with the common risk factor Z, but we dispense with the conditional independence assumption that is typically made in fully specified factor models. We derive easy-to-compute bounds on risk measures such as Value-at-Risk (VaR) and law-invariant convex risk measures (e.g. Tail Value-at-Risk (TVaR)) and demonstrate their asymptotic sharpness. We show that the dependence uncertainty spread is typically reduced substantially and that, contrary to the case in which only marginal information is used, it is not necessarily larger for VaR than for TVaR. © 2017, Springer-Verlag Berlin Heidelberg.}}, 
pages = {631--659}, 
number = {3}, 
volume = {21}
}
@article{10.1088/1742-6596/1217/1/012093, 
year = {2019}, 
title = {{Valuing risk of changes on corn (zea mays) prices by considering skewness and kurtosis parameters}}, 
author = {Rahmawati, R and Tarno and Maruddani, D A I and Hoyyi, A}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1217/1/012093}, 
abstract = {{Value at Risk (VaR) is a measure of risk value by estimating the potential of maximum loss that might occur in future in certain time and a certain level of confidence. The risks faced in agriculture might be in production and marketing of agricultural products. This research aims to estimate the risk value in agricultural commodity-particularly dried shelled corn. Data used was the monthly data of the price of producers in Grobogan in period of November 2015 to July 2018. The data showed the return with the normally undistributed value of skewness and kurtosis. For this, the method of Cornish-Fisher was used in this research to measure the VaR in return that was normally undistributed. At the confidence level of 95\%, with the initial capital of 1 million Rupiah, the risk that probably occurred in the following month (1 period) was by Rp 369,173.40 without considering the skewness and kurtosis, and by Rp 22,439.13 by considering skewness and kurtosis. The result of the research showed that, for the return that was normally undistributed, the level of risk obtained from the measurement of VaR by concerning skewness and kurtosis was found more accurate compared to the one without considering the skewness and kurtosis. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012093}, 
number = {1}, 
volume = {1217}
}
@article{10.1007/978-3-319-00035-0_41, 
year = {2013}, 
title = {{Comparison of some chosen tests of independence of value-at-risk violations}}, 
author = {Piontek, Krzysztof}, 
journal = {Studies in Classification, Data Analysis, and Knowledge Organization}, 
issn = {14318814}, 
doi = {10.1007/978-3-319-00035-0\_41}, 
abstract = {{Backtesting is the necessary statistical procedure to evaluate performance of Value-at-Risk models. A satisfactory test should allow to detect both deviations from the correct probability of violations, as well as their clustering. Many researchers and practitioners underline the importance of the lack of any dependence in the hit series over time. If the independence condition is not met, it may be a signal that the Value at Risk model reacts too slowly to changes in the market. If the violation sequence exhibits a dependence other than first orderMarkov dependence, the classical test of Christoffersen would fail to detect it. This article presents a test based on analysis of duration, having power against more general forms of dependence, based on the same set of information as the Christoffersen test, i.e. hit series. The aim of this article is to analyze presented backtests, focusing on the aspect of limited data sets and the power of tests. Simulated data representing asset returns are used here. This paper is a continuation of earlier research done by the author. © Springer International Publishing Switzerland 2013.}}, 
pages = {407--416}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.cogsys.2019.09.024, 
year = {2020}, 
title = {{On portfolio management with value at risk and uncertain returns via an artificial neural network scheme}}, 
author = {Mohammadi, Sahar and Nazemi, Alireza}, 
journal = {Cognitive Systems Research}, 
issn = {13890417}, 
doi = {10.1016/j.cogsys.2019.09.024}, 
abstract = {{This paper focuses on the computation issue of portfolio optimization with scenario-based Value-at-Risk. The main idea is to replace the portfolio selection models with linear programming problems. According to the convex optimization theory and some concepts of ordinary differential equations, a neural network model for solving linear programming problems is presented. The equilibrium point of the proposed model is proved to be equivalent to the optimal solution of the original problem. It is also shown that the proposed neural network model is stable in the sense of Lyapunov and it is globally convergent to an exact optimal solution of the portfolio selection problem with uncertain returns. Several illustrative examples are provided to show the feasibility and the efficiency of the proposed method in this paper. © 2019 Elsevier B.V.}}, 
pages = {247--263}, 
number = {NA}, 
volume = {59}
}
@article{10.1016/j.jbankfin.2005.07.011, 
year = {2006}, 
title = {{Internal ratings systems, implied credit risk and the consistency of banks' risk classification policies}}, 
author = {Jacobson, Tor and Lindé, Jesper and Roszbach, Kasper}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.07.011}, 
abstract = {{This paper aims at improving our understanding of internal risk rating systems (IRS) at large banks, of the way in which they are implemented, and at verifying if IRS produce consistent estimates of banks' loan portfolio credit risk. An important property of our work is that the size of our data set allows us to derive measures of credit risk without making any assumptions about correlations between loans, by applying Carey's [Carey, Mark, 1998. Credit risk in private debt portfolios. Journal of Finance LIII (4), 1363-1387] non-parametric Monte Carlo re-sampling method. We find substantial differences between the implied loss distributions of two banks with equal "regulatory" risk profiles; both expected losses and the credit loss rates at a wide range of loss distribution percentiles vary considerably. Such variation will translate into different levels of required economic capital. Our results also confirm the quantitative importance of size for portfolio credit risk: for common parameter values, we find that tail risk can be reduced by up to 40\% by doubling portfolio size. Our analysis makes clear that not only the formal design of a rating system, but also the way in which it is implemented (e.g. a rating grade composition; the degree of homogeneity within rating classes) can be quantitatively important for the shape of credit loss distributions and thus for banks' required capital structure. The evidence of differences between lenders also hints at the presence of differentiated market equilibria, that are more complex than might otherwise be supposed: different lending or risk management "styles" may emerge and banks strike their own balance between risk-taking and (the cost of) monitoring (that risk). © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {1899--1926}, 
number = {7}, 
volume = {30}
}
@article{10.3390/e22070789, 
year = {2020}, 
title = {{Looking at extremes without going to extremes: A new self-exciting probability model for extreme losses in financial markets}}, 
author = {Bień-Barkowska, Katarzyna}, 
journal = {Entropy}, 
issn = {10994300}, 
doi = {10.3390/e22070789}, 
pmid = {33286560}, 
pmcid = {PMC7517354}, 
abstract = {{Forecasting market risk lies at the core of modern empirical finance. We propose a new self-exciting probability peaks-over-threshold (SEP-POT) model for forecasting the extreme loss probability and the value at risk. The model draws from the point-process approach to the POT methodology but is built under a discrete-time framework. Thus, time is treated as an integer value and the days of extreme loss could occur upon a sequence of indivisible time units. The SEP-POT model can capture the self-exciting nature of extreme event arrival, and hence, the strong clustering of large drops in financial prices. The triggering effect of recent events on the probability of extreme losses is specified using a discrete weighting function based on the at-zero-truncated Negative Binomial (NegBin) distribution. The serial correlation in the magnitudes of extreme losses is also taken into consideration using the generalized Pareto distribution enriched with the time-varying scale parameter. In this way, recent events affect the size of extreme losses more than distant events. The accuracy of SEP-POT value at risk (VaR) forecasts is backtested on seven stock indexes and three currency pairs and is compared with existing well-recognized methods. The results remain in favor of our model, showing that it constitutes a real alternative for forecasting extreme quantiles of financial returns. © 2020 by the authors.}}, 
pages = {789}, 
number = {7}, 
volume = {22}
}
@article{10.1016/j.insmatheco.2012.09.007, 
year = {2012}, 
title = {{Analytical calculation of risk measures for variable annuity guaranteed benefits}}, 
author = {Feng, Runhuan and Volkmer, Hans W.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.09.007}, 
abstract = {{With the increasing complexity of investment options in life insurance, more and more life insurers have adopted stochastic modeling methods for the assessment and management of insurance and financial risks. The most prevalent approach in market practice, Monte Carlo simulation, has been observed to be time consuming and sometimes extremely costly. In this paper we propose alternative analytical methods for the calculation of risk measures for variable annuity guaranteed benefits on a stand-alone basis. The techniques for analytical calculations are based on the study of geometric Brownian motion and its integral. Another novelty of the paper is to propose a quantitative model which assesses both market risk on the liability side and revenue risk on the asset side in the same framework from the viewpoint of risk management. As we demonstrate by numerous examples on quantile risk measure and conditional tail expectation, the methods and numerical algorithms developed in this paper appear to be both accurate and computationally efficient. © 2012 Elsevier B.V.}}, 
pages = {636--648}, 
number = {3}, 
volume = {51}
}
@article{10.1111/coin.12456, 
year = {2021}, 
title = {{To assess the multiperiod market risk with deep learning method taking the boosting additive quantile regression as an example}}, 
author = {Guan, Min}, 
journal = {Computational Intelligence}, 
issn = {08247935}, 
doi = {10.1111/coin.12456}, 
abstract = {{Compared with current risks, future risks are more important for investment decisions and risk management. This paper modifies the Square Root of Time Rule with the boosting additive quantile regression model to forecast multi-period horizon market risk. In J. P. Morgan's RiskMetrics Model, the k-period horizon value at risk (VaR) equals to (Formula presented.). Since its assumptions are too strict, expected capacity of Risk Metrics is not so well. Taylor relaxed assumptions of this model, used the GARCH model to replace the IGARCH model, and obtained multi-period horizon VaR, which is a nonlinear function of the one-step-ahead volatility forecast (Formula presented.).The conditional mean μt is zero in Taylor's model, but Tsay pointed out that this assumption (μt = 0) does not always hold. Therefore, we relax this assumption about the conditional mean, and obtain the VaR which is mixed function consisting of two parts, one is a linear function of conditional mean, and the other is a nonlinear function of (Formula presented.), given the holding horizon k. For our mixed VaR function, we chose a more appropriate method, the boosting additive quantile regression model, to forecast multi-period horizon VaR. Taking log-returns of the Hang Seng Index from January 1, 2007 to November 1, 2016 as the sample, weforecast the 5-, 10-, 15-, and 20-day horizon VaRs, and compare the prediction accuracy of Morgan's model withour quantile regression model through likelihood ratio tests. Results show that VaR based on the quantile regression model is not only more accurate, but also sensitive to volatility, and is conducive to maintaining a reasonable risk reserve level for financial institutions, enabling them to pay less for regulation andachieve incentive compatibility. © 2021 Wiley Periodicals LLC.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/wsc.2011.6147743, 
year = {2011}, 
title = {{Monte Carlo estimation of value-at-risk, conditional value-at-risk and their sensitivities}}, 
author = {Hong, L. Jeff and Liu, Guanzwu}, 
journal = {Proceedings of the 2011 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2011.6147743}, 
abstract = {{Value-at-risk and conditional value at risk are two widely used risk measures, employed in the financial industry for risk management purposes. This tutorial discusses Monte Carlo methods for estimating value-at-risk, conditional value-at-risk and their sensitivities. By relating the mathematical representation of value-at-risk to that of conditional value-at-risk, it provides a unified view of simulation methodologies for both risk measures and their sensitivities. © 2011 IEEE.}}, 
pages = {95--107}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/ipecon.2010.5696991, 
year = {2010}, 
title = {{Risk assessment of energy trading for a generation company with bilateral contracts}}, 
author = {Nerves, A C and Umali, L M D}, 
journal = {2010 Conference Proceedings IPEC}, 
issn = {NA}, 
doi = {10.1109/ipecon.2010.5696991}, 
abstract = {{A methodology is developed to quantify the financial risk for a multi-unit generation company with bilateral contracts in a competitive electricity market, using Value-at-Risk as a risk measure. A Monte Carlo simulation technique is used to calculate the Value-at-Risk by modeling different pricing scenarios, bids and schedules of generation. An artificial neural network is used to forecast hourly market prices which determine generating unit schedules through a price-based unit commitment method using a genetic algorithm approach. An economic dispatch then determines local generation and purchase schedules using a linear programming solution. The profit and loss distribution that is obtained from the Monte Carlo simulations becomes the basis for Value-at-Risk calculations. Bidding strategies are formulated from a sensitivity analysis of the Value-at-Risk for various market conditions. ©2010 IEEE.}}, 
pages = {1123--1128}, 
number = {NA}, 
volume = {NA}
}
@article{10.22237/jmasm/1525133100, 
year = {2018}, 
title = {{Modeling insurance claims using Flexible skewed and mixture probability distributions}}, 
author = {Leinwander, Aaron J and Aziz, Mohammad A}, 
journal = {Journal of Modern Applied Statistical Methods}, 
issn = {15389472}, 
doi = {10.22237/jmasm/1525133100}, 
abstract = {{The normal distribution comes as a first choice when fitting real data, but it may not be suitable if the assumed distribution deviates from normality. Flexible skewed distributions are capable of including skewness and taking into account multimodality. They may be applied to find appropriate distributions for describing the claim amounts in insurance. The objective is to model insurance claims using a set of flexible skewed and mixture probability distributions, and to test how well they fit the claims. Results indicate the skew-t distribution and alpha-skew Laplace distribution are able to describe unimodal claims accurately, whereas scale mixture of skew-normal and skew-t distributions are better alternatives to both unimodal and bimodal conventional distributions such as skewnormal, alpha skew-normal, and mixture of normal distributions. The tail risk measures such as value at risk and tail value at risk are estimated as judgment criteria to assess the fitness of the models. © 2018 JMASM, Inc.}}, 
number = {1}, 
volume = {17}
}
@article{10.1016/s0377-2217(02)00776-2, 
year = {2003}, 
title = {{Value at risk calculation through ARCH factor methodology: Proposal and comparative analysis}}, 
author = {Semper, J.David Cabedo and Clemente, Ismael Moya}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/s0377-2217(02)00776-2}, 
abstract = {{In this paper we put forward a new method to estimate value at risk (VaR), autoregressive conditional heteroskedastic (ARCH) factor, which combines multivariate analysis with ARCH models. Firstly, from a set of correlated portfolio risk factors, we derive a smaller uncorrelated risk factors set, by applying multivariate analysis. Secondly, we use ARCH schemes to model uncorrelated factors historical behaviour. Thirdly, we use the estimated models to predict future values for factors standard deviation. From them, VaR calculation is immediate. In this way, ARCH factor methodology overcomes the multivariate ARCH models drawbacks, which, in practice, make these unworkable for VaR calculation purposes. We apply the proposed methodology over a set of foreign exchange risk exposed portfolios, obtaining better results than those reached when J.P. Morgan's Riskmetrics is used. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {516--528}, 
number = {3}, 
volume = {150}
}
@article{10.1016/s0169-2070(01)00122-4, 
year = {2002}, 
title = {{Forecasting value at risk allowing for time variation in the variance and kurtosis of portfolio returns}}, 
author = {Guermat, Cherif and Harris, Richard D.F.}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/s0169-2070(01)00122-4}, 
abstract = {{A common approach to forecasting the value at risk (VaR) of a portfolio is to assume a parametric density function for portfolio returns, and to estimate the parameters of the density function by maximum likelihood using historical data. In order to allow for volatility clustering in short horizon returns, this approach is typically combined with a conditional variance model such as EWMA or GARCH. However, these models implicitly assume that while the volatility of returns may be time-varying, the kurtosis of the return distribution is constant, at least over the estimation sample. In this paper, we show that the EWMA variance estimator can be obtained as a special case of a more general, exponentially weighted maximum likelihood (EWML) procedure that potentially allows for time-variation not only in the variance of the return distribution, but also in its higher moments. We use EWML to forecast VaR allowing for time-variation in both the variance and the kurtosis of daily equity returns. Our results show that the EWML based VaR forecasts are generally more accurate than those generated by both the EWMA and GARCH models, particularly at high VaR confidence levels. © 2002 International Institute of Forecasters. Published by Elsevier Science B.V. All rights reserved.}}, 
pages = {409--419}, 
number = {3}, 
volume = {18}
}
@article{10.1016/j.econmod.2016.03.007, 
year = {2016}, 
title = {{Solvency capital requirement for a temporal dependent losses in insurance}}, 
author = {Araichi, Sawssen and Peretti, Christian de and Belkacem, Lotfi}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2016.03.007}, 
abstract = {{This article addresses the appropriate modeling of losses for the insurance sector. In fact, solvency 2 framework has suggested some formulas to evaluate losses and solvency capital using an internal approach. However, these formulas where derived under the assumption of independent losses. Thus, the amount of capital may be inaccurate when losses are dependent, which is the case in practice. The aim of this paper is to investigate temporal dependence structure among claim amounts (losses). For that, a novel model named autoregressive conditional amount (ACA) model handling the dynamic behavior of claim amounts in insurance companies is proposed. Results show that ACA models allow to predict accurately the future claims. Moreover, a measure of risk namely value at risk (VaR) ACA that could hedge daily dependent losses is provided. By backtesting techniques, empirical results show that the new VaR ACA can efficiently evaluate the coverage amount of risks. © 2016 Elsevier B.V.}}, 
pages = {588--598}, 
number = {NA}, 
volume = {58}
}
@article{10.1016/s1042-4431(01)00056-7, 
year = {2002}, 
title = {{Overnight futures trading: Now even Australia and US have common trading hours}}, 
author = {Fong, Kingsley and Martens, Martin}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/s1042-4431(01)00056-7}, 
abstract = {{Overnight futures trading is available in USA, France and Australia. The overnight prices can be used to compute 24 h returns for two international markets over exactly the same time interval, even though the countries in which these markets operate are in completely different time zones. These synchronous returns make it possible to compute accurate daily correlation measures, which can for example be used for daily value-at-risk (VaR). Using Australian overnight index futures prices we find the correlation between USA and Australia to be about 55\%, in stark contrast to near-zero correlation measures obtained from non-synchronous closing prices. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {167--182}, 
number = {2}, 
volume = {12}
}
@article{10.18488/journal.aefr.2020.105.502.515, 
year = {2020}, 
title = {{Probability approach in estimating value at risk of bond portfolios for effective hedging}}, 
author = {Kumar, Bavani Chandra and Ramasamy, Ravindran and Mohamed, Zulkifflee}, 
journal = {Asian Economic and Financial Review}, 
issn = {23052147}, 
doi = {10.18488/journal.aefr.2020.105.502.515}, 
abstract = {{Bond portfolios are growing as the investors like banks, fund managers and institutional investors are more interested in Malaysian Bond market comprises conventional and Islamic (Sukuk) Bonds. The Malaysian Sukuk market has provided greater diversification especially catering to the needs of investors such as Islamic pension funds and Islamic insurance companies which can only invest in Shariah-compliant instruments. As the Sukuk and conventional bonds complement each other in Malaysia, we have chosen from real portfolios framed by popular mutual funds to study the risk management practices adopted by these portfolios and to observe the differences and the reasons for them. The link between duration and price volatility is widely used for managing risk positions and portfolio value protection due to the market volatility which can cause huge losses on large exposures. We have applied standard risk measures of bond portfolios, portfolio duration and portfolio convexity to assess the Value at Risk (VaR) accurately and this will lead to minimum cost of hedging. Fair value hedging requires accurate expected loss, which is arrived at by applying these estimates of duration and convexity. We used Bond funds which invest more than 74\% of funds in Islamic Bond or Conventional Bond which are designed and implemented by popular Banks and Mutual funds. These results helped in computing the VaR which subsequently could be hedged with the available financial instruments such as interest rate futures in Bursa derivative market effectively. © 2020 AESS Publications.}}, 
pages = {502--515}, 
number = {5}, 
volume = {10}
}
@article{10.21314/jop.2020.250, 
year = {2021}, 
title = {{Measurement of operational risk regulatory capital in the banking sector: Developed countries versus emerging markets}}, 
author = {Hassanein, Medhat and Bouaddi, Mohammed and Karim, Talha}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2020.250}, 
abstract = {{This paper addresses operational risk as a fundamental risk type faced by banks in emerging and developed economies. We explore several models to specify the marginal and joint distributions of the types of operational losses that reflect loss frequencies and severity distribution(s), using international data published by a group of banks from developed and emerging economies. Our results reveal that a uniform approach to model operational risk in both types of economy may lead to the overestimation or underestimation of capital losses in banks. This could result in opportunity costs of holding excessive capital to mitigate operational losses, or in extra costs resulting from an underestimation of the capital required. © 2021 Infopro Digital Risk (IP) Limited.}}, 
number = {1}, 
volume = {16}
}
@article{10.1111/j.1467-9965.2009.00377.x, 
year = {2009}, 
title = {{Constant proportion portfolio insurance in the presence of jumps in asset prices}}, 
author = {Cont, Rama and Tankov, Peter}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/j.1467-9965.2009.00377.x}, 
abstract = {{Constant proportion portfolio insurance (CPPI) allows an investor to limit downside risk while retaining some upside potential by maintaining an exposure to risky assets equal to a constant multiple of the cushion, the difference between the current portfolio value and the guaranteed amount. Whereas in diffusion models with continuous trading, this strategy has no downside risk, in real markets this risk is nonnegligible and grows with the multiplier value. We study the behavior of CPPI strategies in models where the price of the underlying portfolio may experience downward jumps. Our framework leads to analytically tractable expressions for the probability of hitting the floor, the expected loss, and the distribution of losses. This allows to measure the gap risk but also leads to a criterion for adjusting the multiplier based on the investor's risk aversion. Finally, we study the problem of hedging the downside risk of a CPPI strategy using options. The results are applied to a jump-diffusion model with parameters estimated from returns series of various assets and indices. © Copyright the Authors. Journal Compilation © 2009 Wiley Periodicals, Inc.}}, 
pages = {379--401}, 
number = {3}, 
volume = {19}
}
@article{10.1016/j.jbankfin.2005.04.013, 
year = {2006}, 
title = {{The hidden dangers of historical simulation}}, 
author = {Pritsker, Matthew}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.04.013}, 
abstract = {{Many large financial institutions compute the Value-at-Risk (VaR) of their trading portfolios using historical simulation based methods, but the methods' properties are not well understood. This paper theoretically and empirically examines the historical simulation method, a variant of historical simulation introduced by Boudoukh et al. [Boudoukh, J., Richardson, M., Whitelaw, R., 1998. The best of both worlds, Risk 11(May) 64-67] (BRW), and the filtered historical simulation method (FHS) of Barone-Adesi et al. [Barone-Adesi, G., Bourgoin F., Giannopoulos, K., 1998. Don't look back. Risk 11(August) 100-104; Barone-Adesi, G., Giannopoulos K., Vosper L., 1999. VaR without correlations for nonlinear portfolios. Journal of Futures Markets 19(April) 583-602]. The historical simulation and BRW methods are both under-responsive to changes in conditional risk; and respond to changes in risk in an asymmetric fashion: measured risk increases when the portfolio experiences large losses, but not when it earns large gains. The FHS method is promising, but its risk estimates are variable in small samples, and its assumption that correlations are constant is violated in large samples. Additional refinements are needed to account for time-varying correlations; and to choose the appropriate length of the historical sample period. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {561--582}, 
number = {2}, 
volume = {30}
}
@article{10.1111/rssb.12254, 
year = {2018}, 
title = {{Estimation of tail risk based on extreme expectiles}}, 
author = {Daouia, Abdelaati and Girard, Stéphane and Stupfler, Gilles}, 
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, 
issn = {13697412}, 
doi = {10.1111/rssb.12254}, 
abstract = {{We use tail expectiles to estimate alternative measures to the value at risk and marginal expected shortfall, which are two instruments of risk protection of utmost importance in actuarial science and statistical finance. The concept of expectiles is a least squares analogue of quantiles. Both are M-quantiles as the minimizers of an asymmetric convex loss function, but expectiles are the only M-quantiles that are coherent risk measures. Moreover, expectiles define the only coherent risk measure that is also elicitable. The estimation of expectiles has not, however, received any attention yet from the perspective of extreme values. Two estimation methods are proposed here, either making use of quantiles or relying directly on least asymmetrically weighted squares. A main tool is first to estimate large values of expectile-based value at risk and marginal expected shortfall within the range of the data, and then to extrapolate the estimates obtained to the very far tails. We establish the limit distributions of both of the resulting intermediate and extreme estimators. We show via a detailed simulation study the good performance of the procedures and present concrete applications to medical insurance data and three large US investment banks. © 2017 Royal Statistical Society}}, 
pages = {263--292}, 
number = {2}, 
volume = {80}
}
@article{10.1504/ijcsm.2020.111119, 
year = {2020}, 
title = {{Jump OpVaR on option liquidity}}, 
author = {Bahiraie, Alireza and Alipour, Mohammad}, 
journal = {International Journal of Computing Science and Mathematics}, 
issn = {17525055}, 
doi = {10.1504/ijcsm.2020.111119}, 
abstract = {{The impact of operational risk on the option pricing through the extension of Mitra’s model with Merton’s jump diffusion model is assessed. A partial integral differential equation (PIDE) is derived and the impact of parameters of Merton’s model on operational risk and option value by operational Value-at-Risk measure, which is derived by Mitra (2013), is studied. The option values in the presence of operational risk on S\&P500 index are computed. The result shows that most operational risks occur around at-the-money options. The result shows that the parameters T, λ, µ and δ have the similar impact on OpVaR, i.e., the OpVaR decreases with increase in the parameters’ values. However, the interest rate showed marginal effect, which decreases with an increase for K < S and OpVaR increases as r increases for K > S. Copyright © 2020 Inderscience Enterprises Ltd.}}, 
pages = {147--156}, 
number = {2}, 
volume = {12}
}
@article{10.1007/s11222-013-9376-6, 
year = {2014}, 
title = {{Percentiles of sums of heavy-tailed random variables: Beyond the single-loss approximation}}, 
author = {Hernández, Lorenzo and Tejero, Jorge and Suárez, Alberto and Carrillo-Menéndez, Santiago}, 
journal = {Statistics and Computing}, 
issn = {09603174}, 
doi = {10.1007/s11222-013-9376-6}, 
abstract = {{A perturbative approach is used to derive approximations of arbitrary order to estimate high percentiles of sums of positive independent random variables that exhibit heavy tails. Closed-form expressions for the successive approximations are obtained both when the number of terms in the sum is deterministic and when it is random. The zeroth order approximation is the percentile of the maximum term in the sum. Higher orders in the perturbative series involve the right-truncated moments of the individual random variables that appear in the sum. These censored moments are always finite. As a result, and in contrast to previous approximations proposed in the literature, the perturbative series has the same form regardless of whether these random variables have a finite mean or not. For high percentiles, and specially for heavier tails, the quality of the estimate improves as more terms are included in the series, up to a certain order. Beyond that order the convergence of the series deteriorates. Nevertheless, the approximations obtained by truncating the perturbative series at intermediate orders are remarkably accurate for a variety of distributions in a wide range of parameters. © 2013 Springer Science+Business Media New York.}}, 
pages = {377--397}, 
number = {3}, 
volume = {24}
}
@article{10.1007/978-3-642-35470-0_1, 
year = {2013}, 
title = {{Stochastic insuring critical path problem with value-at-risk criterion}}, 
author = {Li, ZhenHong and Liu, YanKui and Zang, WenJuan}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-3-642-35470-0\_1}, 
abstract = {{In this paper, we study a class of two-stage stochastic optimization for insuring critical path problems, in which the first-stage objective is to minimize the Value-at-Risk, and the second-stage objective is to maximize the insured task durations. Subsequently, we turn the proposed problem into its equivalent form. For general task duration distributions, the problem is also very complex, so we cannot solve it by conventional optimization methods. We use stochastic simulation method to estimate Value-at-Risk. Furthermore, we employ a hybrid binary particle swarm optimization algorithm (BPSO) to solve it, where the dynamic programming method (DPM) is used in the second-stage problem. Finally, we conduct some numerical experiments to illustrate the feasibility and effectiveness of the designed algorithm. © 2013 Springer-Verlag.}}, 
pages = {3--10}, 
number = {VOL. 3}, 
volume = {225 LNEE}
}
@article{10.1007/978-88-470-2342-0_22, 
year = {2012}, 
title = {{Risk measures and Pareto style tails}}, 
author = {Fiori, Anna Maria and Gianin, Emanuela Rosazza and Spasova, Anna}, 
issn = {NA}, 
doi = {10.1007/978-88-470-2342-0\_22}, 
abstract = {{We discuss asymptotic scaling rules for VaR and CVaR in the context of distributions with Pareto style tails. These relationships are easily turned into semiparametric VaR and CVaR estimates with appealing backtesting properties. © Springer-Verlag Italia 2012.}}, 
pages = {183--191}, 
number = {NA}, 
volume = {NA}
}
@article{10.1155/2021/2704142, 
year = {2021}, 
title = {{Risk Analysis of Gold Prices in Pakistan Using Extreme Value Theory}}, 
author = {Khan, Ghulam Raza and Abdulrahman, Alanazi Talal and Alamri, Osama and Iqbal, Zahid and Ahmad, Maqsood}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2021/2704142}, 
abstract = {{Extreme value theory (EVT) is useful for modeling the impact of crashes or situations of extreme stress on investor portfolios. EVT is mostly utilized in financial modeling, risk management, insurance, and hydrology. The price of gold fluctuates considerably over time, and this introduces a risk on its own. The goal of this study is to analyze the risk of gold investment by applying the EVT to historical daily data for extreme daily losses and gains in the price of gold. We used daily gold prices in the Pakistan Bullion Market from August 1, 2011 to July 30, 2021. This paper covers two methods such as Block Maxima (BM) and Peak Over Threshold (POT) modeling. The risk measures which are adopted in this paper are Value at Risk (VaR) and Expected Shortfall (ES). The point and interval estimates of VaR and ES are obtained by fitting the Generalized Pareto (GPA) distribution. Moreover, in this paper, return-level forecasting is also included for the next 5 and 10 years by analyzing the Generalized Extreme Value (GEV) distribution. © 2021 Ghulam Raza Khan et al.}}, 
pages = {1--18}, 
number = {NA}, 
volume = {2021}
}
@article{10.1016/j.trd.2017.05.007, 
year = {2017}, 
title = {{A Value-at-Risk (VAR) approach to routing rail hazmat shipments}}, 
author = {Hosseini, S. Davod and Verma, Manish}, 
journal = {Transportation Research Part D: Transport and Environment}, 
issn = {13619209}, 
doi = {10.1016/j.trd.2017.05.007}, 
abstract = {{Hazardous materials (hazmat) accidents are rare though the consequences could be disastrous. Given the possibility of low probability – high consequence event, a risk-averse routing hazmat shipment is necessary. We propose a value-at-risk (VaR) approach to route rail hazmat shipments, using the best train configuration, over a given railroad network with limited number of train services such that the transport risk as measured by VaR is minimized. Freight train derailment reports of the Federal Railroad Administration were analyzed to develop expressions that would incorporate characteristics of railroad accidents, and then to estimate the different inputs. The proposed methodology was used to study several problem instances generated using the realistic network of a railroad operator, and to demonstrate that it is possible to develop different routes for shipments depending on the risk preference of the decision maker. © 2017 Elsevier Ltd}}, 
pages = {191--211}, 
number = {NA}, 
volume = {54}
}
@article{10.1016/j.physa.2019.122579, 
year = {2019}, 
title = {{Liquidity-adjusted value-at-risk optimization of a multi-asset portfolio using a vine copula approach}}, 
author = {Janabi, Mazin A.M. Al and Ferrer, Román and Shahzad, Syed Jawad Hussain}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.122579}, 
abstract = {{This paper develops a novel approach to assess liquidity-adjusted Value-at-Risk (LVaR) optimization of multi-asset portfolios based on vine copulas and LVaR models. This framework is applied to stock markets of the G-7 countries, gold, commodities and Bitcoin. The results show that our approach is superior to the classical mean–variance Markowitz portfolio technique in terms of the optimal portfolio selection under a number of realistic operational and budget constraints. We find that both Bitcoin and gold improves the risk-return performance of the G-7 stock portfolio. However, Bitcoin (gold) performs better under a scenario of only long-positions (when short-selling is allowed). © 2019 Elsevier B.V.}}, 
pages = {122579}, 
number = {NA}, 
volume = {536}
}
@article{10.1080/1351847x.2019.1652665, 
year = {2020}, 
title = {{Value-at-Risk dynamics: a copula-VAR approach}}, 
author = {Luca, Giovanni De and Rivieccio, Giorgia and Corsaro, Stefania}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2019.1652665}, 
abstract = {{In financial research and among risk management practitioners the estimation of a correct measure of the Value-at-Risk still proves interesting. A current approach, the multivariate CAViaR allows to provide an accurate measure of VaR modelling the joint dynamics in the Values-at-Risk by capturing the quantile conditional dependence structure to take into account financial contagion risk. The parameter estimates are based on multiple quantile regressions which assume linear combinations of sample quantiles. In this paper we argue that the analysis of multiple time-series aimed to model the time-varying quantile dependence can require non-linear and flexible estimation procedures. To this end, we examine the conditional quantile behaviour of some assets included in the Eurostoxx50 with respect to the quantile of a portfolio representing the market with a new copula-based quantile Vector AutoRegressive approach, and compare the results with the bivariate CAViaR model. Findings show that the copula approach is highly competitive, providing a time-varying model aimed to give a better specification of the Value-at-Risk, especially in terms of loss functions. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--15}, 
number = {2-3}, 
volume = {26}
}
@article{10.1016/j.rfe.2013.05.001, 
year = {2013}, 
title = {{Estimation of tail-related risk measures in the Indian stock market: An extreme value approach}}, 
author = {Karmakar, Madhusudan}, 
journal = {Review of Financial Economics}, 
issn = {10583300}, 
doi = {10.1016/j.rfe.2013.05.001}, 
abstract = {{The purpose of the study is to estimate tail-related risk measures using extreme value theory (EVT) in the Indian stock market. The study employs a two stage approach of conditional EVT originally proposed by McNeil and Frey (2000) to estimate dynamic Value at Risk (VaR) and expected shortfall (ES). The dynamic risk measures have been estimated for different percentiles for negative and positive returns. The estimates of risk measures computed under different quantile levels exhibit strong stability across a range of the selected thresholds, implying the accuracy and reliability of the estimated quantile based risk measures. © 2013 Elsevier Inc.}}, 
pages = {79--85}, 
number = {3}, 
volume = {22}
}
@article{10.1016/j.insmatheco.2010.09.001, 
year = {2011}, 
title = {{The strictest common relaxation of a family of risk measures}}, 
author = {Roorda, Berend and Schumacher, J.M.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2010.09.001}, 
abstract = {{Operations which form new risk measures from a collection of given (often simpler) risk measures have been used extensively in the literature. Examples include convex combination, convolution, and the worst-case operator. Here we study the risk measure that is constructed from a family of given risk measures by the best-case operator; that is, the newly constructed risk measure is defined as the one that is as restrictive as possible under the condition that it accepts all positions that are accepted under any of the risk measures from the family. In fact we define this operation for conditional risk measures, to allow a multiperiod setting. We show that the well-known VaR risk measure can be constructed from a family of conditional expectations by a combination that involves both worst-case and best-case operations. We provide an explicit description of the acceptance set of the conditional risk measure that is obtained as the strictest common relaxation of two given conditional risk measures. © 2010.}}, 
pages = {29--34}, 
number = {1}, 
volume = {48}
}
@article{10.1016/j.sysconle.2008.07.007, 
year = {2009}, 
title = {{Parameter estimation with expected and residual-at-risk criteria}}, 
author = {Calafiore, Giuseppe and Topcu, Ufuk and Ghaoui, Laurent El}, 
journal = {Systems \& Control Letters}, 
issn = {01676911}, 
doi = {10.1016/j.sysconle.2008.07.007}, 
abstract = {{In this paper we study a class of uncertain linear estimation problems in which the data are affected by random uncertainty. We consider two estimation criteria, one based on minimization of the expected ℓ1 or ℓ2 norm residual and one based on minimization of the level within which the ℓ1 or ℓ2 norm residual is guaranteed to lie with an a-priori fixed probability (residual at risk). The random uncertainty affecting the data is characterized by means of its first two statistical moments, and the above criteria are intended in a worst-case probabilistic sense, that is worst-case expectations and probabilities over all possible distribution having the specified moments are considered. The ensuing estimation problems can be solved efficiently via convex programming, yielding exact solutions in the ℓ2 norm case and upper-bounds on the optimal solutions in the ℓ1 case. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {39--46}, 
number = {1}, 
volume = {58}
}
@article{10.1002/for.2756, 
year = {2021}, 
title = {{Forecasting value at risk and conditional value at risk using option market data}}, 
author = {Molino, Annalisa and Sala, Carlo}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2756}, 
abstract = {{We forecast monthly value at risk (VaR) and conditional value at risk (CVaR) using option market data and four different econometric techniques. Independent from the econometric approach used, all models produce quick to estimate forward-looking risk measures that do not depend from the amount of historical data used and that, through the implied moments of options, better reflect the ever-changing market scenario. All proposed option-based approaches outperform or are equally good to different “traditional” forecasts that use historical returns as input. The extensive robustness of our results shows that the real driver of the better forecasts is the use of option market data as inputs for the analysis, more than the type of econometric approach implemented. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {1190--1213}, 
number = {7}, 
volume = {40}
}
@article{10.1016/j.najef.2014.06.006, 
year = {2014}, 
title = {{Modelling and forecasting value at risk and expected shortfall for GCC stock markets: Do long memory, structural breaks, asymmetry, and fat-tails matter?}}, 
author = {Aloui, Chaker and Hamida, Hela ben}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2014.06.006}, 
abstract = {{This paper addresses the question whether dual long memory (LM), asymmetry and structural breaks in stock market returns matter when forecasting the value at risk (VaR) and expected shortfall (ES) for short and long trading positions. We answer this question for the Gulf Cooperation Council (GCC) stock markets. Empirically, we test the occurrence of structural breaks in the GCC return data using the Inclan and Tiao (1994)'s algorithm and we check the relevance of LM using Shimotsu (2006) procedure before estimating the ARFIMA-FIGARCH and ARFIMA-FIAPARCH models with different innovations' distributions and computing VaR and ES. Our results show that all the GCC market's volatilities exhibit significant structural breaks matching mainly with the 2008-2009 global financial crises and the Arab spring. Also, they are governed by LM process either in the mean or in the conditional variance which cannot be due to the occurrence of structural breaks. Furthermore, the forecasting ability analysis shows that the FIAPARCH model under skewed Student- t distribution turn out to improve substantially the VaR and the ES forecasts. © 2014 Elsevier Inc.}}, 
pages = {349--380}, 
number = {NA}, 
volume = {29}
}
@article{10.1016/j.jempfin.2008.08.001, 
year = {2009}, 
title = {{Model averaging in risk management with an application to futures markets}}, 
author = {Pesaran, M. Hashem and Schleicher, Christoph and Zaffaroni, Paolo}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2008.08.001}, 
abstract = {{This paper considers the problem of model uncertainty in the case of multi-asset volatility models and discusses the use of model averaging techniques as a way of dealing with the risk of inadvertently using false models in portfolio management. Evaluation of volatility models is then considered and a simple Value-at-Risk (VaR) diagnostic test is proposed for individual as well as 'average' models. The asymptotic as well as the exact finite-sample distribution of the test statistic, dealing with the possibility of parameter uncertainty, are established. The model averaging idea and the VaR diagnostic tests are illustrated by an application to portfolios of daily returns on six currencies, four equity indices, four ten year government bonds and four commodities over the period 1991-2007. The empirical evidence supports the use of 'thick' model averaging strategies over single models or Bayesian type model averaging procedures. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {280--305}, 
number = {2}, 
volume = {16}
}
@article{10.3182/20100707-3-be-2012.0035, 
year = {2010}, 
title = {{Quality-by-design using a Gaussian mixture density approximation of biological uncertainties}}, 
author = {Rossner, N. and Heine, Th. and King, R.}, 
journal = {IFAC Proceedings Volumes}, 
issn = {14746670}, 
doi = {10.3182/20100707-3-be-2012.0035}, 
abstract = {{In this contribution the uncertainties of a biological process model are taken into account explicitly to calculate optimal process trajectories. For this purpose, the initial condition and the uncertainties of the model parameters are described by a weighted sum of normal distributions. Such a so-called Gaussian mixture density (GMD) approximation is propagated through the nonlinear process model to calculate a second order approximation of the statistical properties of the planed process trajectory. A Value@Risk primary objective is used to obtain an optimal process design procedure in presence of uncertainties. In an extensive simulation study a descriptive fermentation process model is used to compare the classical trajectory planning with the robust design approaches. Here, different degrees of approximation complexity and the influence of the weighting factor in the Value@Risk dual objective criterion is investigated. © 2010 IFAC.}}, 
pages = {7--12}, 
number = {6 PART 1}, 
volume = {43}
}
@article{10.1080/03461230802700897, 
year = {2010}, 
title = {{Extremes on the discounted aggregate claims in a time dependent risk model}}, 
author = {Asimit, Alexandru V. and Badescu, Andrei L.}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461230802700897}, 
abstract = {{This paper presents an extension of the classical compound Poisson risk model for which the inter-claim time and the forthcoming claim amount are no longer independent random variables (rv's). Asymptotic tail probabilities for the discounted aggregate claims are presented when the force of interest is constant and the claim amounts are heavy tail distributed rv's. Furthermore, we derive asymptotic finite time ruin probabilities, as well as asymptotic approximations for some common risk measures associated with the discounted aggregate claims. A simulation study is performed in order to validate the results obtained in the free interest risk model. © 2010 Taylor \& Francis.}}, 
pages = {93--104}, 
number = {2}, 
volume = {NA}
}
@article{10.1109/icumt.2018.8631272, 
year = {2019}, 
title = {{On Value-at-Risk and Expected Shortfall of Financial Asset with Stochastic Pricing}}, 
author = {Fomin, Maxim and Shorokhov, Sergey}, 
journal = {2018 10th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)}, 
issn = {21570221}, 
doi = {10.1109/icumt.2018.8631272}, 
abstract = {{We study the problem of measuring market risk of financial asset with stochastic pricing. Market risk metrics under study are Value-at- Risk and Expected Shortfall. The price of the financial asset is assumed to satisfy a given stochastic differential equation with diffusion coefficient being a function of asset price and time. We investigate various models of asset price dynamics, including well-known lognormal Black-Scholes model, shifted lognormal model, Bachelier and Cox-Ross normal models and a new stochastic model with hyperbolic sine function. For stochastic models under study we derive explicit analytic expressions for loss distribution function, Value-at- Risk and Expected Shortfall. Dependence of derived Value-at- Risk and Expected Shortfall functions on confidence level is shown on the plots. The possibility of using derived formulae for market risk estimation for equity traded on financial markets is demonstrated. We show that the highest estimate of market risk is given by hyperbolic sine and Cox-Ross models, the lowest estimate is given by Black-Scholes model and shifted lognormal model with negative model parameter. © 2018 IEEE.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {2018-November}
}
@article{10.1017/s174849951800009x, 
year = {2018}, 
title = {{A simple isochore model evidencing regulation risk}}, 
author = {Véhel, J. Lévy}, 
journal = {Annals of Actuarial Science}, 
issn = {17484995}, 
doi = {10.1017/s174849951800009x}, 
abstract = {{In this note, we provide a simple example of regulation risk. The idea is that, in certain situations, the very prudential rules (or, rather, some of them) imposed by the regulator in the framework of the Basel II/III Accords or Solvency II directive are themselves the source of a systemic risk. The instance of regulation risk that we bring to light in this work can be summarised as follows: wrongly assuming that prices evolve in a continuous fashion when they may in fact display large negative jumps, and trying to minimise Value at Risk (VaR) under a constraint of minimal volume of activity leads in effect to behaviours that will maximise VaR. Although much stylised, our analysis highlights some pitfalls of model-based regulation. © Institute and Faculty of Actuaries 2018Â.}}, 
pages = {233--248}, 
number = {2}, 
volume = {12}
}
@article{10.1016/s0927-5398(00)00012-8, 
year = {2000}, 
title = {{Estimation of tail-related risk measures for heteroscedastic financial time series: An extreme value approach}}, 
author = {McNeil, Alexander J. and Frey, Rüdiger}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(00)00012-8}, 
abstract = {{We propose a method for estimating Value at Risk (VaR) and related risk measures describing the tail of the conditional distribution of a heteroscedastic financial return series. Our approach combines pseudo-maximum-likelihood fitting of GARCH models to estimate the current volatility and extreme value theory (EVT) for estimating the tail of the innovation distribution of the GARCH model. We use our method to estimate conditional quantiles (VaR) and conditional expected shortfalls (the expected size of a return exceeding VaR), this being an alternative measure of tail risk with better theoretical properties than the quantile. Using backtesting of historical daily return series we show that our procedure gives better 1-day estimates than methods which ignore the heavy tails of the innovations or the stochastic nature of the volatility. With the help of our fitted models we adopt a Monte Carlo approach to estimating the conditional quantiles of returns over multiple-day horizons and find that this outperforms the simple square-root-of-time scaling method. © 2000 Elsevier Science B.V.}}, 
pages = {271--300}, 
number = {3-4}, 
volume = {7}
}
@article{10.1057/jam.2010.28, 
year = {2011}, 
title = {{Dynamic equity asset allocation with liquidity-adjusted market risk criterion: Appraisal of efficient and coherent portfolios}}, 
author = {Janabi, Mazin A M Al}, 
journal = {Journal of Asset Management}, 
issn = {14708272}, 
doi = {10.1057/jam.2010.28}, 
abstract = {{This article extends research literature related to the evaluation of modern portfolio risk management techniques by providing a broad modeling of dynamic equity asset allocation under the supposition of illiquid and adverse market settings. This study analyzes, from a fund manager's perspective, the performance of liquidity adjusted risk modeling in obtaining efficient and coherent equity trading portfolios subject to realistic operational constraints as specified by the fund manager. Specifically, the article proposes a re-engineered and robust approach to equity optimal portfolio selection, in a Liquidity-Adjusted Value at Risk (L-VaR) framework, and particularly from the perspective of trading portfolios that have both long and short trading positions or for trading portfolios that consists merely of long positions. Moreover, in this article, the authors develop a dynamic portfolio selection model and an optimization algorithm that allocates equity assets by minimizing L-VaR subject to the constraints that the expected return, trading volume and liquidation horizon should meet the budget limits set by the fund manager. © 2011 Macmillan Publishers Ltd.}}, 
pages = {378--394}, 
number = {6}, 
volume = {12}
}
@article{10.2139/ssrn.2308787, 
year = {2013}, 
title = {{Intraday volatility forecast in Australian equity market}}, 
author = {Singh, Abhay Kumar and Allen, David E. and Powell, Robert J.}, 
journal = {SSRN Electronic Journal}, 
issn = {NA}, 
doi = {10.2139/ssrn.2308787}, 
abstract = {{On the afternoon of May 6, 2010 Dow Jones Industrial Average (DJIA) plunged about 1000 points (about 9\%) in a matter of minutes before rebounding almost as quickly. This was the biggest one day point decline on an intraday basis in the DJIA's history. An almost similar dramatic change in intraday volatility was observed on April 4, 2000 when DJIA dropped by 4.8\%. These historical events present very compelling argument for the need of robust econometrics models which can forecast intraday asset volatility. There are numerous models available in the finance literature to model financial asset volatility. Various Autoregressive Conditional Heteroskedastic (ARCH) time series models are widely used for modelling daily (end of day) volatility of the financial assets. The family of basic GARCH models work well for modelling daily volatility but they are proven to be not as efficient for intraday volatility. The last two decades has seen some research augmenting the GARCH family of models to forecast intraday volatility, the Multiplicative Component GARCH (MCGARCH) model of Engle \& Sokalska (2012) is the most recent of them. MCGARCH models the conditional variance as the multiplicative product of daily, diurnal, and stochastic intraday volatility of the financial asset. In this paper we use MCGARCH model to forecast intraday volatility of Australia's S\&P/ASX-50 stock market, we also use the model to forecast the intraday Value at Risk. As the model requires a daily volatility component, we test a GARCH based estimate and a Realized Variance based estimate of daily volatility component. © International Congress on Modelling and Simulation, MODSIM 2013.All right reserved.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/scored.2013.7002649, 
year = {2013}, 
title = {{Value at risk estimation of a power system including wind generation}}, 
author = {Usman, M. Dzulhafizi and Shaaban, Mohamed}, 
journal = {2013 IEEE Student Conference on Research and Developement}, 
issn = {NA}, 
doi = {10.1109/scored.2013.7002649}, 
abstract = {{Reckoning the risk of load loss, corresponding to a certain event, into a single MW value can be of premium importance to system operators to quickly initiate a remedial action, if necessary. In this paper, risk to system load loss, due to the inclusion of variable wind generation, is estimated using the value at risk (VaR) concept. Monte Carlo simulation (MCS) is used to construct the wind speed model, through Weibull statistical distribution and a multistate model, as well as the annual load profile, through randomization. A six-bus test system is used to apply the developed notions. Results of incorporating wind turbine generation with the test system, among other conventional generators, have shown that the risk levels increases appreciably. The ease at which the system risk is identified and encapsulated into a quantifiable MW estimate, remains the salient attractive feature of the developed tool. © 2013 IEEE.}}, 
pages = {534--539}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jor.2015.321, 
year = {2015}, 
title = {{Risk measures and the impact of asset price bubbles}}, 
author = {Jarrow, Robert and Silva, Felipe Bastos G}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.321}, 
abstract = {{This paper analyzes the impact of asset price bubbles on a firm’s standard risk measures, including value-at-risk (VaR) and conditional value-at-risk (CVaR). Comparing a bubble economy and a non-bubble economy, it is shown that asset price bubbles may cause a firm’s traditional risk measures such as VaR and CVaR to decline. This decline is due to a reduced standard deviation and an increased right skew of the firm value’s distribution due to bubble expansion. This effect on a firm value’s moments due to the presence of price bubbles documents that traditional risk measures do not adequately capture the impacts of bubble bursting.We propose a new risk measure to account for losses associated with bubble bursting, a phenomenon that must be taken into consideration for the proper determination of equity capital. This additional risk measure is the expected holding period loss. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {35--56}, 
number = {3}, 
volume = {17}
}
@article{10.1080/03610918.2010.546544, 
year = {2011}, 
title = {{A weighted linear estimator of multivariate ARCH parameters}}, 
author = {Iqbal, Farhat}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2010.546544}, 
abstract = {{A weighted linear estimator (WLE) of the parameters of multivariate ARCH models is proposed. The accuracy of WLE in estimating the parameters of multivariate ARCH models is compared with the widely used quasi-maximum likelihood estimator (QMLE) through simulations. Application to real data sets are also presented and forecasts of variance-covariance matrix and value-at-risk (VaR) are obtained. The weighted resampling methods are used to approximate the sampling distribution of the proposed estimator. Our study indicates that the forecasting performance of WLE is not inferior and one-day ahead risk estimates are also found better than the QMLE. Copyright © 2011 Taylor \& Francis Group, LLC.}}, 
pages = {544--560}, 
number = {4}, 
volume = {40}
}
@article{10.1007/s00180-011-0270-4, 
year = {2012}, 
title = {{Modeling fat tails in stock returns: A multivariate stable-GARCH approach}}, 
author = {Bonato, Matteo}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-011-0270-4}, 
abstract = {{In this paper a new multivariate volatility model is proposed. It combines the appealing properties of the stable Paretian distribution to model the heavy tails with the GARCH model to capture the volatility clustering. Returns on assets are assumed to follow a sub-Gaussian distribution, which is a particular multivariate stable distribution. In this way the characteristic function of the fitted returns has a tractable expression and the density function can be recovered by numerical methods. A multivariate GARCH structure is then adopted to model the covariance matrix of the Gaussian vectors underlying the sub-Gaussian system. The model is applied to a bivariate series of daily U. S. stock returns. Value-at-risk for long and short positions is computed and compared with the one obtained using the multivariate normal and the multivariate Student's t distribution. Finally, exploiting the recent developments in the vast dimensional time-varying covariances modeling, possible feasible extensions of our model to higher dimensions are suggested and an illustrative example using the Dow Jones index components is presented. © 2011 Springer-Verlag.}}, 
pages = {499--521}, 
number = {3}, 
volume = {27}
}
@article{10.1109/iccsit.2009.5234557, 
year = {2009}, 
title = {{Estimating market risk under a wavelet-based approach: Mexican case}}, 
author = {Tellez, Jesus C. and Vargas, Teresa and Hernandez, Jose}, 
journal = {2009 2nd IEEE International Conference on Computer Science and Information Technology}, 
issn = {NA}, 
doi = {10.1109/iccsit.2009.5234557}, 
abstract = {{This paper aims to estimate Value-at-Risk and Beyond the VaR for the main Mexican stock index (IPC) under a wavelet-based approach. The wavelet approach is used since financial time series have shown to be non-stationary and in most of the times non-normally distributed, and it allows to face the problem when signals have very high frequency components with short time spans, and low frequency components with long time spans. The IPC variance is decomposed into different timescales using the discrete wavelet transform (DWT) and the least asymmetric (LA) Daubechies wavelet filter. Results show that the decomposed risk measure cannot be rejected at 95\% and 99\% confidence level. © 2009 IEEE.}}, 
pages = {353--357}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11424-014-2095-z, 
year = {2015}, 
title = {{Optimal reinsurance under distortion risk measures and expected value premium principle for reinsurer}}, 
author = {Zheng, Yanting and Cui, Wei and Yang, Jingping}, 
journal = {Journal of Systems Science and Complexity}, 
issn = {10096124}, 
doi = {10.1007/s11424-014-2095-z}, 
abstract = {{This paper discusses optimal reinsurance strategy by minimizing insurer’s risk under one general risk measure: Distortion risk measure. The authors assume that the reinsurance premium is determined by the expected value premium principle and the retained loss of the insurer is an increasing function of the initial loss. An explicit solution of the insurer’s optimal reinsurance problem is obtained. The optimal strategies for some special distortion risk measures, such as value-at-risk (VaR) and tail value-at-risk (TVaR), are also investigated. © 2015, Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg.}}, 
pages = {122--143}, 
number = {1}, 
volume = {28}
}
@article{10.1007/s11146-014-9476-x, 
year = {2015}, 
title = {{Cornish-Fisher Expansion for Commercial Real Estate Value at Risk}}, 
author = {Amédée-Manesme, Charles-Olivier and Barthélémy, Fabrice and Keenan, Donald}, 
journal = {The Journal of Real Estate Finance and Economics}, 
issn = {08955638}, 
doi = {10.1007/s11146-014-9476-x}, 
abstract = {{The computation of Value at Risk has traditionally been a troublesome issue in commercial real estate. Difficulties mainly arise from the lack of appropriate data, the non-normality of returns, and the inapplicability of many of the traditional methodologies. As a result, calculation of this risk measure has rarely been done in the real estate field. However, following a spate of new regulations such as Basel II, Basel III, NAIC and Solvency II, financial institutions have increasingly been required to estimate and control their exposure to market risk. As a result, financial institutions now commonly use “internal” Value at Risk (VaR) models in order to assess their market risk exposure. The purpose of this paper is to estimate distribution functions of real estate VaR while taking into account non-normality in the distribution of returns. This is accomplished by the combination of the Cornish-Fisher expansion with a certain rearrangement procedure. We demonstrate that this combination allows superior estimation, and thus a better VaR estimate, than has previously been obtainable. We also show how the use of a rearrangement procedure solves well-known issues arising from the monotonicity assumption required for the Cornish-Fisher expansion to be applicable, a difficulty which has previously limited the useful of this expansion technique. Thus, practitioners can find a methodology here to quickly assess Value at Risk without suffering loss of relevancy due to any non-normality in their actual return distribution. The originality of this paper lies in our particular combination of Cornish-Fisher expansions and the rearrangement procedure. © 2014, Springer Science+Business Media New York.}}, 
pages = {439--464}, 
number = {4}, 
volume = {50}
}
@article{10.1016/s0210-0266(12)70030-4, 
year = {2012}, 
title = {{Risk optimal single-object auctions}}, 
author = {Alonso, Estrella and Tejada, Juan}, 
journal = {Cuadernos de Economía}, 
issn = {02100266}, 
doi = {10.1016/s0210-0266(12)70030-4}, 
abstract = {{We analyze the preferences of a risk-averse auctioneer over several auction mechanisms with risk-neutral and symmetric bidders. We obtain the value at risk (VaR) for auctioneer revenue in auction mechanisms belonging to a parametric family which includes two classic mechanisms, the first-price auction and second-price auction. By calculating the VaR for revenue an auctioneer can estimate the amount that will be lost within a given confidence level, depending on the number of bidders and the auction mechanism chosen. The contribution of this paper is the calculation of the VaR for auctioneer revenue in some common auction mechanisms that yield the same expected revenue, including first-price auction and second-price auction and the following mechanisms: Santa Claus auction, sad-loser auction and all-pay auction. We describe how to quantify the maximum loss for an auctioneer at a given probability. We study the value at risk of the auctioneer as a criterion to determine which auctions would best suit the auctioneer's interests. © 2012 Asociación Cuadernos de Economía.}}, 
pages = {131--138}, 
number = {99}, 
volume = {35}
}
@article{10.1080/02664760802607517, 
year = {2009}, 
title = {{Nonparametric estimation of value-at-risk}}, 
author = {Jeong, Seok-Oh and Kang, Kee-Hoon}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664760802607517}, 
abstract = {{This paper develops a fully nonparametric method for estimating value-at-risk based on the adaptive volatility estimation and the nonparametric quantile estimation. The proposed method is simple, fast and easy to implement. We evaluated its numerical performance on the basis of Monte Carlo study for numerous models. We also provided an empirical application to KOrean Stock Price Index data, which turned out to be successful by backtesting. © 2009 Taylor \& Francis.}}, 
pages = {1225--1238}, 
number = {11}, 
volume = {36}
}
@article{10.17159/2411-9717/2016/v116n3a3, 
year = {2016}, 
title = {{Determination of value at risk for longterm production planning in open pit mines in the presence of price uncertainty}}, 
author = {Rahmanpour, M and Osanloo, M}, 
journal = {Journal of the Southern African Institute of Mining and Metallurgy}, 
issn = {22256253}, 
doi = {10.17159/2411-9717/2016/v116n3a3}, 
abstract = {{Mine planning is a multidisciplinary procedure that aims to guarantee the profitability of a mining operation in changing and uncertain conditions. Mine plans are normally classified as long-term, intermediate-term, and short-term plans, and many factors affect the preciseness of these plans and cause deviations in reaching the objectives. Commodity price is the heart of mine planning, but it has a changing and uncertain nature. Therefore, the determination of mine plans in the presence of uncertain mineral price is a challenge. A robust mine plan reduces the risk of early mine closure. A procedure is presented to determine the value at risk (VaR) in any possible mine planning alternative. VaR is considered together with downside risk and upside potential in order to select the most profitable and least risky plan. The model is tested on a small iron ore deposit. © The Southern African Institute of Mining and Metallurgy, 2016.}}, 
number = {3}, 
volume = {116}
}
@article{10.1504/ijebr.2021.112004, 
year = {2021}, 
title = {{Alternative capital requirement for insurers: Possibilities and issues}}, 
author = {Zariņa, Ilze and Voronova, Irina and Pettere, Gaida}, 
journal = {International Journal of Economics and Business Research}, 
issn = {17569850}, 
doi = {10.1504/ijebr.2021.112004}, 
abstract = {{Solvency II framework regulates how much capital the insurance companies of the European Union must hold. Although the framework lasts four years, there is still place for improvements considering experience. The goals of the research are to propose an alternative capital model methodology using copulas for reserve risk and to show the case study of potential capital shift impact. To conduct the research, the authors have used the extensive literature review, analytical methods and modelling. Research scope is non-life insurance companies under the Solvency II framework with a focus on reserve risk. The research will help avoid that alternative models are only a modern risk management tool and add risk management reality. Higher capital surplus can be achieved if a copula approach is used for risk aggregation in the Baltic non-life market. The Baltic market does not use alternative capital requirement and internal models. Using an alternative model is the right insurer's approach in modern risk management. Copyright © 2021 Inderscience Enterprises Ltd.}}, 
pages = {41--61}, 
number = {1}, 
volume = {21}
}
@article{10.1080/09599916.2011.577904, 
year = {2011}, 
title = {{Diversification effect of real estate investment trusts: Comparing copula functions with kernel methods}}, 
author = {Chang, Meng-Shiuh and Salin, Victoria and Jin, Yanhong}, 
journal = {Journal of Property Research}, 
issn = {09599916}, 
doi = {10.1080/09599916.2011.577904}, 
abstract = {{Value at Risk estimated with joint distribution methodologies demonstrates that risk is lower for portfolios of real estate investment trusts (REITs) and smallbusiness equities compared with a single-asset holding. Benefits from diversification were largest in 2001-2003 and the smallest from 2006-2008. Previous research using Value at Risk points out the importance of model selection. Various estimation approaches affected results modestly over the entire period (1989-mid 2008). The Value at Risk is -3.1\% for two copula models and -3.2\% for a nonparametric empirical joint density, at a 1\% probability level for weekly returns. After June 1996, the nonparametric copula model consistently returned the lowest risk estimate among the three joint distribution methods. Time-varying risk is a more important driver in the results than model specification. The highest portfolio risk was found for the period after August 2006 (weekly losses of 4.4\% to 5\%). The distribution-based model results were closer to the undiversified model results than in the earlier time periods, which supports the premise that contagion across asset classes characterises the post-2006 real estate bust, but is not a strong characteristic of the market over a longer investment horizon that includes growth phases of the business cycle. © 2011 Taylor \& Francis.}}, 
pages = {189--212}, 
number = {3}, 
volume = {28}
}
@article{10.1002/9781119288992.ch7, 
year = {2017}, 
title = {{Risk Measures Based on Multivariate Skew Normal and Skew t-Mixture Models}}, 
author = {Lee, Sharon X. and McLachlan, Geoffrey J.}, 
issn = {NA}, 
doi = {10.1002/9781119288992.ch7}, 
abstract = {{It is widely recognized that financial stock returns do not always follow the normal distribution. Typ ically, they exhibit non-normal features such as skewness, heavy tails and kurtosis. In this chapter, we consider the application of multivariate non-normal mixture models for modelling the joint distribu tion of the log returns in a portfolio. Formulas are then derived for some commonly used risk measures including probability of shortfall (PS), Value-at-Risk (VaR), expected shortfall (ES) and tail-conditional expectation (TCE), based on these models. Our focus is on skew normal and skew i-component distributions. These families of distributions are generalizations of the normal distribution and i-distribution, respectively, with additional parame ters to accommodate skewness and/or heavy tails, rendering them suitable for handling the asymmetric distributional shape of financial data. As linear transformations of the quantities under consideration also have mixtures of skew-normal or skew i-distributions, the PS, VaR and TCE, and other risk mea sures of an asset portfolio, can be expressed explicitly in terms of the parameters of the fitted mixture models. This approach is demonstrated on a real example of a portfolio of Australian stock returns and the performances of these models are compared with the traditional normal mixture model. © 2018 John Wiley \& Sons Ltd. All rights reserved.}}, 
pages = {152--168}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/13504851.2013.806775, 
year = {2013}, 
title = {{Worldwide equity risk prediction}}, 
author = {Ardia, David and Hoogerheide, Lennart F.}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2013.806775}, 
abstract = {{Various GARCH models are applied to daily returns of more than 1200 constituents of major stock indices worldwide. The value-at-risk forecast performance is investigated for different markets and industries, considering the test for correct conditional coverage using the false discovery rate (FDR) methodology. For most of the markets and industries, we find the same two conclusions. First, an asymmetric GARCH specification is essential when forecasting the 95\% value-at-risk. Second, for both the 95\% and 99\% value-at-risk, it is crucial that the innovations' distribution is fat-tailed (e.g. Student-t or-even better-a nonparametric kernel density estimate). © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1333--1339}, 
number = {14}, 
volume = {20}
}
@article{10.1016/j.econlet.2014.02.008, 
year = {2014}, 
title = {{GARCH models for daily stock returns: Impact of estimation frequency on Value-at-Risk and Expected Shortfall forecasts}}, 
author = {Ardia, David and Hoogerheide, Lennart F.}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2014.02.008}, 
abstract = {{We analyze the impact of the estimation frequency-updating parameter estimates on a daily, weekly, monthly or quarterly basis-for commonly used GARCH models in a large-scale study, using more than twelve years (2000-2012) of daily returns for constituents of the S\&P 500 index. We assess the implication for one-day ahead 95\% and 99\% Value-at-Risk (VaR) forecasts with the test for correct conditional coverage of Christoffersen (1998) and for Expected Shortfall (ES) forecasts with the block-bootstrap test of ES violations of Jalal and Rockinger (2008). Using the false discovery rate methodology of Storey (2002) to estimate the percentage of stocks for which the model yields correct VaR and ES forecasts, we conclude that there is no difference in performance between updating the parameter estimates of the GARCH equation at a daily or weekly frequency, whereas monthly or even quarterly updates are only marginally outperformed. © 2014 Elsevier B.V.}}, 
pages = {187--190}, 
number = {2}, 
volume = {123}
}
@article{10.1108/imefm-11-2018-0388, 
year = {2020}, 
title = {{On the relevance of higher-moments for portfolio-management within Islamic finance}}, 
author = {Shaikh, Omar}, 
journal = {International Journal of Islamic and Middle Eastern Finance and Management}, 
issn = {17538394}, 
doi = {10.1108/imefm-11-2018-0388}, 
abstract = {{Purpose: Using a convenient tail-risk measure of performance, this paper aims to explore the extent to which incorporating higher statistical moments such as an assets skewness and kurtosis, provides further insight into the potential benefits of asset-class diversification within the realm of Islamic finance. Design/methodology/approach: The authors use Engle’s (2002) DCC-GARCH model to study the dynamic conditional correlations between asset classes. Furthermore, the authors use the modified value-at-risk (Favre and Galeano, 2002), which incorporates higher statistical moments, to measure the performance of portfolios during both crisis and bullish regimes. Findings: The most important finding relates to the estimation of portfolio tail-risk. In particular, the authors find that using a standard two-moment value-at-risk (VaR) measure, which assumes normally distributed returns, rather than a four-moment VaR, which incorporates an asset skewness and kurtosis, can lead to a substantial underestimation of portfolio risk during the most extreme market conditions. Originality/value: This paper contributes to the extremely limited research considering higher-moments within the realm of Islamic portfolio-management. The results suggest that Islamic portfolio managers should remain cognisant of the skewness and kurtosis parameters of their assets. Ignoring higher-moments could induce misleading inferences and would, therefore, constitute imprudent risk-management. © 2020, Emerald Publishing Limited.}}, 
pages = {533--552}, 
number = {3}, 
volume = {13}
}
@article{10.1504/ijmef.2014.065099, 
year = {2014}, 
title = {{Selection of the right risk measures for portfolio allocation}}, 
author = {Nguyen, Thanh}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2014.065099}, 
abstract = {{An optimisation framework is proposed to enable investors to select the right risk measures in portfolio selection. Verification is deployed by performing experiments in developed markets (e.g., the US stock market), emerging markets (e.g., the South Korean stock market) and global investments. A preselection procedure dealing with large datasets is also introduced to eliminate stocks that have low diversification potential before running the portfolio optimisation model. Portfolios are evaluated by four performance indices, i.e., the Sortino ratio, the Sharpe ratio, the Stutzer performance index, and the Omega measure. Experimental results demonstrate that high performance and also well-diversified portfolios are obtained if modified value-at-risk, variance, or semi-variance is concerned whereas emphasising only skewness, kurtosis or higher moments in general produces low performance and poorly diversified portfolios. In addition, the preselection applied to large datasets results in portfolios that have not only high performance but also high diversification degree. Copyright © 2014 Inderscience Enterprises Ltd.}}, 
pages = {135}, 
number = {2}, 
volume = {7}
}
@article{10.1504/ijram.2011.04370, 
year = {2011}, 
title = {{Minimising value-at-risk in a portfolio optimisation problem using a multi-objective genetic algorithm}}, 
author = {}, 
issn = {14668297}, 
doi = {10.1504/ijram.2011.04370}, 
abstract = {{In this paper, we develop a general framework for market risk optimisation that focuses on VaR. The reason for this choice is the complexity and problems associated with risk return optimisation (non-convex and non-differential objective function). Our purpose is to obtain VaR efficient frontiers using a multi-objective genetic algorithm (GA) and to show the potential utility of the algorithm to obtain efficient portfolios when the risk measure does not allow calculating an optimal solution. Furthermore, we measure differences between VaR efficient frontiers and variance efficient frontiers in VaR-return space and we evaluate out-sample capacity of portfolios on both bullish and bearish markets. The results indicate the reliability of VaR-efficient portfolios on both bullish and bearish markets and a significant improvement over Markowitz efficient portfolios in the VaR-return space. The improvement decreases as the portfolios level of risk increases. In this particular case, efficient portfolios do not depend on the risk measure minimised. Copyright © 2011 Inderscience Enterprises Ltd.}}, 
number = {5-6}, 
volume = {15}
}
@article{10.1007/s11579-021-00298-x, 
year = {2021}, 
title = {{Risk management with expected shortfall}}, 
author = {Wei, Pengyu}, 
journal = {Mathematics and Financial Economics}, 
issn = {18629679}, 
doi = {10.1007/s11579-021-00298-x}, 
abstract = {{This article studies optimal, dynamic portfolio and wealth/consumption policies of expected utility-maximizing investors who must also manage market-risk exposure which is measured by expected shortfall (ES). We find that ES managers can incur larger losses when losses occur, compared to benchmark managers. A general-equilibrium analysis reveals that the presence of ES managers increases the market volatility during periods of significant financial market stress. We propose weighted shortfall, a coherent and moreover spectral risk measure, that can rectify the shortcomings of ES. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {847--883}, 
number = {4}, 
volume = {15}
}
@article{10.1109/icicip.2016.7885875, 
year = {2017}, 
title = {{Dynamic optimization of value-at-risk portfolios with fuzziness in asset management}}, 
author = {Yoshida, Yuji}, 
journal = {2016 Seventh International Conference on Intelligent Control and Information Processing (ICICIP)}, 
issn = {NA}, 
doi = {10.1109/icicip.2016.7885875}, 
abstract = {{Using fuzzy random variables, a dynamic portfolio model with uncertainty is mentioned for object system. In this approach, the random property is numerated by stochastic expectation and the fuzzy property is also numerated by weights and mean functions. A value-at-risk is introduced to assess the risk of unfavorable paths in investment. Using dynamic programming and mathematical programming, the optimal solutions of a dynamic portfolio problem with VaR is mentioned. An optimization equation is derived and the optimal portfolios are given at each period. © 2016 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0377-2217(01)00039-x, 
year = {2001}, 
title = {{Dynamic value at risk under optimal and suboptimal portfolio policies}}, 
author = {Fusai, Gianluca and Luciano, Elisa}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/s0377-2217(01)00039-x}, 
abstract = {{At present, all value at risk (VaR) implementations - i.e., all risk measures of the "maximum loss at a given level of confidence" type - are based on the assumption that the portfolio mix will not change before the VaR horizon. This hypothesis may be unrealistic, especially when the VaR horizon is established by the regulators (BIS). At the opposite, we measure VaR dynamically, i.e., taking into consideration portfolio mix adjustments over time: adjustments do not occur continuously, since they are costly. We allow both optimal rebalancing policies, which entail changing the portfolio mix whenever it is too far from the optimal one, and suboptimal policies, which mean adjusting at pre-fixed dates. We show that in both cases usual VaR measures underestimate portfolio losses, even if the underlying returns are normal. We study the dependence of the misestimate on the VaR horizon, the initial portfolio mix and the risk aversion of the portfolio manager, which in turn determines the frequency of i nterventions. The bias can be more relevant over one day than over longer horizons and even if the initial portfolio is nearly optimal. We also perform backtesting and estimate a "coherent" risk measure, namely conditional VaR, which confirms the inappropriateness of the usual, static VaR. © 2001 Elsevier Science B.V. All rights reserved.}}, 
pages = {249--269}, 
number = {2}, 
volume = {135}
}
@article{10.1007/s10479-018-2820-4, 
year = {2019}, 
title = {{Pareto-optimal reinsurance policies in the presence of individual risk constraints}}, 
author = {Lo, Ambrose and Tang, Zhaofeng}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-018-2820-4}, 
abstract = {{The notion of Pareto optimality is commonly employed to formulate decisions that reconcile the conflicting interests of multiple agents with possibly different risk preferences. In the context of a one-period reinsurance market comprising an insurer and a reinsurer, both of which perceive risk via distortion risk measures, also known as dual utilities, this article characterizes the set of Pareto-optimal reinsurance policies analytically and visualizes the insurer–reinsurer trade-off structure geometrically. The search of these policies is tackled by translating it mathematically into a functional minimization problem involving a weighted average of the insurer’s risk and the reinsurer’s risk. The resulting solutions not only cast light on the structure of the Pareto-optimal contracts, but also allow us to portray the resulting insurer–reinsurer Pareto frontier graphically. In addition to providing a pictorial manifestation of the compromise reached between the insurer and reinsurer, an enormous merit of developing the Pareto frontier is the considerable ease with which Pareto-optimal reinsurance policies can be constructed even in the presence of the insurer’s and reinsurer’s individual risk constraints. A strikingly simple graphical search of these constrained policies is performed in the special cases of Value-at-Risk and Tail Value-at-Risk. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {395--423}, 
number = {1-2}, 
volume = {274}
}
@article{10.1016/j.jbankfin.2018.04.016, 
year = {2018}, 
title = {{Unbiased estimation of risk}}, 
author = {Pitera, Marcin and Schmidt, Thorsten}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2018.04.016}, 
abstract = {{The estimation of risk measures recently gained a lot of attention, partly because of the backtesting issues of expected shortfall related to elicitability. In this work we shed a new and fundamental light on optimal estimation procedures of risk measures in terms of bias. We show that once the parameters of a model need to be estimated, one has to take additional care when estimating risks. The typical plug-in approach, for example, introduces a bias which leads to a systematic underestimation of risk. In this regard, we introduce a novel notion of unbiasedness to the estimation of risk which is motivated by economic principles. In general, the proposed concept does not coincide with the well-known statistical notion of unbiasedness. We show that an appropriate bias correction is available for many well-known estimators. In particular, we consider value-at-risk and expected shortfall (tail value-at-risk). In the special case of normal distributions, closed-formed solutions for unbiased estimators can be obtained. We present a number of motivating examples which show the outperformance of unbiased estimators in many circumstances. The unbiasedness has a direct impact on backtesting and therefore adds a further viewpoint to established statistical properties. © 2018 Elsevier B.V.}}, 
pages = {133--145}, 
number = {NA}, 
volume = {91}
}
@article{10.1109/tpwrs.2004.836184, 
year = {2005}, 
title = {{Risk assessment of generators bidding in day-ahead market}}, 
author = {Das, D. and Wollenberg, B.F.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2004.836184}, 
abstract = {{Competition in power markets exposes companies which participate to physical and financial uncertainties. Generator companies, bidding to supply power in day-ahead markets may face forced outages after bids are accepted by the system operator. When this happens they have to buy power from the real-time hourly spot market and sell to the ISO at the set day-ahead market clearing price. This paper shows simulations of random forced outages for generators and the resulting risk profiles of generators. Value at Risk (VaR) is calculated at 98\% confidence level as a measure of financial risk. The risk profiles and the VaR of the generators are changed with changes in bidding functions. The simulations do not consider transmission limits or demand side bidding. © 2005 IEEE.}}, 
pages = {416--424}, 
number = {1}, 
volume = {20}
}
@article{10.1007/978-3-642-04962-0_36, 
year = {2009}, 
title = {{Mean-VaR models and algorithms for fuzzy portfolio selection}}, 
author = {Dong, Wen and Peng, Jin}, 
journal = {Communications in Computer and Information Science}, 
issn = {18650929}, 
doi = {10.1007/978-3-642-04962-0\_36}, 
abstract = {{In this paper, value-at-risk (VaR for short) is used as the measure of risk. Based on the concept of VaR, a fuzzy mean-VaR model is proposed. Firstly, we recall some definitions and results of value-at-risk in credibilistic risk analysis. Secondly, we propose the fuzzy mean- VaR model of fuzzy programming, or more precisely, credibilistic programming. Thirdly, a hybrid intelligent algorithm is provided to give a general solution of the optimization problem. Finally, numerical examples are also presented to illustrate the effectiveness of the proposed algorithm. © 2009 Springer-Verlag Berlin Heidelberg.}}, 
pages = {313--319}, 
number = {NA}, 
volume = {51}
}
@article{10.1007/s10690-010-9116-2, 
year = {2010}, 
title = {{Modelling co-movements and tail dependency in the international stock market via copulae}}, 
author = {Ignatieva, Katja and Platen, Eckhard}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-010-9116-2}, 
abstract = {{This paper examines international equity market co-movements using time-varying copulae. We examine distributions from the class of Symmetric Generalized Hyperbolic (SGH) distributions for modelling univariate marginals of equity index returns. We show based on the goodness-of-fit testing that the SGH class outperforms the normal distribution, and that the Student-t assumption on marginals leads to the best performance, and thus, can be used to fit multivariate copula for the joint distribution of equity index returns. We show in our study that the Student-t copula is not only superior to the Gaussian copula, where the dependence structure relates to the multivariate normal distribution, but also outperforms some alternative mixture copula models which allow to reflect asymmetric dependencies in the tails of the distribution. The Student-t copula with Student-t marginals allows to model realistically simultaneous co-movements and to capture tail dependency in the equity index returns. From the point of view of risk management, it is a good candidate for modelling the returns arising in an international equity index portfolio where the extreme losses are known to have a tendency to occur simultaneously. We apply copulae to the estimation of the Value-at-Risk and the Expected Shortfall, and show that the Student-t copula with Student-t marginals is superior to the alternative copula models investigated, as well the Riskmetics approach. © 2010 Springer Science+Business Media, LLC.}}, 
pages = {261--302}, 
number = {3}, 
volume = {17}
}
@article{10.1016/s0925-5273(02)00369-9, 
year = {2003}, 
title = {{VaR as a risk measure for multiperiod static inventory models}}, 
author = {Luciano, Elisa and Peccati, Lorenzo and Cifarelli, Donato M.}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/s0925-5273(02)00369-9}, 
abstract = {{This paper explores the possibility to use value-at-risk (VaR) in the context of inventory management. VaR is being used more and more in financial management as a natural measure of the risk taken with a given position. In the framework of inventory management it can work as well. After having built a decision model, where the choice concerns the quantity to be ordered to face a random demand, in order to optimize the expected result (an expected cost to be minimized or an expected profit to be maximized) the model explores the probability distribution of the result, both via analytical methods and via simulation methods. The analytical exploration of this problem has originated a general method to deduce from inequalities for distribution functions, which are based on the expected value, related tail inequalities, which are particularly useful for VaR problems, where only one tail is involved. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {375--384}, 
number = {NA}, 
volume = {81-82}
}
@article{10.1002/for.2434, 
year = {2017}, 
title = {{Exploiting Spillovers to Forecast Crashes}}, 
author = {Gresnigt, Francine and Kole, Erik and Franses, Philip Hans}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2434}, 
abstract = {{We develop Hawkes models in which events are triggered through self-excitation as well as cross-excitation. We examine whether incorporating cross-excitation improves the forecasts of extremes in asset returns compared to only self-excitation. The models are applied to US stocks, bonds and dollar exchange rates. We predict the probability of crashes in the series and the value at risk (VaR) over a period that includes the financial crisis of 2008 using a moving window. A Lagrange multiplier test suggests the presence of cross-excitation for these series. Out-of-sample, we find that the models that include spillover effects forecast crashes and the VaR significantly more accurately than the models without these effects. Copyright © 2016 John Wiley \& Sons, Ltd. Copyright © 2016 John Wiley \& Sons, Ltd.}}, 
pages = {936--955}, 
number = {8}, 
volume = {36}
}
@article{10.1057/s41283-016-0010-8, 
year = {2017}, 
title = {{Measuring insurers' investment risk taking with asymmetric tail dependencies}}, 
author = {Lai, Gene C. and Lu, Erin P. and Li, Haijun and Chen, Dennis C.}, 
journal = {Risk Management}, 
issn = {14603799}, 
doi = {10.1057/s41283-016-0010-8}, 
abstract = {{This paper provides a new measurement of investment risk taking for the U.S. life insurers and illustrates the computational results. We use a modified portfolio value-at-risk approach, which is based on disaggregated market values of twenty-one types of assets and accounted for dependencies among downside risks of assets, to calculate the respective portfolio risk for 28 mutual insurers and 135 stock insurers over the period from 2007 to 2009. We find more than half of the life insurers in our sample reduced their portfolio risk during the 2008-2009 financial crisis. © 2017 Macmillan Publishers Ltd.}}, 
pages = {1--31}, 
number = {1}, 
volume = {19}
}
@article{10.1007/s10589-008-9231-4, 
year = {2010}, 
title = {{Correlation stress testing for value-at-risk: An unconstrained convex optimization approach}}, 
author = {Qi, Houduo and Sun, Defeng}, 
journal = {Computational Optimization and Applications}, 
issn = {09266003}, 
doi = {10.1007/s10589-008-9231-4}, 
abstract = {{Correlation stress testing is employed in several financial models for determining the value-at-risk (VaR) of a financial institution's portfolio. The possible lack of mathematical consistence in the target correlation matrix, which must be positive semidefinite, often causes breakdown of these models. The target matrix is obtained by fixing some of the correlations (often contained in blocks of submatrices) in the current correlation matrix while stressing the remaining to a certain level to reflect various stressing scenarios. The combination of fixing and stressing effects often leads to mathematical inconsistence of the target matrix. It is then naturally to find the nearest correlation matrix to the target matrix with the fixed correlations unaltered. However, the number of fixed correlations could be potentially very large, posing a computational challenge to existing methods. In this paper, we propose an unconstrained convex optimization approach by solving one or a sequence of continuously differentiable (but not twice continuously differentiable) convex optimization problems, depending on different stress patterns. This research fully takes advantage of the recently developed theory of strongly semismooth matrix valued functions, which makes fast convergent numerical methods applicable to the underlying unconstrained optimization problem. Promising numerical results on practical data (RiskMetrics database) and randomly generated problems of larger sizes are reported. © 2009 Springer Science+Business Media, LLC.}}, 
pages = {427--462}, 
number = {2}, 
volume = {45}
}
@article{10.1007/s13385-021-00300-2, 
year = {2021}, 
title = {{The effect of risk constraints on the optimal insurance policy}}, 
author = {Jiang, Wenjun and Ren, Jiandong}, 
journal = {European Actuarial Journal}, 
issn = {21909733}, 
doi = {10.1007/s13385-021-00300-2}, 
abstract = {{This paper studies the optimal insurance policy that maximizes the decision maker (DM)’s expected utility under distortion risk constraints. To alleviate the ex post moral hazard issues arising from the discontinuity of the indemnity functions in Huang (Geneva Risk Insur Rev 31(2):91–110, 2006) and Bernard and Tian (Geneva Risk Insur Rev 35(1):47–80, 2010) we re-visit their problems under the so called incentive compatibility condition, which requires that both the ceded and retained loss functions are non-decreasing. In addition, we generalize the value-at-risk (VaR) constraints used in the literature to the distortion-risk-measure-based constraints. We first implicitly characterize the optimal indemnity function when the risk constraints are defined in terms of the general distortion risk measure and then provide explicit solutions for the VaR and tail value-at-risk (TVaR) cases. The effect of the risk constraints on the optimal indemnity function are analyzed in great detail. Our results show that under the VaR risk constraints, the DM chooses to ignore the risk which does not contribute to its VaR value and only manages the risk that influences its VaR value. This problem is alleviated under the TVaR risk constraints. © 2021, EAJ Association.}}, 
pages = {1--30}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.najef.2021.101426, 
year = {2021}, 
title = {{Forecasting the Value-at-Risk of REITs using realized volatility jump models}}, 
author = {Odusami, Babatunde O}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2021.101426}, 
abstract = {{This paper examines jump risk in the time series of Real Estate Investment Trusts (REITs). Using high-frequency index-level and firm-level data, the econometric model in this paper integrates jumps into the volatility forecast by estimating jump augmented Heterogeneous Autoregressive (HAR) models of realized volatility. To assess the information value of these specifications, their forecasting accuracies for generating one-step ahead daily Value-at-Risk are also compared with other VaR specifications, including those generated from historical returns, bootstrap technique, and severity loss distribution. © 2021}}, 
pages = {101426}, 
number = {NA}, 
volume = {58}
}
@article{10.1007/978-3-642-24547-3_2, 
year = {2011}, 
title = {{Compensation policies and risk in service level agreements: A value-at-risk approach under the ON-OFF service model}}, 
author = {Mastroeni, Loretta and Naldi, Maurizio}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-24547-3\_2}, 
abstract = {{Service Level Agreements define the obligations of service providers towards their customers. One of such obligations is the compensation that customers receive in the case of service degradation or interruption. This obligation exposes the service provider to the risk of paying large amounts of money in the case of massive disruptions. The evaluation of such risk is preliminary to any countermeasure the service provider may wish to take to mitigate the risk. In this paper we evaluate the probability distribution of economical losses associated to service failures under a Markovian ON-OFF service model. We provide expressions for such distributions under three compensation policies, linked respectively to the number of failures, the number of outages lasting more than a prescribed threshold, and the cumulative downtime over a finite time horizon. In order to provide a single measure of risk, we compute the Value-at-Risk (VaR) for those compensation policies. We show that the VaR provides an accurate view of the risk incurred by the service provider, and allows to differentiate compensation policies, even when they lead to equal average losses. © 2011 Springer-Verlag.}}, 
pages = {2--13}, 
number = {NA}, 
volume = {6995 LNCS}
}
@article{10.21314/jor.2018.385, 
year = {2018}, 
title = {{Estimation window strategies for value-at-risk and expected shortfall forecasting}}, 
author = {Berens, Tobias and Weiß, Gregor N F and Ziggel, Daniel}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.385}, 
abstract = {{Compared with the large number of value-at-risk (VaR) and expected shortfall (ES) forecasting models proposed in the literature, few contributions have been made to address the question of which estimation window strategy is preferable for forecasting these risk measures. To fill this gap, we apply different estimation window strategies to a set of simple parametric, semiparametric and nonparametric industry-standard risk models. Analyzing daily return data on constituents of the German Deutscher Aktienindex (DAX), we evaluate forecasts by backtesting the unconditional coverage and independent and identically distributed properties of VaR violations, the ES forecasting accuracy and the conditional predictive ability. We thereby demonstrate that the selection of the estimation window strategy leads to significant performance differences. The results indicate that forecast combinations are the preferable estimation window strategy. © Infopro Digital Limited 2018. All rights reserved.}}, 
pages = {33--82}, 
number = {5}, 
volume = {20}
}
@article{10.1016/j.jeconom.2008.12.002, 
year = {2009}, 
title = {{Assessing value at risk with CARE, the Conditional Autoregressive Expectile models}}, 
author = {Kuan, Chung-Ming and Yeh, Jin-Huei and Hsu, Yu-Chin}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2008.12.002}, 
abstract = {{In this paper we propose a downside risk measure, the expectile-based Value at Risk (EVaR), which is more sensitive to the magnitude of extreme losses than the conventional quantile-based VaR (QVaR). The index θ of an EVaR is the relative cost of the expected margin shortfall and hence reflects the level of prudentiality. It is also shown that a given expectile corresponds to the quantiles with distinct tail probabilities under different distributions. Thus, an EVaR may be interpreted as a flexible QVaR, in the sense that its tail probability is determined by the underlying distribution. We further consider conditional EVaR and propose various Conditional AutoRegressive Expectile models that can accommodate some stylized facts in financial time series. For model estimation, we employ the method of asymmetric least squares proposed by Newey and Powell [Newey, W.K., Powell, J.L., 1987. Asymmetric least squares estimation and testing. Econometrica 55, 819-847] and extend their asymptotic results to allow for stationary and weakly dependent data. We also derive an encompassing test for non-nested expectile models. As an illustration, we apply the proposed modeling approach to evaluate the EVaR of stock market indices. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {261--270}, 
number = {2}, 
volume = {150}
}
@article{10.1016/j.eneco.2018.10.001, 
year = {2018}, 
title = {{Crude oil risk forecasting: New evidence from multiscale analysis approach}}, 
author = {He, Kaijian and Tso, Geoffrey K.F. and Zou, Yingchao and Liu, Jia}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2018.10.001}, 
abstract = {{Fluctuations in the crude oil price allied to risk have increased significantly over the last decade frequently varying at different risk levels. Although existing models partially predict such variations, so far, they have been unable to predict oil prices accurately in this highly volatile market. The development of an effective, predictive model has therefore become a prime objective of research in this field. Our approach, albeit based in part on previous research, develops an original methodology, in that we have created a risk forecasting model with the ability to predict oil price fluctuations caused by changes in both fundamental and transient risk factors. We achieve this by disintegrating the multi-scale risk-structure of the crude oil market using Variational Mode Decomposition. Normal and transient risk factors are then extracted from the crude oil price using Variational Mode Decomposition and modelled separately using the Quantile Regression Neural Network (QRNN) model. Both risk factors are integrated and ensembled to produce the risk estimates. We then apply our proposed risk forecasting model to predicting future downside risk level in three major crude oil markets, namely the West Taxes Intermediate (WTI), the Brent Market, and the OPEC market. The results demonstrate that our model has the ability to capture downside risk estimates with significantly improved precision, thus reducing estimation errors and increasing forecasting reliability. © 2018}}, 
pages = {574--583}, 
number = {NA}, 
volume = {76}
}
@article{10.1007/s00180-019-00873-3, 
year = {2019}, 
title = {{Two-sided exponential–geometric distribution: inference and volatility modeling}}, 
author = {Altun, Emrah}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-019-00873-3}, 
abstract = {{In this paper, two-sided exponential–geometric (TSEG) distribution is proposed and its statistical properties are studied comprehensively. The proposed distribution is applied to the GJR-GARCH model to introduce a new conditional model in forecasting Value-at-Risk (VaR). Nikkei-225 and BIST-100 indexes are analyzed to demonstrate the VaR forecasting performance of GJR-GARCH-TSEG model against the GJR-GARCH models defined under normal, Student-t, skew-T and generalized error innovation distributions. The backtesting methodology is used to evaluate the out-of-sample performance of VaR models. Empirical findings show that GJR-GARCH-TSEG model produces more accurate VaR forecasts than other competitive models. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {1215--1245}, 
number = {3}, 
volume = {34}
}
@article{10.1007/s12559-017-9516-y, 
year = {2018}, 
title = {{An Online Sequential Learning Non-parametric Value-at-Risk Model for High-Dimensional Time Series}}, 
author = {Zhang, Heng-Guo and Wu, Libo and Song, Yan and Su, Chi-Wei and Wang, Qingping and Su, Fei}, 
journal = {Cognitive Computation}, 
issn = {18669956}, 
doi = {10.1007/s12559-017-9516-y}, 
abstract = {{Online Value-at-Risk (VaR) analysis in high-dimensional space remains a challenge in the era of big data. In this paper, we propose an online sequential learning non-parametric VaR model called OS-GELM which is an autonomous cognitive system. This model uses a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) process and an online sequential extreme learning machine (OS-ELM) to cognitively calculate VaR, which can be used for online risk analysis. The proposed model not only learns the data one-by-one or chunk-by-chunk but also calculates VaR in real time by extending OS-ELM from machine learning to the non-parametric GARCH process. The GARCH process is also extended to one-by-one and chunk-by-chunk mode. In OS-GELM, the parameters of hidden nodes are randomly selected. The output weights are analytically determined based on the sequentially arriving data. In addition, the generalization performance of the OS-GELM model attains a small training error and generates the smallest norm of weights. Experimentally obtained VaRs are compared with those given by GARCH-type models and conventional OS-ELM. The computational results demonstrate that the OS-GELM model obtains more accurate results and is better at forecasting the online VaR. OS-GELM model is an autonomous cognitive system to dynamically calculate Value-at-Risk, which can be used for online financial risk assessment about human being’s behavior. The OS-GELM model can calculate VaR in real time, which can be used as a tool for online risk management. OS-GELM can handle any bounded, non-constant, piecewise-continuous membership function to realize real-time VaR monitoring. © 2017, Springer Science+Business Media, LLC.}}, 
pages = {187--200}, 
number = {2}, 
volume = {10}
}
@article{10.1080/13504851.2021.1958136, 
year = {2021}, 
title = {{Period value at risk and its estimation by Monte Carlo simulation}}, 
author = {Huo, Yanli and Xu, Chunhui and Shiina, Takayuki}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2021.1958136}, 
abstract = {{Most risk indicators for an investment show the risk at a certain future time; they cannot reflect the risk over a time period, which may be more important than the risk at a certain time. We proposed Period Value at Risk (PVaR) for measuring market risk over a period of time, and a historical simulation method to estimate the PVaR of an investment. This paper suggests a method which uses Monte Carlo simulation to estimate PVaR. We can calculate the estimation error with this method, and determine the least number of simulations for getting a qualified estimation. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.najef.2006.06.002, 
year = {2006}, 
title = {{Inadequacy of nation-based and VaR-based safety nets in the European Union}}, 
author = {Kane, Edward J.}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2006.06.002}, 
abstract = {{Considered as a social contract, a financial safety net imposes duties and confers rights on different sectors of the economy. Within a nation, elements of incompleteness inherent in this contract generate principal-agent conflicts that are mitigated by formal agreements, norms, laws, and the principle of democratic accountability. Across nations, additional gaps emerge that are hard to bridge. This paper shows that nationalistic biases and leeway in principles used to measure value-at-risk and bank capital make it unlikely that the crisis-prevention and crisis-resolution schemes incorporated in Basel II and EU Directives could allocate losses imbedded in troubled institutions efficiently or fairly across member nations. © 2006 Elsevier Inc. All rights reserved.}}, 
pages = {375--387}, 
number = {3}, 
volume = {17}
}
@article{10.1109/tencon.2009.5396179, 
year = {2009}, 
title = {{An optimization model for risk management in natural gas supply and energy portfolio of a generation company}}, 
author = {Asif, Usama and Jirutitijarocn, Panida}, 
journal = {TENCON 2009 - 2009 IEEE Region 10 Conference}, 
issn = {NA}, 
doi = {10.1109/tencon.2009.5396179}, 
abstract = {{This paper proposes a mathematical model to manage natural gas supply and energy portfolio of a generation company. The model incorporates financial risks associated with the decision-making process of buying and selling both natural gas and electricity while keeping the interaction between the two markets. Using stochastic programming framework, the problem formulation considers uncertainties associated with electricity prices and natural gas consumption, which results in a large scale mixed integer linear programming problem. The financial risks are measured by the conditional-value-at-risk (CVaR) index. A simplified test system is presented and later solved using Xpress-IVE student edition. Value of stochastic solution is calculated, which provides the value of the stochastic model. ©2009 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-642-20042-7_45, 
year = {2011}, 
title = {{A Cross-Entropy method for Value-at-Risk constrained optimization}}, 
author = {Nguyen, Duc Manh and Thi, Hoai An Le and Dinh, Tao Pham}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-20042-7\_45}, 
abstract = {{In this paper, we consider a portfolio optimization problem with a Value-at-Risk constraint. It is a nonconvex nonsmooth optimization problem which is very hard to solve. We propose an approach based on the Cross-Entropy (CE) method to tackle it. The numerical results show the efficiency of our approach. © 2011 Springer-Verlag Berlin Heidelberg.}}, 
pages = {442--451}, 
number = {PART 2}, 
volume = {6592 LNAI}
}
@article{10.1287/mnsc.1080.0964, 
year = {2011}, 
title = {{Evaluating value-at-risk models with desk-level data}}, 
author = {Berkowitz, Jeremy and Christoffersen, Peter and Pelletier, Denis}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1080.0964}, 
abstract = {{We present new evidence on disaggregated profit and loss (P/L) and value-at-risk (VaR) forecasts obtained from a large international commercial bank. Our data set includes the actual daily P/L generated by four separate business lines within the bank. All four business lines are involved in securities trading and each is observed daily for a period of at least two years. Given this unique data set, we provide an integrated, unifying framework for assessing the accuracy of VaR forecasts. We use a comprehensive Monte Carlo study to assess which of these many tests have the best finite-sample size and power properties. Our desk-level data set provides importance guidance for choosing realistic P/L-generating processes in the Monte Carlo comparison of the various tests. The conditional autoregressive value-at-risk test of Engle and Manganelli (2004) performs best overall, but duration-based tests also perform well in many cases. © 2011 INFORMS.}}, 
pages = {2213--2227}, 
number = {12}, 
volume = {57}
}
@article{10.1109/allerton.2015.7447162, 
year = {2016}, 
title = {{Mean-variance and value at risk in multi-armed bandit problems}}, 
author = {Vakili, Sattar and Zhao, Qing}, 
journal = {2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
issn = {NA}, 
doi = {10.1109/allerton.2015.7447162}, 
abstract = {{We study risk-averse multi-armed bandit problems under different risk measures. We consider three risk mitigation models. In the first model, the variations in the reward values obtained at different times are considered as risk and the objective is to minimize the mean-variance of the observed rewards. In the second and the third models, the quantity of interest is the total reward at the end of the time horizon, and the objective is to minimize the mean-variance and maximize the value at risk of the total reward, respectively. We develop risk-averse online learning policies and analyze their regret performance. We also provide tight lower bounds on regret under the model of mean-variance of observations. © 2015 IEEE.}}, 
pages = {1330--1335}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/risks9060109, 
year = {2021}, 
title = {{Monte carlo simulation of the moments of a copula-dependent risk process with weibull interwaiting time}}, 
author = {Alhabshi, Sharifah Farah Syed Yusoff and Zamzuri, Zamira Hasanah and Ramli, Siti Norafidah Mohd}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks9060109}, 
abstract = {{The widely used Poisson count process in insurance claims modeling is no longer valid if the claims occurrences exhibit dispersion. In this paper, we consider the aggregate discounted claims of an insurance risk portfolio under Weibull counting process to allow for dispersed datasets. A copula is used to define the dependence structure between the interwaiting time and its subsequent claims amount. We use a Monte Carlo simulation to compute the higher-order moments of the risk portfolio, the premiums and the value-at-risk based on the New Zealand catastrophe historical data. The simulation outcomes under the negative dependence parameter θ, shows the highest value of moments when claims experience exhibit overdispersion. Conversely, the underdispersed scenario yields the highest value of moments when θ is positive. These results lead to higher premiums being charged and more capital requirements to be set aside to cope with unfavorable events borne by the insurers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {109}, 
number = {6}, 
volume = {9}
}
@article{10.1109/wsc.2007.4419688, 
year = {2007}, 
title = {{Monte Carlo simulation in financial engineering}}, 
author = {Chen, Nan and Hong, L. Jeff}, 
journal = {2007 Winter Simulation Conference}, 
issn = {08917736}, 
doi = {10.1109/wsc.2007.4419688}, 
abstract = {{This paper reviews the use of Monte Carlo simulation in the field of financial engineering. It focuses on several interesting topics and introduces their recent development, including path generation, pricing American-style derivatives, evaluating Greeks and estimating value-at-risk. The paper is not intended to be a comprehensive survey of the research literature. © 2007 IEEE.}}, 
pages = {919--931}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/14697688.2019.1579924, 
year = {2019}, 
title = {{How to choose the return model for market risk? Getting towards a right magnitude of stressed VaR}}, 
author = {Lichtner, Mark}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2019.1579924}, 
abstract = {{Value at Risk (VaR) and stressed value at Risk (SVaR) or expected shortfall are important risk measures widely used in the financial services industry for risk management and market risk capital computation. Fundamental to any (S)VaR model is the choice of the return type model for each risk factor. Because the resulting SVaR numbers are highly sensitive to the chosen return type model it is important to make a prudent choice on the return type modelling. We propose to estimate the return type model from historic data without making an a priori model assumption on the return model. We explain the fundamentals of return type modelling and how it impacts the magnitude of SVaR. We further show how to obtain a global return type model from a set of similar return type models by using geometric calculus. Numerical simulations and illustrations are provided. In this paper, we consider interest rate data, but the proposed methodology is general and can be applied to any other asset class such as inflation, credit spread, equity or fx. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--17}, 
number = {8}, 
volume = {19}
}
@article{10.1142/s0219024907004214, 
year = {2007}, 
title = {{Time varying sensitivities on a grid architecture}}, 
author = {D'ADDONA, STEFANO and CIPRIAN, MATTIA}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024907004214}, 
abstract = {{We investigate the gains obtained by using GRID, an innovative web-based technology for parallel computing, in a Risk Management application. We show, by estimating a parametric Value at Risk, how GRID computing offers an opportunity to enhance the solution of computationally demanding problems with decentralized data retrieval. Furthermore, we also provide an analysis of the risk factors in the US market, by empirically testing, on the Fama and French database, a classic one factor model augmented with a time varying specification of beta. © World Scientific Publishing Company.}}, 
pages = {307--329}, 
number = {2}, 
volume = {10}
}
@article{10.1007/s00180-019-00934-7, 
year = {2020}, 
title = {{Data driven value-at-risk forecasting using a SVR-GARCH-KDE hybrid}}, 
author = {Lux, Marius and Härdle, Wolfgang Karl and Lessmann, Stefan}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-019-00934-7}, 
eprint = {2009.06910}, 
abstract = {{Appropriate risk management is crucial to ensure the competitiveness of financial institutions and the stability of the economy. One widely used financial risk measure is value-at-risk (VaR). VaR estimates based on linear and parametric models can lead to biased results or even underestimation of risk due to time varying volatility, skewness and leptokurtosis of financial return series. The paper proposes a nonlinear and nonparametric framework to forecast VaR that is motivated by overcoming the disadvantages of parametric models with a purely data driven approach. Mean and volatility are modeled via support vector regression (SVR) where the volatility model is motivated by the standard generalized autoregressive conditional heteroscedasticity (GARCH) formulation. Based on this, VaR is derived by applying kernel density estimation (KDE). This approach allows for flexible tail shapes of the profit and loss distribution, adapts for a wide class of tail events and is able to capture complex structures regarding mean and volatility. The SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold GARCH models coupled with different error distributions. To examine the performance in different markets, 1-day-ahead and 10-days-ahead forecasts are produced for different financial indices. Model evaluation using a likelihood ratio based test framework for interval forecasts and a test for superior predictive ability indicates that the SVR-GARCH-KDE hybrid performs competitive to benchmark models and reduces potential losses especially for 10-days-ahead forecasts significantly. Especially models that are coupled with a normal distribution are systematically outperformed. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {947--981}, 
number = {3}, 
volume = {35}
}
@article{10.1109/ical.2009.5262718, 
year = {2009}, 
title = {{Studies on risk management of the urban infrastructure projects based on the PPP financing model}}, 
author = {Chen, Hanli and Ma, Chao Qun and Liu, Bo and Qin, Tao}, 
journal = {2009 IEEE International Conference on Automation and Logistics}, 
issn = {NA}, 
doi = {10.1109/ical.2009.5262718}, 
abstract = {{With the acceleration of urbanized advancement in China, governments' high investment and low efficiency in the urban infrastructure projects have already become a worldwide problem. The PPP financing model suitable for urban infrastructure projects for theirs unique advantages. The PPP financing model in urban infrastructure projects have greater risks, so their risk management is of great significance. The paper establishes the VAR (Value at Risk) into financing risk assessment of the urban infrastructure projects, and make quantitative risk analysis to the risks; then it applies the method of expert investigation and mark and the fuzzy comprehensive evaluation to setting up the risk assessing model that is based on the VAR. Next, we establish the risk allocation model of urban infrastructure projects. Finally, we carry empirical analysis about the risk problem of an urban infrastructure project. © 2009 IEEE.}}, 
pages = {1614--1618}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.procs.2017.09.029, 
year = {2017}, 
title = {{The Impact of Funding Profiles on Budget Success}}, 
author = {Gideon, Alan K.}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2017.09.029}, 
abstract = {{One of the most valuable assets afforded a program manager is a dependable flow of funding at a rate that allows the team to address the various R\&D, engineering, procurement, and production tasks necessary to deliver the product of interest. This flow of funding with respect to time is known as the funding profile. This study compares the timewise funding profiles of a variety of DoD acquisition programs to evidence of their success in delivering the product at its designated budget. Additionally, a method is proposed for establishing a program's funding profile in terms of the program's known risks.}}, 
pages = {175--182}, 
number = {NA}, 
volume = {114}
}
@article{10.1515/snde-2016-0044, 
year = {2017}, 
title = {{A semiparametric nonlinear quantile regression model for financial returns}}, 
author = {Avdulaj, Krenar and Barunik, Jozef}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2016-0044}, 
abstract = {{Accurately measuring and forecasting value-at-risk (VaR) remains a challenging task at the heart of financial economic theory. Recently, quantile regression models have been used successfully to capture the conditional quantiles of returns and to forecast VaR accurately. In this paper, we further explore nonlinearities in data and propose to couple realized measures with the nonlinear quantile regression framework to explain and forecast the conditional quantiles of financial returns. The nonlinear quantile regression models are implied by the copula specifications and allow us to capture possible nonlinearities, tail dependence, and asymmetries in the conditional quantiles of financial returns. Using high frequency data that covers most liquid US stocks in seven sectors, we provide ample evidence of asymmetric conditional dependence with different levels of dependence, which are characteristic for each industry. The backtesting results of estimated VaR favour our approach. © 2017 Walter de Gruyter GmbH, Berlin/Boston 2017.}}, 
pages = {81--97}, 
number = {1}, 
volume = {21}
}
@article{10.1016/s0377-2217(02)00782-8, 
year = {2003}, 
title = {{The practice of Delta-Gamma VaR: Implementing the quadratic portfolio model}}, 
author = {Castellacci, Giuseppe and Siclari, Michael J.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/s0377-2217(02)00782-8}, 
abstract = {{This paper intends to critically evaluate state-of-the-art methodologies for calculating the value-at-risk (VaR) of non-linear portfolios from the point of view of computational accuracy and efficiency. We focus on the quadratic portfolio model, also known as "Delta-Gamma", and, as a working assumption, we model risk factor returns as multi-normal random variables. We present the main approaches to Delta-Gamma VaR weighing their merits and accuracy from an implementation-oriented standpoint. One of our main conclusions is that the Delta-Gamma-Normal VaR may be less accurate than even Delta VaR. On the other hand, we show that methods that essentially take into account the non-linearity (hence gammas and third or higher moments) of the portfolio values may present significant advantages over full Monte Carlo revaluations. The role of non-diagonal terms in the Gamma matrix as well as the sensitivity to correlation is considered both for accuracy and computational effort. We also qualitatively examine the robustness of Delta-Gamma methodologies by considering a highly non-quadratic portfolio value function. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {529--545}, 
number = {3}, 
volume = {150}
}
@article{10.1002/for.2303, 
year = {2014}, 
title = {{Forecasting VaR models under different volatility processes and distributions of return innovations}}, 
author = {Dendramis, Yiannis and Spungin, Giles E. and Tzavalis, Elias}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2303}, 
abstract = {{This paper provides clear-cut evidence that the out-of-sample VaR (value-at-risk) forecasting performance of alternative parametric volatility models, like EGARCH (exponential general autoregressive conditional heteroskedasticity) or GARCH, and Markov regime-switching models, can be considerably improved if they are combined with skewed distributions of asset return innovations. The performance of these models is found to be similar to that of the EVT (extreme value theory) approach. The performance of the latter approach can also be improved if asset return innovations are assumed to be skewed distributed. The performance of the Markov regime-switching model is considerably improved if this model allows for EGARCH effects, for all different volatility regimes considered. Copyright © 2014 John Wiley \& Sons, Ltd.}}, 
pages = {515--531}, 
number = {7}, 
volume = {33}
}
@article{10.7148/2016-0166, 
year = {2016}, 
title = {{Classical and novel risk measures for a stock index on a developed market}}, 
author = {Nagy, Julia Timea and Nagy, Balint Zsolt and Juhasz, Jacint}, 
journal = {ECMS 2016 Proceedings edited by Thorsten Claus, Frank Herrmann, Michael Manitz, Oliver Rose}, 
issn = {NA}, 
doi = {10.7148/2016-0166}, 
abstract = {{In the present article we conduct an inquiry into several different risk measures, illustrating their advantages and disadvantages, regulatory aspects and apply them on a stock index on a developed market: the DAX index. Specifically we are talking about Value at Risk (VaR), which is now considered a classical measure, its improved version, the Expected Shortfall (ES) and the very novel Entropic Value at Risk (EvaR). The applied computation methods are historic simulation, Monte Carlo simulation and the resampling method, which are all non-parametric methods, yielding robust results. The obtained values are put into the context of the relevant literature, and pertinent conclusions are formulated, especially regarding regulatory applications. © ECMS Thorsten Claus, Frank Herrmann, Michael Manitz, Oliver Rose (Editors).}}, 
pages = {166--171}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2015.05.004, 
year = {2015}, 
title = {{Systemic risk and asymmetric responses in the financial industry}}, 
author = {López-Espinosa, Germán and Moreno, Antonio and Rubia, Antonio and Valderrama, Laura}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2015.05.004}, 
abstract = {{To date, an operational measure of systemic risk capturing nonlinear tail-comovements between system-wide and individual bank returns has not yet been developed. This paper proposes an extension of the CoVaR methodology in Adrian and Brunnermeier (2011) to capture the asymmetric response of the banking system to positive and negative shocks to the market-valued balance sheets of individual banks. Building on a comprehensive sample of U.S. banks in the period 1990-2010, the evidence in this paper shows that ignoring asymmetries that feature tail-interdependences may lead to a severe underestimation of systemic risk. On average, the relative impact on the system of a fall in individual market value is sevenfold that of an increase. Moreover, the downward bias in systemic-risk measuring from ignoring this asymmetric pattern increases with bank size. In particular, the conditional tail-comovement between the banking system and a bank that is losing market value belonging to the top size-sorted decile is nearly 5.5 times larger than the unconditional tail-comovement versus 3.3 times for banks in the bottom decile. The asymmetric model also produces much better fitting, with the restriction that gives rise to the standard symmetric model being rejected for most firms in the sample, particularly, in the segment of large-scale banks. This result is important from a regulatory and supervisory perspective, since the asymmetric generalization enhances the capacity to monitor systemic interdependences. © 2015 Elsevier B.V.}}, 
pages = {471--485}, 
number = {NA}, 
volume = {58}
}
@article{10.1007/s10845-014-0939-y, 
year = {2016}, 
title = {{Robust optimization of supply chain network design in fuzzy decision system}}, 
author = {Bai, Xuejie and Liu, Yankui}, 
journal = {Journal of Intelligent Manufacturing}, 
issn = {09565515}, 
doi = {10.1007/s10845-014-0939-y}, 
abstract = {{This paper presents a new robust optimization method for supply chain network design problem by employing variable possibility distributions. Due to the variability of market conditions and demands, there exist some impreciseness and ambiguousness in developing procurement and distribution plans. The proposed optimization method incorporates the uncertainties encountered in the manufacturing industry. The main motivation for building this optimization model is to make tools available for producers to develop robust supply chain network design. The modeling approach selected is a fuzzy value-at-risk (VaR) optimization model, in which the uncertain demands and transportation costs are characterized by variable possibility distributions. The variable possibility distributions are obtained by using the method of possibility critical value reduction to the secondary possibility distributions of uncertain demands and costs. We also discuss the equivalent parametric representation of credibility constraints and VaR objective function. Furthermore, we take the advantage of structural characteristics of the equivalent optimization model to design a parameter-based domain decomposition method. Using the proposed method, the original optimization problem is decomposed to two equivalent mixed-integer parametric programming sub-models so that we can solve the original optimization problem indirectly by solving its sub-models. Finally, we present an application example about a food processing company with four suppliers, five plants, five distribution centers and five customer zones. We formulate our application example as parametric optimization models and conduct our numerical experiments in the cases when the input data (demands and costs) are deterministic, have fixed possibility distributions and have variable possibility distributions. Experimental results show that our parametric optimization method can provide an effective and flexible way for decision makers to design a supply chain network. © 2014, Springer Science+Business Media New York.}}, 
pages = {1131--1149}, 
number = {6}, 
volume = {27}
}
@article{10.12693/aphyspola.121.b-101, 
year = {2012}, 
title = {{Bayesian value-at-risk and expected shortfall for a large portfolio (Multi- and univariate approaches)}}, 
author = {Pajor, A and Osiewalski, J}, 
journal = {Acta Physica Polonica A}, 
issn = {05874246}, 
doi = {10.12693/aphyspola.121.b-101}, 
abstract = {{Bayesian assessments of value-at-risk and expected shortfall for a given portfolio of dimension n can be based either on the n-variate predictive distribution of future returns of individual assets, or on the univariate model for portfolio volatility. In both cases, the Bayesian VaR and ES fully take into account parameter uncertainty and non-linear relationship between ordinary and logarithmic returns. We use the n-variate type I MSF-SBEKK(1,1) volatility model proposed specially to cope with large n. We compare empirical results obtained using this (more demanding) multivariate approach and the much simpler univariate approach based on modelling volatility of the whole portfolio (of a given structure).}}, 
pages = {B--101-B-109}, 
number = {2 B}, 
volume = {121}
}
@article{10.1017/asb.2017.33, 
year = {2018}, 
title = {{Fast computation of risk measures for variable annuities with additional earnings by conditional moment matching}}, 
author = {Privault, Nicolas and Wei, Xiao}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2017.33}, 
abstract = {{We propose an approximation scheme for the computation of the risk measures of guaranteed minimum maturity benefits (GMMBs) and guaranteed minimum death benefits (GMDBs), based on the evaluation of single integrals under conditional moment matching. This procedure is computationally efficient in comparison with standard analytical methods while retaining a high degree of accuracy, and it allows one to deal with the case of additional earnings and the computation of related sensitivities. © Astin Bulletin 2017.}}, 
pages = {171--196}, 
number = {1}, 
volume = {48}
}
@article{10.1109/pes.2006.1709062, 
year = {2006}, 
title = {{Risk assessment of strategies using total time on test transform}}, 
author = {Zhao, N. and Song, Y.H. and Lu, H.}, 
journal = {2006 IEEE Power Engineering Society General Meeting}, 
issn = {NA}, 
doi = {10.1109/pes.2006.1709062}, 
abstract = {{This paper proposes a new approach to risk assessment of strategies based on the Total Time on Test (TTT) transform techniques. A new concept of risk of loss is introduced, whose value is determined by potential losses and corresponding probabilities of all unfavourable cases. It is closely related to the maximum acceptable (cost) value (MAV) set by a decision maker. How to calculate the value of risk of loss for a given strategy and a MAV is described in depth and a final formula is derived. The values of risk of loss of different strategies can be used for comparisons of these strategies in terms of overall potential losses for the purpose of assisting decision makers in choosing a better strategy. The results of this method can provide specific information about the risk of loss of a strategy. Two examples are used to demonstrate the usage of this method. The scaled TTT plot method and this risk of loss method are applied to the example A and the results of both methods agree with each other. In the example B to which the scaled TTT method is not applicable, the risk of loss method can provide reasonable solutions. This new method can be used for other situations in which the TTT concepts are valid. © 2006 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.econmod.2014.12.022, 
year = {2015}, 
title = {{Value-at-risk estimates of the stock indices in developed and emerging markets including the spillover effects of currency market}}, 
author = {Su, Jung-Bin}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2014.12.022}, 
abstract = {{This study derives the quantiles of the standardized generalized t (GT) in terms of a nonlinear equation which contains a regularized incomplete beta function. Then the quantiles are evaluated by utilizing Secant numerical approach to solve this nonlinear equation. Subsequently, the exponential generalized autoregressive conditional heteroskedasticity (EGARCH) model with GT distribution is utilized to estimate the corresponding volatility, and further estimate the value-at-risk (VaR) of seven stock indices in the developed and emerging markets. Empirical results show that, the stylized facts that appeared in most financial assets are seized effectively by this model and negative return and volatility spillover effects significantly subsist from the currency markets to stock markets. Moreover, the stock indices in emerging market have the higher return and the higher risk. As to VaR performance comparison, the modified historical simulation (MHS) and the EGARCH volatility specification significantly affect the VaR forecast performance for stock indices in the emerging market as compared with the developed market. Moreover, the VaR forecast performance of all models with GT is superior to that with normal return distribution only for stock indices in the developed market and only for 99\% level. Turning to the whole market, the VaR forecast performance is almost the same as that for the emerging market. Finally, the MHS-EGARCH model with GT distribution is the optimal model to forecast the VaR among these eight models, irrespective of which of the three markets are used. This finding can provide the financial institutions to select an appropriate model to forecast and further control the market risk they faced. © 2014 Elsevier B.V.}}, 
pages = {204--224}, 
number = {NA}, 
volume = {46}
}
@article{10.1109/acc.2001.945821, 
year = {2001}, 
title = {{Risk estimates for dynamic hedging using convex probability bounds}}, 
author = {Yamada, Yuji and Primbs, James A.}, 
journal = {Proceedings of the 2001 American Control Conference. (Cat. No.01CH37148)}, 
issn = {07431619}, 
doi = {10.1109/acc.2001.945821}, 
abstract = {{In this paper, we employ convex optimization techniques to compute upper and lower bounds on the tail of an unknown probability distribution given its first m moments. We then apply this to dynamic hedging problems, where we estimate a specific quantile of the wealth balance (hedging error) distribution, known as its Value-at-Risk. A backward recursive algorithm on a multinomial lattice is used to compute the required moments of the wealth balance. Combining this with the convex optimization probability bounds results in an efficient methodology for estimating the risk resulting from a dynamic hedge.}}, 
pages = {843--847}, 
number = {NA}, 
volume = {2}
}
@article{10.1002/qre.655, 
year = {2004}, 
title = {{Modelling operational losses: A Bayesian approach}}, 
author = {Giudici, Paolo and Bilotta, Annalisa}, 
journal = {Quality and Reliability Engineering International}, 
issn = {07488017}, 
doi = {10.1002/qre.655}, 
abstract = {{The exposure of banks to operational risk has increased in recent years. The Basel Committee on Banking Supervision (known as Basel II) has established a capital charge to cover operational risks other than credit and market risk. According to the advanced methods defined in 'The New Basel Capital Accord' to quantify the capital charge, in this paper we present an advanced measurement approach based on a Bayesian network model that estimates an internal measure of risk of the bank. One of the main problems faced when measuring the operational risk is the scarcity of loss data. The methodology proposed solves this critical point because it allows a coherent integration, via Bayes' theorem, of different sources of information, such as internal and external data, and the opinions of 'experts' (process owners) about the frequency and the severity of each loss event. Furthermore, the model corrects the losses distribution by considering the eventual relations between different nodes of the network that represent the losses of each combination of business line/event type/bank/process and the effectiveness of the corresponding internal and external controls. The operational risk capital charge is quantified by multiplying the value at risk (VaR) per event, a percentile of the losses distribution determined, by an estimate of the number of losses that may occur in a given period. Furthermore, it becomes possible to monitor the effectiveness of the internal and external system controls in place at the bank. The methodology we present has been experimented as a pilot project in one of the most important Italian banking groups, Monte dei Paschi di Siena. Copyright © 2004 John Wiley \& Sons, Ltd.}}, 
pages = {407--417}, 
number = {5}, 
volume = {20}
}
@article{10.18187/pjsor.v12i3.1136, 
year = {2016}, 
title = {{Risk forecasting of Karachi Stock Exchange: A comparison of classical and Bayesian GARCH models}}, 
author = {Iqbal, Farhat}, 
journal = {Pakistan Journal of Statistics and Operation Research}, 
issn = {18162711}, 
doi = {10.18187/pjsor.v12i3.1136}, 
abstract = {{This paper is concerned with the estimation, forecasting and evaluation of Value-at-Risk (VaR) of Karachi Stock Exchange before and after the global financial crisis of 2008 using Bayesian method. The generalized autoregressive conditional heteroscedastic (GARCH) models under the assumption of normal and heavy-tailed errors are used to forecast one-day-ahead risk estimates. Various measures and backtesting methods are employed to evaluate VaR forecasts. The observed number of VaR violations using Bayesian method is found close to the expected number of violations. The losses are also found smaller than the competing Maximum Likelihood method. The results showed that the Bayesian method produce accurate and reliable VaR forecasts and can be preferred over other methods.}}, 
pages = {453--465}, 
number = {3}, 
volume = {12}
}
@article{10.1088/1742-6596/1053/1/012112, 
year = {2018}, 
title = {{Are crude oil and natural gas extreme prices interdependent?}}, 
author = {Fuentes, Fernanda and Herrera, Rodrigo}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1053/1/012112}, 
abstract = {{We investigated the relationship between extreme prices of crude oil and natural gas. We found evidence that extreme events in these markets are interdependent and exhibit a self-excited dynamic behavior, where the intensity of extreme events in the oil market influences the occurrence rate and magnitude of extreme events in the natural gas market. In addition, we have determined that it is possible to better manage the Value-at-Risk, when both markets interact. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012112}, 
number = {1}, 
volume = {1053}
}
@article{10.1002/ijfe.2578, 
year = {2021}, 
title = {{Enriching the value-at-risk framework to ensemble empirical mode decomposition with an application to the European carbon market}}, 
author = {Zhu, Bangzhu and Wang, Ping and Chevallier, Julien and Wei, Yi‐Ming}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.2578}, 
abstract = {{Unlike common financial markets, the European carbon market is a typically heterogeneous market, characterised by multiple timescales and affected by extreme events. The traditional value-at-risk (VaR) with single-timescale fails to deal with the multi-timescale characteristics and the effects of extreme events, which can result in the VaR overestimation for carbon market risk. To measure accurately the risk on the European carbon market, we propose ensemble empirical mode decomposition (EEMD) based multiscale VaR approach. First, the EEMD algorithm is utilised to decompose the carbon price return into several intrinsic mode functions (IMFs) with different timescales and a residue, which are modelled respectively using the ARMA-GARCH model to obtain their conditional variances at different timescales. Furthermore, the Iterated Cumulative Sums of Squares algorithm is employed to determine the windows of an extreme event, so as to identify the IMFs influenced by an extreme event and conduct an exponentially weighted moving average on their conditional variations. Finally, the VaRs of various IMFs and the residue are estimated to reconstruct the overall VaR, the validity of which is verified later. Then, we illustrate the results by considering several European carbon futures contracts. Compared with the traditional VaR framework with single timescale, the proposed multiscale VaR-EEMD model can effectively reduce the influences of the heterogeneous environments (such as the influences of extreme events), and obtain a more accurate overall risk measure on the European carbon market. By acquiring the distributions of carbon market risks at different timescales, the proposed multiscale VaR-EEMD estimation is capable of understanding the fluctuation characteristics more comprehensively, which can provide new perspectives for exploring the evolution law of the risks on the European carbon market. © 2021 John Wiley \& Sons, Ltd.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/jrf-10-2019-0203, 
year = {2020}, 
title = {{Valuation of initial margin using bootstrap method}}, 
author = {Seitshiro, Modisane Bennett and Mashele, Hopolang Phillip}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-10-2019-0203}, 
abstract = {{Purpose: The purpose of this paper is to propose the parametric bootstrap method for valuation of over-the-counter derivative (OTCD) initial margin (IM) in the financial market with low outstanding notional amounts. That is, an aggregate outstanding gross notional amount of OTC derivative instruments not exceeding R20bn. Design/methodology/approach: The OTCD market is assumed to have a Gaussian probability distribution with the mean and standard deviation parameters. The bootstrap value at risk model is applied as a risk measure that generates bootstrap initial margins (BIM). Findings: The proposed parametric bootstrap method is in favour of the BIM amounts for the simulated and real data sets. These BIM amounts are reasonably exceeding the IM amounts whenever the significance level increases. Research limitations/implications: This paper only assumed that the OTCD returns only come from a normal probability distribution. Practical implications: The OTCD IM requirement in respect to transactions done by counterparties may affect the entire financial market participants under uncleared OTCD, while reducing systemic risk. Thus, reducing spillover effects by ensuring that collateral (IM) is available to offset losses caused by the default of a OTCDs counterparty. Originality/value: This paper contributes to the literature by presenting a valuation of IM for the financial market with low outstanding notional amounts by using the parametric bootstrap method. © 2020, Emerald Publishing Limited.}}, 
pages = {543--557}, 
number = {5}, 
volume = {21}
}
@article{10.3390/su13095313, 
year = {2021}, 
title = {{The use of hydromulching as an alternative to plastic films in an Artichoke (Cynara Cardunculus cv. symphony) crop: A study of the economic viability}}, 
author = {López-Marín, Josefa and Romero, Miriam and Gálvez, Amparo and Amor, Francisco Moisés del and Piñero, Maria Carmen and Brotons-Martínez, José Manuel}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su13095313}, 
abstract = {{The use of mulching in agriculture suppresses the weeds around crop plants, enhances the nutrients status of soil, controls the soil structure and temperature, and reduces soil water evaporation. Excessive use of low-density polyethylene mulches is contributing to the accumulation of high amounts of plastic wastes, an environmental problem for agricultural ecosystems. Fragments of plastic from such wastes can be found in soils, in water resources, and in organisms, including humans. The objective of this work was to study the economic viability of the use of different hydromulches in an artichoke crop. Three blends were prepared by mixing paper pulp (recycled from used paper) and cardboard (from paper mills) with different additives: wheat straw (WS), rice hulls (RH), and substrate used for mushroom cultivation (MS). These were compared with low-density polyethylene (Pe), a treatment without mulching on bare soil where hand weeding was performed (HW), and a treatment without mulching on bare soil where herbicide was applied (H). The results indicate that the use of hydromulch in an artichoke crop represents a good alternative for reducing plastic waste in agriculture. The net profits of the hydromulch treatments (MS, WS, RH) were higher than for HW and H, and slightly lower than for Pe. The most profitable treatment was Pe (€0.69 m−3), followed by RH (€0.59 m−3), WS (€0.58 m−3), MS (€0.47 m−3), HW (€0.36 m−3), and H (€0.32 m−3). A sensitivity analysis showed a probability of negative results of 0.04 in Pe, 0.13 in SM, 0.08 in WS, and 0.07 in RH, so the probability that the grower will make a profit is greater than 0.9 with the use of mulch (except mushroom substrate) or polyethylene. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {5313}, 
number = {9}, 
volume = {13}
}
@article{10.1002/for.1074, 
year = {2008}, 
title = {{Scalar BEKK and indirect DCC}}, 
author = {Caporin, Massimiliano and McAleer, Michael}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1074}, 
abstract = {{The paper derives the scalar special case of the well-known BEKK multivariate GARCH model using a multivariate extension of the random coefficient autoregressive (RCA) model. This representation establishes the relevant structural and asymptotic properties of the scalar BEKK model using the theoretical results available in the literature for general multivariate GARCH. Sufficient conditions for the (direct) DCC model to be consistent with a scalar BEKK representation are established. Moreover, an indirect DCC model that is consistent with the scalar BEKK representation is obtained, and is compared with the direct DCC model using an empirical example. The paper shows, within an asset allocation and risk measurement framework, that the two models are similar in terms of providing parameter estimates and forecasting value-at-risk thresholds for equally weighted and minimum variance portfolios. Copyright ©2008 John Wiley and Sons, Ltd.}}, 
pages = {537--549}, 
number = {6}, 
volume = {27}
}
@article{10.1002/int.20360, 
year = {2009}, 
title = {{Portfolio management using value at risk: A comparison between genetic algorithms and particle swarm optimization}}, 
author = {Dallagnol, V. A. F. and Berg, J. van den and Mous, L.}, 
journal = {International Journal of Intelligent Systems}, 
issn = {08848173}, 
doi = {10.1002/int.20360}, 
abstract = {{In this paper, it is shown a comparison of the application of particle swarm optimization and genetic algorithms to portfolio management, in a constrained portfolio optimization problem where no short sales are allowed. The objective function to be minimized is the value at risk calculated using historical simulation where several strategies for handling the constraints of the problem were implemented. The results of the experiments performed show that, generally speaking, the methods are capable of consistently finding good solutions quite close to the best solution found in a reasonable amount of time. In addition, it is demonstrated statistically that the algorithms, on average, do not all consistently achieve the same best solution. PSO turned out to be faster than GA, both in terms of number of iterations and in terms of total running time. However, PSO appears to be much more sensitive to the initial position of the particles than GA. Tests were also made regarding the number of particles needed to solve the problem, and 50 particles/chromosomes seem to be enough for problems up to 20 assets. © 2009 Wiley Periodicals, Inc.}}, 
pages = {766--792}, 
number = {7}, 
volume = {24}
}
@article{10.3390/en13143700, 
year = {2020}, 
title = {{Forecasts of value-at-risk and expected shortfall in the crude oil market: A wavelet-based semiparametric approach}}, 
author = {Yang, Lu and Hamori, Shigeyuki}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13143700}, 
abstract = {{We propose the use of wavelet-based semiparametric models for forecasting the value-at-risk (VaR) and expected shortfall (ES) in the crude oil market. We compared the forecast outcomes across different time scales for three semiparametric models, three nonparametric, distribution-based, generalized, autoregressive, conditional, heteroskedasticity (GARCH) models, and three rolling-window models. We found that the GARCH model estimated by the Fissler and Ziegel (FZ) zero loss minimization (GARCH-FZ) model performs the best at forecasting the VaR and ES in the short term, whereas the hybrid model performs the best for mid- and long-term time scales. Thus, long-term investors should consider the hybrid model and short-term investors should employ the GARCH-FZ model in their risk management processes. Overall, our proposed wavelet-based semiparametric models outperform the other models tested for all time scales and market conditions. As such, we suggest that these models are considered for the management of crude oil price risk and in the development of energy policy. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {3700}, 
number = {14}, 
volume = {13}
}
@article{10.1057/gpp.2012.47, 
year = {2013}, 
title = {{Risk measures and capital requirements: A critique of the solvency II approach}}, 
author = {Floreani, Alberto}, 
journal = {The Geneva Papers on Risk and Insurance - Issues and Practice}, 
issn = {10185895}, 
doi = {10.1057/gpp.2012.47}, 
abstract = {{In this paper the Solvency II VaR-based capital requirement is analysed and discussed. The new European risk-based system of prudential regulation for insurers could in fact increase, and not decrease, the fragility of the insurance industry. More specifically, the VaR capital requirement exposes insurance companies to a potentially huge systemic effect, as the bigger/better diversified insurers have high default probabilities in case of market shortfalls. This paper shall suggest and discuss some adjustments to the current Solvency II framework. © 2013 The International Association for the Study of Insurance Economics.}}, 
pages = {189--212}, 
number = {2}, 
volume = {38}
}
@article{10.4028/www.scientific.net/amr.601.464, 
year = {2013}, 
title = {{Estimation of value-at-risk based on ARFIMA-FIAPARCH-SKST model}}, 
author = {Tan, Bin}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.601.464}, 
abstract = {{This paper focus mainly on some important stylized facts in financial market, such as long memory, asymmetry and leverage effect, and so on, and apply ARFIMA-APARCH-SKST model to measure dynamic Value at Risk, at the same time, ARMA-EGARCH(APARCH)-SKST, ARFIMA- FIEGARCH-SKST are used to compare empirical effect of different risk model, at last, we apply LRT method to test accuracy of risk model. Our results indicate that all models used in this paper can measure dynamic VaR at 95\%, 99\% and 99.5\% confidence levels, and there is no significant difference for different risk model for different stock markets. Moreover, we find also that long memory is not more valuable stylized fact than asymmetry for SSEC and S\&P500. © (2013) Trans Tech Publications, Switzerland.}}, 
pages = {464--469}, 
number = {NA}, 
volume = {601}
}
@article{10.1007/978-3-319-89824-7_31, 
year = {2018}, 
title = {{Conditional quantile-located VaR}}, 
author = {Bonaccolto, Giovanni and Caporin, Massimiliano and Paterlini, Sandra}, 
issn = {NA}, 
doi = {10.1007/978-3-319-89824-7\_31}, 
abstract = {{The Conditional Value-at-Risk (CoVaR) has been proposed by Adrian and Brunnermeier (Am Econ Rev 106:1705-1741, 2016) to measure the impact of a company in distress on the Value-at-Risk (VaR) of the financial system. We propose an extension of the CoVaR, that is, the Conditional Quantile-Located VaR (QL-CoVaR), that better deals with tail events, when spillover effects impact the stability of the entire system. In fact, the QL-CoVaR is estimated by assuming that the financial system and the individual companies simultaneously lie in the left tails of their distributions. © Springer International Publishing AG.}}, 
pages = {167--171}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/9781118656501.ch25, 
year = {2013}, 
title = {{Hedge Funds and Risk Management}}, 
author = {Syriopoulos, Theodore}, 
issn = {NA}, 
doi = {10.1002/9781118656501.ch25}, 
abstract = {{Hedge funds have exhibited not only fast growth rates and increased assets under management but also losses and failures. The dynamic investment strategies employed and the complex risk exposures undertaken have turned the issue of efficient risk management into a critical priority. This chapter contributes a concise discussion and critical evaluation of the advantages and limitations of the most appropriate risk tools to apply to hedge funds, such as the variance-based approach, Value at Risk, expected shortfall, extreme value theory, tail analysis, and generalized Pareto distribution. The empirical approaches most widely employed to calculate risk measures are also introduced, including parametric models, Monte Carlo and historical simulations, scenario analysis, stress tests, and copulas. © 2013 John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {495--519}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s001810000062, 
year = {2001}, 
title = {{Conditional value-at-risk: Aspects of modeling and estimation}}, 
author = {Chernozhukov, Victor and Umantsev, Len}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s001810000062}, 
abstract = {{This paper considers flexible conditional (regression) measures of market risk. Value-at-Risk modeling is cast in terms of the quantile regression function - the inverse of the conditional distribution function. A basic specification analysis relates its functional forms to the benchmark models of returns and asset pricing. We stress important aspects of measuring the extremal and intermediate conditional risk. An empirical application characterizes the key economic determinants of various levels of conditional risk.}}, 
pages = {271--292}, 
number = {1}, 
volume = {26}
}
@article{10.5755/j01.ee.27.4.13927, 
year = {2016}, 
title = {{Modelling financial market volatility using asymmetric-skewed-ARFIMAX and -HARX models}}, 
author = {Chin, Wen Cheong and Lee, Min Cherng and Yap, Grace Lee Ching}, 
journal = {Engineering Economics}, 
issn = {13922785}, 
doi = {10.5755/j01.ee.27.4.13927}, 
abstract = {{This study aims to propose an improved modelling framework for high frequency volatitliy in financial stock market. Extended heterogeneous autoregressive (HAR) and fractionally integrated autoregressive moving average (ARFIMA) models are introduced to model the S\&P500 index using various realized volatility measures that are robust to jumps. These measures are the tripower variation volatility, and the realized volatities integrated with the nearest neighbor truncation (NNT) approach, namely the minimum and the median realized volatilities. In order to capture volatility clustering and the asymmetric property of various realized volatilities, the HAR and ARFIMA models are extended with asymmetric GARCH threshold specification. In addition, the asymmetric innovations of various realized volatilities are characterized by a skewed student-t distribution. The empirical findings show that the extended model returns the best performance in the in-sample and out-of-sample forecast evaluations. The forecasted results are used in the determination of value-at-risk for S\&P500 market. © 2016, Kauno Technologijos Universitetas. All rights reserved.}}, 
pages = {373 -- 381}, 
number = {4}, 
volume = {27}
}
@article{10.26845/kjfs.2021.02.50.1.113, 
year = {2021}, 
title = {{Optimal asset allocation of pension funds under a value-at-risk constraint}}, 
author = {Chae, Jiwon and Jang, Bong-Gyu}, 
journal = {Korean Journal of Financial Studies}, 
issn = {20058187}, 
doi = {10.26845/kjfs.2021.02.50.1.113}, 
abstract = {{Pension funds must consider both liabilities and assets simultaneously from the perspective of Asset-Liability Management (ALM) to ensure financial soundness. In the context of such ALM schemes, this study presents an optimal portfolio strategy that maximizes the pension fund managers’ utility score as measured by the funding ratio. Our model includes a Value-at-Risk (or shortfall) constraint at the end of the investment horizon. We extend the model in Kraft and Steffensen (2013) to a model with multiple risky assets. Our examples demonstrate that optimal fund managers with high risk-aversion should increase their investment proportions in risky assets when the funding ratio is slightly below the target value, but should first reduce their risky investment and maintain a certain level if the funding ratio is far away from the target value. In addition, as the correlation coefficient between risky assets decreases, the optimal risky proportions increase and their gap decreases. We can interpret this to mean that the stronger the negative correlation is, the stronger the hedging effect between the risky assets is. © 2021, Korean Securities Association. All rights reserved.}}, 
pages = {113--134}, 
number = {1}, 
volume = {50}
}
@article{10.1016/s0378-4266(97)00047-2, 
year = {1997}, 
title = {{Stress tests of capital requirements}}, 
author = {Dimson, Elroy and Marsh, Paul}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(97)00047-2}, 
abstract = {{This paper examines the performance of the leading methods for setting capital requirements for securities firms' trading books. Tests are conducted on a large sample of UK equity market makers' books over a substantial number of periods of equity market stress from 1985 to 1995. The comprehensive and building-block approaches, favoured by US and European regulators, fail to provide effective cover. Only portfolio-based, value-at-risk (VaR) type models are efficient in providing appropriate levels of capital to cover the position risk of equity trading books.}}, 
pages = {1515--1546}, 
number = {11-12}, 
volume = {21}
}
@article{10.1145/3018661.3018701, 
year = {2017}, 
title = {{Managing risk of bidding in display advertising}}, 
author = {Rijke, Maarten de and Shokouhi, Milad and Tomkins, Andrew and Zhang, Min and Zhang, Haifeng and Zhang, Weinan and Rong, Yifei and Ren, Kan and Li, Wenxin and Wang, Jun}, 
journal = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining}, 
issn = {NA}, 
doi = {10.1145/3018661.3018701}, 
eprint = {1701.02433}, 
abstract = {{In this paper, we deal with the uncertainty of bidding for display advertising. Similar to the financial market trading, real-time bidding (RTB) based display advertising employs an auction mechanism to automate the impression level media buying; and running a campaign is no different than an investment of acquiring new customers in return for obtaining additional converted sales. Thus, how to optimally bid on an ad impression to drive the profit and return-on-investment becomes essential. However, the large randomness of the user behaviors and the cost uncertainty caused by the auction competition may result in a significant risk from the campaign performance estimation. In this paper, we explicitly model the uncertainty of user click-through rate estimation and auction competition to capture the risk. We borrow an idea from finance and derive the value at risk for each ad display opportunity. Our formulation results in two risk-aware bidding strategies that penalize risky ad impressions and focus more on the ones with higher expected return and lower risk. The empirical study on real-world data demonstrates the effectiveness of our proposed risk-aware bidding strategies: yielding profit gains of 15.4\% in offline experiments and up to 17.5\% in an online A/B test on a commercial RTB platform over the widely applied bidding strategies. © 2017 ACM.}}, 
pages = {581--590}, 
number = {NA}, 
volume = {NA}
}
@article{10.1093/jjfinec/nbn015, 
year = {2009}, 
title = {{Linear correlation and EVT: Properties and caveats}}, 
author = {Embrechts, P}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbn015}, 
abstract = {{Due to the current credit crisis, critical questions are being asked concerning some of the quantitative methods used in risk management under the Basel II proposals. In this paper I have given a critical look at Extreme Value Theory and Copulas. Both their potential applications and the possible caveats are discussed, and this mainly with the subprime crisis as a background. © The Author 2008. Published by Oxford University Press. All rights reserved.}}, 
pages = {30--39}, 
number = {1}, 
volume = {7}
}
@article{10.1016/j.jbankfin.2006.01.004, 
year = {2007}, 
title = {{A note on the importance of overnight information in risk management models}}, 
author = {Taylor, Nicholas}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2006.01.004}, 
abstract = {{This paper examines the economic value of overnight information to users of risk management models. In addition to the information revealed by overseas markets that trade during the (domestic) overnight period, this paper exploits information generated via recent innovations in the structure of financial markets. In particular, certain securities (and associated derivative products) can now be traded at any time over a 24-h period. As such, it is now possible to make use of information generated by trading, in (almost) identical securities, during the overnight period. Of the securities that are available over such time periods, S\&P 500 related products are by far the most actively traded and are, therefore, the subject of this paper. Using a variety of conditional volatility models that allow time-dependent information flow within (and across) three different S\&P 500 markets, the results show that overnight information flow has a significant impact on the conditional volatility of daytime traded S\&P 500 securities. Moreover (time-consistent) forecasts from models that incorporate overnight information are shown to have economic value to risk managers. In particular, Value-at-Risk (VaR) models based on these conditional volatility models are shown to be more accurate than VaR models that ignore overnight information. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {161--180}, 
number = {1}, 
volume = {31}
}
@article{10.1016/j.dss.2011.02.009, 
year = {2011}, 
title = {{Profit-maximizing firm investments in customer information security}}, 
author = {Lee, Yong Jick and Kauffman, Robert J. and Sougstad, Ryan}, 
journal = {Decision Support Systems}, 
issn = {01679236}, 
doi = {10.1016/j.dss.2011.02.009}, 
abstract = {{When a customer interacts with a firm, extensive personal information often is gathered without the individual's knowledge. Significant risks are associated with handling this kind of information. Providing protection may reduce the risk of the loss and misuse of private information, but it imposes some costs on both the firm and its customers. Nevertheless, customer information security breaches still may occur. They have several distinguishing characteristics: (1) typically it is hard to quantify monetary damages related to them; (2) customer information security breaches may be caused by intentional attacks, as well as through unintentional organizational and customer behaviors; and (3) the frequency of such incidents typically is low, although they can be very costly when they occur. As a result, predictive models and explanatory statistical analysis using historical data have not been effective. We present a profit optimization model for customer information security investments. Our approach is based on value-at-risk methods and operational risk modeling from financial economics. The main results of this work are that we: (1) provide guidance on the trade-offs between risk and return in customer information security investments; (2) define the range of efficient investments in technology-supported risk indemnification for sellers; (3) model how to handle government-dictated levels of investment versus self-regulation of investments in technology; and (4) characterize customer information security investment levels when the firm is able to pass some of its costs on to consumers. We illustrate our theoretical findings with empirical data from the Open Security Foundation, as a means of grounding our analysis and offering the reader intuition for the managerial interpretation of our theory and main results. The results show that we can narrow the decision set for solution providers and policy-makers based on the estimable risks and losses associated with customer information security. We also discuss the application of our approach in practice. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {904--920}, 
number = {4}, 
volume = {51}
}
@article{10.1142/9789812709691_0014, 
year = {2007}, 
title = {{Stochastic risk capital model for insurance company}}, 
author = {Skiadas, Christos H and Pettere, Gaida}, 
journal = {Recent Advances in Stochastic Modeling and Data Analysis}, 
issn = {NA}, 
doi = {10.1142/9789812709691\_0014}, 
abstract = {{The paper is about stochastic modelling risk capital requirements to cover equity risk for an insurance company by using copulas. We have tried to find the best portfolio structure with copula model and simulation using risk measure VAR. The result is compared with equity shock model given in Solvency I1 documents. © 2007 by World Scientific Publishing Co. Pte. Ltd. All rights reserved.}}, 
pages = {114--121}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.csda.2006.09.017, 
year = {2006}, 
title = {{Accurate value-at-risk forecasting based on the normal-GARCH model}}, 
author = {Hartz, Christoph and Mittnik, Stefan and Paolella, Marc}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2006.09.017}, 
abstract = {{A resampling method based on the bootstrap and a bias-correction step is developed for improving the Value-at-Risk (VaR) forecasting ability of the normal-GARCH model. Compared to the use of more sophisticated GARCH models, the new method is fast, easy to implement, numerically reliable, and, except for having to choose a window length L for the bias-correction step, fully data driven. The results for several different financial asset returns over a long out-of-sample forecasting period, as well as use of simulated data, strongly support use of the new method, and the performance is not sensitive to the choice of L. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {2295--2312}, 
number = {4}, 
volume = {51}
}
@article{10.1108/00214660480001156, 
year = {2004}, 
title = {{Application of credit risk models to agricultural lending}}, 
author = {Zech, Lyubov and Pederson, Glenn}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00214660480001156}, 
abstract = {{A credit risk model suitable for agricultural lenders is identified. The model incorporates sector correlations and is applied to the loan portfolio of an agricultural credit association to create a distribution of loan losses. The distribution is used to derive the lender’s expected and unexpected losses. Results of the analysis indicate that the association is more than adequately capitalized based on 1997S2002 data. Since the capital position of the association is lower than that of most other associations in the Farm Credit System, this raises the issue of overcapitalization in the System. © 2001, Emerald Group Publishing Ltd. All rights reserved.}}, 
pages = {91--106}, 
number = {2}, 
volume = {64}
}
@article{10.1186/s40854-021-00254-0, 
year = {2021}, 
title = {{A note on calculating expected shortfall for discrete time stochastic volatility models}}, 
author = {Grabchak, Michael and Christou, Eliana}, 
journal = {Financial Innovation}, 
issn = {21994730}, 
doi = {10.1186/s40854-021-00254-0}, 
abstract = {{In this paper we consider the problem of estimating expected shortfall (ES) for discrete time stochastic volatility (SV) models. Specifically, we develop Monte Carlo methods to evaluate ES for a variety of commonly used SV models. This includes both models where the innovations are independent of the volatility and where there is dependence. This dependence aims to capture the well-known leverage effect. The performance of our Monte Carlo methods is analyzed through simulations and empirical analyses of four major US indices. © 2021, The Author(s).}}, 
pages = {43}, 
number = {1}, 
volume = {7}
}
@article{10.1142/s021902491250029x, 
year = {2012}, 
title = {{Multivariate heavy-tailed models for value-at-risk estimation}}, 
author = {MARINELLI, CARLO and D'ADDONA, STEFANO and RACHEV, SVETLOZAR T}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s021902491250029x}, 
abstract = {{For purposes of Value-at-Risk estimation, we consider several multivariate families of heavy-tailed distributions, which can be seen as multidimensional versions of Paretian stable and Student's t distributions allowing different marginals to have different indices of tail thickness. After a discussion of relevant estimation and simulation issues, we conduct a backtesting study on a set of portfolios containing derivative instruments, using historical US stock price data. © 2012 World Scientific Publishing Company.}}, 
pages = {1250029}, 
number = {4}, 
volume = {15}
}
@article{10.1002/isaf.1372, 
year = {2016}, 
title = {{VaRSOM: A Tool to Monitor Markets' Stability Based on Value at Risk and Self-Organizing Maps}}, 
author = {Resta, Marina}, 
journal = {Intelligent Systems in Accounting, Finance and Management}, 
issn = {15501949}, 
doi = {10.1002/isaf.1372}, 
abstract = {{We introduce a variant of self-organizing maps (SOMs) termed VaRSOM that evaluates the similarity among inputs and nodes of the map employing value at risk (VaR). In this way we embed risk measurement within a machine-learning architecture, thus becoming particularly well-suited to analysing financial data. We tested the visualization capabilities and the explicative power of VaRSOM on data from the German Stock Exchange; we then evaluated the results in a comparative perspective, opposing the VaRSOM outcomes to those of SOM trained with more conventional similarity measures. The results lead to the conclusion that VaRSOM is a tool particularly well suited to visualize and exploit critical patterns in financial markets. This, in turn, opens perspectives for a general machine-learning framework sensitive to financial distress and contagion effects. © 2016 John Wiley \& Sons, Ltd.}}, 
pages = {47--64}, 
number = {1-2}, 
volume = {23}
}
@article{10.1080/00207543.2013.849823, 
year = {2014}, 
title = {{Incorporating risk measures in closed-loop supply chain network design}}, 
author = {Soleimani, Hamed and Seyyed-Esfahani, Mirmehdi and Kannan, Govindan}, 
journal = {International Journal of Production Research}, 
issn = {00207543}, 
doi = {10.1080/00207543.2013.849823}, 
abstract = {{This paper considers a location-allocation problem in a closed-loop supply chain (CLSC) with two extensions: first, demand and prices of new and return products are regarded as non-deterministic parameters and second, the objective function is developed from expected profit to three types of mean-risk ones. Indeed, design and planning an integrated CLSC in real-world volatile markets is an important and necessary issue. Further, risk-neutral approaches, which are considered expected values, are not efficient for such uncertain conditions. Hence, this paper, copes with the design and planning problem of a CLSC in a two-stage stochastic structure. Besides, risk criteria are considered through using three types of popular and well-behaved risk measures: mean absolute deviation, value at risk and conditional value at risk (CVaR). Consequently, three types of mean-risk models are developed as objective functions and decision-making procedures are undertaken based on the expected values and risk adversity criteria. Finally, performances of the developed mean-risk models are evaluated in various aspects. Results reveal that the inefficiencies of risk-neutral approaches can be overcome. In addition, in terms of quality of solutions, the acceptability of CVaR is proved when it is compared to other risk measures. © 2014 © 2014 Taylor \& Francis.}}, 
pages = {1843--1867}, 
number = {6}, 
volume = {52}
}
@article{10.1016/j.insmatheco.2010.06.002, 
year = {2010}, 
title = {{Optimal investment-reinsurance policy for an insurance company with VaR constraint}}, 
author = {Chen, Shumin and Li, Zhongfei and Li, Kemian}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2010.06.002}, 
abstract = {{This paper investigates an investment-reinsurance problem for an insurance company that has a possibility to choose among different business activities, including reinsurance/new business and security investment. Our main objective is to find the optimal policy to minimize its probability of ruin. The main novelty of this paper is the introduction of a dynamic Value-at-Risk (VaR) constraint. This provides a way to control risk and to fulfill the requirement of regulators on market risk. This problem is formulated as an infinite horizontal stochastic control problem with a constrained control space. The dynamic programming technique is applied to derive the Hamilton-Jacobi-Bellman (HJB) equation and the Lagrange multiplier method is used to tackle the dynamic VaR constraint. Closed-form expressions for the minimal ruin probability as well as the optimal investment-reinsurance/new business policy are derived. It turns out that the risk exposure of the insurance company subject to the dynamic VaR constraint is always lower than otherwise. Finally, a numerical example is given to illustrate our results. © 2010 Elsevier B.V.}}, 
pages = {144--153}, 
number = {2}, 
volume = {47}
}
@article{10.21314/jor.2016.337, 
year = {2016}, 
title = {{The role of model risk in extreme value theory for capital adequacy}}, 
author = {Scheule, Harald and Kellner, Ralf and Rösch, Daniel}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2016.337}, 
abstract = {{In the recent literature, methods from extreme value theory (EVT) have frequently been applied to the estimation of tail risk measures. While previous analyses show that EVT methods often lead to accurate estimates for risk measures, a potential drawback lies in large standard errors of the point estimates in these methods, as only a fraction of the data set is used. Thus, we comprehensively study the impact of model risk on EVT methods when determining the value-at-risk and expected shortfall. We distinguish between first-order effects of model risk, which consist of misspecification and estimation risk, and second-order effects of model risk, which refer to the dispersion of risk measure estimates, and show that EVT methods are less prone to first-order effects. However, they show a greater sensitivity toward secondorder effects.We find that this can lead to severe value-at-risk and expected shortfall underestimations and should be reflected in regulatory capital models. © 2016 Incisive Risk Information (IP) Limited.}}, 
number = {6}, 
volume = {18}
}
@article{10.1109/icmse.2013.6586492, 
year = {2013}, 
title = {{Evaluation of Chinese money funds' performance}}, 
author = {Zhang, Ying and Tan, Min-zhi and Tjong, Andree}, 
journal = {2013 International Conference on Management Science and Engineering 20th Annual Conference Proceedings}, 
issn = {21551847}, 
doi = {10.1109/icmse.2013.6586492}, 
abstract = {{This paper evaluates the performance of nineteen sample funds for the period from 2007 to 2011 using three indices for measurement. Generally, Sharpe Index is the best measurement to evaluate the Chinese funds performance, but it still has some limitations while the averages of returns are not normally distributed. In order to solve this problem, we modified the traditional Sharpe Index into the Sharpe Index modified with VaR method. Furthermore, we also applied Cornish-Fisher expansion into VaR calculation to get better results. In order to assess whether the VaR methods used to evaluate the funds performance are reliable or not, we also conducted the back test. The back test used to assess the accuracy of VaR model modified with Cornish-Fisher expansion approach is the back test using the Traffic Light method and Kupiec's method, and the result of back testing reveals that the VaR model modified with Cornish-Fisher expansion approach could generate accurate risk estimation to the majority of sample funds. After analyzing the data collected, this paper observes that the most outstanding fund from nineteen samples of Chinese money funds according to adjusted Sharpe Index with VaR method is the Da Mo Hua Xin Huo Bi fund. © 2013 IEEE.}}, 
pages = {1682--1688}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/10807039.2012.738154, 
year = {2013}, 
title = {{Catastrophe Risk Analysis: A Financial Perspective}}, 
author = {Luo, Cui Cui and Wu, Dexiang}, 
journal = {Human and Ecological Risk Assessment: An International Journal}, 
issn = {10807039}, 
doi = {10.1080/10807039.2012.738154}, 
abstract = {{This article examines how catastrophe events affect risk analysis from a financial perspective. Data from different industries such as Advanced Sustainable Performance Indices, gold, energy, real estate, and insurance were collected and analyzed. The performance of these funds was compared by using various financial ratios. Then we tested the stock selecting and market timing abilities. We also assessed whether a particular catastrophe event has affected stock prices by analysis of two event studies, the 9/11 terrorist attacks in the United States and the collapse through bankruptcy of Lehman Brothers. We examined how an asset portfolio performs under catastrophic events and under the situation of adding Advanced Sustainable Performance Indices into an investor's portfolio basket. We found that the 9/11 terrorist attacks affected the Dow Jones Real Estate Index's prices a lot. Lehman Brothers' bankruptcy filing had a positive impact on the CBOE Gold Index, and had a large impact on the Fidelity US Bond Index. The ASPI Index in our optimization problem gave us better diversification. From our analysis, we conclude that portfolio diversification is a good way to hedge against catastrophic risk. © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1372--1384}, 
number = {5}, 
volume = {19}
}
@article{10.1016/j.pacfin.2007.08.001, 
year = {2008}, 
title = {{Evaluating the impact of market reforms on Value-at-Risk forecasts of Chinese A and B shares}}, 
author = {Veiga, Bernardo da and Chan, Felix and McAleer, Michael}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2007.08.001}, 
abstract = {{This paper analyses the time-varying conditional correlations between Chinese A and B share returns using the Dynamic Conditional Correlation (DCC) model of Engle [Engle, R.F. (2002), "Dynamic Conditional Correlation: A Simple Class of Multivariate Generalized Autoregressive Conditional Heteroskedasticity Models", Journal of Business and Economic Statistics, 20, 339-350.]. The results show that the conditional correlations increased substantially following the B share market reform, whereby Chinese investors were permitted to purchase B shares. However, this increase in correlations was found to have begun well before the B share market reform. This result has significant implication relating to the structure of the information flow between the markets for the two classes of shares. Value-at-Risk (VaR) threshold forecasts are used to analyse the importance of accommodating dynamic conditional correlations between Chinese A and B shares, and thus reflects the impact of the changes in information flow on the risk evaluation of a diversified portfolio. The competing VaR forecasts are analysed using the Unconditional Coverage, Serial Independence and Conditional Coverage tests of Christoffersen [Christoffersen (1998), "Evaluating Interval Forecasts", International Economic Review, 39, 841-862], and the Time Until First Failure Test of Kupiec [Kupiec, P.H., (1995), "Techniques for Verifying the Accuracy of Risk Measurements Models", Journal of Derivatives, 73-84]. The results offer mild support for the DCC model over its constant conditional correlation counterpart. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {453--475}, 
number = {4}, 
volume = {16}
}
@article{10.1016/j.frl.2018.12.031, 
year = {2020}, 
title = {{Risk measurement of international carbon market based on multiple risk factors heterogeneous dependence}}, 
author = {Zhang, Chen and Yang, Yu and Yun, Po}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2018.12.031}, 
abstract = {{This paper aims to measure carbon market risk considering heterogeneous dependence in multiple risk factors including carbon price, domestic interest rate and exchange rate. Based on vine copula, we analyze dependency and then explore the risk, and we study periodic differences across three stages in the EU ETS. We find vine copula considers risk to a greater extent by heterogeneous dependence in risk factors, and such consideration decreases excessive risk aversion. Moreover, both risk factor dependence and VaR have significant periodic differences. This study provides a more accurate and reasonable risk measurement method to better realize carbon reduction targets. © 2018 Elsevier Inc.}}, 
pages = {101083}, 
number = {NA}, 
volume = {32}
}
@article{10.1016/j.physa.2017.02.080, 
year = {2017}, 
title = {{Exchangeability, extreme returns and Value-at-Risk forecasts}}, 
author = {Huang, Chun-Kai and North, Delia and Zewotir, Temesgen}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2017.02.080}, 
abstract = {{In this paper, we propose a new approach to extreme value modelling for the forecasting of Value-at-Risk (VaR). In particular, the block maxima and the peaks-over-threshold methods are generalised to exchangeable random sequences. This caters for the dependencies, such as serial autocorrelation, of financial returns observed empirically. In addition, this approach allows for parameter variations within each VaR estimation window. Empirical prior distributions of the extreme value parameters are attained by using resampling procedures. We compare the results of our VaR forecasts to that of the unconditional extreme value theory (EVT) approach and the conditional GARCH-EVT model for robust conclusions. © 2017 Elsevier B.V.}}, 
pages = {204--216}, 
number = {NA}, 
volume = {477}
}
@article{10.1016/j.jbankfin.2007.02.007, 
year = {2007}, 
title = {{Basel's value-at-risk capital requirement regulation: An efficiency analysis}}, 
author = {Kaplanski, Guy and Levy, Haim}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2007.02.007}, 
abstract = {{We analyze the optimal portfolio policies of expected utility maximizing agents under VaR Capital Requirement (VaR-CR) regulation in comparison to the optimal policy under exogenously-imposed VaR Limit (VaR-L) and Limited-Expected-Loss (LEL) regulations. With VaR-CR regulation the agent strategy consists of simultaneous decisions on both the portfolio VaR and on the implied amount of required eligible capital. As a result, the performance of VaR-CR regulation depends on its design (the parameter n) and the agent preferences. We show that an optimal VaR-CR regulation allows the regulator on the one hand, to completely eliminate the exposure to the largest losses, which may jeopardize the existence of the institution, and on the other hand, to restrain the portfolio exposure to all other losses. These results rationalize the current Basel regulations. However, the analysis shows also that there is an optimal level of required eligible capital from the regulator standpoint. Counter-intuitively, any requirement above this optimal level is inefficient as it leads to a smaller amount of actually maintained eligible capital and thereby to a larger exposure to the most adverse states of the world. Unfortunately, the current Basel's range of required levels (n = 3-4) is within this inefficient range. Moreover, with an inefficient regulation the agent might employ an inefficient reporting and disclosure procedure. © 2007.}}, 
pages = {1887--1906}, 
number = {6}, 
volume = {31}
}
@article{10.1016/j.physa.2009.12.043, 
year = {2010}, 
title = {{A Bayesian Networks approach to Operational Risk}}, 
author = {Aquaro, V. and Bardoscia, M. and Bellotti, R. and Consiglio, A. and Carlo, F. De and Ferri, G.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2009.12.043}, 
eprint = {0906.3968}, 
abstract = {{A system for Operational Risk management based on the computational paradigm of Bayesian Networks is presented. The algorithm allows the construction of a Bayesian Network targeted for each bank and takes into account in a simple and realistic way the correlations among different processes of the bank. The internal losses are averaged over a variable time horizon, so that the correlations at different times are removed, while the correlations at the same time are kept: the averaged losses are thus suitable to perform the learning of the network topology and parameters; since the main aim is to understand the role of the correlations among the losses, the assessments of domain experts are not used. The algorithm has been validated on synthetic time series. It should be stressed that the proposed algorithm has been thought for the practical implementation in a mid or small sized bank, since it has a small impact on the organizational structure of a bank and requires an investment in human resources which is limited to the computational area. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {1721--1728}, 
number = {8}, 
volume = {389}
}
@article{10.1109/icsai.2012.6223238, 
year = {2012}, 
title = {{Two-stage fuzzy generalized assignment problem with value-at-risk criteria}}, 
author = {Bai, Xuejie and Fan, Yanfang and Zhou, Jing}, 
journal = {2012 International Conference on Systems and Informatics (ICSAI2012)}, 
issn = {NA}, 
doi = {10.1109/icsai.2012.6223238}, 
abstract = {{This paper presents a new type of two-stage fuzzy generalized assignment problem (FGAP) models with critical value-at-risk (VaR) criteria based on credibility theory and two-stage fuzzy optimization method. Since the proposed FGAP model often includes fuzzy coefficients defined through known possibility distributions, it is inherently an infinite-dimensional optimization problem that can rarely be solved directly. Thus, algorithm procedures for solving such an optimization problem must rely on intelligent computing and approximation schemes. In this paper, we employ an approximation approach (AA) to calculate the objective function of the two-stage FGAP, and discuss the convergence about the use of the approximation method. Considering that the approximating FGAP model is neither linear nor convex, we design a hybrid particle swarm optimization (PSO) algorithm. Finally, one numerical example with six tasks and three agents is given to demonstrate the feasibility of the designed algorithm. © 2012 IEEE.}}, 
pages = {1150--1153}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icsssm.2006.320657, 
year = {2006}, 
title = {{Extreme value theory: An empirical analysis of equity risk for shanghai stock market}}, 
author = {Xiu-Min, Li and Fa-Chao, Li}, 
journal = {2006 International Conference on Service Systems and Service Management}, 
issn = {NA}, 
doi = {10.1109/icsssm.2006.320657}, 
abstract = {{Prediction of the frequency of extreme events is of primary importance in many financial applications such as Value-at-Risk(VaR)analysis. We provide an overview of the role of extreme value theory (EVT) in risk management, as a method for modeling and measuring extreme risks. Extreme value theory models the tails of the return distribution rather than the whole distribution, which is more meaningful during the volatile market conditions, under which the distribution of returns almost has a fat tail. In particular, we concentrate on the peaks-over-threshold (POT) model and emphasize the generality of this approach. According to extreme value theory, the POT is a generalized Pareto distribution (GPD) with two parameters, which is widely used for modeling exceedances of a random variable over a high threshold and has proven to be one of the best ways to apply extreme value theory in practice. But the main problem is the selection of the threshold. Extreme value theory tells us that threshold should be high in order to satisfy theorem conditions, however the higher the threshold the less observations are left for the estimation of the parameters of the tail distribution function. The issue of determining the fraction of data belonging to the tail is treated by mean excess function. Tools from exploratory data analysis prove helpful in approaching this problem. Moreover we concentrate on two measures which attempt to describe the tail of a loss distribution-VaR and expected shortfall. VaR is a high quantile of the distribution of losses, typically the 95th or 99th percentile. It provides a kind of upper bound for a loss that only exceeded on a small proportion of occasions. Expected shortfall-the tail conditional expectation is used to estimate the expected size of a loss that exceeds VaR. Finally, the application of EVT is illustrated by Shanghai stock market data. We conclude that EVT is an useful complement to traditional VaR methods. © 2006 IEEE.}}, 
pages = {1073--1077}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.mcm.2012.03.009, 
year = {2013}, 
title = {{Optimal inventory policy with fixed and proportional transaction costs under a risk constraint}}, 
author = {Wang, S.Y. and Yiu, K.F.C. and Mak, K.L.}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/j.mcm.2012.03.009}, 
abstract = {{The traditional inventory models focus on characterizing replenishment policies in order to maximize the total expected profit or to minimize the expected total cost over a planned horizon. However, for many companies, total inventory costs could be accounting for a fairly large amount of invested capital. In particular, raw material inventories should be viewed as a type of invested asset for a manufacturer with suitable risk control. This paper is intended to provide this perspective on inventory management that treats inventory problems within a wider context of financial risk management. In view of this, the optimal inventory problem under a VaR constraint is studied. The financial portfolio theory has been used to model the dynamics of inventories. A diverse portfolio consists of raw material inventories, which involve market risk because of price fluctuations as well as a risk-free bank account. The value-at-risk measure is applied thereto to control the inventory portfolio's risk. The objective function is to maximize the utility of total portfolio value. In this model, the ordering cost is assumed to be fixed and the selling cost is proportional to the value. The inventory control problem is thus formulated as a continuous stochastic optimal control problem with fixed and proportional transaction costs under a continuous value-at-risk (VaR) constraint. The optimal inventory policies are derived by using stochastic optimal control theory and the optimal inventory level is reviewed and adjusted continuously. A numerical algorithm is proposed and the results illustrate how the raw material price, inventory level and VaR constraint are interrelated. © 2012 Elsevier Ltd.}}, 
pages = {1595--1614}, 
number = {9-10}, 
volume = {58}
}
@article{10.1016/j.omega.2014.04.001, 
year = {2014}, 
title = {{Stochastic efficiency analysis with a reliability consideration}}, 
author = {Wei, Guiwu and Chen, Jian and Wang, Jiamin}, 
journal = {Omega}, 
issn = {03050483}, 
doi = {10.1016/j.omega.2014.04.001}, 
abstract = {{Stochastic Data Envelopment Analysis (DEA) models have been introduced in the literature to assess the performance of operating entities with random input and output data. A stochastic DEA model with a reliability constraint is proposed in this study that maximizes the lower bound of an entity[U+05F3]s efficiency score with some pre-selected probability. We define the concept of stochastic efficiency and develop a solution procedure. The economic interpretations of the stochastic efficiency index are presented when the inputs and outputs of each entity follow a multivariate joint normal distribution. © 2014 Elsevier Ltd.}}, 
pages = {1--9}, 
number = {NA}, 
volume = {48}
}
@article{10.1111/1467-6419.00170, 
year = {2002}, 
title = {{Bootstrapping financial time series}}, 
author = {Ruiz, Esther and Pascual, Lorenzo}, 
journal = {Journal of Economic Surveys}, 
issn = {09500804}, 
doi = {10.1111/1467-6419.00170}, 
abstract = {{It is well known that time series of returns are characterized by volatility clustering and excess kurtosis. Therefore, when modelling the dynamic behavior of returns, inference and prediction methods, based on independent and/or Gaussian observations may be inadequate. As bootstrap methods are not, in general, based on any particular assumption on the distribution of the data, they are well suited for the analysis of returns. This paper reviews the application of bootstrap procedures for inference and prediction of financial time series. In relation to inference, bootstrap techniques have been applied to obtain the sample distribution of statistics for testing, for example, autoregressive dynamics in the conditional mean and variance, unit roots in the mean, fractional integration in volatility and the predictive ability of technical trading rules. On the other hand, bootstrap procedures have been used to estimate the distribution of returns which is of interest, for example, for Value at Risk (VaR) models or for prediction purposes. Although the application of bootstrap techniques to the empirical analysis of financial time series is very broad, there are few analytical results on the statistical properties of these techniques when applied to heteroscedastic time series. Furthermore, there are quite a few papers where the bootstrap procedures used are not adequate.}}, 
pages = {271--300}, 
number = {3}, 
volume = {16}
}
@article{10.1080/02331930701761573, 
year = {2009}, 
title = {{A firm's optimizing behaviour under a value-at-risk constraint}}, 
author = {Tulli, Vanda and Weinrich, Gerd}, 
journal = {Optimization}, 
issn = {02331934}, 
doi = {10.1080/02331930701761573}, 
abstract = {{In this article, we employ the concept of value-at-risk to model a kind of risk-averse behaviour of a firm which seeks to maximize profit a la Greenwald-Stiglitz [5]. It is shown that there exists a unique well-defined solution function, which relates output to the firm's net worth, but that this function is not monotone. The latter is due to the fact that whenever the VaR-constraint is not binding, the firm behaves in a risk-neutral fashion. It is also shown that in this context, the Modigliani-Miller theorem applies only in the special case where there is no risk of bankruptcy.}}, 
pages = {213--226}, 
number = {2}, 
volume = {58}
}
@article{10.1109/iscc47284.2019.8969746, 
year = {2019}, 
title = {{Risk Analysis of Blocked Rate Predictions for SDN Load Balancing Using Monte Carlo Simulation}}, 
author = {Alawadi, Aymen Hasan and Molnar, Sandor}, 
journal = {2019 IEEE Symposium on Computers and Communications (ISCC)}, 
issn = {15301346}, 
doi = {10.1109/iscc47284.2019.8969746}, 
abstract = {{The emergence of large data centers and virtualization needs better and smarter solutions for traffic scheduling and load balancing. Data centers benefit from SDN regarding centralized monitoring and management for traffic routing. In general, the traffic in the data center environment can be classified as elephant and mice flow. Researchers showed that there is a significant amount of data carried over elephant flows; therefore, it should be conserved and maintained thoroughly. In this work, we introduce a stochastic performance evaluation model for estimating blocked rate prediction and risk analysis of the elephant flows for a load balancing data center with fat-tree topology using the SDN paradigm. The general procedure of the evaluation includes the estimation of the distribution of the path available bandwidth, including bandwidth error tolerance. The proposed model relies on Monte Carlo simulation to generate future prediction behavior of the load balancing technique. The achieved results examined with Value at Risk (VaR) along with statistics to percept the complete picture of the load balancing behavior. © 2019 IEEE.}}, 
pages = {1028--1033}, 
number = {NA}, 
volume = {2019-June}
}
@article{10.1016/j.cam.2018.10.039, 
year = {2019}, 
title = {{A novel multi period mean-VaR portfolio optimization model considering practical constraints and transaction cost}}, 
author = {Babazadeh, Hossein and Esfahanipour, Akbar}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2018.10.039}, 
abstract = {{The portfolio optimization literature has spent a little effort to consider the fat tail characteristic of asset returns as well as their extreme events. To remove such shortcomings, in this paper, a novel portfolio optimization model is developed in which Value at Risk (VaR) is utilized as a risk measure to account extreme risk so that VaR is estimated use of Extreme Value Theory (EVT). To enrich the practicality of our proposed model, set of real trading constraints are considered such as cardinality, budget, floor and ceiling constraints. Since these modifications lead to a non-convex NP-hard problem which is computationally difficult, a new design of Non-dominated Sorting Genetic Algorithm (NSGA-II) is proposed to solve it. To evaluate the performance of EVT approach in our proposed mean-VaR model, three well-known alternative VaR estimation methods are also considered such as historical simulation, GARCH and t-student GARCH. Experimental results using historical daily financial market data from S \& P 100 indices demonstrates that our proposed NSGA-II has great capability of treating the mean-VaR portfolio optimization problem. In addition, the validation study confirmed that our enhanced NSGA-II not only offers superior result compared with that of delivered by benchmark problem in a much lower solving time, but its performance is better than the original NSGA-II. Also, the results indicate that our proposed model outperforms other mean-VaR models especially in low risk area of Pareto front. Finally, the proposed algorithm is compared with set of Non-dominated-based algorithms including SPEA-II, NSPSO and NSACO which results illustrated that our enhanced NSGA-II suggests superior solutions rather than other algorithms. © 2018 Elsevier B.V.}}, 
pages = {313--342}, 
number = {NA}, 
volume = {361}
}
@article{10.1016/j.physa.2008.01.075, 
year = {2008}, 
title = {{Heavy-tailed value-at-risk analysis for Malaysian stock exchange}}, 
author = {Chin, Wen Cheong}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2008.01.075}, 
abstract = {{This article investigates the comparison of power-law value-at-risk (VaR) evaluation with quantile and non-linear time-varying volatility approaches. A simple Pareto distribution is proposed to account the heavy-tailed property in the empirical distribution of returns. Alternative VaR measurement such as non-parametric quantile estimate is implemented using interpolation method. In addition, we also used the well-known two components ARCH modelling technique under the assumptions of normality and heavy-tailed (student-t distribution) for the innovations. Our results evidenced that the predicted VaR under the Pareto distribution exhibited similar results with the symmetric heavy-tailed long-memory ARCH model. However, it is found that only the Pareto distribution is able to provide a convenient framework for asymmetric properties in both the lower and upper tails. © 2008 Elsevier Ltd. All rights reserved.}}, 
pages = {4285--4298}, 
number = {16-17}, 
volume = {387}
}
@article{10.1016/j.jmateco.2019.07.009, 
year = {2019}, 
title = {{Capital regulation and banking bubbles}}, 
author = {Chevallier, Claire Océane and Joueidi, Sarah El}, 
journal = {Journal of Mathematical Economics}, 
issn = {03044068}, 
doi = {10.1016/j.jmateco.2019.07.009}, 
abstract = {{This paper develops a dynamic general equilibrium model in infinite horizon with a regulated banking sector. We borrow the methodology of Miao and Wang (2015) to analyse how Basel capital requirement recommendations may generate and affect banking bubbles and macroeconomic key variables. We show that when banks face capital requirements based on credit risk, as in Basel I, bubbles cannot exist. Alternatively, under a regulatory framework where capital requirements are based on Value-at-Risk, as in Basel II and III, two different equilibria emerge and can coexist: the bubbleless and the bubbly equilibria. Bubbles can be positive or negative, depending on the tightness of capital requirements based on Value-at-Risk. We find a maximum value of capital requirement below which bubbles are positive and provide a larger welfare compared to the bubbleless equilibrium. Our results also suggest that a change in banking policies might lead to a crisis without external shocks. © 2019 Elsevier B.V.}}, 
pages = {117--129}, 
number = {NA}, 
volume = {84}
}
@article{10.1016/j.mcm.2013.07.002, 
year = {2013}, 
title = {{Parametric VaR with goodness-of-fit tests based on EDF statistics for extreme returns}}, 
author = {Moralles, Herick Fernando and Rebelatto, Daisy Aparecida do Nascimento and Sartoris, Alexandre}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/j.mcm.2013.07.002}, 
abstract = {{Parametric VaR (Value-at-Risk) is widely used due to its simplicity and easy calculation. However, the normality assumption, often used in the estimation of the parametric VaR, does not provide satisfactory estimates for risk exposure. Therefore, this study suggests a method for computing the parametric VaR based on goodness-of-fit tests using the empirical distribution function (EDF) for extreme returns, and compares the feasibility of this method for the banking sector in an emerging market and in a developed one. The paper also discusses possible theoretical contributions in related fields like enterprise risk management (ERM). © 2013 Elsevier Ltd.}}, 
pages = {1648--1658}, 
number = {9-10}, 
volume = {58}
}
@article{10.3390/risks7020050, 
year = {2019}, 
title = {{Practice oriented and monte carlo based estimation of the value-at-risk for operational risk measurement}}, 
author = {Greselin, Francesca and Piacenza, Fabio and Zitikis, Ričardas}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks7020050}, 
abstract = {{We explore the Monte Carlo steps required to reduce the sampling error of the estimated 99.9\% quantile within an acceptable threshold. Our research is of primary interest to practitioners working in the area of operational risk measurement, where the annual loss distribution cannot be analytically determined in advance. Usually, the frequency and the severity distributions should be adequately combined and elaborated with Monte Carlo methods, in order to estimate the loss distributions and risk measures. Naturally, financial analysts and regulators are interested in mitigating sampling errors, as prescribed in EU Regulation 2018/959. In particular, the sampling error of the 99.9\% quantile is of paramount importance, along the lines of EU Regulation 575/2013. The Monte Carlo error for the operational risk measure is here assessed on the basis of the binomial distribution. Our approach is then applied to realistic simulated data, yielding a comparable precision of the estimate with a much lower computational effort, when compared to bootstrap, Monte Carlo repetition, and two other methods based on numerical optimization. © 2019 by the author. Licensee MDPI, Basel, Switzerland.}}, 
pages = {50}, 
number = {2}, 
volume = {7}
}
@article{10.1016/j.matcom.2010.06.016, 
year = {2011}, 
title = {{Value-at-Risk for country risk ratings}}, 
author = {McAleer, Michael and Veiga, Bernardo da and Hoti, Suhejla}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2010.06.016}, 
abstract = {{The country risk literature argues that country risk ratings have a direct impact on the cost of borrowings as they reflect the probability of debt default by a country. An improvement in country risk ratings, or country creditworthiness, will lower a country's cost of borrowing and debt servicing obligations, and vice versa. In this context, it is useful to analyse country risk ratings data, much like financial data, in terms of the time series patterns, as such an analysis would provide policy makers and the industry stakeholders with a more accurate method of forecasting future changes in the risks and returns of country risk ratings. This paper considered an extension of the Value-at-Risk (VaR) framework where both the upper and lower thresholds are considered. The purpose of the paper was to forecast the conditional variance and Country Risk Bounds (CRBs) for the rate of change of risk ratings for 10 countries. The conditional variance of composite risk returns for the 10 countries were forecasted using the Single Index (SI) and Portfolio Methods (PM) of McAleer and da Veiga [10,11]. The results suggested that the country risk ratings of Switzerland, Japan and Australia are much mode likely to remain close to current levels than the country risk ratings of Argentina, Brazil and Mexico. This type of analysis would be useful to lenders/investors evaluating the attractiveness of lending/investing in alternative countries. © 2010 IMACS. Published by Elsevier B.V. All rights reserved.}}, 
pages = {1454--1463}, 
number = {7}, 
volume = {81}
}
@article{10.1057/s41283-017-0023-y, 
year = {2017}, 
title = {{Dependent bootstrapping for value-at-risk and expected shortfall}}, 
author = {Laker, Ian and Huang, Chun-Kai and Clark, Allan Ernest}, 
journal = {Risk Management}, 
issn = {14603799}, 
doi = {10.1057/s41283-017-0023-y}, 
abstract = {{Estimation in extreme financial risk is often faced with challenges such as the need for adequate distributional assumptions, considerations for data dependencies, and the lack of tail information. Bootstrapping provides an alternative that overcomes some of these challenges. It does not assume a distributional form and asymptotically replicates the empirical density for resampled data. Moreover, advanced bootstrapping can cater for dependencies and stationarity in the data. In this paper, we evaluate the use of dependent bootstrapping, both for the original financial time series and for its GARCH innovations (under the Gaussian and Student t noise assumptions), in forecasting value-at-risk and expected shortfall. We also assess the effect of using different window sizes for these procedures. The two datasets used are daily returns of the S\&P500 from NYSE and the ALSI from JSE. © 2017 Macmillan Publishers Ltd.}}, 
pages = {301--322}, 
number = {4}, 
volume = {19}
}
@article{10.1007/s10957-011-9968-2, 
year = {2012}, 
title = {{Entropic Value-at-Risk: A New Coherent Risk Measure}}, 
author = {Ahmadi-Javid, A.}, 
journal = {Journal of Optimization Theory and Applications}, 
issn = {00223239}, 
doi = {10.1007/s10957-011-9968-2}, 
abstract = {{This paper introduces the concept of entropic value-at-risk (EVaR), a new coherent risk measure that corresponds to the tightest possible upper bound obtained from the Chernoff inequality for the value-at-risk (VaR) as well as the conditional value-at-risk (CVaR). We show that a broad class of stochastic optimization problems that are computationally intractable with the CVaR is efficiently solvable when the EVaR is incorporated. We also prove that if two distributions have the same EVaR at all confidence levels, then they are identical at all points. The dual representation of the EVaR is closely related to the Kullback-Leibler divergence, also known as the relative entropy. Inspired by this dual representation, we define a large class of coherent risk measures, called g-entropic risk measures. The new class includes both the CVaR and the EVaR. © 2011 Springer Science+Business Media, LLC.}}, 
pages = {1105--1123}, 
number = {3}, 
volume = {155}
}
@article{10.1016/s0927-0507(03)10001-1, 
year = {2003}, 
title = {{Stochastic Programming Models}}, 
author = {Ruszczyński, Andrzej and Shapiro, Alexander}, 
journal = {Handbooks in Operations Research and Management Science}, 
issn = {09270507}, 
doi = {10.1016/s0927-0507(03)10001-1}, 
abstract = {{In this introductory chapter we discuss some basic approaches to modeling of stochastic optimization problems. We start with motivating examples and then proceed to formulation of linear, and later nonlinear, two stage stochastic programming problems. We give a functional description of two stage programs. After that we proceed to a discussion of multistage stochastic programming and its connections with dynamic programming. We end this chapter by introducing robust and min-max approaches to stochastic programming. Finally, in the appendix, we introduce and briefly discuss some relevant concepts from probability and optimization theories. © 2003 Elsevier Inc. All rights reserved.}}, 
pages = {1--64}, 
number = {C}, 
volume = {10}
}
@article{10.2143/ast.32.2.1028, 
year = {2002}, 
title = {{Analytical Bounds for two Value-at-Risk Functionals}}, 
author = {Hürlimann, Werner}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.32.2.1028}, 
abstract = {{Based on the notions of value-at-risk and conditional value-at-risk, we consider two functionals, abbreviated VaR and CVaR, which represent the economic risk capital required to operate a risky business over some time period when only a small probability of loss is tolerated. These functionals are consistent with the risk preferences of profit-seeking (and risk averse) decision makers and preserve the stochastic dominance order (and the stop-loss order). This result is used to bound the VaR and CVaR functionals by determining their maximal values over the set of all loss and profit functions with fixed first few moments. The evaluation of CVaR for the aggregate loss of portfolios is also discussed. The results of VaR and CVaR calculations are illustrated and compared at some typical situations of general interest. © 2002, International Actuarial Association. All rights reserved.}}, 
pages = {235--265}, 
number = {2}, 
volume = {32}
}
@article{10.1016/j.eneco.2010.11.013, 
year = {2011}, 
title = {{Forecasting petroleum futures markets volatility: The role of regimes and market conditions}}, 
author = {Nomikos, Nikos K. and Pouliasis, Panos K.}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2010.11.013}, 
abstract = {{In this paper we employ regime volatility models to describe time dependency in petroleum markets. Using a sample of NYMEX and ICE futures contracts, we establish the existence of a regime process and link this process to market fundamentals. This formulation results in two distinct states: a highly persistent conditional volatility process, characterised by long memory and low sensitivity to market shocks, and a relatively short-lived nonstationary process with less memory but higher sensitivity to shocks. Moreover, to investigate the relationship between disequilibrium and volatility of oil futures across high and low volatility regimes we use augmented regime GARCH models to address in a realistic way the potential diverse response of volatility to forward curve shocks. The performance of these models is compared to benchmarks, using both statistical tests and risk management loss functions. To test the robustness of the forecasting strategies, we also perform a reality check employing the stationary bootstrap approach. The findings of this paper have important implications for decision making concerning trading and risk management, as well as energy market operations, such as refining and budget planning, by providing valuable information on the oil price volatility dynamics and the ability to predict risk. © 2010 Elsevier B.V.}}, 
pages = {321--337}, 
number = {2}, 
volume = {33}
}
@article{10.1007/s10182-006-0234-0, 
year = {2006}, 
title = {{On the appropriateness of inappropriate VaR models}}, 
author = {Härdle, Wolfgang and Hlávka, Zdeněk and Stahl, Gerhard}, 
journal = {Allgemeines Statistisches Archiv}, 
issn = {00026018}, 
doi = {10.1007/s10182-006-0234-0}, 
abstract = {{The Value-at-Risk calculation reduces the dimensionality of the risk factor space. The main reasons for such simplifications are, e. g., technical efficiency, the logic and statistical appropriateness of the model. In Chapter 2 we present three simple mappings: the mapping on the market index, the principal components models and the model with equally correlated risk factors. The comparison of these models in Chapter 3 is based on the literature on the verification of weather forecasts (Murphy and Winkler, 1992; Murphy, 1997). Some considerations on the quantitative analysis are presented in the fourth chapter. In the last chapter, we present empirical analysis of the DAX data using XploRe. © Springer-Verlag 2006.}}, 
pages = {273--297}, 
number = {2}, 
volume = {90}
}
@article{10.1080/14697681003785942, 
year = {2011}, 
title = {{Volatility forecasts and at-the-money implied volatility: A multi-component ARCH approach and its relation to market models}}, 
author = {Zumbach†, Gilles}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697681003785942}, 
abstract = {{This article explores the relationships between several forecasts for the volatility built from multi-scale linear ARCH processes, and linear market models for the forward variance. This shows that the structures of the forecast equations are identical, but with different dependencies on the forecast horizon. The process equations for the forward variance are induced by the process equations for an ARCH model, but postulated in a market model. In the ARCH case, they are different from the usual diffusive type. The conceptual differences between both approaches and their implication for volatility forecasts are analysed. The volatility forecast is compared with the realized volatility (the volatility that will occur between date t and t + Δ T), and the implied volatility (corresponding to an at-the-money option with expiry at t + Δ T). For the ARCH forecasts, the parameters are set a priori. An empirical analysis across multiple time horizons Δ T shows that a forecast provided by an I-GARCH(1) process (one time scale) does not capture correctly the dynamics of the realized volatility. An I-GARCH(2) process (two time scales, similar to GARCH(1,1)) is better, while a longmemory LM-ARCH process (multiple time scales) replicates correctly the dynamics of the implied and realized volatilities and delivers consistently good forecasts for the realized volatility. © 2011 Taylor \& Francis.}}, 
pages = {101--113}, 
number = {1}, 
volume = {11}
}
@article{10.1109/soli.2008.4686433, 
year = {2008}, 
title = {{Value at risk for repo interest rate based on TGARCH}}, 
author = {He, Qizhi}, 
journal = {2008 IEEE International Conference on Service Operations and Logistics, and Informatics}, 
issn = {NA}, 
doi = {10.1109/soli.2008.4686433}, 
abstract = {{There is a great significance to research the interest rate risk on the background of China's gradual marketization of interest rates. The paper takes the 7 days repo interest rates as the target. First, introduce the calculating method for value at risk. Second, give the sample characters and the dynamic model of the 7 days repo interest rate. Third, using the GARCH and TGARCH model, at 99\% confidence level and 95\% confidence level, calculate the value at risk and the exception rate for the 7 days repo interest rates. The empirical results show that the value at risk of 7days repo interest rate has positive correlation with the level of interest rates, and whatever at 95\% confidence level or at 99\% confidence level, both GARCH model and TGARCH model can cover the real forecasting losses, and the exception rate is lower than the significance level. ©2008 IEEE.}}, 
pages = {425--428}, 
number = {NA}, 
volume = {1}
}
@article{10.1287/mnsc.1060.0505, 
year = {2006}, 
title = {{Reclaiming quasi-Monte Carlo efficiency in portfolio value-at-risk simulation through fourier transform}}, 
author = {Jin, Xing and Zhang, Allen X}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1060.0505}, 
abstract = {{Quasi-Monte Carlo methods overcome the problem of sample clustering in regular Monte Carlo simulation and have been shown to improve simulation efficiency in the derivatives pricing literature when the price is expressed as a multidimensional integration and the integrand is suitably smooth. For portfolio value-at-risk (VaR) problems, the distribution of portfolio value change is based on the expectation of an indicator function, hence the integrand is discontinuous. The purpose of this paper is to smooth the expectation estimation of an indicator function via Fourier transform so that the faster convergence rate of quasi-Monte Carlo methods can be reclaimed theoretically. Under fairly mild assumptions, the simulation of portfolio value-at-risk is fast and accurate. Numerical examples elucidate the advantage of the proposed approach over regular Monte Carlo and quasi-Monte Carlo methods. © 2006 INFORMS.}}, 
pages = {925--938}, 
number = {6}, 
volume = {52}
}
@article{10.1016/j.inteco.2018.03.001, 
year = {2018}, 
title = {{Estimating value-at-risk using a multivariate copula-based volatility model: Evidence from European banks}}, 
author = {Sampid, Marius Galabe and Hasim, Haslifah M.}, 
journal = {International Economics}, 
issn = {21107017}, 
doi = {10.1016/j.inteco.2018.03.001}, 
abstract = {{This paper proposes a multivariate copula-based volatility model for estimating Value-at-Risk (VaR) in the banking sector of selected European countries by combining dynamic conditional correlation (DCC) multivariate GARCH (M-GARCH) volatility model and copula functions. Non-normality in multivariate models is associated with the joint probability of the univariate models' marginal probabilities –the joint probability of large market movements, referred to as tail dependence. In this paper, we use copula functions to model the tail dependence of large market movements and test the validity of our results by performing back-testing techniques. The results show that the copula-based approach provides better estimates than the common methods currently used and captures VaR reasonably well based on the differences in the numbers of exceptions produced during different observation periods at the same confidence level. © 2018 CEPII (Centre d'Etudes Prospectives et d'Informations Internationales), a center for research and expertise on the world economy}}, 
pages = {175--192}, 
number = {NA}, 
volume = {156}
}
@article{10.3390/sym12101639, 
year = {2020}, 
title = {{Formulation of thenon-parametric value at risk portfolio selection problem considering symmetry}}, 
author = {Wang, Dazhi and Chen, Yanhua and Wang, Hongfeng and Huang, Min}, 
journal = {Symmetry}, 
issn = {20738994}, 
doi = {10.3390/sym12101639}, 
abstract = {{In this research, we study the non-parametric portfolio selection problem with Value at Risk (VaR) minimization and establish a new enhanced Mixed Integer Linear Programming (MILP) formulation to obtain the optimal solutions considering the symmetric property of VaR. We identify that the new MILP formulation can significantly reduce the computation burden of the MILP solver CPLEX. To solve larger-scale practical portfolio selection problems in reasonable computation time, we also develop the Particle Swarm Optimization (PSO) algorithm integrating an efficient Fast Feasible Solution Detection (FFSD) scheme to obtain the near-optimal solutions. Using the simulated datasets with different distribution parameters and skewness and kurtosis patterns, some preliminary numerical results are provided to show the efficiency of the new formulation and FFSD scheme. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {1639}, 
number = {10}, 
volume = {12}
}
@article{10.1016/j.frl.2021.102143, 
year = {2021}, 
title = {{Cryptocurrency portfolio optimization with multivariate normal tempered stable processes and Foster-Hart risk}}, 
author = {Kurosaki, Tetsuo and Kim, Young Shin}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2021.102143}, 
eprint = {2010.08900}, 
abstract = {{We study portfolio optimization of four major cryptocurrencies. Our time series model is a generalized autoregressive conditional heteroscedasticity (GARCH) model with multivariate normal tempered stable (MNTS) distributed residuals used to capture the non-Gaussian cryptocurrency return dynamics. Based on the time series model, we optimize the portfolio in terms of Foster-Hart risk. Those sophisticated techniques are not yet documented in the context of cryptocurrency. Statistical tests suggest that the MNTS distributed GARCH model fits better with cryptocurrency returns than the competing GARCH-type models. We find that Foster-Hart optimization yields a more profitable portfolio with better risk-return balance than the prevailing approach. © 2021}}, 
pages = {102143}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.neucom.2008.09.026, 
year = {2009}, 
title = {{Estimating VaR in crude oil market: A novel multi-scale non-linear ensemble approach incorporating wavelet analysis and neural network}}, 
author = {He, Kaijian and Xie, Chi and Chen, Shou and Lai, Kin Keung}, 
journal = {Neurocomputing}, 
issn = {09252312}, 
doi = {10.1016/j.neucom.2008.09.026}, 
abstract = {{Facing the complicated non-linear nature of risk evolutions, current risk measurement approaches offer insufficient explanatory power and limited performance. Thus this paper proposes wavelet decomposed non-linear ensemble value at risk (WDNEVaR), a novel semi-parametric paradigm, incorporating both, wavelet analysis and artificial neural network technique to further improve the modeling accuracy and reliability. Wavelet analysis is utilized to capture the multi-scale data characteristics across scales while artificial neural network technique is utilized to reduce estimation biases following non-linear ensemble algorithms. Experiment results in three major markets suggest that the proposed WDNEVaR is superior to more traditional approaches as it provides value at risk (VaR) estimates at higher reliability and accuracy. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {3428--3438}, 
number = {16-18}, 
volume = {72}
}
@article{10.6138/jit.2015.16.2.20140306, 
year = {2015}, 
title = {{High efficiency privacy-preserving scheme for data distribution in hybrid cloud}}, 
author = {}, 
issn = {16079264}, 
doi = {10.6138/jit.2015.16.2.20140306}, 
abstract = {{Since 2009, the amount of digital data being produced has doubled every year. Cloud computing storage services are an economical approach to dealing with such volumes of data. In cloud storage situations, data encryption and access control are commonly used for the protection of data. Full data encryption is able to ensure privacy; however, this approach makes it difficult to analyze the data in an efficient manner. Data analysis in a cloud environment requires a balance between data security and data analysis capacity. This study proposes a high-efficiency, privacy-preservation scheme for data distribution in hybrid clouds to improve data protection, expand the availability of data, and reduce cloud system resources consumption. Data classification and selective data protection mechanisms are also addressed. Results of functional analysis and comparison revealed that the proposed scheme reduces the time required for encryption and decryption by as much as 82.76\% and decreases the number of privacy content by up to 92.5\% while allowing data mining without compromising privacy. © 2015, Taiwan Academic Network Management Committee. All rights reserved.}}, 
number = {2}, 
volume = {16}
}
@article{10.1198/016214506000000799, 
year = {2007}, 
title = {{A sturdy reduced-bias extreme quantile (VaR) estimator}}, 
author = {Gomes, M Ivette and Pestana, Dinis}, 
journal = {Journal of the American Statistical Association}, 
issn = {01621459}, 
doi = {10.1198/016214506000000799}, 
abstract = {{The main objective of statistics of extremes lies in the estimation of quantities related to extreme events. In many areas of application, such as statistical quality control, insurance, and finance, a typical requirement is to estimate a high quantite, that is, the value at risk at a level p (VaR p), high enough so that the chance of an exceedance of that value is equal to p. small. In this article we deal with the semiparametric estimation of VaRp for heavy tails. The classical semiparametric estimators of parameters characterizing the tail behavior of the underlying model F usually exhibit a high bias for low thresholds, that is, for large values of k, the number of top order statistics used for the estimation. We shall here deal with bias reduction techniques for heavy tails, trying to improve the performance of the classical high quantite estimators through the use of an adequate bias-corrected tail index estimator. The new high quantile estimators have a mean squared error smaller than that of the classical estimators, even for small values of k. They are, thus, alternatives to the classical estimators not only around optimal levels but also for other levels. The asymptotic distributional properties of the proposed classes of estimators are derived. The estimators are compared with alternative ones, not only asymptotically but also for finite samples, through Monte Carlo techniques. An application to the analysis of different datasets in the field of finance is also provided. © 2007 American Statistical Association.}}, 
pages = {280--292}, 
number = {477}, 
volume = {102}
}
@article{10.1057/s41260-017-0060-9, 
year = {2018}, 
title = {{Tail Event Driven ASset allocation: Evidence from equity and mutual funds' markets}}, 
author = {Härdle, Wolfgang Karl and Lee, David Kuo Chuen and Nasekin, Sergey and Petukhina, Alla}, 
journal = {Journal of Asset Management}, 
issn = {14708272}, 
doi = {10.1057/s41260-017-0060-9}, 
abstract = {{The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. Recently introduced TEDAS - Tail Event Driven ASset allocation approach determines the dependence between assets at different tail measures. TEDAS uses adaptive Lasso-based quantile regression in order to determine an active set of negative coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. In this research, authors aim to develop TEDAS, by introducing three TEDAS modifications differing in allocation weights' determination: a Cornish-Fisher Value-at-Risk minimization, Markowitz diversification rule or naïve equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds. © 2017 Macmillan Publishers Ltd.}}, 
pages = {49--63}, 
number = {1}, 
volume = {19}
}
@article{10.1108/jfrc-10-2012-0042, 
year = {2013}, 
title = {{Capital requirements for market risks: Value-at-risk models and stressed-VaR after the financial crisis}}, 
author = {Burchi, Alberto}, 
journal = {Journal of Financial Regulation and Compliance}, 
issn = {13581988}, 
doi = {10.1108/jfrc-10-2012-0042}, 
abstract = {{Purpose – The financial crisis has led the Basel Committee to improve the system of capital requirements for market risks. This paper aims to investigate the effects of different models to estimate the market risk in the management of the trading book. The study takes into account the events occurring in the financial markets and the new prudential rules. Design/methodology/approach – The author compares different models and proposes an opportunity cost function able to evaluate the cost related to capital requirements. He identified seven asset classes and studies the effects of different models for estimating VaR simulating financial portfolios with increasing risk. The series consists of the daily return from 01/01/2002 to 06/30/2012. Findings – The results show that it is possible to identify a wide area between aggressive and conservative approach where the bank management must choose. The regulation does not encourage intermediaries to the use of complex models that could better evaluate the risk in financial markets. The revision of the market risk framework increases the capital requirement and reduces the incentive to use models with more predictive power for regulatory purposes. Originality/value – The work differs from previous contributions for three characteristics: first, it uses a set of extended data and more consistent with the actual operation. Secondly, the author presents an opportunity cost function in order to evaluate the estimation models. Third, he calculates the effect of stressed-VaR after a year and a half of adoption. © 2013, © Emerald Group Publishing Limited.}}, 
pages = {284--304}, 
number = {3}, 
volume = {21}
}
@article{10.1080/00036846.2018.1488073, 
year = {2018}, 
title = {{Score-driven Markov-switching EGARCH models: an application to systematic risk analysis}}, 
author = {Blazsek, Szabolcs and Ho, Han-Chiang and Liu, Su-Ping}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2018.1488073}, 
abstract = {{We introduce new Markov-switching (MS) dynamic conditional score (DCS) exponential generalized autoregressive conditional heteroscedasticity (EGARCH) models, to be used by practitioners for forecasting value-at-risk (VaR) and expected shortfall (ES) in systematic risk analysis. We use daily log-return data from the Standard \& Poor’s 500 (S\&P 500) index for the period 1950–2016. The analysis of the S\&P 500 is useful, for example, for investors of (i) well-diversified US equity portfolios; (ii) S\&P 500 futures and options traded at Chicago Mercantile Exchange Globex; (iii) exchange traded funds (ETFs) related to the S\&P 500. The new MS DCS-EGARCH models are alternatives to of the recent MS Beta-t-EGARCH model that uses the symmetric Student’s t distribution for the error term. For the new models, we use more flexible asymmetric probability distributions for the error term: Skew-Gen-t (skewed generalized t), EGB2 (exponential generalized beta of the second kind) and NIG (normal-inverse Gaussian) distributions. For all MS DCS-EGARCH models, we identify high- and low-volatility periods for the S\&P 500. We find that the statistical performance of the new MS DCS-EGARCH models is superior to that of the MS Beta-t-EGARCH model. As a practical application, we perform systematic risk analysis by forecasting VaR and ES. Abbreviation Single regime (SR); Markov-switching (MS); dynamic conditional score (DCS); exponential generalized autoregressive conditional heteroscedasticity (EGARCH); value-at-risk (VaR); expected shortfall (ES); Standard \& Poor's 500 (S\&P 500); exchange traded funds (ETFs); Skew-Gen-t (skewed generalized t); EGB2 (exponential generalized beta of the second kind); NIG (normal-inverse Gaussian); log-likelihood (LL); standard deviation (SD); partial autocorrelation function (PACF); likelihood-ratio (LR); ordinary least squares (OLS); heteroscedasticity and autocorrelation consistent (HAC); Akaike information criterion (AIC); Bayesian information criterion (BIC); Hannan-Quinn criterion (HQC). © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--14}, 
number = {56}, 
volume = {50}
}
@article{10.7148/2017-0100, 
year = {2017}, 
title = {{A margin calculation method for illiquid products}}, 
author = {Beli, Marcell and Szanyi, Csilla and Varadi, Kata}, 
journal = {ECMS 2017 Proceedings edited by Zita Zoltay Paprika, Péter Horák, Kata Váradi, Péter Tamás Zwierczyk, Ágnes Vidovics-Dancs, János Péter Rádics}, 
issn = {NA}, 
doi = {10.7148/2017-0100}, 
abstract = {{The role of the central counterparties (CCPs) on the market is to take over the counterparty risk during the trading on stock exchanges. CCPs use a multilevel guarantee system to manage this risk. The margin has a key role in this guarantee system, and the paper will focus only on this level. The main motivation of this paper is to introduce a potential margin calculation method which is compliant with the EMIR regulation and also does not put unnecessary burden on the market participants. We will introduce this method for two special type of products: (1) the illiquid products and (2) for the case of initial public offerings (IPOs). The specialty of these two product types, that there is no available historical time series of the securities' prices, so no risk management models can be used by the CCPs to calculate the margin. © ECMS Zita Zoltay Paprika, Péter Horák, Kata Váradi,Péter Tamás Zwierczyk, Ágnes Vidovics-Dancs, János Péter Rádics (Editors).}}, 
pages = {100--105}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/07474938.2014.977093, 
year = {2017}, 
title = {{Bayesian analysis of multivariate stochastic volatility with skew return distribution}}, 
author = {Nakajima, Jouchi}, 
journal = {Econometric Reviews}, 
issn = {07474938}, 
doi = {10.1080/07474938.2014.977093}, 
abstract = {{Multivariate stochastic volatility models with skew distributions are proposed. Exploiting Cholesky stochastic volatility modeling, univariate stochastic volatility processes with leverage effect and generalized hyperbolic skew t-distributions are embedded to multivariate analysis with time-varying correlations. Bayesian modeling allows this approach to provide parsimonious skew structure and to easily scale up for high-dimensional problem. Analyses of daily stock returns are illustrated. Empirical results show that the time-varying correlations and the sparse skew structure contribute to improved prediction performance and Value-at-Risk forecasts. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {1--23}, 
number = {5}, 
volume = {36}
}
@article{10.1080/03610926.2019.1710197, 
year = {2021}, 
title = {{Optimal reinsurance from the perspectives of both insurers and reinsurers under the VaR risk measure and Vajda condition}}, 
author = {Chen, Yanhong and Hu, Yijun}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2019.1710197}, 
abstract = {{In this article, we revisit the optimal reinsurance problem by minimizing the convex combination of the VaRs of the insurer’s loss and the reinsurer’s loss. To prevent moral hazard and to reflect the spirit of reinsurance, we assume that the set of admissible ceded loss function is the class of ceded loss functions such that the retained loss functions are increasing and the ceded loss functions satisfy Vajda condition. We analyze the optimal solutions for a wide class of reinsurance premium principles that satisfy the following three properties: law invariance, risk loading property and stop-loss ordering preserving. Meanwhile, we use the expected value premium principle to derive the explicit expressions for the optimal reinsurance treaties. Finally, we construct a numerical example to illustrate our results. © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--18}, 
number = {15}, 
volume = {50}
}
@article{10.1016/j.spl.2016.04.006, 
year = {2016}, 
title = {{Artifactual unit root behavior of Value at risk (VaR)}}, 
author = {Chan, Ngai Hang and Sit, Tony}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2016.04.006}, 
abstract = {{An effective model for time-varying quantiles of a time series is of considerable practical importance across various disciplines. In particular, in financial risk management, computation of Value-at-risk (VaR), one of the most popular risk measures, involves knowledge of quantiles of portfolio returns. This paper examines the random walk behavior of VaRs constructed under two most common approaches, viz. historical simulation and the parametric approach using GARCH models. We find that sequences of historical VaRs appear to follow a unit root model, which can be an artifact under some settings, whereas its counterpart constructed via the parametric approach does not follow a random walk model by default. © 2016 Elsevier B.V..}}, 
pages = {88--93}, 
number = {NA}, 
volume = {116}
}
@article{10.1080/14697688.2015.1059469, 
year = {2016}, 
title = {{Optimal capital growth with convex shortfall penalties}}, 
author = {MacLean, Leonard C. and Zhao, Yonggan and Ziemba, William T.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2015.1059469}, 
abstract = {{The optimal capital growth strategy or Kelly strategy has many desirable properties such as maximizing the asymptotic long-run growth of capital. However, it has considerable short-run risk since the utility is logarithmic, with essentially zero Arrow–Pratt risk aversion. It is common to control risk with a Value-at-Risk (VaR) constraint defined on the end of horizon wealth. A more effective approach is to impose a VaR constraint at each time on the wealth path. In this paper, we provide a method to obtain the maximum growth while staying above an ex-ante discrete time wealth path with high probability, where shortfalls below the path are penalized with a convex function of the shortfall. The effect of the path VaR condition and shortfall penalties is a lower growth rate than the Kelly strategy, but the downside risk is under control. The asset price dynamics are defined by a model with Markov transitions between several market regimes and geometric Brownian motion for prices within a regime. The stochastic investment model is reformulated as a deterministic programme which allows the calculation of the optimal constrained growth wagers at discrete points in time. © 2015 Taylor \& Francis.}}, 
pages = {101--117}, 
number = {1}, 
volume = {16}
}
@article{10.1109/cdc.2015.7402831, 
year = {2015}, 
title = {{A randomised approach to flood control using Value-at-Risk}}, 
author = {Nasir, Hasan Arshad and Carè, Algo and Weyer, Erik}, 
journal = {2015 54th IEEE Conference on Decision and Control (CDC)}, 
issn = {07431546}, 
doi = {10.1109/cdc.2015.7402831}, 
abstract = {{Flooding is one of the major risks associated with rivers, and a typical operational goal is to reduce the risk of severe floods while at the same time not being overly cautious. In this paper we consider a Stochastic Model Predictive Control (S-MPC) based strategy which is well suited for rivers with uncertain in- and out-flows. In order to reduce the risk of floods, Value-at-Risk (VaR) is used as a risk measure and incorporated as a chance-constraint in the control optimisation problem. A computationally tractable scenario-based iterative optimisation and testing algorithm is proposed for solving the corresponding S-MPC problem, and its usefulness is demonstrated on a simulated river example. © 2015 IEEE.}}, 
pages = {3939--3944}, 
number = {NA}, 
volume = {54rd IEEE Conference on Decision and Control,CDC 2015}
}
@article{10.1017/s174849952000007x, 
year = {2020}, 
title = {{Longevity trend risk over limited time horizons}}, 
author = {Richards, Stephen J. and Currie, Iain D. and Kleinow, Torsten and Ritchie, Gavin P.}, 
journal = {Annals of Actuarial Science}, 
issn = {17484995}, 
doi = {10.1017/s174849952000007x}, 
abstract = {{We consider various aspects of longevity trend risk viewed through the prism of a finite time window. We show the broad equivalence of value-at-risk (VaR) capital requirements at a p-value of 99.5\% to conditional tail expectations (CTEs) at 99\%. We also show how deferred annuities have higher risk, which can require double the solvency capital of equivalently aged immediate anuities. However, results vary considerably with the choice of model and so longevity trend-risk capital can only be determined through consideration of multiple models to inform actuarial judgement. This model risk is even starker when trying to value longevity derivatives. We briefly discuss the importance of using smoothed models and describe two methods to considerably shorten VaR and CTE run times. ©}}, 
pages = {262--277}, 
number = {2}, 
volume = {14}
}
@article{10.1109/isspa.2012.6310455, 
year = {2012}, 
title = {{Application of entropic value-at-risk in machine learning with corrupted input data}}, 
author = {Ahmadi-Javid, Amir}, 
journal = {2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)}, 
issn = {NA}, 
doi = {10.1109/isspa.2012.6310455}, 
abstract = {{The entropic value-at-risk (EVaR) is a coherent risk measure that is efficiently computable for the sum of independent random variables. This paper shows how this risk measure can be used in machine learning when uncertainty affects the input data. For this purpose, we consider here a support vector machine with corrupted input data. © 2012 IEEE.}}, 
pages = {1104--1107}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/03610926.2020.1767141, 
year = {2020}, 
title = {{Optimal investment of DC pension plan with two VaR constraints}}, 
author = {Zhu, Shunqing and Dong, Yinghui and Wu, Sang}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2020.1767141}, 
abstract = {{In this paper, we investigate an optimal investment problem under two value-at-risk (VaR) constraints faced by a defined contribution (DC) pension fund manager. We apply a concavification technique and a Lagrange dual method to solve the problem and derive the closed-form representations of the optimal wealth and portfolio processes in terms of the state price density. Theoretical and numerical results show that the two VaR constraints can significantly impact the distribution of the optimal terminal wealth. © 2020, © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--20}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/03610926.2012.746984, 
year = {2015}, 
title = {{Moderate deviations for estimators of financial risk under an asymmetric Laplace law}}, 
author = {Cai, Yujie and Gao, Fuqing and Wang, Shaochen}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2012.746984}, 
abstract = {{We study moderate deviations of estimators of financial risk under an asymmetric Laplace law. The moderate deviation principles of two kinds of estimators of Value-at-Risk (VaR) and conditional Value-at-Risk (CVaR) are obtained by the approximation method and the delta method in large deviations. Some numerical comparisons of the estimators are also presented. Copyright © 2015 Taylor \& Francis Group, LLC.}}, 
pages = {329--339}, 
number = {2}, 
volume = {44}
}
@article{10.1016/j.eswa.2012.07.064, 
year = {2013}, 
title = {{Random fuzzy multi-objective linear programming: Optimization of possibilistic value at risk (pVaR)}}, 
author = {Katagiri, Hideki and Uno, Takeshi and Kato, Kosuke and Tsuda, Hiroshi and Tsubaki, Hiroe}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2012.07.064}, 
abstract = {{This paper considers multiobjective linear programming problems (MOLPP) where random fuzzy variables are contained in objective functions and constraints. A new decision making model optimizing possibilistic value at risk (pVaR) is proposed by incorporating the concept of value at risk (VaR) into possibility theory. It is shown that the original MOLPPs involving random fuzzy variables are transformed into deterministic problems. An interactive algorithm is presented to derive a satisficing solution for a decision maker (DM) from among a set of Pareto optimal solutions. Each Pareto optimal solution that is a candidate of the satisficing solution is exactly obtained by using convex programming techniques. A simple numerical example is provided to show the applicability of the proposed methodology to real-world problems with multiple objectives in uncertain environments. © 2012 Elsevier Ltd. All rights reserved.}}, 
pages = {563--574}, 
number = {2}, 
volume = {40}
}
@article{10.1504/ijram.2008.019743, 
year = {2008}, 
title = {{Copulae and operational risks}}, 
author = {Fantazzini, Dean and Valle, Luciana Dalla and Giudici, Paolo}, 
journal = {International Journal of Risk Assessment and Management}, 
issn = {14668297}, 
doi = {10.1504/ijram.2008.019743}, 
abstract = {{The motivation of this paper is to develop efficient statistical methods aimed at measuring operational risk. A number of recent legislations and market practices are motivating such developments. For instance, 'the New Basel Capital Accord' (Basel II, 2001), published by Basel Committee on Banking supervision, requires financial institutions to measure operational risks, defined as "the risk of loss resulting from inadequate or failed internal processes, people and systems or from external events". The main aim of operational risk models is to set aside an amount of capital that can cover unexpected losses. This is typically achieved estimating a loss distribution deriving functions of interest from it (such as the Value at Risk (VaR)). Statistical models for operational risk estimation face the difficulty that there are a large number of loss distributions to be estimated (in the Basel II framework, at least 56). Current approaches treat these distributions as perfectly dependent, thus overestimating VaR. We propose to use copula distributions to model high dimensional operational risks in a more flexible way, that takes (partial) dependence into account. Copyright © 2008 Inderscience Enterprises Ltd.}}, 
pages = {238}, 
number = {3}, 
volume = {9}
}
@article{10.1109/icime.2010.5478093, 
year = {2010}, 
title = {{Analysis of value at risk on CSI 300 index via Extreme Value Theory}}, 
author = {Ding, Sheng}, 
journal = {2010 2nd IEEE International Conference on Information Management and Engineering}, 
issn = {NA}, 
doi = {10.1109/icime.2010.5478093}, 
abstract = {{Value at Risk (VaR) is a widely used tool in risk anagement. It is the most important method to measure marke trisk. In this paper we use Extreme Value Theory (EVT) method to calculate VaR compared with Variance-Covariance method, and then apply empirical analysis on CSI 300 index. The results show that using EVT method always acquires more accurate VaR under extreme conditions. © 2010 IEEE.}}, 
pages = {426--429}, 
number = {NA}, 
volume = {6}
}
@article{10.1080/14697688.2013.835860, 
year = {2014}, 
title = {{Alternative modeling for long term risk}}, 
author = {Guégan, Dominique and Zhao, Xin}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2013.835860}, 
abstract = {{In this paper, we propose an alternative approach to estimate long-term risk. Instead of using the static square root of time method, we use a dynamic approach based on volatility forecasting by non-linear models. We explore the possibility of improving the estimations using different models and distributions. By comparing the estimations of two risk measures, value at risk and expected shortfall, with different models and innovations at short-, median- and long-term horizon, we find that the best model varies with the forecasting horizon and that the generalized Pareto distribution gives the most conservative estimations with all the models at all the horizons. The empirical results show that the square root method underestimates risk at long horizons and our approach is more competitive for risk estimation over a long term. © 2013, Taylor \& Francis.}}, 
pages = {2237--2253}, 
number = {12}, 
volume = {14}
}
@article{10.1080/02331934.2013.854785, 
year = {2013}, 
title = {{Static and dynamic VaR constrained portfolios with application to delegated portfolio management}}, 
author = {Pınar, Mustafa Ç.}, 
journal = {Optimization}, 
issn = {02331934}, 
doi = {10.1080/02331934.2013.854785}, 
abstract = {{We give a closed-form solution to the single-period portfolio selection problem with a Value-at-Risk (VaR) constraint in the presence of a set of risky assets with multivariate normally distributed returns and the risk-less account, without short sales restrictions. The result allows to obtain a very simple, myopic dynamic portfolio policy in the multiple period version of the problem. We also consider mean-variance portfolios under a probabilistic chance (VaR) constraint and give an explicit solution. We use this solution to calculate explicitly the bonus of a portfolio manager to include a VaR constraint in his/her portfolio optimization, which we refer to as the price of a VaR constraint. © 2013 © 2013 Taylor \& Francis.}}, 
pages = {1419--1432}, 
number = {11}, 
volume = {62}
}
@article{10.1016/j.redee.2015.11.003, 
year = {2016}, 
title = {{Hedging foreign exchange rate risk: Multi-currency diversification}}, 
author = {Álvarez-Díez, Susana and Alfaro-Cid, Eva and Fernández-Blanco, Matilde O.}, 
journal = {European Journal of Management and Business Economics}, 
issn = {24448451}, 
doi = {10.1016/j.redee.2015.11.003}, 
abstract = {{This article proposes a multi-currency cross-hedging strategy that minimizes the exchange risk. The use of derivatives in small and medium-sized enterprises (SMEs) is not common but, despite its complexity, can be interesting for those with international activities. In particular, the reduction in the exchange risk borne through the use of natural multi-currency cross-hedging is measured, considering Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR) for measuring market risk instead of the variance. CVaR is minimized using linear programmes, while a multiobjective genetic algorithm is designed for minimizing VaR, considering two scenarios for each currency. The results obtained show that the optimal hedge strategy that minimizes VaR is different from the minimum CVaR hedge strategy. A very interesting point is that, just by investing in other currencies, a significant risk reduction in VaR and CVaR can be obtained. © 2015 AEDEM}}, 
pages = {2--7}, 
number = {1}, 
volume = {25}
}
@article{10.1016/j.insmatheco.2018.01.008, 
year = {2018}, 
title = {{Optimal investment under VaR-Regulation and Minimum Insurance}}, 
author = {Chen, An and Nguyen, Thai and Stadje, Mitja}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2018.01.008}, 
abstract = {{We look at an optimal investment problem of a financial institution operating under a joint Value-at-Risk and a portfolio insurance constraint. This analysis is particularly relevant for an insurance company operating under the Solvency II regulation which aims to maximize the expected utility of its shareholders, while at the same time being required to provide its policyholders a minimum guaranteed amount. Using static Lagrangian method, we solve the pointwise utility optimization problem to achieve the global maximum by carefully comparing the local maximizers with the jump point or the boundary. Our theoretical and numerical results show that contrary to a pure Value-at-Risk regulation, an insurance company that operates not only under a Solvency II VaR constraint but additionally has to serve a minimal guarantee admits a comprehensive but not too costly protection, and at the same time displays prudent investment behavior. This result holds for both constant and stochastic volatility settings. © 2018 Elsevier B.V.}}, 
pages = {194--209}, 
number = {NA}, 
volume = {79}
}
@article{10.1007/978-3-319-09114-3_15, 
year = {2015}, 
title = {{Worst-Case Scenario Portfolio Optimization Given the Probability of a Crash}}, 
author = {Menkens, Olaf}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-09114-3\_15}, 
abstract = {{Korn and Wilmott [9] introduced the worst-case scenario portfolio problem. Although Korn and Wilmott assume that the probability of a crash occurring is unknown, this paper analyzes how the worst—case scenario portfolio problem is affected if the probability of a crash occurring is known. The result is that the additional information of the known probability is not used in the worst—case scenario. This leads to a q-quantile approach (instead of a worst case), which is a value at risk-style approach in the optimal portfolio problem with respect to the potential crash. Finally, it will be shown that-under suitable conditions-every stochastic portfolio strategy has at least one superior deterministic portfolio strategy within this approach. © The Author(s) 2015.}}, 
pages = {267--288}, 
number = {NA}, 
volume = {99}
}
@article{10.1016/j.econmod.2013.04.031, 
year = {2013}, 
title = {{Operational risk of option hedging}}, 
author = {Mitra, Sovan}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2013.04.031}, 
abstract = {{Operational risk is increasingly being recognised as a significant area of risk and regulation, yet there exists relatively little research on it. In this paper we show that operational risk represents a fundamental risk to option hedging and investigate it by proposing a new theoretical model. We derive an exposure indicator for the operational risk of option hedging and the resulting operational risk distribution. We obtain analytical results for various risk measures including the Value at Risk equation; this includes deriving a new analytical result for the quantile function of the half-normal distributions (which will be of interest to Statisticians in general). We determine an analytical solution to the price of options under operational risk. We conduct numerical experiments on empirical option data to validate our model and estimate the operational Value at Risk for option hedging. © 2013 Elsevier B.V.}}, 
pages = {194--203}, 
number = {NA}, 
volume = {33}
}
@article{10.1109/ciced.2018.8592587, 
year = {2018}, 
title = {{Load cluster management considering response uncertainty}}, 
author = {Zeng, Wei and Sun, Min and Chen, Bo and He, Wei and Huang, Yangqi and Yu, Kansheng}, 
journal = {2018 China International Conference on Electricity Distribution (CICED)}, 
issn = {21617481}, 
doi = {10.1109/ciced.2018.8592587}, 
abstract = {{The elastic load on demand side can change with the fluctuation electricity price, so it could be modeled as the adjustable load cluster and involved into power dispatching with traditional generation unit, which can improve the flexibility of the power system. Due to the influence of various factors, such as the response to electricity price is not timely, signal transmission delay, the user's habits is difficult to be changed, and the response amount and expected value always exist in a certain range of deviation. In this paper, the value at risk (VaR) is used to measure the risk loss caused by the uncertainty of the load cluster response, then VaR is introduced as a loss measure to participate in the optimization of the power dispatching model and establish a joint scheduling model. Through the actual example analysis, the results show that, due to the risk loss of the adjustable load cluster is considered, the risk loss caused by uncertainty response is reduced. The research has practical application prospect to the implementation of DSM projects. © 2018 IEEE}}, 
pages = {2865--2869}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1354-7798.2004.00266.x, 
year = {2004}, 
title = {{Analysing perceived downside risk: The component value-at-risk framework}}, 
author = {Hallerbach, Winfried G. and Menkveld, Albert J.}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/j.1354-7798.2004.00266.x}, 
abstract = {{Multinational companies face increasing risks arising from external risk factors, e.g. exchange rates, interest rates and commodity prices, which they have learned to hedge using derivatives. However, despite increasing disclosure requirements, a firm’s net risk profile may not be transparent to shareholders. We develop the ‘Component Value-at-Risk (VaR)’ framework for companies to identify the multi-dimensional downside risk profile as perceived by shareholders. This framework allows for decomposing downside risk into components that are attributable to each of the underlying risk factors. The firm can compare this perceived VaR, including its composition and dynamics, to an internal VaR based on net exposures as it is known to the company. Any differences may lead to surprises at times of earnings announcements and thus constitute a litigation threat to the firm. It may reduce this information asymmetry through targeted communication efforts. © 2004 Blackwell Publishing Ltd.}}, 
pages = {567--591}, 
number = {4}, 
volume = {10}
}
@article{10.1287/opre.51.4.543.16101, 
year = {2003}, 
title = {{Worst-case value-at-risk and robust portfolio optimization: A conic programming approach}}, 
author = {Ghaoui, Laurent El and Oks, Maksim and Oustry, Francois}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.51.4.543.16101}, 
abstract = {{Classical formulations of the portfolio optimization problem, such as mean-variance or Value-at-Risk (VaR) approaches, can result in a portfolio extremely sensitive to errors in the data, such as mean and covariance matrix of the returns. In this paper we propose a way to alleviate this problem in a tractable manner. We assume that the distribution of returns is partially known, in the sense that only bounds on the mean and covariance matrix are available. We define the worst-case Value-at-Risk as the largest VaR attainable, given the partial information on the returns' distribution. We consider the problem of computing and optimizing the worst-case VaR, and we show that these problems can be cast as semidefinite programs. We extend our approach to various other partial information on the distribution, including uncertainty in factor models, support constraints, and relative entropy information.}}, 
pages = {543--556}, 
number = {4}, 
volume = {51}
}
@article{10.1142/s0129183121500996, 
year = {2021}, 
title = {{Non-extensive value-at-risk estimation during times of crisis}}, 
author = {Hajihasani, Ahmad and Namaki, Ali and Asadi, Nazanin and Tehrani, Reza}, 
journal = {International Journal of Modern Physics C}, 
issn = {01291831}, 
doi = {10.1142/s0129183121500996}, 
eprint = {2005.09036}, 
abstract = {{Value-at-risk (VaR) is a crucial subject that researchers and practitioners extensively use to measure and manage uncertainty in financial markets. Although VaR is a standard risk control instrument, there are criticisms about its performance. One of these cases, which has been studied in this research, is the VaR underestimation during times of crisis. In these periods, the non-Gaussian behavior of markets intensifies, and the estimated VaRs by typical models are lower than the real values. A potential approach that can be used to describe the non-Gaussian behavior of return series is the Tsallis entropy framework and nonextensive statistical methods. This paper has used the nonextensive models for analyzing financial markets' behavior during crisis times. By applying the q-Gaussian probability density function for emerging and mature markets over 20 years, we can see a better VaR estimation than the regular models, especially during crisis times. We have shown that the q-Gaussian models composed of VaR and Expected Shortfall (ES) estimate risk better than the standard models. By comparing the ES, VaR, q-VaR and q-ES for emerging and mature markets, we see in confidence levels more than 0.98, the outputs of q models are more real, and the q-ES model has lower errors than the other ones. Also, it is evident that in the mature markets, the difference of VaR between normal condition and nonextensive approach increases more than one standard deviation during times of crisis. Still, in the emerging markets, we cannot see a specific pattern. The findings of this paper are useful for analyzing the risk of financial crises in different markets. © 2021 World Scientific Publishing Company.}}, 
pages = {2150099}, 
number = {7}, 
volume = {32}
}
@article{10.1016/j.insmatheco.2003.07.004, 
year = {2003}, 
title = {{High volatility, thick tails and extreme value theory in value-at-risk estimation}}, 
author = {Gençay, Ramazan and Selçuk, Faruk and Ulugülyaǧci, Abdurrahman}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2003.07.004}, 
abstract = {{In this paper, the performance of the extreme value theory in value-at-risk calculations is compared to the performances of other well-known modeling techniques, such as GARCH, variance-covariance (Var-Cov) method and historical simulation in a volatile stock market. The models studied can be classified into two groups. The first group consists of GARCH(1, 1) and GARCH(1, 1)- t models which yield highly volatile quantile forecasts. The other group, consisting of historical simulation, Var-Cov approach, adaptive generalized Pareto distribution (GPD) and nonadaptive GPD models, leads to more stable quantile forecasts. The quantile forecasts of GARCH(1, 1) models are excessively volatile relative to the GPD quantile forecasts. This makes the GPD model be a robust quantile forecasting tool which is practical to implement and regulate for VaR measurements. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {337--356}, 
number = {2}, 
volume = {33}
}
@article{10.1016/j.ijdrr.2020.101932, 
year = {2021}, 
title = {{Quantile-based individual risk measures for rockfall-prone areas}}, 
author = {Farvacque, Manon and Eckert, Nicolas and Bourrier, Franck and Corona, Christophe and Lopez-Saez, Jérôme and Toe, David}, 
journal = {International Journal of Disaster Risk Reduction}, 
issn = {22124209}, 
doi = {10.1016/j.ijdrr.2020.101932}, 
abstract = {{In mountain environments, precise rockfall risk evaluation is crucial to reduce death tolls and costs. However, to date, existing rockfall risk assessment procedures remain scarce, as they focus only on existing elements at risk and with the damage expectation as sole risk measure. Here, we propose an approach to evaluate the distribution of damages on an individual basis as a continuous function of space. Furthermore, rockfall risk is evaluated through (i) the damage expectation and based on (ii) the value-at-risk (VAR) and (iii) the expected shortfall (ES). VaR and ES risk measures allow better assessing the risk due to extreme events and accounting for various short-term/long-term constraints faced by stakeholders. This procedure is applied to Le Brocey slope (municipality of Crolles, French Alps), frequently affected by rockfall events, illustrating its potential for land-use planning. Notably, obtained individual risk values can be confronted with acceptability thresholds to perform legal zoning on sound basis. Also, they allow comparison of risk management strategies as function of different behaviors towards risk, budgetary constraints and/or temporal horizons. Hence, the approach provides valuable tools for future land-use planning and decision-making. It could easily be transferred to other hazards as a wider contribution to the determination of the best balance between safety and sustainability. © 2020 Elsevier Ltd}}, 
pages = {101932}, 
number = {NA}, 
volume = {53}
}
@article{10.3917/fina.403.0045, 
year = {2019}, 
title = {{Long-term project valuation in capital-constrained firms}}, 
author = {Leland, Hayne}, 
journal = {Finance}, 
issn = {07526180}, 
doi = {10.3917/fina.403.0045}, 
abstract = {{This paper values correlated future cash flows when idiosyncratic risk earns a premium. For example, single period RAROC-style valuations used by financial institutions can be extended to multiple periods. Properties of the valuation differ considerably from traditional NPV analysis. Cash flow valuations are non-additive, and asset values vary inversely with cash flow variances and covariances. Negative valuations are possible even when expected cash flows are positive. The valuation of normally distributed cash flows is provided in matrix form, as well as the period-specific risk charge allocations. Simplified perpetuity formulas are also developed for cash flows that follow random walks. © 2020 Presses universitaires de Grenoble.}}, 
pages = {45}, 
number = {3}, 
volume = {40}
}
@article{10.1016/j.mulfin.2007.12.003, 
year = {2008}, 
title = {{How useful is intraday data for evaluating daily Value-at-Risk?. Evidence from three Euro rates}}, 
author = {McMillan, David G. and Speight, Alan E.H. and Evans, Kevin P.}, 
journal = {Journal of Multinational Financial Management}, 
issn = {1042444X}, 
doi = {10.1016/j.mulfin.2007.12.003}, 
abstract = {{Previous research concerned with the investigation of intraday data has typically sought to model that data using techniques to control for intraday periodicity, has applied models of short-horizon and long-horizon dependencies, or has utilised intraday data in the construction of realised variance. Using Euro exchange rate data, we apply these different modelling strategies in forecasting daily volatility and calculating Value-at-Risk measures, benchmarked against a standard GARCH model for daily and raw intraday returns. Our results suggest that the use of intraday data provides improved daily volatility and VaR forecasts relative to daily data and daily realised volatility. Further, use of the raw intraday data, or intraday data subjected to a simple standardisation procedure, provides better forecasts and VaR measures than more complicated models for intraday periodicity. These results also hold in a multi-asset portfolio setting. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {488--503}, 
number = {5}, 
volume = {18}
}
@article{10.1063/1.4976117, 
year = {2017}, 
title = {{Optimal operation of distribution networks with presence of distributed generations and battery energy storage systems considering uncertainties and risk analysis}}, 
author = {Afshan, Razieh and Salehi, Javad}, 
journal = {Journal of Renewable and Sustainable Energy}, 
issn = {19417012}, 
doi = {10.1063/1.4976117}, 
abstract = {{This paper presents a probabilistic framework for the operation of distribution networks considering distributed generations (DGs) and battery energy storage systems. This framework considers the uncertainty of electricity prices and output power of DGs. To account for uncertainties, the probability distribution function is used, which generates scenarios by Monte Carlo Simulation at each hour. The aim of this paper is to calculate the distribution of daily profit and risk analysis. Value at Risk (VaR) is used as risk measure, and sensitivity of VaR with respect to changes in standard deviation is assessed. Numerical examples for two case studies are presented to illustrate the impact of uncertainty of DGs and prices on the profit and risk. © 2017 Author(s).}}, 
pages = {014102}, 
number = {1}, 
volume = {9}
}
@article{10.23919/icif.2017.8009872, 
year = {2017}, 
title = {{Flexible estimation of risk metric using copula model for the joint severity-frequency loss framework}}, 
author = {Guharay, Sabyasachi and Chang, K.C. and Xu, Jie}, 
journal = {2017 20th International Conference on Information Fusion (Fusion)}, 
issn = {NA}, 
doi = {10.23919/icif.2017.8009872}, 
abstract = {{Predictive analytics and data fusion techniques are being regularly used for analysis in Quantitative Risk Management (QRM). The primary risk metric of interest, Value-at-Risk (VaR), has always been difficult to robustly estimate for different data types. The classical Monte Carlo simulation (MCS) approach (denoted henceforth as classical approach) assumes the independence of loss severity and loss frequency. In practice, this assumption may not always hold. To overcome this limitation and more robustly estimate the corresponding VaR, we propose a new approach known as Copula-based Parametric Modeling of Frequency and Severity (CPFS). The proposed approach is verified via large-scale MCS experiments and validated on three publicly available datasets. We compare CPFS with the classical approach and a Data-driven Partitioning of Frequency and Severity (DPFS) approach for robust VaR estimation. We observe that the classical approach estimates VaR poorly while both the DPFS and the CPFS methodologies attain VaR estimates for real-world data. These studies provide real-world evidence that the CPFS and DPFS methodologies have merits for its use to accurately estimate VaR. © 2017 International Society of Information Fusion (ISIF).}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/drpt.2008.4523428, 
year = {2008}, 
title = {{Risk assessment and control for distribution companies in electricity market environment}}, 
author = {Wang, Qin and Li, Chunhua and Wen, Fushuan and Xu, Nan}, 
journal = {2008 Third International Conference on Electric Utility Deregulation and Restructuring and Power Technologies}, 
issn = {NA}, 
doi = {10.1109/drpt.2008.4523428}, 
abstract = {{This paper presents a methodology to manage the market risk faced by a distribution company in the emerging electricity market environment. The risk is due to uncertainty in load forecasting, direct power-purchase for large users, and credit default of users. An optimization model that integrates these three kinds of uncertainty is built aiming at minimizing the loss of the distribution company concerned. This model permits the representation of an integrated risk management problem by means of value-at-risk. Under the stochastic programming framework, a numerical example is provided to illustrate the feasibility of our approach. © 2008 DRPT.}}, 
pages = {335--339}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/07474938.2019.1697087, 
year = {2020}, 
title = {{Where does the tail begin? An approach based on scoring rules}}, 
author = {Hoga, Yannick}, 
journal = {Econometric Reviews}, 
issn = {07474938}, 
doi = {10.1080/07474938.2019.1697087}, 
abstract = {{Learning about the tail shape of time series is important in, e.g., economics, finance, and risk management. However, it is well known that estimates of the tail index can be very sensitive to the choice of the number k of tail observations used for estimation. We propose a procedure that determines where the tail begins by choosing k in a data-driven fashion using scoring rules. So far, scoring rules have mainly been used to compare density forecasts. We also demonstrate how our proposal can be used in multivariate applications in the system risk literature. The advantages of our choice of k are illustrated in simulations and an empirical application to Value-at-Risk forecasts for five U.S. blue-chip stocks. © 2019, © 2019 Taylor \& Francis Group, LLC.}}, 
pages = {1--23}, 
number = {6}, 
volume = {39}
}
@article{10.1504/ijcee.2020.108384, 
year = {2020}, 
title = {{Performance evaluation of the Bayesian and classical value at risk models with circuit breakers set up}}, 
author = {Haddad, GholamReza Keshavarz and Heidari, Hadi}, 
journal = {International Journal of Computational Economics and Econometrics}, 
issn = {17571170}, 
doi = {10.1504/ijcee.2020.108384}, 
abstract = {{Circuit breakers, like price limits and trading suspensions, are used to reduce price volatility in security markets. When returns hit price limits or missed, observed returns deviate from equilibrium returns. This creates a challenge for predicting stock returns and modelling value at risk (VaR). In Tehran Stock Exchange (TSE), the circuit breakers are applied to control for the excess price volatilities. This paper intend to address which models and what methodology should be applied by risk analysts to calculate the VaR when the returns are unobservable. To this end, we extend Wei’s (2002) model, in the framework of Bayesian Censored and Missing-GARCH approach, to estimate VaR for a share index in TSE. Using daily data over June 2006 to June 2016, we show that the Censored and Missing- GARCH model with student-t distribution outperforms the other VaR estimation metods. Kullback-Leibler (KLIC), Kupic (1995) test and Lopez score (1998) outcomes show that estimated VaR by Censored and missing- GARCH model with student-t distribution is of the most accuracy among the other GARCH family estimated models. Copyright © 2020 Inderscience Enterprises Ltd.}}, 
pages = {222--241}, 
number = {3}, 
volume = {10}
}
@article{10.1007/978-88-470-0704-8_10, 
year = {2008}, 
title = {{A liability adequacy test for mathematical provision}}, 
author = {Cocozza, Rosa and Lorenzo, Emilia Di and Orlando, Abina and Sibillo, Marilena}, 
issn = {NA}, 
doi = {10.1007/978-88-470-0704-8\_10}, 
abstract = {{This paper deals with the application of the Value at Risk of the mathematical provision within a fair valuation context. Through the VaR calculation, the estimate of an appropriate contingency reserve is connected to the predicted worst case additional cost, at a specific confidence level, projected over a fixed accounting period. The numerical complexity is approached by means of a simulation methodology, particularly suitable also in the case of a large number of risk factors. © Springer 2008, Milan.}}, 
pages = {75--81}, 
number = {NA}, 
volume = {NA}
}
@article{10.16081/j.issn.1006-6047.2018.05.011, 
year = {2018}, 
title = {{Optimal AGC model with risk constraints of market power [考虑市场力风险约束的最优AGC控制模型]}}, 
author = {}, 
issn = {10066047}, 
doi = {10.16081/j.issn.1006-6047.2018.05.011}, 
abstract = {{Aming at the problem that the market power risk is difficult to be considered in the traditional AGC(Automatic Generation Control) optimization model,the VaR(Value at Risk) for measuring market risk is introduced,and the AGC optimization model of mixed integer nonlinear programming under the constraint of ancillary service cost with VaR as the limit value is constructed. For dealing with the nonlinearity of the model,the two-state auxiliary variables are introduced to equivalently transform the three-state variables of the unit,which realizes the linearization of the model and thus effectively reduces the difficulty of solving the model. Taking the operation data of Guangxi Power Grid as an example,the performances of AGC under different risk confidence levels are compared. The results show that the higher the confidence level is,the greater the regulation cost will be,but with better control effect. However,whether the confidence level is high or not,the proposed model can ensure the CPS(Control Performance Standard) qualification,verifying the validity of the model. It can provide a useful reference for the AGC in the power market environment. © 2018, Electric Power Automation Equipment Press. All right reserved.}}, 
number = {5}, 
volume = {38}
}
@article{10.1016/j.ejor.2012.06.001, 
year = {2012}, 
title = {{International portfolio management with affine policies}}, 
author = {Fonseca, Raquel J. and Rustem, Berç}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2012.06.001}, 
abstract = {{While dynamic decision making has traditionally been represented as scenario trees, these may become severely intractable and difficult to compute with an increasing number of time periods. We present an alternative tractable approach to multiperiod international portfolio optimization based on an affine dependence between the decision variables and the past returns. Because local asset and currency returns are modeled separately, the original model is non-linear and non-convex. With the aid of robust optimization techniques, however, we develop a tractable semidefinite programming formulation of our model, where the uncertain returns are contained in an ellipsoidal uncertainty set. We add to our formulation the minimization of the worst case value-at-risk and show the close relationship with robust optimization. Numerical results demonstrate the potential gains from considering a dynamic multiperiod setting relative to a single stage approach. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {177--187}, 
number = {1}, 
volume = {223}
}
@article{10.1080/07474938.2010.481972, 
year = {2010}, 
title = {{On some models for value-at-risk}}, 
author = {Yu, Philip L. H. and Li, Wai Keung and Jin, Shusong}, 
journal = {Econometric Reviews}, 
issn = {07474938}, 
doi = {10.1080/07474938.2010.481972}, 
abstract = {{The idea of statistical learning can be applied in financial risk management. In recent years, value-at-risk (VaR) has become the standard tool for market risk measurement and management. For better VaR estimation, Engle and Manganelli (2004) introduced the conditional autoregressive value-at-risk (CAViaR) model to estimate the VaR directly by quantile regression. To entertain the nonlinearity and structural change in the VaR, we extend the CAViaR idea using two approaches: the threshold GARCH (TGARCH) and the mixture-GARCH models. The estimation method of these models are proposed. Our models should possess all the advantages of the CAViaR model and enhance the nonlinear structure. The methods are applied to the S\&P500, Hang Seng, Nikkei and Nasdaq indices to illustrate our models. © Taylor \& Francis Group, LLC.}}, 
pages = {622--641}, 
number = {5}, 
volume = {29}
}
@article{10.1016/j.asoc.2017.12.003, 
year = {2018}, 
title = {{On risk measures and capital allocation for distributions depending on parameters with interval or fuzzy uncertainty}}, 
author = {Vernic, Raluca}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2017.12.003}, 
abstract = {{Since risks are regarded as emerging from uncertainty, they can be modeled using probabilistic, interval or fuzzy methods. The probabilistic literature on risk measures, though well developed, quantifies the risks by single values, which could seem restrictive for risk managers who would like to have more insight into the phenomena, like, e.g., an interval covering the single value. Therefore, in this paper, we study the VaR and TVaR risk measures for distributions with parameters of interval type, further extended to fuzzy numbers. In particular, we concentrate on the class of location and/or scale parameters, showing that in this case, the resulting risk measures are also in the form of intervals or, respectively, fuzzy numbers. Moreover, we apply the results to the capital allocation problem and detail the procedure for the normal, Pareto and Farlie–Gumbel–Morgenstern particular distributions. The formulas are numerically illustrated on interval and fuzzy parameters for some classical distributions; in this sense, some applications on real data sets are discussed. © 2017 Elsevier B.V.}}, 
pages = {199--215}, 
number = {NA}, 
volume = {64}
}
@article{10.1016/j.jkss.2018.08.005, 
year = {2019}, 
title = {{Quantile forecasts for financial volatilities based on parametric and asymmetric models}}, 
author = {Choi, Ji-Eun and Shin, Dong Wan}, 
journal = {Journal of the Korean Statistical Society}, 
issn = {12263192}, 
doi = {10.1016/j.jkss.2018.08.005}, 
abstract = {{For financial volatilities such as realized volatility and volatility index, a new parametric quantile forecast strategy is proposed, focusing on forecast interval and value at risk (VaR) forecast. This fully addresses asymmetries in 3 parts: mean, volatility and distribution. The asymmetries are addressed by the LHAR (leverage heterogeneous autoregressive) model of McAleer and Medeiros (2008) and Corsi and Reno (2009) for the mean part, by the EGARCH model for the volatility part, and by the skew-t distribution for the error distribution part. The method is applied to the realized volatilities and the volatility indexes of the US S\&P 500 index, the US NASDAQ index, the Korea KOSPI index in which significant asymmetries are identified. Considerable out-of-sample forecast improvements of the forecast interval and VaR forecast are demonstrated for the volatilities: forecast intervals of volatilities have better coverages with shorter lengths and VaR forecasts of volatility indexes have better violations if asymmetries are properly addressed rather than ignored. The proposed parametric method reveals considerably better out-of-sample performance than the recently proposed semiparametric quantile regression approach of Zikes and Barunik (2016). © 2018 The Korean Statistical Society}}, 
pages = {68--83}, 
number = {1}, 
volume = {48}
}
@article{10.21314/jem.2020.207, 
year = {2020}, 
title = {{Performance of value-at-risk averaging in the nordic power futures market}}, 
author = {Westgaard, Sjur and Frydenberg, Stein and Sveinsson, Jørgen Andersen and Aaløkken, Maurits}, 
journal = {The Journal of Energy Markets}, 
issn = {17563607}, 
doi = {10.21314/jem.2020.207}, 
abstract = {{We investigate the performance of various value-at-risk (VaR) models in the context of the highly volatile Nordic power futures market, examining whether simple averages of models provide better results than the individual models them-selves. The individual models used are normally distributed GARCH, t-distributed GARCH, t-distributed GJR–GARCH, a quantile regression using RiskMetrics, a quantile regression using t-distributed GARCH, RiskMetrics with Cornish–Fisher and a filtered historical simulation using t-distributed GARCH. We find that Risk-Metrics with Cornish–Fisher and normally distributed GARCH perform worse than the other individual models. The average models generally outperform the individual models at a 5\% significance level. The conditional independence test reveals that the models are only partially capable of accounting for the volatility clustering of the Nordic power futures. Investors in the Nordic electricity markets should therefore use several methods and average them to be more confident in their VaR estimates. © 2020 Infopro Digital Risk (IP) Limited.}}, 
number = {3}, 
volume = {13}
}
@article{10.1088/1742-6596/974/1/012029, 
year = {2018}, 
title = {{Value-at-Risk analysis using ARMAX GARCHX approach for estimating risk of banking subsector stock return's}}, 
author = {Ratih, Iis Dewi and Ulama, Brodjol Sutijo Supri and Prastuti, Mike}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/974/1/012029}, 
abstract = {{Value at Risk (VaR) is one of the statistical methods used to measure market risk by estimating the worst losses in a given time period and level of confidence. The accuracy of this measuring tool is very important in determining the amount of capital that must be provided by the company to cope with possible losses. Because there is a greater losses to be faced with a certain degree of probability by the greater risk. Based on this, VaR calculation analysis is of particular concern to researchers and practitioners of the stock market to be developed, thus getting more accurate measurement estimates. In this research, risk analysis of stocks in four banking sub-sector, Bank Rakyat Indonesia, Bank Mandiri, Bank Central Asia and Bank Negara Indonesia will be done. Stock returns are expected to be influenced by exogenous variables, namely ICI and exchange rate. Therefore, in this research, stock risk estimation are done by using VaR ARMAX-GARCHX method. Calculating the VaR value with the ARMAX-GARCHX approach using window 500 gives more accurate results. Overall, Bank Central Asia is the only bank had the estimated maximum loss in the 5\% quantile. © 2018 Published under licence by IOP Publishing Ltd.}}, 
pages = {012029}, 
number = {1}, 
volume = {974}
}
@article{10.1142/s0217590819500668, 
year = {2019}, 
title = {{AN AI APPROACH to MEASURING FINANCIAL RISK}}, 
author = {Yu, Lining and Härdle, Wolfgang Karl and Borke, Lukas and Benschop, Thijs}, 
journal = {The Singapore Economic Review}, 
issn = {02175908}, 
doi = {10.1142/s0217590819500668}, 
eprint = {2009.13222}, 
abstract = {{AI artificial intelligence brings about new quantitative techniques to assess the state of an economy. Here, we describe a new measure for systemic risk: the Financial Risk Meter (FRM). This measure is based on the penalization parameter (λ) of a linear quantile lasso regression. The FRM is calculated by taking the average of the penalization parameters over the 100 largest US publicly-traded financial institutions. We demonstrate the suitability of this AI-based risk measure by comparing the proposed FRM to other measures for systemic risk, such as VIX, SRISK and Google Trends. We find that mutual Granger causality exists between the FRM and these measures, which indicates the validity of the FRM as a systemic risk measure. The implementation of this project is carried out using parallel computing, the codes are published on www.quantlet.de with keyword FRM. The R package RiskAnalytics is another tool with the purpose of integrating and facilitating the research, calculation and analysis methods around the FRM project. The visualization and the up-to-date FRM can be found on hu.Berlin/frm. © 2019 World Scientific Publishing Company.}}, 
pages = {1--21}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2012.12.001, 
year = {2013}, 
title = {{Optimal reinsurance with general premium principles}}, 
author = {Chi, Yichun and Tan, Ken Seng}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.12.001}, 
abstract = {{In this paper, we study two classes of optimal reinsurance models from the perspective of an insurer by minimizing its total risk exposure under the criteria of value at risk (VaR) and conditional value at risk (CVaR), assuming that the reinsurance premium principles satisfy three basic axioms: distribution invariance, risk loading and stop-loss ordering preserving. The proposed class of premium principles is quite general in the sense that it encompasses eight of the eleven commonly used premium principles listed in Young (2004). Under the additional assumption that both the insurer and reinsurer are obligated to pay more for larger loss, we show that layer reinsurance is quite robust in the sense that it is always optimal over our assumed risk measures and the prescribed premium principles. We further use the Wang's and Dutch premium principles to illustrate the applicability of our results by deriving explicitly the optimal parameters of the layer reinsurance. These two premium principles are chosen since in addition to satisfying the above three axioms, they exhibit increasing relative risk loading, a desirable property that is consistent with the market convention on reinsurance pricing. © 2012 Elsevier B.V.}}, 
pages = {180--189}, 
number = {2}, 
volume = {52}
}
@article{10.1016/j.ejor.2021.02.041, 
year = {2021}, 
title = {{A one-sided Vysochanskii-Petunin inequality with financial applications}}, 
author = {Mercadier, Mathieu and Strobel, Frank}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2021.02.041}, 
abstract = {{We derive a one-sided Vysochanskii-Petunin inequality, providing probability bounds for random variables analogous to those given by Cantelli's inequality under the additional assumption of unimodality, potentially relevant for applied statistical practice across a wide range of disciplines. As a possible application of this inequality in a financial context, we examine refined bounds for the individual risk measure of Value-at-Risk, providing a potentially useful alternative benchmark with interesting regulatory implications for the Basel multiplier. © 2021 Elsevier B.V.}}, 
pages = {374--377}, 
number = {1}, 
volume = {295}
}
@article{10.1109/cybersa.2019.8899431, 
year = {2019}, 
title = {{Quantile based risk measures in cyber security}}, 
author = {Carfora, Maria Francesca and Orlando, Albina}, 
journal = {2019 International Conference on Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA)}, 
issn = {NA}, 
doi = {10.1109/cybersa.2019.8899431}, 
abstract = {{Measures and methods used in financial sector to quantify risk, have been recently applied to cyber world. The aim is to help organizations to improve risk management strategies and to wisely plan investments in cyber security. On the other hand, they are useful instruments for insurance companies in pricing cyber insurance contracts and setting the minimum capital requirements defined by the regulators. In this paper we propose an estimation of Value at Risk (VaR), referred to as Cyber Value at Risk in cyber security domain, and Tail Value at risk (TVaR). The data breach information we use is obtained from the 'Chronology of data breaches' compiled by the Privacy Rights Clearinghouse. © 2019 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/978-3-642-12133-3_18, 
year = {2010}, 
title = {{Design of a financial application driven multivariate Gaussian random number generator for an FPGA}}, 
author = {Saiprasert, Chalermpol and Bouganis, Christos-Savvas and Constantinides, George A.}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-12133-3\_18}, 
abstract = {{A Multivariate Gaussian random number generator (MVGRNG) is a pre-requisite for most Monte Carlo simulations for financial applications, especially those that involve many correlated assets. In recent years, Field Programmable Gate Arrays (FPGAs) have received a lot of attention as a target platform for the implementation of such a generator due to the high throughput performance that can be achieved. In this work it is demonstrated that the choice of the objective function employed for the hardware optimization of the MVRNG core, has a considerable impact on the final performance of the application of interest. Two of the most important financial applications, Value-at-Risk estimation and option pricing are considered in this paper. Experimental results have shown that the suitability of the chosen objective function for the optimization of the hardware MVRNG core depends on the structure of the targeted distribution. An improvement in performance of up to 96\% is reported for VaR calculation while up to 81\% improvement is observed for option pricing when a suitable objective function for the optimization of the MVRNG core is considered while maintaining the same level of hardware resources. © 2010 Springer-Verlag.}}, 
pages = {182--193}, 
number = {NA}, 
volume = {5992 LNCS}
}
@article{10.1108/03074351111167956, 
year = {2011}, 
title = {{Risk management of risk under the Basel Accord: forecasting value-at-risk of VIX futures}}, 
author = {Chang, Chia‐lin and Jiménez‐Martín, Juan‐Ángel and McAleer, Michael and Pérez‐Amaral, Teodosio}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/03074351111167956}, 
abstract = {{Purpose – The Basel II Accord requires that banks and other authorized deposit-taking institutions (ADIs) communicate their daily risk forecasts to the appropriate monetary authorities at the beginning of each trading day, using one or more risk models to measure value-at-risk (VaR). The risk estimates of these models are used to determine capital requirements and associated capital costs of ADIs, depending in part on the number of previous violations, whereby realized losses exceed the estimated VaR. The purpose of this paper is to address the question of risk management of risk, namely VaR of VIX futures prices. Design/methodology/approach – The authors examine how different risk management strategies performed before, during and after the 2008-2009 global financial crisis (GFC). Findings – The authors find that an aggressive strategy of choosing the supremum of the univariate model forecasts is preferred to the other alternatives, and is robust during the GFC. Originality/value – The paper examines how different risk management strategies performed before, during and after the 2008-2009 GFC, and finds that an aggressive strategy of choosing the supremum of the univariate model forecasts is preferred to the other alternatives, and is robust during the GFC. © 2011, © Emerald Group Publishing Limited.}}, 
pages = {1088--1106}, 
number = {11}, 
volume = {37}
}
@article{10.1109/icbmei.2011.5917056, 
year = {2011}, 
title = {{Multiple product newsvendor problem based on different risk measurements}}, 
author = {Zhou, Yan-Ju and Chen, Qian}, 
journal = {2011 International Conference on Business Management and Electronic Information}, 
issn = {NA}, 
doi = {10.1109/icbmei.2011.5917056}, 
abstract = {{Different risk constraint represents the different decision-making process of decision-makers. This paper use the traditional Mean-Variance, Value at Risk and Conditional Value at Risk which are from financial engineering to establish the optimum order risk decision model under the assumption of normal distribution. In order to illustrate the characteristics of different risk measurement constraint, the solutions among those three models is compared by numerical examples and their decision-making behaviors are summarized accordingly. In addition, Monte Carlo technique is used to get the sample simulation. According to the result of the three models, this paper compares these risk measurement and analyzes the relative advantages of Conditional Value at Risk to Mean-Variance and Value at Risk. © 2011 IEEE.}}, 
pages = {794--798}, 
number = {NA}, 
volume = {1}
}
@article{10.1205/psep.05195, 
year = {2007}, 
title = {{Value at risk perspective on layers of protection analysis}}, 
author = {Fang, J.S. and Mannan, M.S. and Ford, D.M. and Logan, J. and Summers, A.}, 
journal = {Process Safety and Environmental Protection}, 
issn = {09575820}, 
doi = {10.1205/psep.05195}, 
abstract = {{Layers of protection analysis (LOPA) is an established tool for designing, characterizing, and evaluating risk in the chemical process industry. Value at risk (VaR) is a method first introduced in the financial sector for modeling potential loss in a complex venture. In this paper we demonstrate the application of VaR principles to the LOPA of an ethylene refrigeration compressor. We calculate the changes in risk profile (probability versus loss) associated with adding or removing different safety interlocks around the compressor. The VaR analysis shows that the benefits of a given layer of protection are not necessarily captured by a single average number, since the entire probability - value curve is affected. This type of analysis will aid in the allocation of limited resources to process risk interventions. © 2007 Institution of Chemical Engineers.}}, 
pages = {81--87}, 
number = {1 B}, 
volume = {85}
}
@article{10.1111/eufm.12079, 
year = {2016}, 
title = {{Empirical Analysis of the Intertemporal Relationship between Downside Risk and Expected Returns: Evidence from Time-varying Transition Probability Models}}, 
author = {Chen, Cathy Yi‐Hsuan and Chiang, Thomas C.}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/eufm.12079}, 
abstract = {{This paper examines the intertemporal relationship between downside risks and expected stock returns for five advanced markets. Using Value-at-Risk (VaR) as a measure of downside risk, we find a positive and significant relationship between VaR and the expected return before the world financial crisis (September 2008). However, when we estimate the model using a sample after this date, the results show a negative risk–return relationship. Evidence from a two-state Markov regime-switching model indicates that as uncertainty rises, the sign of the risk–return relationship turns negative. Evidence suggests that the Markov regime-switching model helps to resolve the conflicting signs in the risk–return relationship. © 2015 John Wiley \& Sons, Ltd.}}, 
pages = {749--796}, 
number = {5}, 
volume = {22}
}
@article{10.1111/j.2041-6156.2009.tb00029.x, 
year = {2009}, 
title = {{An importance sampling method to evaluate value-at-risk for assets with jump risks}}, 
author = {Wang, Ren‐Her and Lin, Shih‐Kuei and Fuh, Cheng‐Der}, 
journal = {Asia‐Pacific Journal of Financial Studies}, 
issn = {12261165}, 
doi = {10.1111/j.2041-6156.2009.tb00029.x}, 
abstract = {{Risk management is an important issue when there is a catastrophic event that affects asset price in the market such as a sub-prime financial crisis or other financial crisis. By adding a jump term in the geometric Brownian motion, the jump diffusion model can be used to describe abnormal changes in asset prices when there is a serious event in the market. In this paper, we propose an importance sampling algorithm to compute the Value-at-Risk for linear and nonlinear assets under a multi-variate jump diffusion model. To be more precise, an efficient computational procedure is developed for estimating the portfolio loss probability for linear and nonlinear assets with jump risks. And the titling measure can be separated for the diffusion and the jump part under the assumption of independence. The simulation results show that the efficiency of importance sampling improves over the naive Monte Carlo simulation from 7 times to 285 times under various situations. We also show the robustness of the importance sampling algorithm by comparing it with the EVT-Copula method proposed by Oh and Moon (2006).}}, 
pages = {745--772}, 
number = {5}, 
volume = {38}
}
@article{10.1016/j.jempfin.2006.06.009, 
year = {2008}, 
title = {{Asymmetric and leptokurtic distribution for heteroscedastic asset returns: The SU-normal distribution}}, 
author = {Choi, Pilsun and Nam, Kiseok}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2006.06.009}, 
abstract = {{This paper proposes the SU-normal distribution to describe non-normality features embedded in financial time series, such as: asymmetry and fat tails. Applying the SU-normal distribution to the estimation of univariate and multivariate GARCH models, we test its validity in capturing asymmetry and excess kurtosis of heteroscedastic asset returns. We find that the SU-normal distribution outperforms the normal and Student-t distributions for describing both the entire shape of the conditional distribution and the extreme tail shape of daily exchange rates and stock returns. The goodness-of-fit (GoF) results indicate that the skewness and excess kurtosis are better captured by the SU-normal distribution. The exceeding ratio (ER) test results indicate that the SU-normal is superior to the normal and Student-t distributions, which consistently underestimate both the lower and upper extreme tails, and tend to overestimate the lower tail in general. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {41--63}, 
number = {1}, 
volume = {15}
}
@article{10.1080/02331888.2018.1551895, 
year = {2019}, 
title = {{The generalized Gudermannian distribution: inference and volatility modelling}}, 
author = {Altun, Emrah}, 
journal = {Statistics}, 
issn = {02331888}, 
doi = {10.1080/02331888.2018.1551895}, 
abstract = {{In this paper, we introduce a new distribution, called generalized Gudermannian (GG) distribution, and its skew extension for GARCH models in modelling daily Value-at-Risk (VaR). Basic structural properties of the proposed distribution are obtained including probability density and cumulative distribution functions, moments, and stochastic representation. The maximum likelihood method is used to estimate unknown parameters of the proposed model and finite sample performance of maximum likelihood estimates are evaluated by means of Monte-Carlo simulation study. The real data application on Nikkei 225 index is given to demonstrate the performance of GARCH model specified under skew extension of GG innovation distribution against normal, Student's-t, skew normal and generalized error and skew generalized error distributions in terms of the accuracy of VaR forecasts. The empirical results show that the GARCH model with GG innovation distribution produces the most accurate VaR forecasts for all confidence levels. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--23}, 
number = {2}, 
volume = {53}
}
@article{10.1016/j.forpol.2019.04.003, 
year = {2019}, 
title = {{Economic impact of growth effects in mixed stands of Norway spruce and European beech – A simulation based study}}, 
author = {Friedrich, Stefan and Paul, Carola and Brandl, Susanne and Biber, Peter and Messerer, Katharina and Knoke, Thomas}, 
journal = {Forest Policy and Economics}, 
issn = {13899341}, 
doi = {10.1016/j.forpol.2019.04.003}, 
abstract = {{Productivity in mixed forest stands is often higher than that in pure forest stands. Economic analyses usually exclude this phenomenon. In our study, we assess the consequences of an increased productivity in mixed forests for the economically optimal proportions of tree species in forest stands. The economic model applied centers around the Modern Portfolio Theory with the Value-at-Risk as the objective function. The methodological approach encompasses a structured Monte Carlo simulation to generate distributions of returns for different scenarios. Risks include price fluctuations for raw wood as well as natural hazards. The study's major novelty is that it combines well-researched productivity relations in mixed forests with recent empirical survival models for mixed and pure stands to address site variability across Bavaria. We parametrized the model with simulated growth data for Norway spruce and European beech for various climatic and geological conditions. From the results, we conclude that decision makers can increase the Value-at-Risk of forest portfolios when overyielding occurs, by choosing higher shares of beech. Climatic conditions and the extent of overyielding strongly influence optimal stand composition. Using Value-at-Risk as a risk measure that regards both expected returns and their variation, however, leads to the observation that increased risk aversion rather favors higher shares of spruce in forest stands, despite its higher biophysical risks. © 2019 Elsevier B.V.}}, 
pages = {65--80}, 
number = {NA}, 
volume = {104}
}
@article{10.1007/s10182-009-0118-1, 
year = {2010}, 
title = {{De copulis non est disputandum: Copulae: An overview}}, 
author = {Härdle, Wolfgang Karl and Okhrin, Ostap}, 
journal = {AStA Advances in Statistical Analysis}, 
issn = {18638171}, 
doi = {10.1007/s10182-009-0118-1}, 
abstract = {{Normal distribution of residuals is a traditional assumption in multivariate models. It is, however, not very often consistent with real data. Copulae allow for an extension of dependency models to nonellipticity and for separation of margins from the dependency. This paper provides a survey of copulae where different copula classes, estimation and simulation techniques and goodness-of-fit tests are considered. In the empirical section we apply different copulae to the static and dynamic Value-at-Risk of portfolio returns and Profit-and-Loss function. © Springer-Verlag 2009.}}, 
pages = {1--31}, 
number = {1}, 
volume = {94}
}
@article{10.1007/s10693-007-0006-3, 
year = {2007}, 
title = {{A critique of revised basel II}}, 
author = {Jarrow, Robert A.}, 
journal = {Journal of Financial Services Research}, 
issn = {09208550}, 
doi = {10.1007/s10693-007-0006-3}, 
abstract = {{This paper critiques the revised Basel II capital requirements for banks. To provide a framework for analysis, the XYZ theory of regulatory capital is formulated. Independent of the XYZ theory, we argue that the revised Basel II capital rule for credit risk is not a good approximation to the ideal rule. Based on this, and using the XYZ theory, we argue that: (1) the revised Basel II rules should not replace the existing approaches for determining minimal capital standards, but should be used in conjunction with them, and (2) that calibrating the capital rules to maintain aggregate market capital is a prudent procedure. © 2007 Springer Science+Business Media, LLC.}}, 
pages = {1--16}, 
number = {1-2}, 
volume = {32}
}
@article{10.1239/aap/1222868182, 
year = {2008}, 
title = {{EVT-based estimation of risk capital and convergence of high quantiles}}, 
author = {Degen, Matthias and Embrechts, Paul}, 
journal = {Advances in Applied Probability}, 
issn = {00018678}, 
doi = {10.1239/aap/1222868182}, 
abstract = {{We discuss some issues regarding the accuracy of a quantile-based estimation of risk capital. In this context, extreme value theory (EVT) emerges naturally. The paper sheds some further light on the ongoing discussion concerning the use of a semi-parametric approach like EVT and the use of specific parametric models such as the g-and-h. In particular, we discusses problems and pitfalls evolving from such parametric models when using EVT and highlight the importance of the underlying second-order tail behavior. © Applied Probability Trust 2008.}}, 
pages = {696--715}, 
number = {3}, 
volume = {40}
}
@article{10.1080/03610926.2015.1116572, 
year = {2017}, 
title = {{Tabulations for value at risk and expected shortfall}}, 
author = {Nadarajah, Saralees and Chan, Stephen and Afuecheta, Emmanuel}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2015.1116572}, 
abstract = {{Value at risk and expected shortfall are the two most popular measures of financial risk. Here, we tabulate expressions for both these measures for over 100 parametric distributions, including all commonly known distributions, and illustrate a data application. We expect that this collection of expressions could serve as a source of reference and encourage further research with respect to measures of financial risk. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {5956--5984}, 
number = {12}, 
volume = {46}
}
@article{10.1080/03610926.2018.1549253, 
year = {2020}, 
title = {{Extreme tail risk estimation with the generalized Pareto distribution under the peaks-over-threshold framework}}, 
author = {Zhao, Xu and Cheng, Weihu and Zhang, Pengyue}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2018.1549253}, 
abstract = {{Modeling excesses over a high threshold and estimating extreme tail risk are two utmost studies in the extreme value literature. Traditional techniques are limited on handling these two challenges. To better analyze this type of data, we propose a novel approach which utilizes the generalized Pareto distribution (GPD) in the peaks-over-threshold (POT) framework. Under the proposed approach, by using partial L-moments (PL-moments), computational efficient estimators are derived for the parameters in the GPD. Additionally, we propose method to estimate the tail expectiles and apply a recently developed stopping rule to find the optimal threshold. Various simulation researches show that the proposed approach outperforms the traditional techniques in some aspects. Last, we apply the proposed method to the Shanghai Stock Exchange data for comprehensively illustrating the details and providing guidance for future applications. © 2018, © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--18}, 
number = {4}, 
volume = {49}
}
@article{10.1007/s13595-018-0793-8, 
year = {2019}, 
title = {{Climate change and mixed forests: how do altered survival probabilities impact economically desirable species proportions of Norway spruce and European beech?}}, 
author = {Paul, Carola and Brandl, Susanne and Friedrich, Stefan and Falk, Wolfgang and Härtl, Fabian and Knoke, Thomas}, 
journal = {Annals of Forest Science}, 
issn = {12864560}, 
doi = {10.1007/s13595-018-0793-8}, 
abstract = {{Key message: Economic consequences of altered survival probabilities under climate change should be considered for regeneration planning in Southeast Germany. Findings suggest that species compositions of mixed stands obtained from continuous optimization may buffer but not completely mitigate economic consequences. Mixed stands of Norway spruce (Picea abiesL. Karst.) and European beech (Fagus sylvaticaL.) (considering biophysical interactions between tree species) were found to be more robust, against both perturbations in survival probabilities and economic input variables, compared to block mixtures (excluding biophysical interactions). Context: Climate change is expected to increase natural hazards in European forests. Uncertainty in expected tree mortality and resulting potential economic consequences complicate regeneration decisions. Aims: This study aims to analyze the economic consequences of altered survival probabilities for mixing Norway spruce (Picea abies L. Karst.) and European beech (Fagus sylvatica L.) under different climate change scenarios. We investigate whether management strategies such as species selection and type of mixture (mixed stands vs. block mixture) could mitigate adverse financial effects of climate change. Methods: The bio-economic modelling approach combines a parametric survival model with modern portfolio theory. We estimate the economically optimal species mix under climate change, accounting for the biophysical and economic effects of tree mixtures. The approach is demonstrated using an example from Southeast Germany. Results: The optimal tree species mixtures under simulated climate change effects could buffer but not completely mitigate undesirable economic consequences. Even under optimally mixed forest stands, the risk-adjusted economic value decreased by 28\%. Mixed stands economically outperform block mixtures for all climate scenarios. Conclusion: Our results underline the importance of mixed stands to mitigate the economic consequences of climate change. Mechanistic bio-economic models help to understand consequences of uncertain input variables and to design purposeful adaptation strategies. © 2019, INRA and Springer-Verlag France SAS, part of Springer Nature.}}, 
pages = {14}, 
number = {1}, 
volume = {76}
}
@article{10.1016/j.insmatheco.2008.08.001, 
year = {2009}, 
title = {{Additivity properties for Value-at-Risk under Archimedean dependence and heavy-tailedness}}, 
author = {Embrechts, Paul and Nešlehová, Johanna and Wüthrich, Mario V.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2008.08.001}, 
abstract = {{Mainly due to new capital adequacy standards for banking and insurance, an increased interest exists in the aggregation properties of risk measures like Value-at-Risk (VaR). We show how VaR can change from sub to superadditivity depending on the properties of the underlying model. Mainly, the switch from a finite to an infinite mean model gives a completely different asymptotic behaviour. Our main result proves a conjecture made in Barbe et al. [Barbe, P., Fougères, A.L., Genest, C., 2006. On the tail behavior of sums of dependent risks. ASTIN Bull. 36(2), 361-374]. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {164--169}, 
number = {2}, 
volume = {44}
}
@article{10.1016/j.cam.2018.03.038, 
year = {2018}, 
title = {{Computation of market risk measures with stochastic liquidity horizon}}, 
author = {Colldeforns-Papiol, Gemma and Ortiz-Gracia, Luis}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2018.03.038}, 
abstract = {{The Basel Committee of Banking Supervision has recently set out the revised standards for minimum capital requirements for market risk. The Committee has focused, among other things, on the two key areas of moving from Value-at-Risk (VaR) to Expected Shortfall (ES) and considering a comprehensive incorporation of the risk of market illiquidity by extending the risk measurement horizon. The estimation of the ES for several trading desks and taking into account different liquidity horizons is computationally very involved. We present a novel numerical method to compute the VaR and ES of a given portfolio within the stochastic holding period framework. Two approaches are considered, the delta–gamma approximation, for modelling the change in value of the portfolio as a quadratic approximation of the change in value of the risk factors, and some of the state-of-the-art stochastic processes for driving the dynamics of the log-value change of the portfolio like the Merton jump–diffusion model and the Kou model. Central to this procedure is the application of the SWIFT method developed for option pricing, that appears to be a very efficient and robust Fourier inversion method for risk management purposes. © 2018 Elsevier B.V.}}, 
pages = {431--450}, 
number = {NA}, 
volume = {342}
}
@article{10.21314/jrmv.2017.173, 
year = {2017}, 
title = {{New historical bootstrap value-at-risk model}}, 
author = {Radivojevic, Nikola and Sobat-Matic, Zorana and Mirjanic, Borjana B}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2017.173}, 
abstract = {{In this paper, the authors present a newvalue-at-risk (VaR) model for the estimation of market risk in banks and other financial institutions. The model is labeled a new historical bootstrapVaR model, since it shares the same theoretical basis as the historical simulation (HS) and bootstrap approaches. This paper aims to answer the question of whether incorporating the bootstrap method into the HS model contributes to improving the applicability of the HS approach in terms of meeting the backtesting rules of the Basel Accord. In order to obtain an answer to this question, we test the applicability and compare the performances of the HS500 and the autoregressive moving average-generalized autoregressive conditional heteroscedasticity-BootstrapHS500 (ARMA-GARCH-BootstrapHS500) models on the capital markets of Serbia, Croatia, Greece, Spain, Germany, Slovakia, the Czech Republic, Romania and Hungary. The results of our research show that the new model performed better than the HS model. Thus, we can conclude that incorporating the bootstrap approach into the HS approach contributes to improving the applicability of the HS approach. © 2017 Infopro Digital Risk (IP) Limited.}}, 
pages = {57--75}, 
number = {4}, 
volume = {11}
}
@article{10.1017/asb.2013.19, 
year = {2013}, 
title = {{Var-based optimal partial hedging}}, 
author = {Cong, Jianfa and Tan, Ken Seng and Weng, Chengguo}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2013.19}, 
abstract = {{Hedging is one of the most important topics in finance. When a financial market is complete, every contingent claim can be hedged perfectly to eliminate any potential future obligations. When the financial market is incomplete, the investor may eliminate his risk exposure by superhedging. In practice, both hedging strategies are not satisfactory due to their high implementation costs, which erode the chance of making any profit. A more practical and desirable strategy is to resort to the partial hedging, which hedges the future obligation only partially. The quantile hedging of Föllmer and Leukert (Finance and Stochastics, vol. 3, 1999, pp. 251-273), which maximizes the probability of a successful hedge for a given budget constraint, is an example of the partial hedging. Inspired by the principle underlying the partial hedging, this paper proposes a general partial hedging model by minimizing any desirable risk measure of the total risk exposure of an investor. By confining to the value-at-risk (VaR) measure, analytic optimal partial hedging strategies are derived. The optimal partial hedging strategy is either a knock-out call strategy or a bull call spread strategy, depending on the admissible classes of hedging strategies. Our proposed VaR-based partial hedging model has the advantage of its simplicity and robustness. The optimal hedging strategy is easy to determine. Furthermore, the structure of the optimal hedging strategy is independent of the assumed market model. This is in contrast to the quantile hedging, which is sensitive to the assumed model as well as the parameter values. Extensive numerical examples are provided to compare and contrast our proposed partial hedging to the quantile hedging. © 2013 astin Bulletin.}}, 
pages = {271--299}, 
number = {3}, 
volume = {43}
}
@article{10.47654/v24y2020i3p142-217, 
year = {2020}, 
title = {{Risk aversion: Differential conditions for the iso-utility curves with positive slope in transformed two-parameter distributions}}, 
journal = {Advances in Decision Sciences}, 
issn = {20903359}, 
doi = {10.47654/v24y2020i3p142-217}, 
abstract = {{The condition of Risk Aversion implies that the Utility Function must be concave. We take into account the dependence of the Utility Function on the return that has any type of two-parameter distribution; it is possible to define Risk and Target, the first one may be the Standard Deviation of the return and the last one usually is the Expected value of the return, as a generic function of these two parameters. Considering the 3D space of Risk, Target and Expected Utility, this paper determines the Differential Conditions for these three functions so that the Expected Utility Function depends decreasingly on Risk and increasingly on Target, that means the iso-utility curves have positive slope in the plane of Risk and Target. As a particular case, we discuss these conditions in the case of the CRRA Utility Function and the Truncated Normal distribution. Furthermore, different measures of Risk are chosen, as Value at Risk (VaR) and Expected Shortfall (ES), to verify if these measures maintain a positive slope of the Iso-utility curves in the Risk-Target plane. © 2020 Hindawi Limited. All rights reserved.}}, 
pages = {142--217}, 
number = {3}, 
volume = {24}
}
@article{10.1109/cit.2016.93, 
year = {2017}, 
title = {{A risk evaluation framework for service level agreements}}, 
author = {Yadranjiaghdam, Babak and Hotwani, Komal and Tabrizi, Nasseh}, 
journal = {2016 IEEE International Conference on Computer and Information Technology (CIT)}, 
issn = {NA}, 
doi = {10.1109/cit.2016.93}, 
abstract = {{Selecting the right cloud service provider is crucial to minimize the downtime/outage risk and prevent Service Level Agreement (SLA) violation. The main purpose of this paper is to introduce a framework to identify and control the associated risks using requirements, priorities, service level parameters, and cost in order to identify eligible cloud providers. The methodology uses Value at Risk (VaR) to measure the risk involved in cloud services associated with a service in SLA. It is expected that the results will assist customers in selecting right cloud service providers with minimum risk of cloud outages. © 2016 IEEE.}}, 
pages = {681--685}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jspi.2012.11.008, 
year = {2013}, 
title = {{Sparse moving maxima models for tail dependence in multivariate financial time series}}, 
author = {Tang, Rui and Shao, Jun and Zhang, Zhengjun}, 
journal = {Journal of Statistical Planning and Inference}, 
issn = {03783758}, 
doi = {10.1016/j.jspi.2012.11.008}, 
abstract = {{The multivariate maxima of moving maxima (M4) model has the potential to model both the cross-sectional and temporal tail-dependence for a rich class of multivariate time series. The main difficulty of applying M4 model to real data is due to the estimation of a large number of parameters in the model and the intractability of its joint likelihood. In this paper, we consider a sparse M4 random coefficient model (SM4R), which has a parsimonious number of parameters and it can potentially capture the major stylized facts exhibited by devolatized asset returns found in empirical studies. We study the probabilistic properties of the newly proposed model. Statistical inference can be made based on the Generalized Method of Moments (GMM) approach. We also demonstrate through real data analysis that the SM4R model can be effectively used to improve the estimates of the Value-at-Risk (VaR) for portfolios consisting of multivariate financial returns while ignoring either temporal or cross-sectional tail dependence could potentially result in serious underestimate of market risk. © 2012 Elsevier B.V.}}, 
pages = {882--895}, 
number = {5}, 
volume = {143}
}
@article{10.1111/j.1574-0862.2009.00408.x, 
year = {2009}, 
title = {{Integration of VaR and expected utility under departures from normality}}, 
author = {Barry, Peter J. and Sherrick, Bruce J. and Zhao, Jianmei}, 
journal = {Agricultural Economics}, 
issn = {01695150}, 
doi = {10.1111/j.1574-0862.2009.00408.x}, 
abstract = {{This article identifies the level of the expected utility (EU) risk aversion and Value-at-Risk (VaR) confidence level that yield the same choice from a given distribution of outcomes, and thus allow for consistent application of the two criteria. The result for a given distribution is an explicit mapping between risk aversion under EU and VaR, for both normal and nonnormal distributions. The Cornish-Fisher expansion is used to establish adjusted mean-deviates for nonnormal outcome distributions and the investor's preference function is expanded to include elements for variance, skewness, and excess kurtosis. A farm-level application with nonnormal revenue distribution illustrates these approaches. © 2009 International Association of Agricultural Economists.}}, 
pages = {691--699}, 
number = {6}, 
volume = {40}
}
@article{10.1515/snde-2019-0096, 
year = {2021}, 
title = {{A new bivariate Archimedean copula with application to the evaluation of VaR}}, 
author = {Guloksuz, Cigdem Topcu and Kumar, Pranesh}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2019-0096}, 
abstract = {{In this paper, a new generator function is proposed and based on this function a new Archimedean copula is introduced. The new Archimedean copula along with three representatives of Archimedean copula family which are Clayton, Gumbel and Frank copulas are considered as models for the dependence structure between the returns of two stocks. These copula models are used to simulate daily log-returns based on Monte Carlo (MC) method for calculating value at risk (VaR) of the financial portfolio which consists of two market indices, Ford and General Motor Company. The results are compared with the traditional MC simulation method with the bivariate normal assumption as a model of the returns. Based on the backtesting results, describing the dependence structure between the returns by the proposed Archimedean copula provides more reliable results over the considered models in calculating VaR of the studied portfolio. © 2020 Walter de Gruyter GmbH, Berlin/Boston 2020.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jedc.2018.03.016, 
year = {2018}, 
title = {{Improving daily Value-at-Risk forecasts: The relevance of short-run volatility for regulatory quality assessment}}, 
author = {Berger, Theo and Gençay, Ramazan}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/j.jedc.2018.03.016}, 
abstract = {{In this paper, we present a novel perspective on data filtering and present an innovative wavelet-based approach that leads to improved Value-at-Risk (VaR) forecasts. A separation of financial conditional volatility into short-, mid- and long-run components allows us to study the relevance of these frequency components with respect to a regulatory quality assessment for daily VaR forecasts. A simulation study and an analysis of daily market prices suggest that short- and mid-run information components cover the relevant information that is necessary for estimating adequate daily VaR. Excluding long-run information components reduces daily VaR forecasts by (up to) 4\% and does not impact the quality of regulatory back-testing. © 2018 Elsevier B.V.}}, 
pages = {30--46}, 
number = {NA}, 
volume = {92}
}
@article{10.1016/s0378-4266(02)00272-8, 
year = {2002}, 
title = {{Expected shortfall and beyond}}, 
author = {Tasche, Dirk}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00272-8}, 
abstract = {{Financial institutions have to allocate so-called economic capital in order to guarantee solvency to their clients and counterparties. Mathematically speaking, any methodology of allocating capital is a risk measure, i.e. a function mapping random variables to the real numbers. Nowadays value-at-risk (VaR), which is defined as a fixed level quantile of the random variable under consideration, is the most popular risk measure. Unfortunately, it fails to reward diversification, as it is not subadditive. In the search for a suitable alternative to VaR, expected shortfall (ES) (or conditional VaR or tail VaR) has been characterized as the smallest coherent and law invariant risk measure to dominate VaR. We discuss these and some other properties of ES as well as its generalization to a class of coherent risk measures which can incorporate higher moment effects. Moreover, we suggest a general method on how to attribute ES risk contributions to portfolio components. © 2002, Elsevier Science B.V. All rights reserved.}}, 
pages = {1519--1533}, 
number = {7}, 
volume = {26}
}
@article{10.1080/02331888.2021.1897984, 
year = {2021}, 
title = {{Transformed central quantile subspace}}, 
author = {Christou, Eliana}, 
journal = {Statistics}, 
issn = {02331888}, 
doi = {10.1080/02331888.2021.1897984}, 
abstract = {{Quantile regression (QR) is a well-established method of tail analysis. Application of QR can become very challenging when dealing with high-dimensional data, thus requiring dimension reduction techniques. While the current literature on these techniques focuses on extracting linear combinations of the predictor variables that contain all the information about the conditional quantile, non-linear features can potentially achieve greater dimension reduction. We, therefore, present the first application of transformed dimension reduction for conditional quantiles, which serves as an intermediate step between linear and nonlinear dimension reduction. The idea is to transform the predictors monotonically and then look for low-dimensional projections by applying linear dimension reduction techniques. The performance of the proposed methodology is demonstrated through simulation examples and a real data application. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--17}, 
number = {2}, 
volume = {55}
}
@article{10.1109/ebiss.2010.5473537, 
year = {2010}, 
title = {{Dynamic assessment and VaR-based quantification of information security risk}}, 
author = {Qi, Wenjing and Liu, Xue and Zhang, Jian and Yuan, Weihua}, 
journal = {2010 2nd International Conference on E-business and Information System Security}, 
issn = {NA}, 
doi = {10.1109/ebiss.2010.5473537}, 
abstract = {{Risk assessment and quantification is crucial to the effectiveness of information security measure deployed in an organization. A dynamic risk assessment process is presented in this paper to cope with the variation and diversity of threats in the information system. To give a clear perspective of the information risk without confusing by the complexity of so many risk factors, an risk quantification model and a VaR-based risk measure are presented, through which, risk can be represented by the prospective maximum daily loss under certain confidence level. We test our risk quantification model and measure in a real network environment. ©2010 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icnc.2010.5582625, 
year = {2010}, 
title = {{A new method for dynamic portfolio choice based on copulas}}, 
author = {Qifa, Xu and Jingdong, Liu and Shaojie, Liu}, 
journal = {2010 Sixth International Conference on Natural Computation}, 
issn = {NA}, 
doi = {10.1109/icnc.2010.5582625}, 
abstract = {{Multivariate volatility modeling is always a hot topic in academic research. It is difficult to consider how to construct multivariate joint distribution. Copulas, a statistic method, can be used to decompose multivariate joint distribution into marginal distribution and correlation structure. This advantage is applied into calculating the dynamic VaR of a portfolio in the paper. Furthermore, a new model for dynamic portfolio choice based on copulas is proposed, and empirical analysis is operated in the end. © 2010 IEEE.}}, 
pages = {2729--2733}, 
number = {NA}, 
volume = {5}
}
@article{10.1080/13518470500039121, 
year = {2006}, 
title = {{Stochastic volatility and GARCH: A comparison based on UK stock data}}, 
author = {Pederzoli, Chiara}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/13518470500039121}, 
abstract = {{This paper compares two types of volatility models for returns, ARCH-type and stochastic volatility (SV) models, both from a theoretical and an empirical point of view. In particular a GARCH(1,1) model, an EGARCH(1,1) model and a log-normal AR(1) stochastic volatility model are considered. The three models are estimated on UK stock data: a series of the British equity index FTSE100 is used to estimate the relevant parameters. Diagnostic tests are implemented to evaluate how well the models fit the data. The models are used to obtain daily volatility forecasts and these volatilities are used to estimate the "VaR on a simple one-unit position on FTSE100. The VaR accuracy is tested by means of a backtest. While the results do not lead to a straightforward preference between GARCH(1,1) and SV, the EGARCH shows the best performance.}}, 
pages = {41--59}, 
number = {1}, 
volume = {12}
}
@article{10.1016/j.insmatheco.2015.09.002, 
year = {2015}, 
title = {{A directional multivariate value at risk}}, 
author = {Torres, Raúl and Lillo, Rosa E. and Laniado, Henry}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.09.002}, 
eprint = {1502.00908}, 
abstract = {{In economics, insurance and finance, value at risk (VaR) is a widely used measure of the risk of loss on a specific portfolio of financial assets. For a given portfolio, time horizon, and probability α, the 100α\% VaR is defined as a threshold loss value, such that the probability that the loss on the portfolio over the given time horizon exceeds this value is α. That is to say, it is a quantile of the distribution of the losses, which has both good analytic properties and easy interpretation as a risk measure. However, its extension to the multivariate framework is not unique because a unique definition of multivariate quantile does not exist. In the current literature, the multivariate quantiles are related to a specific partial order considered in Rn, or to a property of the univariate quantile that is desirable to be extended to Rn. In this work, we introduce a multivariate value at risk as a vector-valued directional risk measure, based on a directional multivariate quantile, which has recently been introduced in the literature. The directional approach allows the manager to consider external information or risk preferences in her/his analysis. We derive some properties of the risk measure and we compare the univariate VaR over the marginals with the components of the directional multivariate VaR. We also analyze the relationship between some families of copulas, for which it is possible to obtain closed forms of the multivariate VaR that we propose. Finally, comparisons with other alternative multivariate VaR given in the literature, are provided in terms of robustness. © 2015 Elsevier B.V..}}, 
pages = {111--123}, 
number = {NA}, 
volume = {65}
}
@article{10.1007/s00186-005-0045-1, 
year = {2006}, 
title = {{Time consistent dynamic risk measures}}, 
author = {Boda, Kang and Filar, Jerzy A.}, 
journal = {Mathematical Methods of Operations Research}, 
issn = {14322994}, 
doi = {10.1007/s00186-005-0045-1}, 
abstract = {{We introduce the time-consistency concept that is inspired by the so-called "principle of optimality" of dynamic programming and demonstrate - via an example - that the conditional value-at-risk (CVaR) need not be time-consistent in a multi-stage case. Then, we give the formulation of the target-percentile risk measure which is time-consistent and hence more suitable in the multi-stage investment context. Finally, we also generalize the value-at-risk and CVaR to multi-stage risk measures based on the theory and structure of the target-percentile risk measure.}}, 
pages = {169--186}, 
number = {1}, 
volume = {63}
}
@article{10.1109/icicic.2007.526, 
year = {2007}, 
title = {{Soft approach for optimization in portfolio management}}, 
author = {Xu, Chunhui}, 
journal = {Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)}, 
issn = {NA}, 
doi = {10.1109/icicic.2007.526}, 
abstract = {{The present paper introduces the soft approach I advocated for solving complicated optimization models in portfolio management problems. ©2007 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eap.2019.03.002, 
year = {2019}, 
title = {{Value-at-risk methodologies for effective energy portfolio risk management}}, 
author = {Halkos, George E. and Tsirivis, Apostolos S.}, 
journal = {Economic Analysis and Policy}, 
issn = {03135926}, 
doi = {10.1016/j.eap.2019.03.002}, 
abstract = {{Research has shown that the prediction of future variance through advanced GARCH type models is essential for an effective energy portfolio risk management. Still there has been a failure to provide a clear view on the specific amount of capital that is at risk on behalf of the investor or any party directly affected by the price fluctuations of specific or multiple energy commodities. Thus, it is necessary for risk managers to make one further step, determining the most robust and effective approach that will enable them to precisely monitor and accurately estimate the portfolio's value-at-risk (VaR) which by definition, provides a good measure of the total actual amount at stake. Nevertheless, despite the variety of the variance models that have been developed and the range of various methodologies, most researchers have concluded that there is no model or specific methodology that outperforms all others. We find the best approach to minimize risk and accurately forecast the future potential losses is to adopt a methodology which takes into consideration the particular features which characterize the trade of energy products. © 2019 Economic Society of Australia, Queensland}}, 
pages = {197--212}, 
number = {NA}, 
volume = {62}
}
@article{10.1002/asmb.825, 
year = {2011}, 
title = {{Percentile residual life orders}}, 
author = {Franco‐Pereira, Alba M. and Lillo, Rosa E. and Romo, Juan and Shaked, Moshe}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.825}, 
abstract = {{In this paper we study a family of stochastic orders of random variables defined via the comparison of their percentile residual life functions. Some interpretations of these stochastic orders are given, and various properties of them are derived. The relationships to other stochastic orders are also studied. Finally, some applications in reliability theory and finance are described. Copyright © 2010 John Wiley \& Sons, Ltd.}}, 
pages = {235--252}, 
number = {3}, 
volume = {27}
}
@article{10.1016/j.eswa.2018.01.001, 
year = {2018}, 
title = {{An integrated inverse adaptive neural fuzzy system with Monte-Carlo sampling method for operational risk management}}, 
author = {Peña, Alejandro and Bonet, Isis and Lochmuller, Christian and Chiclana, Francisco and Góngora, Mario}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2018.01.001}, 
abstract = {{Operational risk refers to deficiencies in processes, systems, people or external events, which may generate losses for an organization. The Basel Committee on Banking Supervision has defined different possibilities for the measurement of operational risk, although financial institutions are allowed to develop their own models to quantify operational risk. The advanced measurement approach, which is a risk-sensitive method for measuring operational risk, is the financial institutions preferred approach, among the available ones, in the expectation of having to hold less regulatory capital for covering operational risk with this approach than with alternative approaches. The advanced measurement approach includes the loss distribution approach as one way to assess operational risk. The loss distribution approach models loss distributions for business-line-risk combinations, with the regulatory capital being calculated as the 99,9\% operational value at risk, a percentile of the distribution for the next year annual loss. One of the most important issues when estimating operational value at risk is related to the structure (type of distribution) and shape (long tail) of the loss distribution. The estimation of the loss distribution, in many cases, does not allow to integrate risk management and the evolution of risk; consequently, the assessment of the effects of risk impact management on loss distribution can take a long time. For this reason, this paper proposes a flexible integrated inverse adaptive fuzzy inference model, which is characterized by a Monte-Carlo behavior, that integrates the estimation of loss distribution and different risk profiles. This new model allows to see how the management of risk of an organization can evolve over time and it effects on the loss distribution used to estimate the operational value at risk. The experimental study results, reported in this paper, show the flexibility of the model in identifying (1) the structure and shape of the fuzzy input sets that represent the frequency and severity of risk; and (2) the risk profile of an organization. Therefore, the proposed model allows organizations or financial entities to assess the evolution of their risk impact management and its effect on loss distribution and operational value at risk in real time. © 2018 Elsevier Ltd}}, 
pages = {11--26}, 
number = {NA}, 
volume = {98}
}
@article{10.1016/j.jbankfin.2011.02.013, 
year = {2011}, 
title = {{The pernicious effects of contaminated data in risk management}}, 
author = {Frésard, Laurent and Pérignon, Christophe and Wilhelmsson, Anders}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2011.02.013}, 
abstract = {{Banks hold capital to guard against unexpected surges in losses and long freezes in financial markets. The minimum level of capital is set by banking regulators as a function of the banks'. own estimates of their risk exposures. As a result, a great challenge for both banks and regulators is to validate internal risk models. We show that a large fraction of US and international banks uses contaminated data when testing their models. In particular, most banks validate their market risk model using profit-and-loss (P/L) data that include fees and commissions and intraday trading revenues. This practice is inconsistent with the definition of the employed market risk measure. Using both bank data and simulations, we find that data contamination has dramatic implications for model validation and can lead to the acceptance of misspecified risk models. Moreover, our estimates suggest that the use of contaminated data can significantly reduce (market-risk induced) regulatory capital. © 2011 Elsevier B.V.}}, 
pages = {2569--2583}, 
number = {10}, 
volume = {35}
}
@article{10.1016/j.insmatheco.2005.02.006, 
year = {2005}, 
title = {{Risk measure and fair valuation of an investment guarantee in life insurance}}, 
author = {Barbarin, Jérome and Devolder, Pierre}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2005.02.006}, 
abstract = {{Investment guarantees are amongst the most important topics in the pricing and management of life insurance. Traditionally, two ways of analyzing the risk are possible: on the one hand, the financial approach based on risk-neutral measure and leading to option pricing and continuous hedging strategy and on the other hand, a more actuarial approach based on ruin probability and distribution of surplus. The purpose of this paper is to try to integrate these two approaches in the management of life insurance contracts with profits. First, we analyze in terms of value at risk and conditional value at risk the effect of putting an investment guarantee. This will be done in an ALM framework, based on different investment strategies of the insurer in terms of risk and matching between assets and liabilities. The liability side will be represented by a guaranteed technical rate; the asset side will be a mix of stocks, cash and bonds in a Gaussian environment with different matching strategies. Consequences of an investment choice in terms of ruin probability and level of solvency will be illustrated. In a second step, fair valuation principles are used in order to compute the market value of the contract and fix the participation rate of the contract. © 2005 Published by Elsevier B.V.}}, 
pages = {297--323}, 
number = {2 SPEC. ISS.}, 
volume = {37}
}
@article{10.1002/for.2719, 
year = {2020}, 
title = {{Value-at-risk forecasting via dynamic asymmetric exponential power distributions}}, 
author = {Ou, Lu and Zhao, Zhibiao}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2719}, 
abstract = {{In the value-at-risk (VaR) literature, many existing works assume that the noise distribution is the same over time. To take into account the potential time-varying dynamics of stock returns, we propose a dynamic asymmetric exponential distribution-based framework. The new method includes a time-varying shape parameter to control the dynamic shape of the distribution, a time-varying probability parameter to control the dynamic proportion of positive returns, and a time-varying scale parameter to control the dynamic volatility. We combine the generalized method of moments and the exponentially weighted moving average (EWMA) approach to derive specifications for these time-varying parameters. Empirical applications demonstrate the superior performance of the proposed method when compared with various GARCH and EWMA approaches without time variation in the innovations. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {291--300}, 
number = {NA}, 
volume = {NA}
}
@article{10.1515/snde-2019-0081, 
year = {2021}, 
title = {{A monitoring procedure for detecting structural breaks in factor copula models}}, 
author = {Manner, Hans and Stark, Florian and Wied, Dominik}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2019-0081}, 
abstract = {{We propose a new monitoring procedure based on moving sums (MOSUM) for detecting single or multiple structural breaks in factor copula models. The test compares parameter estimates from a rolling window to those from a historical data set and analyzes the behavior under the null hypothesis of no parameter change. The case of multiple breaks is also treated. In the model, the joint copula is given by the copula of random variables which arise from a factor model. This is particularly useful for analyzing high dimensional data. Parameters are estimated with the simulated method of moments (SMM). We analyze the behavior of the monitoring procedure in Monte Carlo simulations and a real data application. We consider an online procedure for predicting the day-ahead Value-at-risk based on the suggested monitoring procedure. © 2020 Walter de Gruyter GmbH, Berlin/Boston.}}, 
pages = {171--192}, 
number = {4}, 
volume = {25}
}
@article{10.1002/9781118650318.ch11, 
year = {2016}, 
title = {{On the Estimation of the Distribution of Aggregated Heavy-Tailed Risks: Application to Risk Measures}}, 
author = {Kratz, Marie}, 
issn = {NA}, 
doi = {10.1002/9781118650318.ch11}, 
abstract = {{The presence of heavy tails has been long recognized for financial and insurance data, which makes the gaussian distribution a poor approximation of the extreme risks distribution. The main objective of this study is to tackle this problem by, on one hand, obtaining the most accurate evaluations of the aggregated risks distribution and thus the risk measures used in solvency regulations, and, on the other hand, by providing practical solutions for estimating high quantiles of aggregated risks. In this chapter, we explore theoretically as well as numerically new approaches to handle this question, based on properties of upper order statistics and on trimmed sums. We show that these approaches compare very favorably to existing methods, for instance with the one based on the Generalized Central Limit Theorem. © 2017 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {239--282}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jrmv.2018.196, 
year = {2018}, 
title = {{A comprehensive evaluation of value-at-risk models and a comparison of their performance in emerging markets}}, 
author = {Shaker-Akhtekhane, Saeed and Seighali, Mohsen and Poorabbas, Solmaz}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2018.196}, 
abstract = {{This paper aims to evaluate the performance of different value-at-risk (VaR) calculation methods, allowing us to identify models that are valid for use in emerging markets. We apply several widely used methods for calculating VaR, including both parametric and nonparametric methods. We consider different confidence levels for the VaR as well as different sample sizes. To test our models' validity, we use both unconditional and conditional coverage backtests. In addition, we use a ranking method (which entails a backtesting approach based on the regulatory loss function) to appropriately compare the VaR calculation methods. Obtained from data for three different indexes (namely, Iranian, Turkish and Russian), our backtesting results indicate that parametric models from the generalized autoregressive conditional heteroscedasticity family, with asymmetric effects and fat tails (associated with their useof a t distribution), display the best performance. That is, the best-performing models under emerging market conditions are those that satisfy three important criteria simultaneously. First, they account for the time-varying variance. Second, they capture the asymmetric nature of shocks. Third, they are able to deal with fat tails in the distribution. These can also be regarded as the main features of emerging markets. © 2018 Infopro Digital Risk (IP) Limited.}}, 
pages = {1--16}, 
number = {4}, 
volume = {12}
}
@article{10.1109/tfuzz.2011.2144599, 
year = {2011}, 
title = {{Fuzzy-portfolio-selection models with value-at-risk}}, 
author = {Wang, Bo and Wang, Shuming and Watada, Junzo}, 
journal = {IEEE Transactions on Fuzzy Systems}, 
issn = {10636706}, 
doi = {10.1109/tfuzz.2011.2144599}, 
abstract = {{Based on fuzzy value-at-risk (VaR), this paper proposes a new portfolio-selection model (PSM) called the VaR-based fuzzy PSM (VaR-FPSM). Compared with the existing FPSMs, the VaR can directly reflect the greatest loss of a selected case under a given confidence level. In this study, when the security returns are taken as trapezoidal, triangular, and Gaussian fuzzy numbers, several crisp equivalent models of the VaR-FPSM are derived, which can be handled by any linear programming solvers. In general situations, an improved particle swarm optimization algorithm on the basis of fuzzy simulation is designed to search for the approximate optimal solutions. To illustrate the proposed model and the behavior of the improved particle swarm optimization algorithm, two numerical examples are provided, and the results are discussed. Furthermore, the proposed algorithm is compared with some existing approaches to fuzzy portfolio selection, such as the genetic algorithm and simulated annealing. © 2006 IEEE.}}, 
pages = {758--769}, 
number = {4}, 
volume = {19}
}
@article{10.1016/j.irfa.2019.04.005, 
year = {2019}, 
title = {{Backtesting VaR and ES under the magnifying glass}}, 
author = {Argyropoulos, Christos and Panopoulou, Ekaterini}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2019.04.005}, 
abstract = {{Backtesting provides the means of determining the accuracy of risk forecasts and the corresponding risk model. Given that the actual return generating process is unknown, the evaluation methods rely on various assumptions in order to quantify the models inefficiencies and proceed with the model evaluation. These method specific assumptions, in conjunction with the regulatory policies can introduce distortions in the evaluation process, which affect the reliability of the evaluation results. To investigate such effects from a practitioner's perspective, this paper reviews the major Value at Risk and Expected Shortfall forecast evaluation methods and evaluates their performance under a common simulation and financial application framework. Our findings suggest that focusing on specific individual hypothesis tests provides a more reliable alternative than the corresponding conditional coverage ones. In addition, selecting a two-year out-of-sample period provides a significantly better power to relevance ratio than the more relevant but powerless regulatory one-year specification. © 2019 Elsevier Inc.}}, 
pages = {22--37}, 
number = {NA}, 
volume = {64}
}
@article{10.1111/j.1540-6288.2009.00223.x, 
year = {2009}, 
title = {{The impact of large changes in asset prices on intra-market correlations in the domestic and international markets}}, 
author = {Ronn, Ehud I. and Sayrak, Akin and Tompaidis, Stathis}, 
journal = {Financial Review}, 
issn = {07328516}, 
doi = {10.1111/j.1540-6288.2009.00223.x}, 
abstract = {{We consider the impact of “large” changes in asset prices on intra-market correlations in domestic and international markets. Assuming normally distributed asset returns, we show that the absolute magnitude of the correlation, conditional on a change is greater than or equal to a given absolute size of one of the variables, is monotonically increasing in the magnitude of that absolute change. Empirical tests using domestic and international-market data support this theoretical result. These results have significant implications for portfolio management, hedging interest rate risk, tests of asset pricing models, Roll’s concern with asset pricing models’ explanatory power, and implementation of Value-at-Risk. © 2009, The Eastern Finance Association.}}, 
pages = {405--436}, 
number = {3}, 
volume = {44}
}
@article{10.18267/j.polek.514, 
year = {2005}, 
title = {{Comparison of approaches for value-at-risk estimation of foreign exchange portfolios [Porovnanie prístupov na výpočet hodnoty v riziku menových portfólií]}}, 
author = {Rimarčík, Marián}, 
journal = {Politická ekonomie}, 
issn = {00323233}, 
doi = {10.18267/j.polek.514}, 
abstract = {{In this paper historical performance of eleven approaches for estimation of one-day 95\% value-at-risk is evaluated. Random sample of 1000 foreign exchange portfolios consisting of positions in EUR and USD with CZK as a base currency was considered. Since foreign exchange portfolio consists of linear instruments, historical simulation and the variance-covariance method for VaR estimation were investigated. Performance of all approaches was evaluated using seven performance criteria. With minimal degree of simplification we can say that variance-covariance method using exponentially weighted averages with λ=0.94 is the best approach. This approach produces risk estimates which are systematically lowest ones and with high time volatility. It also achieves perfect coverage (95 \%) and the highest correlation between risk measure and absolute value of the outcome. RiskMetrics variance-covariance approach was dominant in spite of violation of normality assumption.}}, 
pages = {323--336}, 
number = {3}, 
volume = {53}
}
@article{10.1007/s12182-018-0279-1, 
year = {2019}, 
title = {{Risk measurement of international oil and gas projects based on the Value at Risk method}}, 
author = {Cheng, Cheng and Wang, Zhen and Liu, Ming-Ming and Ren, Xiao-Hang}, 
journal = {Petroleum Science}, 
issn = {16725107}, 
doi = {10.1007/s12182-018-0279-1}, 
abstract = {{International oil and gas projects feature high capital-intensity, high risks and contract diversity. Therefore, in order to help decision makers make more reasonable decisions under uncertainty, it is necessary to measure the risks of international oil and gas projects. For this purpose, this paper constructs a probabilistic model that is based on the traditional economic evaluation model, and introduces value at risk (VaR) which is a valuable risk measure tool in finance, and applies VaR to measure the risks of royalty contracts, production share contracts and service contracts of an international oil and gas project. Besides, this paper compares the influences of different risk factors on the net present value (NPV) of the project by using the simulation results. The results indicate: (1) risks have great impacts on the project’s NPV, therefore, if risks are overlooked, the decision may be wrong. (2) A simulation method is applied to simulate the stochastic distribution of risk factors in the probabilistic model. Therefore, the probability is related to the project’s NPV, overcoming the inherent limitation of the traditional economic evaluation method. (3) VaR is a straightforward risk measure tool, and can be applied to evaluate the risks of international oil and gas projects. It is helpful for decision making. © 2018, The Author(s).}}, 
pages = {199--216}, 
number = {1}, 
volume = {16}
}
@article{10.5267/j.ijiec.2014.6.003, 
year = {2014}, 
title = {{A simulation optimization approach to apply value at risk analysis on the inventory routing problem with backlogged demand}}, 
author = {Abdollahi, Mohammad and Arvan, Meysam and Omidvar, Aschkan and Ameri, Fatemeh}, 
journal = {International Journal of Industrial Engineering Computations}, 
issn = {19232926}, 
doi = {10.5267/j.ijiec.2014.6.003}, 
abstract = {{Inventory Routing Problem (IRP) is defined as the combination of vehicle routing, inventory management and delivery scheduling decisions. In this study, a model for a basic inventory routing problem is proposed, which controls the risk of exceeding capital dedicated to an IRP with a value at risk (VaR) measure. It is assumed that the structure of the basic model is one to many, the time horizon is single period and fleet composition is homogeneous. The objective of the model is to minimize the expected total cost over a planning horizon at the same time considering the affordable risk. Since a stochastic IRP is an NP-hard problem and considering VaR makes it more complex, it is not feasible to solve the large-scale problems through an exact model. Hence, a new simulation optimization procedure is proposed to solve the problem. Finally, a numerical example is presented to demonstrate its accuracy and applicability. © 2014 Growing Science Ltd. All rights reserved.}}, 
pages = {603--620}, 
number = {4}, 
volume = {5}
}
@article{10.1214/aoap/1019487360, 
year = {2000}, 
title = {{Conservative delta hedging}}, 
author = {Mykland, Per Aslak}, 
journal = {The Annals of Applied Probability}, 
issn = {10505164}, 
doi = {10.1214/aoap/1019487360}, 
abstract = {{It is common to have interval predictions for volatilities and other quantities governing securities prices. The purpose of this paper is to provide an exact method for converting such intervals into arbitrage based prices of financial derivatives or industrial or contractual options. We call this procedure conservative delta hedging. The proposed approach will permit an institution's management a greater oversight of its exposure to risk.}}, 
pages = {664--683}, 
number = {2}, 
volume = {10}
}
@article{10.1109/cinti.2010.5672272, 
year = {2010}, 
title = {{Neural fuzzy system for default forecasts}}, 
author = {Bozsik, József}, 
journal = {2010 11th International Symposium on Computational Intelligence and Informatics (CINTI)}, 
issn = {NA}, 
doi = {10.1109/cinti.2010.5672272}, 
abstract = {{A novel Neural Network Based Fuzzy Inference System for financial default forecast will be introduced. A wide range of financial forecasts is known. This method is focusing on the economical default forecast, but the method can be used generally for other financial forecasts as well, for example for calculating the Value at Risk. This hybrid method is combined by two classical methods: the artificial neural networks and fuzzy systems. In this article the structure of the hybrid method will be shown, the problems which occurred during the construction of the model and the solutions for the problems. The results of the model will be detailed and compared with the results of another financial default forecast model. The results and the reliability of the method will be analysed and it will be shown how the parameters can influence the reliability of the results. ©2010 IEEE.}}, 
pages = {69--74}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ejor.2017.07.055, 
year = {2018}, 
title = {{Risk tomography}}, 
author = {Prékopa, András and Lee, Jinwook}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2017.07.055}, 
abstract = {{New multivariate risk measures are introduced, suitable for optimal management of multidimensional assets. Risk is measured along lines through a given reference point in a multidimensional Euclidean space, and then maximum (minimum in financial planning) or mixture is taken with respect to lines lying in cones. We use VaR and CVaR as univariate risk measures but the construction allows for the use any of them. In some case numéraire is used to value the assets. Some of the new measures enjoy the coherence property for sums and also for composition, where assets are put together to form higher dimensional vectors. Numerical calculations of them are tractable as shown for certain multivariate distributions. Applications are presented for the agricultural industry using USDA database, as well as a financial portfolio problem using recent US stock market data. © 2017 Elsevier B.V.}}, 
pages = {149--168}, 
number = {1}, 
volume = {265}
}
@article{10.1109/pes.2007.385968, 
year = {2007}, 
title = {{Managing price risk while bidding in a multimarket environment}}, 
author = {Menniti, D. and Musmanno, R. and Scordino, N. and Sorrentino, N. and Violi, A.}, 
journal = {2007 IEEE Power Engineering Society General Meeting}, 
issn = {NA}, 
doi = {10.1109/pes.2007.385968}, 
abstract = {{In the deregulated marketplace, generation companies sell energy through auctions in a daily market. The daily-price volatility, together with the bids acceptance process uncertainty, make arise the need of performing risk assessment. A a multi-stage mixed-integer stochastic programming model with linear constraints, able to detect the profitability and risk of bidding in a multi auction energy market, is proposed for power producers. At this purpose, a particularly effective risk measure as the Conditional Value at Risk has been chosen: a discrete formulation has been proposed and evaluated for quantifying the risk of a producer portfolio. The intuitive scenario tree formulation has been adopted to represent the evolution of the random clearing prices and quantities. The Italian zonal pricing has been used as pricing system, to manage network congestions. The proposed model so provides suppliers with an efficient tool able to handle daily market-price uncertainties and to capture, in powerful aggregate risk measures, all relevant portfolio effects of market-risk exposure. Simulations are carried out in a 24-hour time frame on a representative test problem. © 2007 IEEE.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {NA}
}
@article{10.1504/ijram.2003.003827, 
year = {2003}, 
title = {{A model of optimal dynamic asset allocation in a Value-at-Risk framework}}, 
author = {Wang, Ching-Ping and Shyu, David and Liao, Y Chris and Chen, Ming-Chi and Chen, Miao-Ling}, 
journal = {International Journal of Risk Assessment and Management}, 
issn = {14668297}, 
doi = {10.1504/ijram.2003.003827}, 
abstract = {{This study focuses on the problem of investors in optimising dynamic asset allocation to maximise expected utility under the value-at-risk (VaR) constraint. Although Basak and Shapiro [1] presented this topic, they assumed a complete market and employed the martingale approach to determine a dynamic asset allocation strategy. However, a complete market does not exist in the real world and the martingale approach is not suitable for portfolio selection. Consequently, this study relaxes these limitations and firstly provides a solving method to derive the dynamic asset allocation under the VaR constraint. A simple case and a general case of derivation of optimal dynamic asset allocation are explored. A continuous probability distribution also can be approximated by the discrete probability distribution discussed in this study. © 2003 Inderscience Enterprises Ltd.}}, 
pages = {301}, 
number = {4}, 
volume = {4}
}
@article{10.1080/00036846.2020.1828566, 
year = {2021}, 
title = {{Correlation between Shanghai crude oil futures, stock, foreign exchange, and gold markets: a GARCH-vine-copula method}}, 
author = {He, Chaohua and Li, Guangchen and Fan, Hai and Wei, Weixian}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2020.1828566}, 
abstract = {{The relationship between markets has always been a topic of heated debate among scholars from various countries. One of the most important concerns is the need to model the relationships between the crude oil market and other markets. Based on daily return observations from 2018 to 2019, we apply a GARCH-vine-copula approach to probe the linkage between Shanghai crude oil futures, stock, foreign exchange, and gold markets. We find that obvious tail dependencies do exist between these markets. And the crude oil futures market occupies a dominant position. Moreover, when the Shanghai crude oil futures market is taken as the known condition, the links between different markets reduce to some extent. Finally, value at risk results denote that the risk of the Shanghai crude oil futures market is relatively high, but portfolio investment can effectively reduce the risk. Moreover, the model fitting results at different confidence levels have passed the Kupiec backtest, indicating that the model in this paper fits the relationship between these markets well. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--15}, 
number = {11}, 
volume = {53}
}
@article{10.1080/02664760802520785, 
year = {2009}, 
title = {{Archimedean copulae for risk measurement}}, 
author = {Luca, Giovanni De and Rivieccio, Giorgia}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664760802520785}, 
abstract = {{In this paper some Archimedean copula functions for bivariate financial returns are studied. The choice of this family is due to their ability to capture the tail dependence, which is an association measure we can detect in many bivariate financial time-series. A time-varying version of these copulae is also investigated. Finally, the Value-at-Risk is computed and its performance is compared across different copula specifications. © 2009 Taylor \& Francis.}}, 
pages = {907--924}, 
number = {8}, 
volume = {36}
}
@article{10.1016/j.spl.2020.108803, 
year = {2020}, 
title = {{Approximating sums of products of dependent random variables}}, 
author = {Gajek, Lesław and Krajewska, Elżbieta}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2020.108803}, 
abstract = {{Stochastic approximation of a given time series \{∑j=1 kXjYj\} by a linear combination of simpler sequences \{∑j=1 kXj\} and \{∑j=1 kYj\} is treated uniformly over k∈\{1,…,n\}. A maximal inequality is proven in order to find a sharp bound on Value-at-Risk of max1≤k≤n|∑j=1 kXjYj|. © 2020 The Authors}}, 
pages = {108803}, 
number = {NA}, 
volume = {164}
}
@article{10.1080/03461230701766882, 
year = {2008}, 
title = {{On finite-time ruin probabilities for classical risk models}}, 
author = {Lefèvre, Claude and Loisel, Stéphane}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461230701766882}, 
abstract = {{This paper examines the problem of ruin in the classical compound binomial and compound Poisson risk models. Our primary purpose is to extend to those models an exact formula derived by Picard \& Lefèvre (1997) for the probability of (non-)ruin within finite time. First, a standard method based on the ballot theorem and an argument of Seal-type provides an initial (known) formula for that probability. Then, a concept of pseudo-distributions for the cumulated claim amounts, combined with some simple implications of the ballot theorem, leads to the desired formula. Two expressions for the (non-)ruin probability over an infinite horizon are also deduced as corollaries. Finally, an illustration within the framework of Solvency II is briefly presented. © 2008 Taylor \& Francis.}}, 
pages = {41--60}, 
number = {1}, 
volume = {NA}
}
@article{10.1145/1143997.1144129, 
year = {2006}, 
title = {{Comparison of multi-objective evolutionary algorithms in optimizing combinations of reinsurance contracts}}, 
author = {Oesterreicher, Ingo and Mitschele, Andreas and Schlottmann, Frank and Seese, Detlef}, 
journal = {Proceedings of the 8th annual conference on Genetic and evolutionary computation - GECCO '06}, 
issn = {NA}, 
doi = {10.1145/1143997.1144129}, 
abstract = {{Our paper concerns optimal combinations of different types of reinsurance contracts. We introduce a novel approach based on the Mean-Variance-Criterion to solve this task. Two state-of-the-art MOEAs are used to perform an optimization of yet unresolved problem instances. In addition to that, we focus on finding a dense set of solutions to derive analogies to theoretic results of easier problem instances.}}, 
pages = {747--748}, 
number = {NA}, 
volume = {1}
}
@article{10.3303/cet1651106, 
year = {2016}, 
title = {{Stock market risk measurement method based on improved genetic algorithm}}, 
author = {}, 
issn = {22839216}, 
doi = {10.3303/cet1651106}, 
abstract = {{In this paper, we concentrate on the problem of stock market risk measurement, which is of great importance for the healthy development of the stock market. The main innovations of this paper lie in that 1) we introduce the GARCH model in stock market risk measuring, and 2) we utilize the Value-at-Risk (VaR) as the stock market risk measure. In order to solve the volatility prediction problem, GARCH model is developed to allow for a great more flexible lag structure and VaR is defined as the loss associated with the low percentile of the return distribution. As the performance of the GARCH model highly depends on the parameter selection, we propose an improved genetic algorithm to estimate optimal parameters for the GARCH model. Finally, to test the effectiveness of the proposed GA-GARCH algorithm, we choose Shanghai composite index and Shenzhen Compositional Index to make performance evaluation. Experimental results demonstrate that proposed GA-GARCH model can effectively cover the stock market risk. Copyright © 2016, AIDIC Servizi S.r.l.}}, 
number = {NA}, 
volume = {51}
}
@article{10.1016/s0378-4266(03)00127-4, 
year = {2004}, 
title = {{A new approach to modeling the dynamics of implied distributions: Theory and evidence from the S\&P 500 options}}, 
author = {Panigirtzoglou, Nikolaos and Skiadopoulos, George}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(03)00127-4}, 
abstract = {{This paper presents a new approach to modeling the dynamics of implied distributions. First, we obtain a parsimonious description of the dynamics of the S\&P 500 implied cumulative distribution functions by applying principal components analysis. Subsequently, we develop new arbitrage-free Monte Carlo simulation methods that model the evolution of the whole distribution through time as a diffusion process. Our approach generalizes the conventional approaches of modeling only the first two moments as diffusion processes, and it has important implications for "smile-consistent" option pricing and for risk management. The out-of-sample performance within a Value-at-Risk framework is examined. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {1499--1520}, 
number = {7}, 
volume = {28}
}
@article{10.3390/econometrics6040047, 
year = {2018}, 
title = {{Interval estimation of value-at-risk based on nonparametric models}}, 
author = {Khraibani, Hussein and Nehme, Bilal and Strauss, Olivier}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics6040047}, 
abstract = {{Value-at-Risk (VaR) has become the most important benchmark for measuring risk in portfolios of different types of financial instruments. However, as reported by many authors, estimating VaR is subject to a high level of uncertainty. One of the sources of uncertainty stems from the dependence of the VaR estimation on the choice of the computation method. As we show in our experiment, the lower the number of samples, the higher this dependence. In this paper, we propose a new nonparametric approach called maxitive kernel estimation of the VaR. This estimation is based on a coherent extension of the kernel-based estimation of the cumulative distribution function to convex sets of kernel. We thus obtain a convex set of VaR estimates gathering all the conventional estimates based on a kernel belonging to the above considered convex set. We illustrate this method in an empirical application to daily stock returns. We compare the approach we propose to other parametric and nonparametric approaches. In our experiment, we show that the interval-valued estimate of the VaR we obtain is likely to lead to more careful decision, i.e., decisions that cannot be biased by an arbitrary choice of the computation method. In fact, the imprecision of the obtained interval-valued estimate is likely to be representative of the uncertainty in VaR estimate. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {47}, 
number = {4}, 
volume = {6}
}
@article{10.1007/s10182-016-0270-3, 
year = {2017}, 
title = {{How risky is the optimal portfolio which maximizes the Sharpe ratio?}}, 
author = {Bodnar, Taras and Zabolotskyy, Taras}, 
journal = {AStA Advances in Statistical Analysis}, 
issn = {18638171}, 
doi = {10.1007/s10182-016-0270-3}, 
abstract = {{In this paper, we investigate the properties of the optimal portfolio in the sense of maximizing the Sharpe ratio (SR) and develop a procedure for the calculation of the risk of this portfolio. This is achieved by constructing an optimal portfolio which minimizes the Value-at-Risk (VaR) and at the same time coincides with the tangent (market) portfolio on the efficient frontier which is related to the SR portfolio. The resulting significance level of the minimum VaR portfolio is then used to determine the risk of both the market portfolio and the corresponding SR portfolio. However, the expression of this significance level depends on the unknown parameters which have to be estimated in practice. It leads to an estimator of the significance level whose distributional properties are investigated in detail. Based on these results, a confidence interval for the suggested risk measure of the SR portfolio is constructed and applied to real data. Both theoretical and empirical findings document that the SR portfolio is very risky since the corresponding significance level is smaller than 90 \% in most of the considered cases. © 2016, Springer-Verlag Berlin Heidelberg.}}, 
pages = {1--28}, 
number = {1}, 
volume = {101}
}
@article{10.1016/j.amc.2020.125351, 
year = {2020}, 
title = {{Model-free computation of risk contributions in credit portfolios}}, 
author = {Leitao, Álvaro and Ortiz-Gracia, Luis}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2020.125351}, 
abstract = {{In this work, we propose a non-parametric density estimation technique for measuring the risk in a credit portfolio, aiming at efficiently computing the marginal risk contributions. The novel method is based on wavelets, and we derive closed-form expressions to calculate the Value-at-Risk (VaR), the Expected Shortfall (ES) as well as the individual risk contributions to VaR (VaRC) and ES (ESC). We consider the multi-factor Gaussian and t-copula models for driving the defaults. The results obtained along the numerical experiments show the impressive accuracy and speed of this method when compared with crude Monte Carlo simulation. The presented methodology applies in the same manner regardless of the used model, and the computational performance is invariant under a considerable change in the dimension of the selected model. The speed-up with respect to the classical Monte Carlo approach ranges from twenty-five to one-thousand depending on the used model. © 2020 Elsevier Inc.}}, 
pages = {125351}, 
number = {NA}, 
volume = {382}
}
@article{10.1002/asmb.671, 
year = {2007}, 
title = {{Stochastic optimization for allocation problems with shortfall risk constraints}}, 
author = {Casarin, Roberto and Billio, Monica}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.671}, 
abstract = {{One of the crucial aspects in asset allocation problems is the assumption concerning the probability distribution of asset returns. Financial managers generally suppose normal distribution, even if extreme realizations usually have an higher frequency than in the Gaussian case. The aim of this paper is to propose a general Monte Carlo simulation approach able to solve an asset allocation problem with shortfall constraint, and to evaluate the exact portfolio risk-level when managers assume a misspecified return behaviour. We assume that returns are generated by a multivariate skewed Student-t distribution where each marginal can have different degrees of freedom. The stochastic optimization allows us to value the effective risk for managers. In the empirical application we consider a symmetric and heterogeneous case, and interestingly note that a multivariate Student-t with heterogeneous marginal distributions produces in the optimization problem a shortfall probability and a shortfall return level that can be adequately approximated by assuming a multivariate Student-t with common degrees of freedom. Thus, the proposed simulation-based approach could be an important instrument for investors who require a qualitative assessment of the reliability and sensitivity of their investment strategies in the case their models could be potentially misspecified. Copyright © 2007 John Wiley \& Sons, Ltd.}}, 
pages = {247--271}, 
number = {3}, 
volume = {23}
}
@article{10.1109/ccdc.2010.5499021, 
year = {2010}, 
title = {{CVaR-based decision-making model for interruptible load management}}, 
author = {An, Xuena and Zhang, Shaohua and Wang, Xian}, 
journal = {2010 Chinese Control and Decision Conference}, 
issn = {NA}, 
doi = {10.1109/ccdc.2010.5499021}, 
abstract = {{Integration of risk factors into decision-making model for interruptible load management plays a valuable role for power suppliers to mitigate price risks and improve their profitability in electricity market environment. A decision-making model is developed for power suppliers to determine the interruptible load volume purchased from interruptible consumers. The power suppliers' tradeoff between the wholesale market and interruptible load is considered as a portfolio selection problem. The conditional value-at-risk is employed to describe and measure the risk faced by power suppliers. In addition, a risk constraint is added to the decision-making model to take into account the risk preference of power suppliers. The Monte Carlo simulation-based numerical examples are presented to validate the reasonableness and effectiveness of the proposed model. It is also shown that the more averse to the risk, the larger volume of interruptible load the power supplier will choose. Furthermore, in order to hedge the wholesale market risk, the volume of interruptible load increases with increasing uncertainty in the wholesale market price. ©2010 IEEE.}}, 
pages = {448--451}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11408-012-0192-3, 
year = {2012}, 
title = {{Any regulation of risk increases risk}}, 
author = {Maymin, Philip Z. and Maymin, Zakhar G.}, 
journal = {Financial Markets and Portfolio Management}, 
issn = {15554961}, 
doi = {10.1007/s11408-012-0192-3}, 
abstract = {{We show that any objective risk measurement algorithm mandated by central banks for regulated financial entities will result in more risk being taken by those financial entities than would otherwise be the case. Furthermore, the risks taken by the regulated financial entities are far more systemically concentrated than they would have been otherwise, making the entire financial system more fragile. This result leaves three options for the future of financial regulation: (1) continue regulating by enforcing risk measurement algorithms at the cost of occasional severe crises, (2) regulate more severely and subjectively by fully nationalizing all financial entities, or (3) abolish all central banking regulations, including deposit insurance, thus allowing risk to be determined by the entities themselves and, ultimately, by their depositors through voluntary market transactions, rather than by the taxpayers through enforced government participation. © 2012 Swiss Society for Financial Market Research.}}, 
pages = {299--313}, 
number = {3}, 
volume = {26}
}
@article{10.5897/ijps11.660, 
year = {2011}, 
title = {{A study of electricity market volatility using long memory heteroscedastic model}}, 
author = {Cheong, Chin Wen}, 
journal = {AFRICAN JOURNAL OF BUSINESS MANAGEMENT}, 
issn = {19921950}, 
doi = {10.5897/ijps11.660}, 
abstract = {{An accurate wholesale electricity market forecast has become an essential tool in bidding and hedging strategies in competitive electricity markets. This paper provides a dynamic asymmetric long memory heteroscedastic model to account the high volatile daily wholesale electricity markets in New England and Louisiana. This model implemented power Cox-Box transformation (Tse, 1998) under the Chung's (1999) model specification to the time-varying volatility. The model is able to capture various empirical stylized facts that commonly observed in electricity markets including clustering volatility, news impact, heavy-tailed and long memory volatility. Under the forecast evaluations, the long memory model outperformed the traditional model in all the forecast time-horizons. Finally, the outcome of the analysis is further applied in quantifying the market risk in term of value-at-risk. © 2011 Academic Journals.}}, 
number = {31}, 
volume = {6}
}
@article{10.1016/j.jfi.2006.12.004, 
year = {2007}, 
title = {{Optimal capital allocation using RAROC™ and EVA®}}, 
author = {Stoughton, Neal M. and Zechner, Josef}, 
journal = {Journal of Financial Intermediation}, 
issn = {10429573}, 
doi = {10.1016/j.jfi.2006.12.004}, 
abstract = {{Equity capital allocation plays a particularly important role for financial institutions such as banks, who issue equity infrequently but have continuous access to debt capital. In such a context this paper shows that EVA and RAROC based capital budgeting mechanisms have economic foundations. We derive optimal capital allocation under asymmetric information and in the presence of outside managerial opportunities for an institution with a risky and a riskless division. It is shown that the results extend in a consistent manner to the multidivisional case of decentralized investment decisions with a suitable redefinition of economic capital. The decentralization leads to a charge for economic capital based on the division's own realized risk. Outside managerial opportunities increase the usage of capital and lead to overinvestment in risky projects; at the same time more capital is raised but risk limits are binding in more states. An institution with a single risky division should base its hurdle rate for capital allocated on the cost of debt. In contrast, the hurdle rate tends to the cost of equity for a diversified multidivisional firm. The analysis shows that hurdle rates have a common component in contrast to the standard perfect markets result with division-specific hurdle rates. © 2007 Elsevier Inc. All rights reserved.}}, 
pages = {312--342}, 
number = {3}, 
volume = {16}
}
@article{10.1016/j.jedc.2017.07.002, 
year = {2017}, 
title = {{Impact of value-at-risk models on market stability}}, 
author = {Llacay, Bàrbara and Peffer, Gilbert}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/j.jedc.2017.07.002}, 
abstract = {{Financial institutions around the world use value-at-risk (VaR) models to manage their market risk and calculate their capital requirements under Basel Accords. VaR models, as any other risk management system, are meant to keep financial institutions out of trouble by, among other things, guiding investment decisions within established risk limits so that the viability of a business is not put unduly at risk in a sharp market downturn. However, some researchers have warned that the widespread use of VaR models creates negative externalities in financial markets, as it can feed market instability and result in what has been called endogenous risk, that is, risk caused and amplified by the system itself, rather than being the result of an exogenous shock. This paper aims at analyzing the potential of VaR systems to amplify market disturbances with an agent-based model of fundamentalist and technical traders which manage their risk with a simple VaR model and must reduce their positions when the risk of their portfolio goes above a given threshold. We analyse the impact of the widespread use of VaR systems on different financial instability indicators and confirm that VaR models may induce a particular price dynamics that rises market volatility. These dynamics, which we have called `VaR cycles’, take place when a sufficient number of traders reach their VaR limit and are forced to simultaneously reduce their portfolio; the reductions cause a sudden price movement, raise volatility and force even more traders to liquidate part of their positions. The model shows that market is more prone to suffer VaR cycles when investors use a short-term horizon to calculate asset volatility or a not-too-extreme value for their risk threshold. © 2017 Elsevier B.V.}}, 
pages = {223--256}, 
number = {NA}, 
volume = {82}
}
@article{10.1016/j.insmatheco.2013.05.007, 
year = {2013}, 
title = {{Simple risk measure calculations for sums of positive random variables}}, 
author = {Guillén, Montserrat and Sarabia, José María and Prieto, Faustino}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.05.007}, 
abstract = {{Closed-form expressions for basic risk measures, such as value-at-risk and tail value-at-risk, are given for a family of statistical distributions that are specially suitable for right-skewed positive random variables. This is useful for risk aggregation in many insurance and financial applications that model positive losses, where the Gaussian assumption is not valid. Our results provide a direct and flexible parametric approach to multivariate risk quantification, for sums of correlated positive loss distributions, that can be readily implemented in a spreadsheet. © 2013 Elsevier B.V.}}, 
pages = {273--280}, 
number = {1}, 
volume = {53}
}
@article{10.1016/j.jbankfin.2018.01.002, 
year = {2018}, 
title = {{Multinomial VaR backtests: A simple implicit approach to backtesting expected shortfall}}, 
author = {Kratz, Marie and Lok, Yen H. and McNeil, Alexander J.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2018.01.002}, 
abstract = {{Under the Fundamental Review of the Trading Book, capital charges are based on the coherent Expected Shortfall (ES) risk measure, which is sensitive to tail risk. We argue that backtesting of the forecasting models used to derive ES can be based on a multinomial test of Value-at-Risk (VaR) exceptions at several levels. Using simulation experiments with heavy-tailed distributions and GARCH volatility models, we design a statistical procedure to show that at least four VaR levels are required to obtain tests for misspecified trading book models that are more powerful than single-level (or even two-level) binomial exception tests. A traffic-light system for model approval is proposed and illustrated with three real-data examples spanning the 2008 financial crisis. © 2018 Elsevier B.V.}}, 
pages = {393--407}, 
number = {NA}, 
volume = {88}
}
@article{10.1080/13518470902853566, 
year = {2009}, 
title = {{From Markowitz to modern risk management}}, 
author = {Alexander, Gordon J.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/13518470902853566}, 
abstract = {{Nobel Laureate Harry Markowitz is often referred to as the 'founder of Modern portfolio theory' and deservedly so given his enormous influence on the money management industry. However, it is my contention that he should also be referred to as the 'founder of Modern Risk Management' since his contributions to portfolio theory formed the basis for how risk is currently viewed and managed. More specifically, Markowitz argued that a portfolio of securities should be viewed through the lens of statistics where the probability distribution of its rate of return is evaluated in terms of its expected value and standard deviation. Since the ultimate selection of a portfolio involves the evaluation and management of risk as measured by standard deviation, it is clear that Markowitz's process of portfolio selection represents the birth of modern risk management whereby risk is quantified and controlled. In this paper, I will first, introduce value-at-risk as a measure of risk and how it relates to standard deviation, the risk measure at the heart of the model of Markowitz. Second, I will similarly introduce conditional value-at-risk (also known as expected shortfall) as a measure of risk and compare it with VaR. Third, I will briefly introduce stress testing as a supplemental means of controlling risk and will then present my conclusions. © 2009 Taylor \& Francis.}}, 
pages = {451--461}, 
number = {5-6}, 
volume = {15}
}
@article{10.1007/s00500-021-05638-z, 
year = {2021}, 
title = {{Improved multiobjective bat algorithm for the credibilistic multiperiod mean-VaR portfolio optimization problem}}, 
author = {Jiang, Manrui and Liu, Weiyi and Xu, Wen and Chen, Wei}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-021-05638-z}, 
abstract = {{This paper deals with a multiperiod multiobjective fuzzy portfolio selectiossn problem based on credibility theory. A credibilistic multiobjective mean-VaR model is formulated for the multiperiod portfolio selection problem, whereby the return is quantified by the credibilistic mean and the risk is measured by the credibilistic VaR. We also consider liquidity, cardinality, and upper and lower bound constraints to obtain a more realistic model. Furthermore, to solve the proposed model efficiently, an improved multiobjective bat algorithm termed IMBA is designed, in which three new strategies, i.e., the global best solution selection strategy, candidate solution generation strategy, and competitive learning strategy, are proposed to increase the convergence speed and improve the solution quality. Finally, comparative experiments are presented to show the applicability and superiority of the proposed approaches from two aspects. First, the designed IMBA is compared with seven typical algorithms, i.e., multiobjective particle swarm optimization, multiobjective artificial bee colony, multiobjective firefly algorithm, multiobjective differential evolution, multiobjective bat, the non-dominated sorting genetic algorithm (NSGA-II) and strength pareto evolutionary algorithm 2 (SPEA2), on a number of benchmark test problems. Second, the applicability of the proposed model to practical applications of portfolio selection is given under different circumstances. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.}}, 
pages = {6445--6467}, 
number = {8}, 
volume = {25}
}
@article{10.3934/jimo.2012.8.531, 
year = {2012}, 
title = {{Optimal investment with a value-at-risk constraint}}, 
author = {Liu, Jingzhen and Bai, Lihua and Yiu, Ka-Fai Cedric}, 
journal = {Journal of Industrial and Management Optimization}, 
issn = {15475816}, 
doi = {10.3934/jimo.2012.8.531}, 
abstract = {{We consider constrained investment problem with the objective of minimizing the ruin probability. In this paper, we formulate the cash reserve and investment model for the insurance company and analyze the value-at-risk (VaR) in a short time horizon. For risk regulation, we impose it as a risk constraint dynamically. Then the problem becomes minimizing the probability of ruin together with the imposed risk constraint. By solving the corresponding Hamilton-Jacobi-Bellman equations, we derive analytic expressions for the optimal value function and the corresponding optimal strategies. Looking at the value-at-risk alone, we show that it is possible to reduce the overall risk by an increased exposure to the risky asset. This is aligned with the risk diversification effect for negative correlated or uncorrelated risky asset with the stochastic of the fundamental insurance business. Moreover, studying the optimal strategies, we find that a different investment strategy will be in place depending on the Sharpe ratio of the risky asset.}}, 
pages = {531--547}, 
number = {3}, 
volume = {8}
}
@article{10.1109/wsc.2017.8247963, 
year = {2017}, 
title = {{A sequential elimination approach to value-at-risk and conditional value-at-risk selection}}, 
author = {Hepworth, Adam J. and Atkinson, Michael P. and Szechtman, Roberto}, 
journal = {2017 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2017.8247963}, 
abstract = {{Conditional Value-at-Risk (CVaR) is a widely used metric of risk in portfolio analysis, interpreted as the expected loss when the loss is larger than a threshold defined by a quantile. This work is motivated by situations where the CVaR is given, and the objective is to find the portfolio with the largest or smallest quantile that meets the CVaR constraint. We define our problem within the classic stochastic multi-armed bandit (MAB) framework, and present two algorithms. One that can be used to find the portfolio with largest or smallest loss threshold that satisfies the CVaR constraint with high probability, and another that determines the portfolio with largest or smallest probability of exceeding a loss threshold implied by a CVaR constraint, also at some desired probability level. © 2017 IEEE.}}, 
pages = {2324--2335}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/02102412.2019.1622066, 
year = {2020}, 
title = {{Do investors obtain better results selecting mutual funds through quantitative ratings? [La selección de fondos de inversión mediante ratings cuantitativos proporciona mejores resultados a los inversores]}}, 
author = {Otero-González, Luis and Santomil, Pablo Durán and Correia-Domingues, Renato Heitor}, 
journal = {Spanish Journal of Finance and Accounting / Revista Española de Financiación y Contabilidad}, 
issn = {02102412}, 
doi = {10.1080/02102412.2019.1622066}, 
abstract = {{The objective of this paper is to analyse if ratings are reliable tools in selecting mutual funds. Our sample contains the European equity funds rated by Morningstar from 2003 to 2014. Our conclusions support the ability of ratings to predict future performance in the short and medium term. In this sense, we have found that on average, funds with lower ratings have a worse out of sample performance in terms of risk-adjusted measures and annual return. The strongest predictability is observed for one year ahead but is also good for the three-years period. The inclusion of costs and other variables like turnover, age and size, reflect the importance of considering other factors to explain future performance. Finally, the best ratings have a better behaviour in terms of VaR (value at risk) showing that investment in good rated funds can better help to preserve investors’ wealth under unfavourable market conditions. © 2019 Asociación Española de Contabilidad y Administración de Empresas (AECA).}}, 
pages = {1--27}, 
number = {3}, 
volume = {49}
}
@article{10.1016/j.jempfin.2019.06.001, 
year = {2019}, 
title = {{Sovereign bond-backed securities: A VAR-for-VaR and marginal expected shortfall assessment}}, 
author = {Perea, Maite De Sola and Dunne, Peter G. and Puhl, Martin and Reininger, Thomas}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2019.06.001}, 
abstract = {{The risk reducing benefits of the sovereign bond-backed security (SBBS) proposal of Brunnermeier et al. (2016) have been assessed in terms of the likely losses that different kinds of holders would suffer under simulated default scenarios. However, the effects of mark-to-market losses that may occur when there is rising uncertainty about defaults, or when self-fulfilling destabilising dynamics are prevalent, have not yet been examined. We apply the “VAR-for-VaR” method of Manganelli et al. (2015) and the Marginal Expected Shortfall (MES) approach of Brownlees and Engle (2012, 2017) to estimated yields of SBBS to assess how ex ante exposures and marginal contributions to systemic risk are likely to play-out for different SBBS tranches under various securitisation structures. We compare these with exposures/MES of single sovereigns and a diversified portfolio of sovereigns. We find that the senior SBBS has extremely low ex ante tail risk and that, like the low-risk sovereigns, it acts as a hedge against extreme market-wide yield movements. The mezzanine SBBS has tail risk exposure similar to that of Italian and Spanish bonds. Yields on SBBS appear to be adequate compensation for their risks when compared with single sovereigns or a diversified portfolio. © 2019 Elsevier B.V.}}, 
pages = {33--52}, 
number = {NA}, 
volume = {53}
}
@article{10.1080/00949655.2011.563239, 
year = {2012}, 
title = {{The use of Jeffreys priors for the Student-t distribution}}, 
author = {Ho, Kwok-Wah}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2011.563239}, 
abstract = {{The Jeffreys-rule prior and the marginal independence Jeffreys prior are recently proposed in Fonseca et al. [Objective Bayesian analysis for the Student-t regression model, Biometrika 95 (2008), pp. 325-333] as objective priors for the Student-t regression model. The authors showed that the priors provide proper posterior distributions and perform favourably in parameter estimation. Motivated by a practical financial risk management application, we compare the performance of the two Jeffreys priors with other priors proposed in the literature in a problem of estimating high quantiles for the Student-t model with unknown degrees of freedom. Through an asymptotic analysis and a simulation study, we show that both Jeffreys priors perform better in using a specific quantile of the Bayesian predictive distribution to approximate the true quantile. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1015--1021}, 
number = {7}, 
volume = {82}
}
@article{10.1142/s0219024910005875, 
year = {2010}, 
title = {{The VAR at risk}}, 
author = {GALICHON, ALFRED}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024910005875}, 
eprint = {2102.02577}, 
abstract = {{I show that the structure of the firm is not neutral with respect to regulatory capital budgeted under rules which are based on the Value-at-Risk. Indeed, when a holding company has the liberty to divide its risk into as many subsidiaries as needed, and when the subsidiaries are subject to capital requirements according to the Value-at-Risk budgeting rule, then there is an optimal way to divide risk which is such that the total amount of capital to be budgeted by the shareholder is zero. This result may lead to regulatory arbitrage by some firms. © 2010 World Scientific Publishing Company.}}, 
pages = {503--506}, 
number = {4}, 
volume = {13}
}
@article{10.24818/18423264/54.1.20.03, 
year = {2020}, 
title = {{Weather risk management in the weather-var approach. Assumptions of value-at-risk modeling}}, 
author = {YURIY, BILAN and GRZEGORZ, MENTEL and DALIA, STREIMIKIENE and BEATA, SZETELA}, 
journal = {ECONOMIC COMPUTATION AND ECONOMIC CYBERNETICS STUDIES AND RESEARCH}, 
issn = {0424267X}, 
doi = {10.24818/18423264/54.1.20.03}, 
abstract = {{In this paper, an attempt is made to implement market-based risk measures in the process of weather risk management. A weather-VaR plays a significant role in the risk evaluation of non-extreme weather events and the process of its management, even in terms of weather derivatives. The innovative nature of the work results from the approach to model the weather factor as a "causative" instrument based on the specific historical data and not specific knowledge that typical weather forecasters have. The use of the bootstrap method to verify the indications of the VaR method is another advantage of the presented model. The obtained additional confidence interval is strengthening the VaR indications. The implementation of the weather VaR concept to derivative valuation may significantly influence the market of forward-looking weather contracts. © 2020, Bucharest University of Economic Studies. All rights reserved.}}, 
pages = {31--48}, 
number = {1}, 
volume = {54}
}
@article{10.1080/14697688.2019.1588469, 
year = {2019}, 
title = {{Estimation of risk contributions with MCMC}}, 
author = {Koike, Takaaki and Minami, Mihoko}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2019.1588469}, 
abstract = {{Determining risk contributions of unit exposures to portfolio-wide economic capital is an important task in financial risk management. Computing risk contributions involves difficulties caused by rare-event simulations. In this study, we address the problem of estimating risk contributions when the total risk is measured by value-at-risk (VaR). Our proposed estimator of VaR contributions is based on the Metropolis-Hasting (MH) algorithm, which is one of the most prevalent Markov chain Monte Carlo (MCMC) methods. Unlike existing estimators, our MH-based estimator consists of samples from the conditional loss distribution given a rare event of interest. This feature enhances sample efficiency compared with the crude Monte Carlo method. Moreover, our method has consistency and asymptotic normality, and is widely applicable to various risk models having a joint loss density. Our numerical experiments based on simulation and real-world data demonstrate that in various risk models, even those having high-dimensional (≈500) inhomogeneous margins, our MH estimator has smaller bias and mean squared error when compared with existing estimators. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--19}, 
number = {9}, 
volume = {19}
}
@article{10.1007/978-3-7908-2604-3_55, 
year = {2010}, 
title = {{Modeling operational risk: Estimation and effects of dependencies}}, 
author = {Mittnik, Stefan and Paterlini, Sandra and Yener, Tina}, 
issn = {NA}, 
doi = {10.1007/978-3-7908-2604-3\_55}, 
abstract = {{Being still in its early stages, operational risk modeling has, so far, mainly been concentrated on the marginal distributions of frequencies and severities within the context of the Loss Distribution Approach (LDA). In this study, drawing on a fairly large real-world data set, we analyze the effects of competing strategies for dependence modeling. In particular, we estimate tail dependence both via copulas as well as nonparametrically, and analyze its effect on aggregate risk- capital estimates. © Springer-Verlag Berlin Heidelberg 2010.}}, 
pages = {541--548}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/9781119186229.ch28, 
year = {2015}, 
title = {{A multidimensional framework for risk analysis}}, 
author = {Fong, Gifford and Vasicek, Oldrich A. and Vasicek, Oldrich A}, 
issn = {NA}, 
doi = {10.1002/9781119186229.ch28}, 
abstract = {{The variety and complexity of portfolio holdings have given rise to the need for additional analyses for purposes of risk management. A framework for risk analysis includes three dimensions: sensitivity analysis, value at risk (VaR), and stress testing. This article describes each dimension and suggests a procedure for achieving a VaR measure. Once individual holdings are analyzed, attention can be directed to portfolio-level analyses and the types of output suitable for monitoring purposes. In combination, this framework can capture the important features of portfolio risk. © 2016 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {247--260}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2017.11.010, 
year = {2018}, 
title = {{Solvency II, or how to sweep the downside risk under the carpet}}, 
author = {Weber, Stefan}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2017.11.010}, 
abstract = {{Under Solvency II the computation of capital requirements is based on value at risk (V@R). V@R is a quantile-based risk measure and neglects extreme risks in the tail. V@R belongs to the family of distortion risk measures. A serious deficiency of V@R is that firms can hide their total downside risk in corporate networks, unless a consolidated solvency balance sheet is required for each economic scenario. In this case, they can largely reduce their total capital requirements via appropriate transfer agreements within a network structure consisting of sufficiently many entities and thereby circumvent capital regulation. We prove several versions of such a result for general distortion risk measures of V@R-type, explicitly construct suitable allocations of the network portfolio, and finally demonstrate how these findings can be extended beyond distortion risk measures. We also discuss why consolidation requirements cannot completely eliminate this problem. Capital regulation should thus be based on coherent or convex risk measures like average value at risk or expectiles. © 2018 Elsevier B.V.}}, 
pages = {191--200}, 
number = {NA}, 
volume = {82}
}
@article{10.1002/fut.22154, 
year = {2020}, 
title = {{Stochastic multifactor models in risk management of energy futures}}, 
author = {Guo, Zi‐Yi}, 
journal = {Journal of Futures Markets}, 
issn = {02707314}, 
doi = {10.1002/fut.22154}, 
abstract = {{We adopt Schwartz and Smith's model to calculate risk measures of Brent oil and light sweet crude oil (WTI) futures contracts and Mirantes, Poblacion, and Serna's model to calculate risk measures of natural gas, gasoil, heating oil, RBOB gasoline, PJM Western Hub peak, and off-peak electricity futures contracts. The models generate well in-sample goodness of fit and satisfactory out-of-sample Value-at-Risk and expected shortfall forecasts for all the eight of the analyzed commodities. A simple and flexible estimation method improving upon existing estimation methods is developed. © 2020 Wiley Periodicals LLC}}, 
pages = {1918--1934}, 
number = {12}, 
volume = {40}
}
@article{10.1016/j.eswa.2011.09.108, 
year = {2012}, 
title = {{Ensemble forecasting of Value at Risk via Multi Resolution Analysis based methodology in metals markets}}, 
author = {He, Kaijian and Lai, Kin Keung and Yen, Jerome}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2011.09.108}, 
abstract = {{Subject to shocks worldwide, the metals markets in the era of structural changes and globalization have seen a very competitive and volatile market environment. Proper risk measurement and management in the metals markets are of critical value to the investors belonging to different parts of the economy due to their unique role as important industry inputs to the manufacturing process. Although traditional risk management methodologies have worked in the past, we are now facing the challenge of rapidly changing market conditions. Markets now demand the methodologies that estimate more reliable and accurate VaRs. This paper proposes a Multi Resolution Analysis (MRA) based nonlinear ensemble methodology for Value at Risk Estimates (MRNEVaR). The MRA using wavelet analysis is introduced to analyze the dynamic risk evolution at a finer time scale domain and provide insights into different aspects of the underlying risk evolution. The nonlinear ensemble approach using the artificial neural network technique is introduced to determine the optimal ensemble weights and stabilize the forecasts. Performances of the proposed MRNEVaR and more traditional ARMA-GARCH VaR are evaluated and compared during empirical studies in three major metals markets using Kupiec backtesting and Diebold-Mariano test procedures. Experiment results confirm that VaR estimates produced by MRNEVaR provide superior forecasts that are significantly more reliable and accurate than traditional methods. © 2011 Elsevier Ltd. All rights reserved.}}, 
pages = {4258--4267}, 
number = {4}, 
volume = {39}
}
@article{10.1007/978-3-030-61146-0_12, 
year = {2020}, 
title = {{Volatility and Value at Risk of Crypto Versus Fiat Currencies}}, 
author = {Naimy, Viviane and Chidiac, Johnny El and Khoury, Rim El}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-030-61146-0\_12}, 
abstract = {{This paper examines the behavior of Bitcoin and Ripple compared to the three fiat currencies, EURUSD, GBPUSD and CNYUSD, by comparing their volatility and VaR during the period extending from March 01, 2016 to February 28, 2019. EWMA, GARCH (1, 1), GARCH (p, q) and EGARCH (1, 1) were used to forecast volatilities. EWMA model outperformed the rest of the models for all of the selected fiat and cryptocurrencies during the in-sample period and for EURUSD, GBPUSD and Bitcoin during the out-of-sample period. GARCH (p, q) was the optimal model for the CNYUSD and Ripple in the out-of-sample period. Bitcoin and Ripple exhibit an asymmetry in their volatility which is significantly higher than all the volatilities of the studied currencies. When estimated volatilities were compared to the implied volatility, the GARCH (1, 1), GARCH (6, 6) and EWMA were the optimal models for the EURUSD, CNYUSD and GBPUSD, respectively for the in-sample period. VaR results were accepted for the EURUSD, GBPUSD and Bitcoin at all confidence levels. For the CNYUSD, VaR measures underestimated the risk at the 99\% confidence level unlike Ripple’s VaR that was accepted at the 90\% and 99\% confidence levels. Our results suggest that Bitcoin and generally the cryptocurrencies market cannot act as alternatives to fiat currencies at the moment. © 2020, Springer Nature Switzerland AG.}}, 
pages = {145--157}, 
number = {NA}, 
volume = {394}
}
@article{10.1109/itapp.2010.5566130, 
year = {2010}, 
title = {{The quantitative analytic research of extenics by VAR on the risks of the financial derivatives markets}}, 
author = {Lin, Shiguang}, 
journal = {2010 International Conference on Internet Technology and Applications}, 
issn = {NA}, 
doi = {10.1109/itapp.2010.5566130}, 
abstract = {{By the basic methods and principles of VAR, the paper quantifies and analyses the financial derivatives markets and builds up the risks control model of the financial derivatives markets. Under the insufficiency development of the Chinese local derivatives markets, the lack of the products quantities on the sorts of the derivatives, and also enterprises always defeated in the "zero-sum game". It presents the risks control methods of the active regulation risks and ltimately achieves rational investment and the objectives on the risks self-control. ©2010 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.orl.2018.11.005, 
year = {2019}, 
title = {{Concentration bounds for empirical conditional value-at-risk: The unbounded case}}, 
author = {Kolla, Ravi Kumar and L.A., Prashanth and Bhat, Sanjay P. and Jagannathan, Krishna}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2018.11.005}, 
abstract = {{Conditional Value-at-Risk (CVaR) is a popular risk measure for modelling losses in the case of a rare but extreme event. We consider the problem of estimating CVaR from i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or sub-exponential. We derive a novel one-sided concentration bound for a natural sample-based CVaR estimator in this setting. Our bound relies on a concentration result for a quantile-based estimator for Value-at-Risk (VaR), which may be of independent interest. © 2018 Elsevier B.V.}}, 
pages = {16--20}, 
number = {1}, 
volume = {47}
}
@article{10.1109/tia.2019.2958302, 
year = {2020}, 
title = {{Two-Stage Bidding Strategy for Peer-to-Peer Energy Trading of Nanogrid}}, 
author = {Zhang, Zhenyuan and Tang, Haoyue and Wang, Peng and Huang, Qi and Lee, Wei-Jen}, 
journal = {IEEE Transactions on Industry Applications}, 
issn = {00939994}, 
doi = {10.1109/tia.2019.2958302}, 
abstract = {{With more distributed energy resources penetrated into the residential community, nanogrid based peer-to-peer (P2P) energy market has rapidly emerged over recent years. Due to the complexities on the decision-making process of each market participant, an efficient, fair and beneficial oriented bidding strategy is thus necessary. In this article, a two-stage bidding strategy for P2P trading of nanogrid is proposed. To overcome the limitations of traditional methods, in the first stage, a supply-demand relationship considered two-step price predictor, which aims to promote the usage of local renewable energy, is formulated to provide the guidance on transaction adjustment. In the second stage, trading preference based simultaneous game-theoretic approach is fully introduced, which can optimize the market equilibrium and then increase the social welfare of the P2P market. Additionally, to mitigate the possible failure of price matching, value-at-risk is implemented through the trading process as a risk hedging tool. To verify the effectiveness of the proposed strategy, usages of local renewable energy, economic benefits and success rates of transaction is compared against the traditional method for various cases. © 1972-2012 IEEE.}}, 
pages = {1000--1009}, 
number = {2}, 
volume = {56}
}
@article{10.1016/s0148-6195(00)00026-6, 
year = {2000}, 
title = {{Value at risk using hyperbolic distributions}}, 
author = {Bauer, Christian}, 
journal = {Journal of Economics and Business}, 
issn = {00129933}, 
doi = {10.1016/s0148-6195(00)00026-6}, 
abstract = {{This article deals with the Value at Risk concept as it is used in practice. We show that, like the Gaussian distribution, elliptical distributions lend themselves to simple practical computations. All necessary computations are detailed for the symmetric hyperbolic distributions. A test on real stock market and exchange rate data shows the new distributions fit the data better and outperform equivalent estimators used in RiskMetrics™.1 © 2000 Elsevier Science Inc.}}, 
pages = {455--467}, 
number = {5}, 
volume = {52}
}
@article{10.1109/upec.2015.7339808, 
year = {2015}, 
title = {{Risk assessment of major storm situation in distribution system}}, 
author = {Dehghani, Nemat and Supponen, Antti and Repo, Sami}, 
journal = {2015 50th International Universities Power Engineering Conference (UPEC)}, 
issn = {NA}, 
doi = {10.1109/upec.2015.7339808}, 
abstract = {{Risk and reliability have a significant connection in meaning; both of them are the facts for one inference. High level of risk is resource of lower reliability. Risk management in power system has a variety of different subjects including models, methods and applications. Risk is a mixture of probability of disturbance event and the negative effect of that occurrence. Usually it counted for random accident which has harmful effect on people's life and environment. In this paper risks study of storm situation modelled. Random failures in power system are the origin of risk and cannot control by staff. Monte-Carlo Simulation (MCS) has used to model the fault frequencies and outage time of customers. The two tools which use in financial studies to make investment decision and applicable in power systems are Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) Result of study compared to the actual reliability which confirm the improvement in the reliability of system. It is not possible to predict the precise amount of load value, Concerns of power outage in local area and possibility of a general blackout. © 2015 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {2015-November}
}
@article{10.1093/jjfinec/nbr010, 
year = {2012}, 
title = {{Microinformation, nonlinear filtering, and granularity}}, 
author = {Gagliardini, P and Gourieroux, C and Monfort, A}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbr010}, 
abstract = {{The recursive prediction and filtering formulas of the Kalman filter are difficult to implement in nonlinear state space models since they require the updating of a function. The aim of this paper is to consider the situation of a large number n of individual measurements, called microinformation, and to take advantage of the large cross-sectional size to get closed-form prediction and filtering formulas at order 1/n. The state variables have a macrofactor interpretation. The results are applied to maximum likelihood estimation of a macroparameter and to computation of a granularity adjusted Value-at-Risk (VaR) for large portfolios. The granularity adjustment for VaR is illustrated by an application of the value of the firm model Merton, 1974, Journal of Finance 29, 449-470) taking into account both default and loss given default. © The Author 2011. Published by Oxford University Press. All rights reserved.}}, 
pages = {1--53}, 
number = {1}, 
volume = {10}
}
@article{10.1080/03610926.2018.1476707, 
year = {2019}, 
title = {{The Performance of Gaussian and non Gaussian dynamic models in assessing market risk: The Implications for risk management}}, 
author = {Nawaz, Faisal and Shahzad, Faisal and Rehman, Ijaz Ur and Shujahat, Muhammad and Hyder, Shabir and Barghouthi, Sameer Al}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2018.1476707}, 
abstract = {{The developing markets are more volatile, unstable illiquid, and more prone to the external shocks. The non Gaussian VaR model gives more accurate risk models than Gaussian VaR models. Hence, the purpose of this study is to test if and how non Gaussian VaR models are comparatively better fit for risk modeling of developing markets than the Gaussian VaR models. The study measures the market risk for the daily closing price of Karachi Stock Exchange 100 index over the period of 2004–2016. The evaluation of VaR models suggests that non Gaussian dynamic model outperformed the Gaussian VaR models in developing markets. © 2018, © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--17}, 
number = {13}, 
volume = {48}
}
@article{10.1016/j.jfi.2013.08.004, 
year = {2013}, 
title = {{Incentives and financial crises: Microfounded macroprudential regulation}}, 
author = {Iasio, Giovanni di}, 
journal = {Journal of Financial Intermediation}, 
issn = {10429573}, 
doi = {10.1016/j.jfi.2013.08.004}, 
abstract = {{We provide a micro-based rationale for macroprudential capital regulation of financial intermediaries (banks) by developing a model in which bankers can privately undertake a costly effort and reduce the probability of adverse shocks to their asset holdings that force liquidation (deterioration risk). A decline in the fundamental risk of assets ameliorates funding conditions, boosting the banks' ability to expand their balance sheets. In principle, a higher continuation value would improve incentives to put effort. However, the rise in asset demand and prices also increases the payoff in liquidation, eventually reducing the equilibrium optimal effort. Poor incentives impose socially inefficient liquidation and can be corrected through a regulatory capital requirement. We show that the requirement should be high when fundamental risk is low. Therefore, the model suggests a theoretical foundation for macroprudential regulation and the countercyclical capital buffer of Basel III. © 2013 Elsevier Inc.}}, 
pages = {627--638}, 
number = {4}, 
volume = {22}
}
@article{10.1007/s11766-009-2249-2, 
year = {2009}, 
title = {{Modeling dependence based on mixture copulas and its application in risk management}}, 
author = {Ouyang, Zi-sheng and Liao, Hui and Yang, Xiang-qun}, 
journal = {Applied Mathematics-A Journal of Chinese Universities}, 
issn = {10051031}, 
doi = {10.1007/s11766-009-2249-2}, 
abstract = {{This paper is concerned with the statistical modeling of the dependence structure of multivariate financial data using the copula, and the application of copula functions in VaR valuation. After the introduction of the pure copula method and the maximum and minimum mixture copula method, authors present a new algorithm based on the more generalized mixture copula functions and the dependence measure, and apply the method to the portfolio of Shanghai stock composite index and Shenzhen stock component index. Comparing with the results from various methods, one can find that the mixture copula method is better than the pure Gaussian copula method and the maximum and minimum mixture copula method on different VaR level. © 2009 Editorial Committee of Applied Mathematics.}}, 
pages = {393}, 
number = {4}, 
volume = {24}
}
@article{10.3390/risks6010017, 
year = {2018}, 
title = {{Lambda value at risk and regulatory capital: A dynamic approach to tail risk}}, 
author = {Hitaj, Asmerilda and Mateus, Cesario and Peri, Ilaria}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks6010017}, 
abstract = {{This paper presents the first methodological proposal of estimation of the ΛVaR. Our approach is dynamic and calibrated to market extreme scenarios, incorporating the need of regulators and financial institutions in more sensitive risk measures. We also propose a simple backtesting methodology by extending the VaR hypothesis-testing framework. Hence, we test our ΛVaR proposals under extreme downward scenarios of the financial crisis and different assumptions on the profit and loss distribution. The findings show that our ΛVaR estimations are able to capture the tail risk and react to market fluctuations significantly faster than the VaR and expected shortfall. The backtesting exercise displays a higher level of accuracy for our ΛVaR estimations. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {17}, 
number = {1}, 
volume = {6}
}
@article{10.21314/jor.2015.306, 
year = {2015}, 
title = {{Applying the Cornish-fisher expansion to value-at-risk estimation in Islamic banking}}, 
author = {Izhar, Hylmun}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.306}, 
abstract = {{Methodological-empirical analysis of risk management in Islamic banking is rather scarce. This is due to either a lack of comprehensive understanding in the latest development of empirical modeling or analysts’failure to grasp the underlying assumptions that underpin the model. Such an inability could be detrimental, particularly in the investigation of operational risk exposures in Islamic banking. This study therefore contributes to this field by deliberating upon a proposed delta-gamma sensitivity analysis-extreme value theory (DGSA-EVT) model that focuses on the assessment of risk exposures represented by the value of value-at-risk (VaR) in three incomegenerating channels: one in investment, one in financing and one in services. A data analysis demonstrates that risk variables in this study exhibit nonnormality, which understandably does not fit with the underlying assumption of VaR, namely the normality of the variables under the analysis. Hence, in order to mitigate this, our study employs a parametric approach called the Cornish-Fisher expansion, under which the confidence interval of assigned risk variables is a function of their respective skewness, kurtosis and volatility. Such an approach has proven to be effective in mitigating the likelihood of over- or underestimating theVaR in the income-generation channels analyzed as a result of misaddressing the nonnormality of risk variables. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {51--72}, 
number = {6}, 
volume = {17}
}
@article{10.1007/978-3-642-28466-3_41, 
year = {2012}, 
title = {{A dynamic computing research for Value at Risk (VaR) of Shanghai stock market based on the GARCH model}}, 
author = {Xia, Shi}, 
journal = {Advances in Intelligent and Soft Computing}, 
issn = {18675662}, 
doi = {10.1007/978-3-642-28466-3\_41}, 
abstract = {{This paper selects the Shanghai index of 2006 listed companies after share-trading reform, to analyze the VaR of Shanghai stock market based on GARCH model under different distribution assumptions. The results show that the difference of distribution hypothesis has a great impact on the VaR based on GRACH model. The VaR of Shanghai stock market after share-trading reform can be better calculated after using GRACH model; the VaR got under T-distribution assumptions is too conservative, which a bit overstated risk; the VaR estimations under normal distribution, generalized error distribution (GED) have no big difference and both underestimated risk. © 2012 Springer-Verlag GmbH.}}, 
pages = {299--308}, 
number = {NA}, 
volume = {146 AISC}
}
@article{10.1108/00215000380001141, 
year = {2003}, 
title = {{Using extreme value theory to estimate Value-at-Risk}}, 
author = {Odening, Martin and Hinrichs, Jan}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00215000380001141}, 
abstract = {{This study examines problems that may occur when conventional Value-at-Risk (VaR) estimators are used to quantify market risks in an agricultural context. For example, standard VaR methods, such as the variance-covariance method or historical simulation, can fail when the return distribution is fat tailed. This problem is aggravated when long-term VaR forecasts are desired. Extreme Value Theory (EVT) is proposed to overcome these problems. The application of EVT is illustrated by an example from the German hog market. Multi-period VaR forecasts derived by EVT are found to deviate considerably from standard forecasts. We conclude that EVT is a useful complement to traditional VaR methods. © 2001, Emerald Group Publishing Ltd. All rights reserved.}}, 
pages = {55--73}, 
number = {1}, 
volume = {63}
}
@article{10.1080/00036846.2014.982853, 
year = {2015}, 
title = {{What should the value of lambda be in the exponentially weighted moving average volatility model?}}, 
author = {Bollen, Bernard}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2014.982853}, 
abstract = {{Forecasting volatility is fundamental to forecasting parametric models of value-at-risk. The exponentially weighted moving average (EWMA) volatility model is the recommended model for forecasting volatility by the Riskmetrics group. For monthly data, the lambda parameter of the EWMA model is recommended to be set to 0.97. In this study, we empirically investigate if this is the optimal value of lambda in terms of forecasting volatility. Employing monthly realized volatility as the benchmark for testing the value of lambda, it is found that a value of lambda of 0.97 is far from optimal. The tests are robust to a variety of test statistics. It is further found that the optimal value of lambda is time varying and should be based upon recent historical data. The article offers a practical method to increase the reliability and accuracy of value-at-risk forecasts that can be easily implemented within an Excel spreadsheet. © 2014 Taylor \& Francis.}}, 
pages = {853--860}, 
number = {8}, 
volume = {47}
}
@article{10.1002/net.22059, 
year = {2021}, 
title = {{The multifacility center problems with random demand weights}}, 
author = {Berman, Oded and Wang, Jiamin}, 
journal = {Networks}, 
issn = {00283045}, 
doi = {10.1002/net.22059}, 
abstract = {{We study two p-center models on a network with probabilistic demand weights. In the first, which is called the maximum probability p-center problem, the objective is to maximize the probability that the maximum demand-weighted distance between the demand and the open facilities does not exceed a given threshold value. In the second, referred to as the β-VaR p-center problem, the objective is to minimize the value-at-risk of the maximum demand-weighted distance with a pre-selected confidence level. It is shown that both models are NP-hard. We develop algorithms for solving the two models and conduct computational experiments to compare their performance. We recommend that the branch and bound algorithm be applied to solve the first model, and an ensemble optimization method to solve the second model. The solution approaches presented can be easily extended to the case where the random demand weights are not independent. © 2021 Wiley Periodicals LLC}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/qre.2397, 
year = {2019}, 
title = {{Risk-based metrics for performance evaluation of control charts}}, 
author = {Weiß, Christian H. and Testik, Murat Caner}, 
journal = {Quality and Reliability Engineering International}, 
issn = {07488017}, 
doi = {10.1002/qre.2397}, 
abstract = {{Control charts are commonly evaluated in terms of their average run length (ARL). However, since run length distributions are typically strongly skewed, the ARL gives a very limited impression about the actual run length performance. In this study, it is proposed to evaluate a control chart's performance using risk metrics, specifically the value at risk and the tail conditional expectation. When a control chart is evaluated for an in-control performance, the risk is an early occurrence of a false alarm, whereas in an out-of-control state, there is a risk of a delayed detection. For these situations, risk metric computations are derived and exemplified for diverse types of control charts. It is demonstrated that the use of such risk metrics leads to important new insights into a control chart's performance. In addition to the cases of known process parameters for control chart design, these risk metrics are further used to analyze the estimation uncertainty in evaluating a control chart's performance if the design parameters rely on a phase 1 analysis. Benefits of the risk-based metrics are discussed thoroughly, and these are recommended as supplements to the traditional ARL metric. © 2018 John Wiley \& Sons, Ltd.}}, 
pages = {280--291}, 
number = {1}, 
volume = {35}
}
@article{10.3390/en13164178, 
year = {2020}, 
title = {{Economic Feasibility of Semi-Underground Pumped Storage Hydropower Plants in Open-Pit Mines}}, 
author = {Wessel, Michael and Madlener, Reinhard and Hilgers, Christoph}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13164178}, 
abstract = {{This work aims at the economic evaluation of a semi-underground pumped hydro storage power plant erected in an abandoned open-pit mine. For the exploratory model-based analysis, we develop and apply both a simple deterministic and a stochastic net present value (NPV) approach, the latter of which uses a monte Carlo simulation to account for revenue uncertainty from electricity price fluctuations. The analytical framework developed is applied to two promising sites in the Rheinland region in Germany, Hambach and Inden, making reasonable parameter value assumptions and considering and ignoring the lengthy duration of lower reservoir flooding. The investor’s value-at-risk is computed for alternative performance indicators (NPV, net cash recovery, profit-to-investment ratio, and specific production costs) to compare the different outcomes in terms of the project’s financial risk distribution. Calculations show that a semi-underground pumped hydro storage power plant in an abandoned open-pit mine can be constructed at reasonably low investment costs and operated at low specific production costs. However, because the investment has to be made long before the pit lake is (naturally) flooded—a process that for realistic flow rates may take up to 20 years—the project is highly uneconomical and would require substantial subsidies, as compared to a situation where flooding happens immediately. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {4178}, 
number = {6}, 
volume = {13}
}
@article{10.1631/jzus.2005.as0163, 
year = {2005}, 
title = {{Dynamic decision model for portfolio investment and assets management}}, 
author = {Qian, Edward Y. and Ying, Feng and James, Higgision}, 
journal = {Journal of Zhejiang University-SCIENCE A}, 
issn = {10093095}, 
doi = {10.1631/jzus.2005.as0163}, 
abstract = {{This paper addresses a dynamic portfolio investment problem. It discusses how we can dynamically choose candidate assets, achieve the possible maximum revenue and reduce the risk to the minimum level. The paper generalizes Markowitz's portfolio selection theory and Sharpe's rule for investment decision. An analytical solution is presented to show how an institutional or individual investor can combine Markowitz's portfolio selection theory, generalized Sharpe's rule and Value-at-Risk (VaR) to find candidate assets and optimal level of position sizes for investment (dis-investment). The result shows that the generalized Markowitz's portfolio selection theory and generalized Sharpe's rule improve decision making for investment.}}, 
pages = {163--171}, 
number = {SUPPL.}, 
volume = {6 A}
}
@article{10.1080/13504851.2014.920467, 
year = {2014}, 
title = {{Histogram-valued data on value at risk measures: a symbolic approach for risk attribution}}, 
author = {Toque, Carole and Terraza, Virginie}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2014.920467}, 
abstract = {{In this article, we develop the concept of histogram-valued data on value at risk for the classification of hedge fund risk. By using recent developments in data mining, it is a question of the classification of heterogeneous data in order to sort hedge funds by risk class. In practical terms, risk levels relative to measures of histogram-valued data on VaR are calculated as an aid to decision-making. The empirical study was carried out on 1023 HFR-based hedge funds, where we had estimated monthly ARMA-GARCH or asymmetric GARCH VaR and CVaR measures between 01 January 2003 and 31 December 2008. We identify two sub-periods: from 2003 to 2005, and from 2006 to 2008 in order to identify a recovery period after the 2001–2002 crisis and the impact of the 2007–2008 crisis. First, the symbolic approach allows us to construct the measures of histogram-valued data on VaR by optimizing the definition of categories. A symbolic principal component analysis shows that the indices coming from the VaR of the GARCH and asymmetrical GARCH are the most pertinent. Second, we apply a criterion of inter-class inertia and retain a partitioning of hedge funds into three classes by dynamic K-means cluster analysis. For each of our sub-periods and for each class, a risk level is defined on the basis of the categories of the most discriminating variable. © 2014, Taylor \& Francis.}}, 
pages = {1243--1251}, 
number = {17}, 
volume = {21}
}
@article{10.1109/nswctc.2009.332, 
year = {2009}, 
title = {{The variances of VaR for the Poisson-Gumbel compound extreme value distribution and for the Poisson-Generalized Pareto compound peaks over threshold distribution}}, 
author = {Han, Yueli and Shi, Daoji}, 
journal = {2009 International Conference on Networks Security, Wireless Communications and Trusted Computing}, 
issn = {NA}, 
doi = {10.1109/nswctc.2009.332}, 
abstract = {{In this paper we compared the variances of Value at Risk (VaR) of loss distribution models: one based on the Poisson-Gumbel compound extreme value distribution and another based on the Poisson-Generalized Pareto (GP) compound peaks over threshold distribution. The data used in this study are records of exchange rates between US Dollars and British Pounds from January 2, 1990 to December 29, 2006. By comparison, we found that the variance of VaR for the Poisson-Gumbel compound extreme value distribution is less than the variance of VaR for the Poisson-GP compound peaks over threshold distribution when the variances of other parameter estimates are assumed to be similar. We concluded that if both distribution models can be used to model the loss sample data, then the Poisson-Gumbel compound extreme value distribution is superior than the Poisson-GP compound peaks over threshold distribution. © 2009 IEEE.}}, 
pages = {748--753}, 
number = {NA}, 
volume = {1}
}
@article{10.1007/s00186-020-00707-9, 
year = {2020}, 
title = {{Qualitative robustness of set-valued value-at-risk}}, 
author = {Crespi, Giovanni Paolo and Mastrogiacomo, Elisa}, 
journal = {Mathematical Methods of Operations Research}, 
issn = {14322994}, 
doi = {10.1007/s00186-020-00707-9}, 
abstract = {{Risk measures are defined as functionals of the portfolio loss distribution, thus implicitly assuming the knowledge of such a distribution. However, in practical applications, the need for estimation arises and with it the need to study the effects of mis-specification errors, as well as estimation errors on the final conclusion. In this paper we focus on the qualitative robustness of a sequence of estimators for set-valued risk measures. These properties are studied in detail for two well-known examples of set-valued risk measures: the value-at-risk and the maximum average value-at-risk. Our results illustrate, in particular, that estimation of set-valued value-at-risk can be given in terms of random sets. Moreover, we observe that historical set-valued value-at-risk, while failing to be sub-additive, leads to a more robust procedure than alternatives such as the maximum likelihood average value at-risk. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {25--54}, 
number = {1}, 
volume = {91}
}
@article{10.1016/j.orl.2018.02.006, 
year = {2018}, 
title = {{Vector-valued multivariate conditional value-at-risk}}, 
author = {Meraklı, Merve and Küçükyavuz, Simge}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2018.02.006}, 
eprint = {1708.01324}, 
abstract = {{In this study, we propose a new definition of multivariate conditional value-at-risk (MCVaR) as a set of vectors for arbitrary probability spaces. We explore the properties of the vector-valued MCVaR (VMCVaR) and show the advantages of VMCVaR over the existing definitions particularly for discrete random variables. © 2018 Elsevier B.V.}}, 
pages = {300--305}, 
number = {3}, 
volume = {46}
}
@article{10.1002/asmb.424, 
year = {2001}, 
title = {{A comparison of several time-series models for assessing the value at risk of shares}}, 
author = {Zucchini, Walter and Neumann, Kristin}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.424}, 
abstract = {{The objective of this investigation was to assess the suitability of some standard time-series models to perform a specific task in the context of recent change in banking regulations in Germany. The task is to estimate the value at risk (VaR) associated with financial assets on a daily basis. The procedure employed by the supervisory authorities to monitor whether a model used for this purpose adequately performs this task is outlined. Nine time-series models were investigated using share prices from the Frankfurt Stock exchange. The models were compared in terms of criteria that are derived from the new regulations. The results are reported. Copyright © 2001 John Wiley \& Sons, Ltd.}}, 
pages = {135--148}, 
number = {1}, 
volume = {17}
}
@article{10.1080/14697688.2014.882070, 
year = {2014}, 
title = {{Scaling laws: a viable alternative to value at risk?}}, 
author = {Chopping, Thomas}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2014.882070}, 
abstract = {{Recent research has found a number of scaling law relationships in foreign exchange data. These relationships, estimated using simple ordinary least squares, can be used to forecast losses in foreign exchange time series from as little as one month's tick data. We compare the loss forecasts from a new scaling law against six parametric Value at Risk models. Compared to these models, the new scaling law is easier to fit, provides more stable forecasts and is very accurate. © 2014 © 2014 Taylor \& Francis.}}, 
pages = {889--911}, 
number = {5}, 
volume = {14}
}
@article{10.1016/j.ejor.2016.04.001, 
year = {2016}, 
title = {{Generalized asymmetric linguistic term set and its application to qualitative decision making involving risk appetites}}, 
author = {Zhou, Wei and Xu, Zeshui}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2016.04.001}, 
abstract = {{The linguistic term set is an applicable and flexible technique in qualitative decision making (QDM). To further develop the linguistic term set, this paper proposes a generalized asymmetric linguistic term set (GALTS) based on the asymmetric sigmoid semantics, which belongs to an asymmetric and non-uniform linguistic term set, and can be used to address the QDM problems involving risk appetites of the decision maker (DM). Then, a value-at-risk fitting (VARF) approach is designed for obtaining the risk appetite parameters of the GALTS and six desirable properties of the GALTS are analyzed, i.e., asymmetry, non-uniformity, generality, variability, range consistency, and diminishing-utility. Based on the above approaches and the generalized asymmetric linguistic preference relations (GALPRs), a QDM process involving risk appetites of the DM is designed. Because the GALPRs consist of subjective information provided by the DM, the process is not perfectly consistent and is usually difficult to change or repeat. Thus, a transitivity improvement approach is investigated, and the corresponding calculation steps are provided. Finally, an example dealing with the problem of investment decision making is provided, and the results fully demonstrate the validity of the proposed methods for QDM involving risk appetites. © 2016 Elsevier B.V. All rights reserved.}}, 
pages = {610--621}, 
number = {2}, 
volume = {254}
}
@article{10.1111/acfi.12795, 
year = {2021}, 
title = {{An empirical investigation of the quality of value-at-risk disclosure in Australia}}, 
author = {Campbell, Angus and Smith, Daniel R.}, 
journal = {Accounting \& Finance}, 
issn = {08105391}, 
doi = {10.1111/acfi.12795}, 
abstract = {{We study the level and quality of value-at-risk (VaR) disclosure at Australian banks. We find that Australian banks have increased disclosure about their VaR recently, reaching a level post-crisis that is similar to other regulatory jurisdictions. We find that the actual VaR estimates produced by banks are generally rejected by standard backtesting procedures. During quiet periods bank VaRs are too high, while during high volatility stress periods bank VaRs are too low. We are able to reject the null hypothesis that the daily VaRs for two banks are the 1st percentile using a quantile regression-based test. © 2021 Accounting and Finance Association of Australia and New Zealand}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/72.935098, 
year = {2001}, 
title = {{Cost functions and model combination for VaR-based asset allocation using neural networks}}, 
author = {Chapados, Nicolas and Bengio, Yoshua}, 
journal = {IEEE Transactions on Neural Networks}, 
issn = {10459227}, 
doi = {10.1109/72.935098}, 
pmid = {18249920}, 
abstract = {{We introduce an asset-allocation framework based on the active control of the value-at-risk of the portfolio. Within this framework, we compare two paradigms for making the allocation using neural networks. The first one uses the network to make a forecast of asset behavior, in conjunction with a traditional mean-variance allocator for constructing the portfolio. The second paradigm uses the network to directly make the portfolio allocation decisions. We consider a method for performing soft input variable selection, and show its considerable utility. We use model combination (committee) methods to systematize the choice of hyperparameters during training. We show that committees using both paradigms are significantly outperforming the benchmark market performance.}}, 
pages = {890}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.frl.2009.03.004, 
year = {2009}, 
title = {{Analytical Value-at-Risk and Expected Shortfall under regime-switching}}, 
author = {Taamouti, Abderrahim}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2009.03.004}, 
abstract = {{It is well known that the use of Gaussian models to assess financial risk leads to an underestimation of risk. The reason is because these models are unable to capture some important facts such as heavy tails and volatility clustering which indicate the presence of large fluctuations in returns. An alternative way is to use regime-switching models, the latter are able to capture the previous facts. Using regime-switching model, we propose an analytical approximation for multi-horizon conditional Value-at-Risk and a closed-form solution for conditional Expected Shortfall. By comparing the Value-at-Risks and Expected Shortfalls calculated analytically and using simulations, we find that the both approaches lead to almost the same result. Further, the analytical approach is less time and computer intensive compared to simulations, which are typically used in risk management. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {138--151}, 
number = {3}, 
volume = {6}
}
@article{10.1111/j.1467-6419.2012.00744.x, 
year = {2014}, 
title = {{Extreme value theory in finance: A survey}}, 
author = {Rocco, Marco}, 
journal = {Journal of Economic Surveys}, 
issn = {09500804}, 
doi = {10.1111/j.1467-6419.2012.00744.x}, 
abstract = {{Extreme value theory is concerned with the study of the asymptotic distribution of extreme events, that is to say events which are rare in frequency and huge in magnitude with respect to the majority of observations. Statistical methods derived from it have been employed increasingly in finance, especially for risk measurement. This paper surveys some of those main applications, namely for testing different distributional assumptions for the data, for Value-at-Risk and Expected Shortfall calculations, for asset allocation under safety-first type constraints, and for the study of contagion and dependence across markets under conditions of stress. © 2012 John Wiley \& Sons Ltd.}}, 
pages = {82--108}, 
number = {1}, 
volume = {28}
}
@article{10.1080/15140326.2006.12040653, 
year = {2006}, 
title = {{Central Bank's Value at Risk and financial crises: An application to the 2001 Argentine crisis}}, 
author = {Nocetti, Diego}, 
journal = {Journal of Applied Economics}, 
issn = {15140326}, 
doi = {10.1080/15140326.2006.12040653}, 
abstract = {{Blejer and Schumacher (1999) were the first to suggest that Central Bank's Value at Risk (VaR), a widely used composite measure of potential portfolio losses in the corporate sector, could be used as an early warning indicator of financial crises. We extend their research in two aspects. First, we develop an operational model to calculate Central Bank's VaR and illustrate the methodology using data from the recent financial crisis in Argentina. Second, we compare the predictive performance of diverse measures based on the VaR approach to that of another well known early warning system, the signals approach, and several univariate leading indicators. The results reveal a strong relationship between the measures proposed and the crisis. Furthermore, one of the measures provides higher accuracy and announces the probability of a crisis sooner than the competing indicators.}}, 
pages = {381--402}, 
number = {2}, 
volume = {9}
}
@article{10.1016/j.jempfin.2009.09.005, 
year = {2010}, 
title = {{GHICA - Risk analysis with GH distributions and independent components}}, 
author = {Chen, Ying and Härdle, Wolfgang and Spokoiny, Vladimir}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2009.09.005}, 
abstract = {{Over recent years, a study on risk management has been prompted by the Basel committee for regular banking supervisory. There are however limitations of some widely-used risk management methods that either calculate risk measures under the Gaussian distributional assumption or involve numerical difficulty. The primary aim of this paper is to present a realistic and fast method, GHICA, which overcomes the limitations in multivariate risk analysis. The idea is to first retrieve independent components (ICs) out of the observed high-dimensional time series and then individually and adaptively fit the resulting ICs in the generalized hyperbolic (GH) distributional framework. For the volatility estimation of each IC, the local exponential smoothing technique is used to achieve the best possible accuracy of estimation. Finally, the fast Fourier transformation technique is used to approximate the density of the portfolio returns. The proposed GHICA method is applicable to covariance estimation as well. It is compared with the dynamic conditional correlation (DCC) method based on the simulated data with d = 50 GH distributed components. We further implement the GHICA method to calculate risk measures given 20-dimensional German DAX portfolios and a dynamic exchange rate portfolio. Several alternative methods are considered as well to compare the accuracy of calculation with the GHICA one. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {255--269}, 
number = {2}, 
volume = {17}
}
@article{10.1016/j.econlet.2013.10.025, 
year = {2014}, 
title = {{VaR-implied tail-correlation matrices}}, 
author = {Mittnik, Stefan}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2013.10.025}, 
abstract = {{Empirical evidence suggests that asset returns correlate more strongly in bear markets than conventional correlation estimates imply. We propose a method for determining complete tail-correlation matrices based on Value-at-Risk (VaR) estimates. We demonstrate how to obtain more efficient tail-correlation estimates by use of overidentification strategies and how to guarantee positive semidefiniteness, a property required for valid risk aggregation and Markowitz-type portfolio optimization. An empirical application to a 30-asset universe illustrates the practical applicability and relevance of the approach in portfolio management. © 2013 Elsevier B.V.}}, 
pages = {69--73}, 
number = {1}, 
volume = {122}
}
@article{10.1080/10293523.2015.1126916, 
year = {2016}, 
title = {{Do long memory and asymmetries matter when assessing downside return risk?}}, 
author = {Katzke, Nico and Garbers, Chris}, 
journal = {Investment Analysts Journal}, 
issn = {10293523}, 
doi = {10.1080/10293523.2015.1126916}, 
abstract = {{In this paper we set out to test whether, on sector level, returns series in South Africa exhibit long memory and asymmetries and, more specifically, whether these effects should be accounted for when assessing downside risk. The purpose of this analysis is not to identify the most optimal downside risk assessment model or to reaffirm the often regarded stylised fact of the existence of long memory and asymmetry in asset returns series. Rather, we set out to establish whether accounting for these effects and allowing for more flexibility in second order persistence models actually leads to improved downside risk assessments. We use several variants of the widely used GARCH family of second order persistence models that control for these effects, and compare the downside risk estimates using Value-at-Risk measures of different model formulations and compare the out-of-sample performances. Our findings confirm that controlling for asymmetries and long memory in volatility models improve risk management calculations. © 2016 Investment Analysts Society of South Africa.}}, 
pages = {1--26}, 
number = {3}, 
volume = {45}
}
@article{10.1007/978-3-319-70942-0_6, 
year = {2018}, 
title = {{Bayesian forecasting for tail risk}}, 
author = {Chen, Cathy W. S. and Sun, Yu-Wen}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-319-70942-0\_6}, 
abstract = {{This paper evaluates the performances of Value-at-Risk (VaR) and expected shortfall, as well as volatility forecasts in a class of risk models, specifically focusing on GARCH, integrated GARCH, and asymmetric GARCH models (GJR-GARCH, exponential GARCH, and smooth transition GARCH models). Most of the models incorporate four error probability distributions: Gaussian, Student’s t, skew Student’s t, and generalized error distribution (GED). We employ Bayesian Markov chain Monte Carlo sampling methods for estimation and forecasting. We further present backtesting measures for both VaR and expected shortfall forecasts and implement two loss functions to evaluate volatility forecasts. The empirical results are based on the S\&P500 in the U.S. and Japan’s Nikkei 225. A VaR forecasting study reveals that at the 1\% level the smooth transition model with a second-order logistic function and skew Student’s t error compares most favorably in terms of violation rates for both markets. For the volatility predictive abilities, the EGARCH model with GED error is the best model in both markets. © Springer International Publishing AG 2018.}}, 
pages = {122--145}, 
number = {NA}, 
volume = {753}
}
@article{10.1142/s0219024910005693, 
year = {2010}, 
title = {{Implication of the kelly criterion for multi-dimensional processes}}, 
author = {LV, YINGDONG and MEISTER, BERNHARD K}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024910005693}, 
abstract = {{In this paper, we study the Kelly criterion in the continuous time framework building on the work of E.O. Thorp and others. The existence of an optimal strategy is proven in a general setting and the corresponding optimal wealth process is found. A simple formula is provided for calculating the optimal portfolio in terms of drift, short term risk-free rate and correlations for a set of generic multi-dimensional diffusion processes satisfying some simple conditions. Properties of the optimal investment strategy are studied. The paper ends with a short discussion of the implications of these ideas for financial markets. © 2010 World Scientific Publishing Company.}}, 
pages = {93--112}, 
number = {1}, 
volume = {13}
}
@article{10.1080/14697680902814225, 
year = {2010}, 
title = {{Portfolio optimization for student t and skewed t returns}}, 
author = {Hu, Wenbo and Kercheval, Alec N.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680902814225}, 
abstract = {{It is well-established that equity returns are not Normally distributed, but what should the portfolio manager do about this, and is it worth the effort? It is now feasible to employ better multivariate distribution families that capture heavy tails and skewness in the data; we argue that among the best are the Student t and skewed t distributions. These can be efficiently fitted to data, and show a much better fit to real returns than the Normal distribution. By examining efficient frontiers computed using different distributional assumptions, we show, using for illustration five stocks chosen from the Dow index, that the choice of distribution has a significant effect on how much available return can be captured by an optimal portfolio on the efficient frontier. © 2010 Taylor \& Francis.}}, 
pages = {91--105}, 
number = {1}, 
volume = {10}
}
@article{10.1080/1351847x.2021.1900888, 
year = {2021}, 
title = {{Nonlinearity everywhere: implications for empirical finance, technical analysis and value at risk}}, 
author = {Amini, Shima and Hudson, Robert and Urquhart, Andrew and Wang, Jian}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2021.1900888}, 
abstract = {{We show that expected returns on US stocks and all major global stock market indices have a particular form of non-linear dependence on previous returns. The expected sign of returns tends to reverse after large price movements and trends tend to continue after small movements. The observed market properties are consistent with various models of investor behaviour and can be captured by a simple polynomial model. We further discuss a number of important implications of our findings. Incorrectly fitting a simple linear model to the data leads to a substantial bias in coefficient estimates. We show through the polynomial model that well-known short-term technical trading rules may be substantially driven by the non-linear behaviour observed. The behaviour also has implications for the appropriate calculation of important risk measures such as value at risk. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--24}, 
number = {13}, 
volume = {27}
}
@article{10.1016/j.eneco.2020.105061, 
year = {2021}, 
title = {{The good, the bad and the ugly relation between oil and commodities: An analysis of asymmetric volatility connectedness and portfolio implications}}, 
author = {Maitra, Debasish and Guhathakurta, Kousik and Kang, Sang Hoon}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2020.105061}, 
abstract = {{This study examines the direction and extent of asymmetric volatility connectedness between the oil and commodity markets, using five-minute interval data from the oil, natural gas, and 21 commodity futures markets. We also analyze the positive and negative volatility connectedness through network diagrams to determine the magnitude and strength of the volatility spillover. We suggest optimal portfolios for several oil-commodity pairs minimizing value-at-risk and conditional value-at-risk with higher hedge effectiveness. The results are of significant interest to investors and policymakers. © 2020 Elsevier B.V.}}, 
pages = {105061}, 
number = {NA}, 
volume = {94}
}
@article{10.3390/econometrics7030030, 
year = {2019}, 
title = {{Evaluating approximate point forecasting of count processes}}, 
author = {Homburg, Annika and Weiß, Christian H. and Alwan, Layth C. and Frahm, Gabriel and Göb, Rainer}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics7030030}, 
abstract = {{In forecasting count processes, practitioners often ignore the discreteness of counts and compute forecasts based on Gaussian approximations instead. For both central and non-central point forecasts, and for various types of count processes, the performance of such approximate point forecasts is analyzed. The considered data-generating processes include different autoregressive schemes with varying model orders, count models with overdispersion or zero inflation, counts with a bounded range, and counts exhibiting trend or seasonality. We conclude that Gaussian forecast approximations should be avoided. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {30}, 
number = {3}, 
volume = {7}
}
@article{10.1016/j.insmatheco.2015.01.009, 
year = {2015}, 
title = {{On the effectiveness of natural hedging for insurance companies and pension plans}}, 
author = {Li, Jackie and Haberman, Steven}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.01.009}, 
abstract = {{Natural hedging is one possible method to reduce longevity risk exposure for an annuity provider or a pension plan. In this paper, we provide an assessment of the effectiveness of natural hedging between annuity and life products, using the correlated Poisson Lee-Carter model, Poisson common factor model, product-ratio model, and historical simulation. Our analysis is based on the mortality experience of UK assured lives, pensioners, and annuitants, and the national population of England and Wales. We consider a range of different scenarios, and find that the level of risk reduction is significant in general, with an average of around 60\%. These results have important implications for those insurers, reinsurers, and pension plan sponsors who are seeking ways to hedge their unwanted risk exposures. © 2015 Elsevier B.V.}}, 
pages = {286--297}, 
number = {NA}, 
volume = {61}
}
@article{10.1016/j.insmatheco.2014.07.002, 
year = {2014}, 
title = {{Second order risk aggregation with the Bernstein copula}}, 
author = {Coqueret, Guillaume}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.07.002}, 
abstract = {{We analyze the tail of the sum of two random variables when the dependence structure is driven by the Bernstein family of copulas. We consider exponential and Pareto distributions as marginals. We show that the first term in the asymptotic behavior of the sum is not driven by the dependence structure when a Pareto random variable is involved. Consequences on the Value-at-Risk are derived and examples are discussed. © 2014 Elsevier B.V.}}, 
pages = {150--158}, 
number = {NA}, 
volume = {58}
}
@article{10.1109/icnc.2010.5583173, 
year = {2010}, 
title = {{Value-at-risk forecasting with combined neural network model}}, 
author = {Huapu, Lu and Xinxin, Yu and Jianan, Zhu and Xiaoqiang, Zhao and Nan, Cheng}, 
journal = {2010 Sixth International Conference on Natural Computation}, 
issn = {NA}, 
doi = {10.1109/icnc.2010.5583173}, 
abstract = {{This paper develops a neural network model for solving the Value-at-risk forecasting problems. The application of forecasting methods in neural network models is discussed, which involves normal-GARCH model and grey forecasting model. Compared to the use of traditional models, the new method is fast, easy to implement, numerically reliable. After describing the model, experimental results from Chinese equity market verify the effectiveness and applicability of the proposed work. © 2010 IEEE.}}, 
pages = {746--750}, 
number = {NA}, 
volume = {2}
}
@article{10.1002/for.2374, 
year = {2016}, 
title = {{Forecasting High-Frequency Risk Measures}}, 
author = {Banulescu, Denisa and Colletaz, Gilbert and Hurlin, Christophe and Tokpavi, Sessi}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2374}, 
abstract = {{This article proposes intraday high-frequency risk (HFR) measures for market risk in the case of irregularly spaced high-frequency data. In this context, we distinguish three concepts of value-at-risk (VaR): the total VaR, the marginal (or per-time-unit) VaR and the instantaneous VaR. Since the market risk is obviously related to the duration between two consecutive trades, these measures are completed with a duration risk measure, i.e. the time-at-risk (TaR). We propose a forecasting procedure for VaR and TaR for each trade or other market microstructure event. Subsequently, we perform a backtesting procedure specifically designed to assess the validity of the VaR and TaR forecasts on irregularly spaced data. The performance of the HFR measure is illustrated in an empirical application for two stocks (Bank of America and Microsoft) and an exchange-traded fund based on Standard \& Poor's 500 index. We show that the intraday HFR forecasts capture accurately the volatility and duration dynamics for these three assets. Copyright © 2015 John Wiley \& Sons, Ltd.}}, 
pages = {224--249}, 
number = {3}, 
volume = {35}
}
@article{10.1142/9789811202391_0058, 
year = {2020}, 
title = {{An empirical investigation of the long memory effect on the relation of downside risk and stock returns}}, 
author = {Lee, Cheng Few and Lee, John C and Chen, Cathy Yi-Hsuan and Chiang, Thomas C}, 
issn = {NA}, 
doi = {10.1142/9789811202391\_0058}, 
abstract = {{This chapter resolves an inconclusive issue in the empirical literature about the relationship between downside risk and stock returns for Asian markets. This study demonstrates that the mixed signs on the risk coefficient stem from the fact that the excess stock return series is assumed to be stationary with a short memory, which is inconsistent with the downside risk series featuring a long memory process. After we appropriately model the long memory property of downside risk and apply a fractional difference to downside risk, the evidence consistently supports a significant and positive risk-return relation. This holds true for downside risk not only in the domestic market but also across markets. The evidence suggests that the risk premium is higher if the risk originates in a dominant market, such as the US. These findings are robust even when we consider the leverage effect, value-at-risk feedback, and the long memory effect in the conditional variance. © 2021 by World Scientific Publishing Co. Pte. Ltd.}}, 
pages = {2107--2140}, 
number = {NA}, 
volume = {NA}
}
@article{10.1088/2057-1976/abb4bc, 
year = {2020}, 
title = {{A new optimization algorithm for HDR brachytherapy that improves DVH-based planning: Truncated conditional value-at-risk (TCVaR)}}, 
author = {Wu, Victor W and Epelman, Marina A and Pasupathy, Kalyan S and Sir, Mustafa Y and Deufel, Christopher L}, 
journal = {Biomedical Physics \& Engineering Express}, 
issn = {20571976}, 
doi = {10.1088/2057-1976/abb4bc}, 
abstract = {{Purpose: To introduce a new optimization algorithm that improves DVH results and is designed for the type of heterogeneous dose distributions that occur in brachytherapy. Methods: The new optimization algorithm is based on a prior mathematical approach that uses mean doses of the DVH metric tails. The prior mean dose approach is referred to as conditional value-at-risk (CVaR), and unfortunately produces noticeably worse DVH metric results than gradient-based approaches. We have improved upon the CVaR approach, using the so-called Truncated CVaR (TCVaR), by excluding the hottest or coldest voxels in the structure from the calculations of the mean dose of the tail. Our approach applies an iterative sequence of convex approximations to improve the selection of the excluded voxels. Data Envelopment Analysis was used to quantify the sensitivity of TCVaR results to parameter choice and to compare the quality of a library of 256 TCVaR plans created for each of prostate, breast, and cervix treatment sites with commercially-generated plans. Results: In terms of traditional DVH metrics, TCVaR outperformed CVaR and the improvements increased monotonically as more iterations were used to identify and exclude the hottest/coldest voxels from the optimization problem. TCVaR also outperformed the Eclipse-Brachyvision TPS, with an improvement in PTVD95\% (for equivalent organ-at-risk doses) of up to 5\% (prostate), 3\% (breast), and 1\% (cervix). Conclusions: A novel optimization algorithm for HDR treatment planning produced plans with superior DVH metrics compared with a prior convex optimization algorithm as well as Eclipse-Brachyvision. The algorithm is computationally efficient and has potential applications as a primary optimization algorithm or quality assurance for existing optimization approaches. © 2020 IOP Publishing Ltd}}, 
pages = {065007}, 
number = {6}, 
volume = {6}
}
@article{10.1080/03610926.2011.562782, 
year = {2011}, 
title = {{Adaptive reduced-bias tail index and VaR estimation via the bootstrap methodology}}, 
author = {Gomes, M Ivette and Mendonça, Sandra and Pestana, Dinis}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2011.562782}, 
abstract = {{In this article, we deal with the estimation, under a semi-parametric framework, of a positive extreme value index , the primary parameter in Statistics of Extremes, and associated estimation of the Value at Risk (VaR) at a level p, the size of the loss occurred with a small probability p. We consider second-order minimum-variance reduced-bias (MVRB) estimators, and propose the use of bootstrap computer-intensive methods for the adaptive choice of thresholds, both for γ and Var p. Applications in the fields of insurance and finance, as well as a small-scale simulation study of the bootstrap adaptive estimators behaviour, are also provided. © 2011 Taylor and Francis Group, LLC.}}, 
pages = {2946--2968}, 
number = {16}, 
volume = {40}
}
@article{10.1007/s10479-007-0306-x, 
year = {2009}, 
title = {{Moment based approaches to value the risk of contingent claim portfolios}}, 
author = {Iaquinta, Gaetano and Lamantia, Fabio and Massabò, Ivar and Ortobelli, Sergio}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-007-0306-x}, 
abstract = {{In this paper we describe and apply the estimating function methodology to value the risk of asset derivative portfolios. We first implement the Li's model based on the first four moments and then we show the limits of this model in forecasting the maximum loss of contingent claims. In addition, we show that four moments are not enough to describe the behavior of the lower percentiles of derivatives. Finally, we propose a model that considers the first six moments and we compare the performances of these models proposing a backtest analysis on several historical and truncated asset derivative portfolios. © 2008 Springer Science+Business Media, LLC.}}, 
pages = {97--121}, 
number = {1}, 
volume = {165}
}
@article{10.3390/risks4040036, 
year = {2016}, 
title = {{Nested MC-based risk measurement of complex portfolios: Acceleration and energy efficiency}}, 
author = {Desmettre, Sascha and Korn, Ralf and Varela, Javier Alejandro and Wehn, Norbert}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks4040036}, 
abstract = {{Risk analysis and management currently have a strong presence in financial institutions, where high performance and energy efficiency are key requirements for acceleration systems, especially when it comes to intraday analysis. In this regard, we approach the estimation of the widely-employed portfolio risk metrics value-at-risk (VaR) and conditional value-at-risk (cVaR) by means of nested Monte Carlo (MC) simulations. We do so by combining theory and software/hardware implementation. This allows us for the first time to investigate their performance on heterogeneous compute systems and across different compute platforms, namely central processing unit (CPU), many integrated core (MIC) architecture XeonPhi, graphics processing unit (GPU), and field-programmable gate array (FPGA). To this end, the OpenCL framework is employed to generate portable code, and the size of the simulations is scaled in order to evaluate variations in performance. Furthermore, we assess different parallelization schemes, and the targeted platforms are evaluated and compared in terms of runtime and energy efficiency. Our implementation also allowed us to derive a new algorithmic optimization regarding the generation of the required random number sequences. Moreover, we provide specific guidelines on how to properly handle these sequences in portable code, and on how to efficiently implement nested MC-based VaR and cVaR simulations on heterogeneous compute systems. © 2016 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {36}, 
number = {4}, 
volume = {4}
}
@article{10.1109/isra.2012.6219107, 
year = {2012}, 
title = {{Short selling mechanism, market risk reduced: Evidence from a share market of China}}, 
author = {Wang, Xingyu and Wang, Fan}, 
journal = {2012 IEEE Symposium on Robotics and Applications (ISRA)}, 
issn = {NA}, 
doi = {10.1109/isra.2012.6219107}, 
abstract = {{This paper use VaR (value at risk) as the main risk measure and evaluate it by the quartile regression model, choosing the data from A share market for positive analysis. The results show that the VaR has decreased markedly since short selling mechanism was set up; the VaR series have remained volatile and displayed no trend without short selling mechanism; the VaR series have obvious decreasing tendency under short selling mechanism. © 2012 IEEE.}}, 
pages = {13--15}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jrmv.2018.197, 
year = {2018}, 
title = {{Back to backtesting: Integrated backtesting for value-at-risk and expected shortfall in practice}}, 
author = {Wehn, Carsten S}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2018.197}, 
abstract = {{This paper aims to reflect the current state of the discussion on the validation of market risk forecasts by means of backtesting. Due to the upcoming Fundamental Review of the Trading Book (FRTB), the importance of expected shortfall (ES) as a risk measure will be significantly strengthened, which, in turn, should influence the discussion on appropriate approaches for its proper validation. While procedures based on forecast distribution and backtesting for value-at-risk have already been established and discussed intensively, the development of a standard for backtesting ES is still in full swing. In addition to a classification in this context, our paper offers a practical application for backtesting via an example portfolio with three comparative models and shows how to integrate the different backtesting methods in a holistic framework. © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.econmod.2018.11.022, 
year = {2019}, 
title = {{Modelling the spreading process of extreme risks via a simple agent-based model: Evidence from the China stock market}}, 
author = {Ji, Jingru and Wang, Donghua and Xu, Dinghai}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2018.11.022}, 
abstract = {{This paper focuses on investigating financial asset returns' extreme risks, which are defined as the negative log-returns over a certain threshold. A simple agent-based model is constructed to explain the behavior of the market traders when extreme risks occur. We consider both the volatility clustering and the heavy tail characteristics when constructing the model. Empirical study uses the China securities index 300 daily level data and applies the method of simulated moments to estimate the model parameters. The stationarity and ergodicity tests provide evidence that the proposed model is good for estimation and prediction. The goodness-of-fit measures show that our proposed model fits the empirical data well. Our estimated model performs well in out-of-sample Value-at-Risk prediction, which contributes to the risk management. © 2018 Elsevier B.V.}}, 
pages = {383--391}, 
number = {NA}, 
volume = {80}
}
@article{10.3390/en13051045, 
year = {2020}, 
title = {{Optimization of electric energy sales strategy based on probabilistic forecasts}}, 
author = {Janczura, Joanna and Michalak, Aleksandra}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13051045}, 
abstract = {{In this paper we propose an optimization scheme for a selling strategy of an electricity producer who in advance decides on the share of electricity sold on the day-ahead market. The remaining part is sold on the complementary (intraday/balancing) market. To this end, we use probabilistic forecasts of the future selling price distribution. Next, we find an optimal share of electricity sold on the day-ahead market using one of the three objectives: maximization of the overall profit, minimization of the sellers risk, or maximization of the median of portfolio values. Using data from the Polish day-ahead and balancing markets, we show that the assumed objective is achieved, as compared to the naive strategy of selling the whole produced electricity only on the day-ahead market. However, an increase of the profit is associated with a significant increase of the risk. © 2020 by the authors.}}, 
pages = {1045}, 
number = {5}, 
volume = {13}
}
@article{10.21307/stattrans-2016-049, 
year = {2016}, 
title = {{Interval estimation of higher order quantiles. Analysis of accuracy of selected procedures}}, 
author = {Pekasiewicz, Dorota}, 
journal = {Statistics in Transition New Series}, 
issn = {12347655}, 
doi = {10.21307/stattrans-2016-049}, 
abstract = {{In the paper selected nonparametric and semiparametric estimation methods of higher orders quantiles are considered. The construction of nonparametric confidence intervals is based on order statistics of appropriate ranks from random samples or from generated bootstrap samples. Semiparametric bootstrap methods are characterized by double bootstrap simulations. The values of bootstrap sample below the prearranged threshold are generated by the empirical distribution and the values above this threshold are generated by the distribution based on the asymptotic properties of the tail of the random variable distribution. The results of the study allow one to draw conclusions about the effectiveness of the considered procedures and to compare these methods.}}, 
pages = {737--748}, 
number = {4}, 
volume = {17}
}
@article{10.1007/978-3-642-38279-6_9, 
year = {2013}, 
title = {{Risk behavior of stock markets before and after the subprime crisis}}, 
author = {Thomasz, Esteban Otto and Bariviera, Aurelio Fernández}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-642-38279-6\_9}, 
abstract = {{The aim of this paper is to present a simple classification of traditional risk indicators of stock markets. The last 10 years provides extensive evidence of changes of behavior of markets around the globe, combining a period of continuous growth with low volatility, an extreme crisis and a recuperation period. We present a simple analysis of risk indicators, such as implied volatility, value at risk, measuring of extreme events, etc. of 30 stock markets around the globe. Additionally we make a comparative analysis of such risk measures before and after the subprime crisis. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {83--90}, 
number = {NA}, 
volume = {145}
}
@article{10.1109/epetsg.2018.8658380, 
year = {2019}, 
title = {{Risk Based Benefit Analysis of RES Aggregator in Restructured Energy Market}}, 
author = {Pandey, Harsh Wardhan and Gadham, Kumar Raja and Ghose, T.}, 
journal = {2018 2nd International Conference on Power, Energy and Environment: Towards Smart Technology (ICEPE)}, 
issn = {NA}, 
doi = {10.1109/epetsg.2018.8658380}, 
abstract = {{In the market environment where a number of microgrids are involved, the operational policies of the decision making entities demand an in depth analysis of the situation that can arise during different events. Microgrid is an active network where different types of Distributed Energy Sources(DES) are used. Most of the DES used earlier was Diesel generators but the concern for the environment along with the high operational cost of diesel generators has encouraged the use of alternatives such asRenewable Energy Sources (RES). The uncertain nature of the power generated by different RES is not easy to deal with. This paper aims to realize the risk in power generation and the financial benefit extracted from the generated power. While deciding the day-ahead power to be committed in the wholesale energy market, therisk associated with it should be considered. In this paper two of the well-known risk assessment techniques VaR and CVaR are used to predict the power generated by the RES at a particular level of risk. The segregation of the aggregator has also been done to show the different business strategies taken by different type of aggregator working in risky environment. © 2018 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s00180-016-0646-6, 
year = {2016}, 
title = {{Comparison of Value-at-Risk models using the MCS approach}}, 
author = {Bernardi, Mauro and Catania, Leopoldo}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-016-0646-6}, 
abstract = {{This paper compares the Value-at-Risk (VaR) forecasts delivered by alternative model specifications using the Model Confidence Set (MCS) procedure recently developed by Hansen et al. (Econometrica 79(2):453–497, 2011). The direct VaR estimate provided by the Conditional Autoregressive Value-at-Risk (CAViaR) models of Engle and Manganelli (J Bus Econ Stat 22(4):367–381, 2004) are compared to those obtained by the popular Autoregressive Conditional Heteroskedasticity (ARCH) models of Engle (Econometrica 50(4):987–1007, 1982) and to the Generalised Autoregressive Score (GAS) models recently introduced by Creal et al. (J Appl Econom 28(5):777–795, 2013) and Harvey (Dynamic models for volatility and heavy tails: with applications to financial and economic time series. Cambridge University Press, Cambridge, 2013). The MCS procedure consists in a sequence of tests which permits to construct a set of “superior” models, where the null hypothesis of Equal Predictive Ability (EPA) is not rejected at a certain confidence level. Our empirical results, suggest that, during the European Sovereign Debt crisis of 2009–2010, highly non-linear volatility models deliver better VaR forecasts for the European countries as opposed to other regional indexes. Model comparisons have been performed using the (Formula presented.) package MCS developed by the authors and freely available at the CRAN website. © 2016, Springer-Verlag Berlin Heidelberg.}}, 
pages = {579--608}, 
number = {2}, 
volume = {31}
}
@article{10.1061/41139(387)297, 
year = {2010}, 
title = {{Study of price risk value of inventory financing}}, 
author = {Hu, Qifan and Hu, Min and Liu, Yuhao}, 
journal = {ICLEM 2010}, 
issn = {NA}, 
doi = {10.1061/41139(387)297}, 
abstract = {{Inventory financing is a new type of value adding service which is provided by a logistic firm to their customers who do not satisfy the requirements for the conventional mortgages. The customers hand over their own product to a logistic firm in order to acquire a short term loan. Inventory Financing's development is based on the efficiency of its risk control; and the premise of price risk control is to precisely predict the value of quality materials at risk. This article uses the historical simulation method to calculate the value at risk of Chang Jiang 1\# copper spot in various conditions and implement review test on the results, analyze the product and summarize the elements that affect the calculation results. © 2010 ASCE.}}, 
pages = {2132--2140}, 
number = {NA}, 
volume = {387}
}
@article{10.1016/j.csda.2013.09.028, 
year = {2014}, 
title = {{Sovereign credit ratings, market volatility, and financial gains}}, 
author = {Afonso, António and Gomes, Pedro and Taamouti, Abderrahim}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2013.09.028}, 
abstract = {{The reaction of EU bond and equity market volatilities to sovereign rating announcements (Standard \& Poor's, Moody's, and Fitch) is investigated using a panel of daily stock market and sovereign bond returns. The parametric volatilities are defined using EGARCH specifications. The estimation results show that upgrades do not have significant effects on volatility, but downgrades increase stock and bond market volatility. Contagion is present, and sovereign rating announcements create interdependence among European financial markets with upgrades (downgrades) in one country leading to a decrease (increase) in volatility in other countries. The empirical results show also a financial gain and risk (value-at-risk) reduction for portfolio returns when taking into account sovereign credit ratings' information for volatility modelling, with financial gains decreasing with higher risk aversion. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {20--33}, 
number = {NA}, 
volume = {76}
}
@article{10.1016/s0377-2217(00)00265-4, 
year = {2001}, 
title = {{Linear and nonlinear dependence in Turkish equity returns and its consequences for financial risk management}}, 
author = {Harris, Richard D.F. and Küçüközmen, C.Coskun}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/s0377-2217(00)00265-4}, 
abstract = {{This paper investigates the dynamic behaviour of daily aggregate returns of one of Europe's largest and fastest growing emerging equity markets, the Istanbul Stock Exchange (ISE). It is found that ISE returns exhibit significant linear and nonlinear dependence. We investigate the nature and source of the nonlinear dependence and find that it is due primarily to linear dependence in the conditional variance of returns, rather than nonlinear dependence in the conditional mean. We analyse the implications of our findings for financial risk management. We show that by exploiting the nonlinear dependence in ISE returns, the average capital required to cover against unexpected portfolio losses can be considerably reduced. In contrast, exploiting the linear dependence in ISE returns has only a negligible impact on capital requirements. © 2001 Elsevier Science B.V. All rights reserved.}}, 
pages = {481--492}, 
number = {3}, 
volume = {134}
}
@article{10.20965/jaciii.2013.p0520, 
year = {2013}, 
title = {{Ordered weighted averages on intervals and the sub/super-additivity}}, 
author = {Yoshida, Yuji and {Japan, Faculty of Economics and Business Administration, University of Kitakyushu, 4-2-1 Kitagata, Kokuraminami, Kitakyushu 802-8577,}}, 
journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics}, 
issn = {13430130}, 
doi = {10.20965/jaciii.2013.p0520}, 
abstract = {{This paper deals with continuous Ordered Weighted Averages (OWA) on a closed interval and investigates their fundamental properties. In this paper, we focus on OWA with a truncation weight and derive the subadditivity of a top-concentrated average. We then deal with OWA from the bottom and investigate their relations. The subadditivity for OWA with monotone weights is also discussed, then OWA based on probability are demonstrated and value-at-risks are explained as an example.}}, 
pages = {520--525}, 
number = {4}, 
volume = {17}
}
@article{10.1016/j.pacfin.2018.06.003, 
year = {2018}, 
title = {{Modeling extreme risks in commodities and commodity currencies}}, 
author = {Fuentes, Fernanda and Herrera, Rodrigo and Clements, Adam}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2018.06.003}, 
abstract = {{This paper examines extreme co-movements between the Australian and Canadian currencies, often known as commodity currencies, and gold and oil markets respectively. Here, two main approaches based on extreme value theory are compared in the context of explaining the co-movements between the markets in times of market instability. On the one hand, the intensity of the extreme events is represented by self-exciting marked point processes using a multivariate extension of the Hawkes-POT model, while contemporaneous co-movements are characterized utilizing a more traditional multivariate volatility model, the DBEKK-EVT model. It is found that intensity and volatility follow similar paths through time. The Hawkes-POT model reveals the unidirectional influence of the commodity on the currency, consistent with previous literature. Hawkes-POT model produces slightly more accurate Value at Risk results in the in-sample period, while the results are mixed in the backtesting period. Overall it seems as though the simpler multivariate volatility based approach produce forecasts of extreme risk that are comparable to the more complex Hawkes model. © 2018}}, 
pages = {108--120}, 
number = {NA}, 
volume = {51}
}
@article{10.1016/j.csda.2015.12.008, 
year = {2016}, 
title = {{Estimating extreme tail risk measures with generalized Pareto distribution}}, 
author = {Park, Myung Hyun and Kim, Joseph H.T.}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2015.12.008}, 
abstract = {{The generalized Pareto distribution (GPD) has been widely used in modelling heavy tail phenomena in many applications. The standard practice is to fit the tail region of the dataset to the GPD separately, a framework known as the peaks-over-threshold (POT) in the extreme value literature. In this paper we propose a new GPD parameter estimator, under the POT framework, to estimate common tail risk measures, the Value-at-Risk (VaR) and Conditional Tail Expectation (also known as Tail-VaR) for heavy-tailed losses. The proposed estimator is based on a nonlinear weighted least squares method that minimizes the sum of squared deviations between the empirical distribution function and the theoretical GPD for the data exceeding the tail threshold. The proposed method properly addresses a caveat of a similar estimator previously advocated, and further improves the performance by introducing appropriate weights in the optimization procedure. Using various simulation studies and a realistic heavy-tailed model, we compare alternative estimators and show that the new estimator is highly competitive, especially when the tail risk measures are concerned with extreme confidence levels. © 2015 Elsevier B.V. All rights reserved.}}, 
pages = {91--104}, 
number = {NA}, 
volume = {98}
}
@article{10.1016/s0140-9883(02)00111-1, 
year = {2003}, 
title = {{Estimating oil price 'Value at Risk' using the historical simulation approach}}, 
author = {Cabedo, J. David and Moya, Ismael}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/s0140-9883(02)00111-1}, 
abstract = {{In this paper we propose using Value at Risk (VaR) for oil price risk quantification. VaR provides an estimation for the maximum oil price change associated with a likelihood level, and can be used for designing risk management strategies. We analyse three VaR calculation methods: The historical simulation standard approach, the historical simulation with ARMA forecasts (HSAF) approach, developed in this paper, and the variance-covariance method based on autoregressive conditional heteroskedasticity models forecasts. The results obtained indicate that HSAF methodology provides a flexible VaR quantification, which fits the continuous oil price movements well and provides an efficient risk quantification. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {239--253}, 
number = {3}, 
volume = {25}
}
@article{10.1016/j.jbankfin.2021.106097, 
year = {2021}, 
title = {{A new approach to credit ratings}}, 
author = {Pertaia, Giorgi and Prokhorov, Artem and Uryasev, Stan}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2021.106097}, 
abstract = {{Credit ratings are fundamental in assessing the credit risk of a security or debtor. The failure of the Collateralized Debt Obligation (CDO) ratings during the financial crisis of 2007-2008 and the massive undervaluation of corporate risk leading up to the crisis resulted in a review of rating approaches. Yet the fundamental metric that guides the construction of credit ratings has not changed. We study the inadequacies of the old metric in simple models of investment and in structured finance portfolio optimization tasks, and we propose a new methodology based on a buffered probability of exceedance. The new approach offers a conservative risk assessment, with substantial conceptual and computational benefits. We illustrate the new approach using several examples and report the results of a structuring step-up CDO case study, with details available in an online Supplement. © 2021 Elsevier B.V.}}, 
pages = {106097}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11846-011-0071-8, 
year = {2013}, 
title = {{Stochastic orders and non-Gaussian risk factor models}}, 
author = {Höse, Steffi and Huschens, Stefan}, 
journal = {Review of Managerial Science}, 
issn = {18636683}, 
doi = {10.1007/s11846-011-0071-8}, 
abstract = {{The main results of this paper are monotonicity statements about the risk measures value-at-risk (VaR) and tail value-at-risk (TVaR) with respect to the parameters of single and multi risk factor models, which are standard models for the quantification of credit and insurance risk. In the context of single risk factor models, non-Gaussian distributed latent risk factors are allowed. It is shown that the TVaR increases with increasing claim amounts, probabilities of claims and correlations, whereas the VaR is in general not monotone in the correlation parameters. To compare the aggregated risks arising from single and multi risk factor models, the usual stochastic order and the increasing convex order are used in this paper, since these stochastic orders can be interpreted as being induced by the VaR-concept and the TVaR-concept, respectively. To derive monotonicity statements about these risk measures, properties of several further stochastic orders are used and their relation to the usual stochastic order and to the increasing convex order are applied. © 2011 Springer-Verlag.}}, 
pages = {99--140}, 
number = {2}, 
volume = {7}
}
@article{10.2202/1553-779x.2161, 
year = {2009}, 
title = {{Strategic bidding and risk assessment using genetic algorithm in electricity markets}}, 
author = {Jain, Arvind Kumar and Srivastava, S.C.}, 
journal = {International Journal of Emerging Electric Power Systems}, 
issn = {1553779X}, 
doi = {10.2202/1553-779x.2161}, 
abstract = {{In an electricity market, suppliers are more concerned with maximizing their profit and minimizing the financial risk, which can be achieved through strategic bidding. In this paper, Equal Incremental Cost Criteria (EICC) has been used for developing the optimal bidding strategy. The rival's bidding behavior has been formulated using a stochastic optimization model. Genetic Algorithm (GA), along with ac sensitivity factors, has been used to decide the optimal bidding strategy including congestion management to maximize the profit of the suppliers, considering single sided as well as double sided bidding. Both pure as well as probabilistic strategies have been simulated. Results with Sequential Quadratic Programming (SQP), a classical optimization method, and dc sensitivity factors have also been obtained to compare and establish the effectiveness of proposed method. Value at Risk (VaR) has been calculated as a measure of financial risk. © 2009 The Berkeley Electronic Press. All rights reserved.}}, 
number = {5}, 
volume = {10}
}
@article{10.1080/07350015.2021.1929249, 
year = {2021}, 
title = {{Realized Quantiles*}}, 
author = {Dimitriadis, Timo and Halbleib, Roxana}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1080/07350015.2021.1929249}, 
abstract = {{This article proposes a simple approach to estimate quantiles of daily financial returns directly from high-frequency data. We denote the resulting estimator as realized quantile (RQ) and use it to forecast tail risk measures, such as Value at Risk (VaR) and Expected Shortfall (ES). The RQ estimator is built on the assumption that financial logarithm prices are subordinated self-similar processes in intrinsic time. The intrinsic time dimension stochastically transforms the clock time in order to capture the real “heartbeat” of financial markets in accordance with their trading activity and/or riskiness. The self-similarity assumption allows to compute daily quantiles by simply scaling up their intraday counterparts, while the subordination technique can easily accommodate numerous empirical features of financial returns, such as volatility persistence and fat-tailedness. Our method, which is built on a flexible assumption, is simple to implement and exploits the rich information content of high-frequency data from another time perspective than the classical clock time. In a comprehensive empirical exercise, we show that our forecasts of VaR and ES are more accurate than the ones from a large set of up-to-date comparative models, for both, stocks and foreign exchange rates. © 2021 The Author(s). Published with license by Taylor \& Francis Group, LLC.}}, 
pages = {1--43}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/02102412.2005.10779567, 
year = {2005}, 
title = {{Risk analysis measurement in life insurance portfolios [Análisis y medición del riesgo financiero en carteras de vida]}}, 
author = {González, José Luis Otero}, 
journal = {Spanish Journal of Finance and Accounting / Revista Española de Financiación y Contabilidad}, 
issn = {02102412}, 
doi = {10.1080/02102412.2005.10779567}, 
abstract = {{The purpose of this paper is analysing financial risk in life insurance sector. First of all we study the different techniques that could be used and we select those that fit better with the sector. Then we propose a system based upon the selected techniques that could be used with different ob­jectives in management and determination of risk adjusted capital needs. We finish with a review of the state of art in practice in our country. © 2005, Routledge. All rights reserved.}}, 
pages = {925--950}, 
number = {127}, 
volume = {34}
}
@article{10.3390/e21020102, 
year = {2019}, 
title = {{Using high-frequency entropy to forecast Bitcoin's daily value at risk}}, 
author = {Pele, Daniel Traian and Mazurencu-Marinescu-Pele, Miruna}, 
journal = {Entropy}, 
issn = {10994300}, 
doi = {10.3390/e21020102}, 
pmid = {33266818}, 
pmcid = {PMC7514585}, 
abstract = {{In this paper we investigate the ability of several econometrical models to forecast value at risk for a sample of daily time series of cryptocurrency returns. Using high frequency data for Bitcoin, we estimate the entropy of intraday distribution of logreturns through the symbolic time series analysis (STSA), producing low-resolution data from high-resolution data. Our results show that entropy has a strong explanatory power for the quantiles of the distribution of the daily returns. Based on Christoffersen's tests for Value at Risk (VaR) backtesting, we can conclude that the VaR forecast build upon the entropy of intraday returns is the best, compared to the forecasts provided by the classical GARCH models. © 2019 by the authors.}}, 
pages = {102}, 
number = {2}, 
volume = {21}
}
@article{10.13334/j.0258-8013.pcsee.2015.18.012, 
year = {2015}, 
title = {{Optimal sizing of energy storage for microgrids considering energy management of electric vehicles}}, 
author = {}, 
issn = {02588013}, 
doi = {10.13334/j.0258-8013.pcsee.2015.18.012}, 
abstract = {{The engineering method of energy storage sizing and the operation and management of electric vehicles (EVs) were studied considering the moving load and storage characteristics of EVs in microgrids. The value at risk (VaR) theory was applied to evaluating the risk of the configured storage. Three EV regional energy management modes including uncoordinated charging, coordinated charging and coordinated charging/discharging were formulated utilizing a price incentive model. The optimal storage size was calculated by minimizing the comprehensive cost of microgrid in a day in grid-connected mode. The impacts of interactive power penalty coefficient and fluctuation on selecting storage size were also analyzed. VaR theory was applied to analyzing the islanded operation risk in various energy managements and the feasibility of the configured storage size was verified. The simulation result validates that compared with uncoordinated charging mode, coordinated charging and coordinated charging /discharging modes reduces storage size, lowers the comprehensive cost and decreased the fluctuation of interactive power. Compared with no EV participating in islanded microgrid, the length of islanded time is extended and the risk of autonomous islanded operation is decreased significantly when encouraging EV to discharge. © 2015 Chin. Soc. for Elec. Eng.}}, 
number = {18}, 
volume = {35}
}
@article{10.1109/ccdc.2009.5192638, 
year = {2009}, 
title = {{The stock portfolios simulated annealing genetic algorithm based on RAROC}}, 
author = {LI, Yun-fei and Wei, GUO}, 
journal = {2009 Chinese Control and Decision Conference}, 
issn = {NA}, 
doi = {10.1109/ccdc.2009.5192638}, 
abstract = {{Within the mean-variance model of Markowitz portfolio framework, we propose a betterment portfolio optimize model, the optimize model take the risk value as the tools of risk measurement and use the risk adjustment return as the optimization function, at the same time solve portfolio by simulated annealing genetic algorithm and validate the model's validity in reality by empirical study. The model accord with the risk return rule of investment tool in theory, the genetic algorithm make the validity of utilize this model in reality. This model and the algorithm have high validity in the empirical study of A-share stock market. © 2009 IEEE.}}, 
pages = {3574--3578}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/iciii.2012.6339654, 
year = {2012}, 
title = {{Portfolio Value-at-Risk estimating on markov regime switching copula-autoregressive conditional jump intensity-threshold generalized autoregressive conditional heteroscedasticity model}}, 
author = {Xie, Chi and Yao, Lin}, 
journal = {2012 International Conference on Information Management, Innovation Management and Industrial Engineering}, 
issn = {NA}, 
doi = {10.1109/iciii.2012.6339654}, 
abstract = {{Considering asymmetric dependence between assets and jump dynamics and asymmetric volatility in asset returns, in this paper, we develop a markov regime switching copula-autoregressive conditional jump intensity-threshold generalized autoregressive conditional heteroscedasticity model in order to estimate the Value-a-Risk of industry indices portfolios. The empirical research shows that, the markov regime switching copula-autoregressive conditional jump intensity-threshold generalized autoregressive conditional heteroscedasticity model can comprehensively reflect the probability of extreme returns, which makes it optimal in estimating Value-at-Risk and outperforming other models which ignore jump dynamics in assets returns or asymmetric dependence between assets or asymmetric volatility in asset returns. Those above suggest that the model we construct can improve the accuracy of VaR estimating, and help investors make cautious decisions to manage risk effectively. © 2012 IEEE.}}, 
pages = {278--281}, 
number = {NA}, 
volume = {1}
}
@article{10.1198/073500103288619232, 
year = {2003}, 
title = {{Maximum Likelihood Estimation and Inference in Multivariate Conditionally Heteroscedastic Dynamic Regression Models with Student t Innovations}}, 
author = {Fiorentini, Gabriele and Sentana, Enrique and Calzolari, Giorgio}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/073500103288619232}, 
abstract = {{We provide numerically reliable analytical expressions for the score, Hessian, and information matrix of conditionally heteroscedastic dynamic regression models when the conditional distribution is multivariate t. We also derive one-sided and two-sided Lagrange multiplier tests for multivariate normality versus multivariate t based on the first two moments of the squared norm of the standardized innovations evaluated at the Gaussian pseudo-maximum likelihood estimators of the conditional mean and variance parameters. Finally, we illustrate our techniques through both Monte Carlo simulations and an empirical application to 26 U.K. sectorial stock returns that confirms that their conditional distribution has fat tails.}}, 
pages = {532--546}, 
number = {4}, 
volume = {21}
}
@article{10.1016/j.intfin.2011.02.001, 
year = {2011}, 
title = {{Intertemporal risk-return trade-off in foreign exchange rates}}, 
author = {Christiansen, Charlotte}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2011.02.001}, 
abstract = {{We investigate the intertemporal risk-return trade-off of foreign exchange (FX) rates for ten currencies quoted against the USD. For each currency, we use three risk measures simultaneously that pertain to that currency; its realized volatility, its realized skewness, and its value-at-risk. We apply monthly FX excess returns and risk measures calculated from daily observations. We find that there is a significant contemporaneous risk-return trade-off for the currencies under investigation. There is no evidence of noncontemporaneous risk-return trade-off. We pay special attention to the risk-return trade-off during the recent financial crisis. © 2011 Elsevier B.V.}}, 
pages = {535--549}, 
number = {4}, 
volume = {21}
}
@article{10.1016/j.jbankfin.2013.07.022, 
year = {2014}, 
title = {{Unexpected tails in risk measurement: Some international evidence}}, 
author = {Tolikas, Konstantinos}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.07.022}, 
abstract = {{Risk management critically depends on the assumptions made about the distribution of stock returns. This paper applies extreme value methods to investigate the limiting distribution of the extreme returns of the NIKKEI225, FTSE100 and S\&P500 indices as well as the indices of some of largest sectors in Japan, UK and US. The results indicate that the much celebrated Generalised Extreme Value distribution does not provide the most accurate description of the minima since the Generalised Logistic distribution performs better due to its ability to better capture the fat tails of returns. The time varying nature of extremes is also confirmed while a simulation exercise adds to the robustness of our results. It is also shown that the findings may have important implications for risk models, such as VaR and Expected Shortfall, since risk measures which cannot capture the fatness of tails of the empirical distribution function of returns may lead to serious underestimation of downside risk. © 2013 .}}, 
pages = {476--493}, 
number = {1}, 
volume = {40}
}
@article{10.1007/11428848_10, 
year = {2005}, 
title = {{Financial computations on clusters using web services}}, 
author = {Chinchalkar, Shirish and Coleman, Thomas F. and Mansfield, Peter}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/11428848\_10}, 
abstract = {{The pricing of a portfolio of financial instruments is a common and important computational problem in financial engineering. In addition to pricing, a portfolio or risk manager may be interested in determining an effective hedging strategy, computing the value at risk, or valuing the portfolio under several different scenarios. Because of the size of many practical portfolios and the complexity of modern financial instruments the computing time to solve these problems can be several hours. We demonstrate a powerful and practical method for solving these problems on clusters using web services. © Springer-Verlag Berlin Heidelberg 2005.}}, 
pages = {76--83}, 
number = {II}, 
volume = {3515}
}
@article{10.1016/j.ecolecon.2020.106747, 
year = {2020}, 
title = {{The Economy-Wide Value-at-Risk from the Exposure of Natural Capital to Climate Change and Extreme Natural Events: The Case of Wind Damage and Forest Recreational Services in New Zealand}}, 
author = {Monge, Juan J. and McDonald, Garry W.}, 
journal = {Ecological Economics}, 
issn = {09218009}, 
doi = {10.1016/j.ecolecon.2020.106747}, 
abstract = {{The long-term management of natural capital is essential for the stable and resilient flow of ecosystem services for future generations facing climatic uncertainty. Understanding its resilience to extreme natural events, using biological principles, and integrating them into more holistic systems-wide modelling techniques is crucial to evaluate and design the wider economic impacts from alternative management interventions. Here we developed a probabilistic and system-wide bio-economic model to measure extreme economic impacts from natural events using the financial concept of Value at Risk. We have applied the new extreme economic impact metric to measure the economy-wide impact from the disruption of mountain-biking recreational services in the Whakarewarewa peri-urban forest in Rotorua, New Zealand from a potential windthrow event. We identified that extreme natural events, such as windthrows, could exert a substantial impact on the urban forest-dependent regional economy. This impact could potentially be intensified by the CO2 forest fertilization effects predicted under future climate scenarios and by adopting low-intensity silvicultural regimes currently rewarded by national climate change mitigation policies. Some of the insights from this study could be the precursors to develop robust adaptation strategies using robust decision-making techniques considering inclusive environmental-economic accounting frameworks. © 2020 Elsevier B.V.}}, 
pages = {106747}, 
number = {NA}, 
volume = {176}
}
@article{10.1016/j.jbankfin.2007.01.019, 
year = {2007}, 
title = {{Mean-variance portfolio selection with 'at-risk' constraints and discrete distributions}}, 
author = {Alexander, Gordon J. and Baptista, Alexandre M. and Yan, Shu}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2007.01.019}, 
abstract = {{We examine the impact of adding either a VaR or a CVaR constraint to the mean-variance model when security returns are assumed to have a discrete distribution with finitely many jump points. Three main results are obtained. First, portfolios on the VaR-constrained boundary exhibit (K + 2)-fund separation, where K is the number of states for which the portfolios suffer losses equal to the VaR bound. Second, portfolios on the CVaR-constrained boundary exhibit (K + 3)-fund separation, where K is the number of states for which the portfolios suffer losses equal to their VaRs. Third, an example illustrates that while the VaR of the CVaR-constrained optimal portfolio is close to that of the VaR-constrained optimal portfolio, the CVaR of the former is notably smaller than that of the latter. This result suggests that a CVaR constraint is more effective than a VaR constraint to curtail large losses in the mean-variance model. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {3761--3781}, 
number = {12}, 
volume = {31}
}
@article{10.1007/s10690-013-9171-6, 
year = {2013}, 
title = {{Does Cross-Listing Benefit the Shareholders? Evidence from Companies in the GCC Countries?}}, 
author = {Bahlous, Mejda}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-013-9171-6}, 
abstract = {{The goal of this study is to estimate the impact of cross-listing on stock returns, on liquidity, and on risk. A sample of 24 companies from the Gulf Cooperation Council countries which cross-listed their stocks in a foreign market over the period 2000-2010 were chosen for study. An event study estimating abnormal returns related to the cross-listing event as well as parametric and nonparametric tests find that there is (1) a significant abnormal return of about 6 \% that lasts until 6 days after the cross-listing day and starts fading away thereafter (2) a significant increase in liquidity during the event period for most firms and (3) on average a decrease in risk. Our results also suggest that cross-listing had a small impact on market risk measured by the average beta but led to a decrease in the total risk measured by standard deviation of returns and a decrease in the potential loss measured by the average value at risk at the 5 \% confidence. Additionally, an analysis based on the foreign market of secondary listing suggests that the benefit of cross-listing varies with the market of secondary listing. The positive abnormal return is more obvious for companies that cross-listed in Kuwait, Bahrain, and London. The most obvious increase in liquidity is for firms that cross-listed in London or in Bahrain and the biggest decrease in risk is for companies that cross-listed in London. We conclude overall that cross-listing in London benefits the shareholders the most as it leads to positive significant abnormal returns, an increase in liquidity, and a decrease in risk. © 2013 Springer Science+Business Media New York.}}, 
pages = {345--381}, 
number = {4}, 
volume = {20}
}
@article{10.1111/mafi.12097, 
year = {2017}, 
title = {{MODEL UNCERTAINTY AND SCENARIO AGGREGATION}}, 
author = {Cambou, Mathieu and Filipović, Damir}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12097}, 
abstract = {{This paper provides a coherent method for scenario aggregation addressing model uncertainty. It is based on divergence minimization from a reference probability measure subject to scenario constraints. An example from regulatory practice motivates the definition of five fundamental criteria that serve as a basis for our method. Standard risk measures, such as value-at-risk and expected shortfall, are shown to be robust with respect to minimum divergence scenario aggregation. Various examples illustrate the tractability of our method. © 2015 Wiley Periodicals, Inc.}}, 
pages = {534--567}, 
number = {2}, 
volume = {27}
}
@article{10.1002/9781118650318.ch21, 
year = {2016}, 
title = {{EVT Seen by a Vet: A Practitioner's Experience on Extreme Value Theory}}, 
author = {Boulier, Jean‐François}, 
issn = {NA}, 
doi = {10.1002/9781118650318.ch21}, 
abstract = {{This chapter situates the development of quantitative finance with the development of financial regulation and internal risk management in financial institutions. Related to extreme events, he discusses the concept of "stress scenarios", which complements the value-at-risk measure. He argues that while models based on normality do their job of computing the VaR (associated with market shocks appearing every four years on average), extreme value theory adds value for designing "stress scenarios" (associated with extreme market shocks appearing every 20 or 50 years on average). Finally, Jean-François asks the question: what could EVT additionally bring to the party? © 2017 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {525--528}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2004.08.010, 
year = {2005}, 
title = {{Value-at-risk versus expected shortfall: A practical perspective}}, 
author = {Yamai, Yasuhiro and Yoshiba, Toshinao}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2004.08.010}, 
abstract = {{Value-at-Risk (VaR) has become a standard risk measure for financial risk management. However, many authors claim that there are several conceptual problems with VaR. Among these problems, an important one is that VaR disregards any loss beyond the VaR level. We call this problem the "tail risk". In this paper, we illustrate how the tail risk of VaR can cause serious problems in certain cases, cases in which expected shortfall can serve more aptly in its place. We discuss two cases: concentrated credit portfolio and foreign exchange rates under market stress. We show that expected shortfall requires a larger sample size than VaR to provide the same level of accuracy. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {997--1015}, 
number = {4}, 
volume = {29}
}
@article{10.20965/jaciii.2012.p0800, 
year = {2012}, 
title = {{A dynamic risk allocation of value-at-risks with portfolios}}, 
author = {Yoshida, Yuji and {Japan, Faculty of Economics and Business Administration, University of Kitakyushu, 4-2-1 Kitagata, Kokuraminami, Kitakyushu 802-8577,}}, 
journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics}, 
issn = {13430130}, 
doi = {10.20965/jaciii.2012.p0800}, 
abstract = {{A mathematical dynamic portfolio model with uncertainty is discussed by use of value-at-risks. The risk criterion is composed by the sum of unexpected shortterm risks which occur suddenly in each period. By dynamic programming approach, we derive an optimality condition for the optimal value-at-risk portfolio in a stochastic decision process. It is shown that the optimal value-at-risk is a solution of the optimality equation under a reasonable assumption, and an optimal trading strategy is obtained from the equation. A numerical example is given to illustrate our idea.}}, 
pages = {800--806}, 
number = {7}, 
volume = {16}
}
@article{10.1016/j.najef.2018.12.012, 
year = {2019}, 
title = {{Extreme dependence and risk spillovers across north american equity markets}}, 
author = {Warshaw, Evan}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2018.12.012}, 
abstract = {{This study analyzes risk spillovers across North American equity markets over 1995–2016. Downside and upside Conditional Value-at-Risk (CoVaR) are estimated after modeling the dynamic dependence structure for each equity market pair using generalized autoregressive score (GAS) copulas. US-CAN and CAN-MX dynamic correlations trend upwards over the sample period while the US-CAN correlation fluctuates around a higher long-run average. Conditional tail dependence is symmetric and significantly higher following the Global Financial Crisis (GFC) in all cases, implying greater co-movement under extreme economic conditions. Downside and upside risk spillovers are significant and asymmetric along two dimensions for each equity market pair, where downside risk spillovers are more severe and the degree of asymmetry by conditioning direction is rank ordered by relative equity market sizes. Aside from the US-CAN pair, downside and upside risk spillovers are significantly larger following the GFC as compared to the pre-crisis period. Observed asymmetric and time-varying behavior is consistent across high/low risk and high/low risk spillover sub-periods. © 2018 Elsevier Inc.}}, 
pages = {237--251}, 
number = {NA}, 
volume = {47}
}
@article{10.1016/j.ijforecast.2020.07.009, 
year = {2021}, 
title = {{Observation-driven models for realized variances and overnight returns applied to Value-at-Risk and Expected Shortfall forecasting}}, 
author = {Opschoor, Anne and Lucas, André}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2020.07.009}, 
abstract = {{We present a new model to decompose total daily return volatility into high-frequency-based open-to-close volatility and a time-varying scaling factor. We use score-driven dynamics based on fat-tailed distributions to obtain robust volatility dynamics. Applying our new model to a 2001–2018 sample of individual stocks and stock indices, we find substantial in-sample variation of the daytime-to-total volatility ratio over time. We apply the model to out-of-sample forecasting, evaluated in terms of Value-at-Risk and Expected Shortfall. Models with a non-constant volatility ratio typically perform best, particularly in terms of Value-at-Risk. Our new model performs especially well during turbulent times. All results are generally stronger for individual stocks than for index returns. © 2020 International Institute of Forecasters}}, 
pages = {622--633}, 
number = {2}, 
volume = {37}
}
@article{10.1080/13504860701718448, 
year = {2008}, 
title = {{Return and value at risk using the Dirichlet process}}, 
author = {Zarepour, Mahmoud and Bédard, Thierry and Dabrowski, André R.}, 
journal = {Applied Mathematical Finance}, 
issn = {1350486X}, 
doi = {10.1080/13504860701718448}, 
abstract = {{There exists a wide variety of models for return, and the chosen model determines the tool required to calculate the value at risk (VaR). This paper introduces an alternative methodology to model-based simulation by using a Monte Carlo simulation of the Dirichlet process. The model is constructed in a Bayesian framework, using properties initially described by Ferguson. A notable advantage of this model is that, on average, the random draws are sampled from a mixed distribution that consists of a prior guess by an expert and the empirical process based on a random sample of historical asset returns. The method is relatively automatic and similar to machine learning tools, e.g. the estimate is updated as new data arrive. © 2008 Taylor \& Francis.}}, 
pages = {205--218}, 
number = {3}, 
volume = {15}
}
@article{10.1007/s10693-005-4356-4, 
year = {2005}, 
title = {{Credit risk versus capital requirements under basel II: Are SME loans and retail credit really different?}}, 
author = {Jacobson, Tor and Lindé, Jesper and Roszbach, Kasper}, 
journal = {Journal of Financial Services Research}, 
issn = {09208550}, 
doi = {10.1007/s10693-005-4356-4}, 
abstract = {{Under Basel II, retail and SME credit (R\&SME) receive special treatment because of a supposedly smaller exposure to systemic risk. Most research on this issue has been based on parameterized credit risk models. We present new evidence by applying Carey's (Carey, Mark. "Credit Risk in Private Debt Portfolios." Journal of Finance 53, no. 4 (1998), 1363-1387.) nonparametric Monte-Carlo resampling method to two banks' complete loan portfolios. By exploiting that a sub-sample of all borrowers has been assigned an internal rating by both banks, we can compare the credit loss distributions for the three credit types, and compute both economic and regulatory capital under Basel II. We also test if our conclusions are sensitive to the definitions of R\&SME credit. Our findings show that R\&SME portfolios are usually riskier than corporate credit. Special treatment under Basel II is thus not justified. © 2005 Springer Science + Business Media, Inc.}}, 
pages = {43--75}, 
number = {1-3}, 
volume = {28}
}
@article{10.1016/j.jbankfin.2014.07.005, 
year = {2014}, 
title = {{A new set of improved Value-at-Risk backtests}}, 
author = {Ziggel, Daniel and Berens, Tobias and Weiß, Gregor N.F. and Wied, Dominik}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2014.07.005}, 
abstract = {{We propose a new set of formal backtests for VaR-forecasts that significantly improve upon existing backtesting procedures. Our new test of unconditional coverage can be used for both one-sided and two-sided testing, which leads to a significantly increased power. Second, we stress the importance of testing the property of independent and identically distributed (i.i.d.) VaR-exceedances and propose a simple approach that explicitly tests for the presence of clusters in VaR-violation processes. Results from a simulation study indicate that our tests significantly outperform competing backtests in several distinct settings. © 2014 Elsevier B.V.}}, 
pages = {29--41}, 
number = {NA}, 
volume = {48}
}
@article{10.1007/s11704-010-0501-9, 
year = {2010}, 
title = {{Modeling default risk via a hidden Markov model of multiple sequences}}, 
author = {Ching, Wai-Ki and Leung, Ho-Yin and Wu, Zhenyu and Jiang, Hao}, 
journal = {Frontiers of Computer Science in China}, 
issn = {16737350}, 
doi = {10.1007/s11704-010-0501-9}, 
abstract = {{Default risk in commercial lending is one of the major concerns of the creditors. In this article, we introduce a new hidden Markov model (HMM) with multiple observable sequences (MHMM), assuming that all the observable sequences are driven by a common hidden sequence, and utilize it to analyze default data in a network of sectors. Efficient estimation method is then adopted to estimate the model parameters. To further illustrate the advantages of MHMM, we compare the hidden risk state process obtained by MHMM with that from the traditional HMMs using credit default data. We then consider two applications of our MHMM. The calculation of two important risk measures: Value-at-risk (VaR) and expected shortfall (ES) and the prediction of global risk state. We first compare the performance of MHMM and HMM in the calculation of VaR and ES in a portfolio of default-prone bonds. A logistic regression model is then considered for the prediction of global economic risk using our MHMM with default data. Numerical results indicate our model is effective for both applications. © 2010 Higher Education Press and Springer-Verlag Berlin Heidelberg.}}, 
pages = {187--195}, 
number = {2}, 
volume = {4}
}
@article{10.1504/ijram.2008.019314, 
year = {2008}, 
title = {{A practical approach to market risk analysis and control: Empirical test of the Mexican foreign exchange and stock markets}}, 
author = {Janabi, Mazin A M Al}, 
journal = {International Journal of Risk Assessment and Management}, 
issn = {14668297}, 
doi = {10.1504/ijram.2008.019314}, 
abstract = {{The Mexican financial markets, like many other emerging markets, are generally portrayed as illiquid, volatile, segmented and politically unpredictable. Despite all these shortcomings, portfolio investments and trading activities in emerging markets have immense rewards for markets' participants. These investment alternatives may create unique expected return opportunities and substantial inherent risks. Risk measurement, management and control in such economies are in fact wearisome tasks; however, it may be addressed through art and science risk management practices. In this research paper, key market risk management methods and procedures that financial institutions, regulators and policymakers should consider in devising their daily market risk management objectives are examined and are adapted to the specific needs of emerging financial markets. The proper use of Value At Risk (VAR) and stress-testing methods are illustrated with real-world examples and practical reports of market risk analysis and control. The calculations and conclusions that are presented herein where applied to both, the Mexican foreign exchange and stock markets. To this end, several case studies were achieved with the objective of setting a practical framework of market risk measurement and control reports in addition to the inception of procedures for the setting of VAR's limits. The effects of hedging equity trading exposures with reciprocal foreign exchange trading positions were examined and quantified. Copyright © 2008 Inderscience Enterprises Ltd.}}, 
pages = {70}, 
number = {1-2}, 
volume = {9}
}
@article{10.12011/1000-6788(2017)02-0303-08, 
year = {2017}, 
title = {{VaR measurement for stock portfolio based on BEMD-Copula-GARCH model}}, 
author = {}, 
issn = {10006788}, 
doi = {10.12011/1000-6788(2017)02-0303-08}, 
abstract = {{Given that stock fluctuation has significant multi-scale features, a novel Value-at-Risk (VaR) model is proposed by combining the binary empirical mode decomposition (BEMD) and Copula-GARCH algorithm, i.e., BEMD-Copula-GARCH model. In the proposed model, three main steps are included, i.e., data decomposition, individual risk measurement, and total risk integration. First, the binary EMD technique is employed to decomposed the pair of complex and interactive stock series into pairs of relatively simple and independent components, to reduce the modeling difficulty. Second, Copula-GARCH model is introduced to individually capture the dynamic dependence between the decomposed components in pairs, in terms of VaR at different time-scale. Finally, the individual results are integrated into the final VaR measure. In the empirical study, the portfolio with equal-weighted Hang Seng Index and Shanghai Composite index is analyzed, and the results indicate that the proposed model outperform the benchmark DCC-GARCH model and Copula-GARCH model in terms of the risk measurement accuracy. © 2017, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.}}, 
number = {2}, 
volume = {37}
}
@article{10.1109/tdcla.2006.311411, 
year = {2006}, 
title = {{Risk management in the commercialization activity in Brazil - An approach by using markowitz, VaR and CVaR}}, 
author = {Oliveira, M. F. de and Arfux, G. A. B. and Teive, R. C. G.}, 
journal = {2006 IEEE/PES Transmission \& Distribution Conference and Exposition: Latin America}, 
issn = {NA}, 
doi = {10.1109/tdcla.2006.311411}, 
abstract = {{In the new competitive environment of the electricity market, risk analysis is a powerful tool to guide investors under both contract uncertainties and energy prices of the spot market. This paper compares three risk measures: medium variance, maximum loss and medium maximum loss applied to the energy commercialization problem. These methodologies are used to support the decision-making process in the investment analysis problem, considering the definition of the best contracts portfolio. It is illustrated in this paper that, techniques presented by Markowitz, Value-at-Risk and Conditional Value-at-Risk theories can be used in a complementary way, improving the quality of decision in the energy commercialization problem. © 2006 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.iref.2014.09.001, 
year = {2014}, 
title = {{Managing extreme risk in some major stock markets: An extreme value approach}}, 
author = {Karmakar, Madhusudan and Shukla, Girja K.}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2014.09.001}, 
abstract = {{The study investigates the relative performance of Value-at-Risk (VaR) models using daily share price index data from six different countries across Asia, Europe and the United States for a period of 10. years from January 01, 2000 to December 31, 2009. The main emphasis of the study has been given to Extreme Value Theory (EVT) and to evaluate how well Conditional EVT model performs in modeling tails of distributions and in estimating and forecasting VaR measures. We have followed McNeil and Frey's (2000) two stage approach called Conditional EVT to estimate dynamic VaR. In stage 1, we model the conditional volatility of each series using an appropriate asymmetric GARCH model which serves to filter the return series such that the asymmetric GARCH residuals are closer to iid than the raw return series. In stage 2, we apply EVT to model the fat tails of the asymmetric GARCH residuals. We have compared the accuracy of Conditional EVT approach to VaR estimation with other competing models. The best performing model is found to be the Conditional EVT for the entire sample. To confirm whether the Conditional EVT would still be the best for a sub-period, we have compared the forecasting accuracy for the sub-sample of bull market. Here too the Conditional EVT maintains its superiority even more precisely. Since the Conditional EVT approach clearly dominates other competing models in terms of VaR forecasting, we would advocate the use of the model when managing tail related market risk in such equity markets. © 2014 Elsevier Inc.}}, 
pages = {1--25}, 
number = {NA}, 
volume = {35}
}
@article{10.1080/01621459.2000.10473909, 
year = {2000}, 
title = {{Safe and Effective Importance Sampling}}, 
author = {Owen, Art and Zhou, Yi}, 
journal = {Journal of the American Statistical Association}, 
issn = {01621459}, 
doi = {10.1080/01621459.2000.10473909}, 
abstract = {{We present two improvements on the technique of importance sampling. First, we show that importance sampling from a mixture of densities, using those densities as control variates, results in a useful upper bound on the asymptotic variance. That bound is a small multiple of the asymptotic variance of importance sampling from the best single component density. This allows one to benefit from the great variance reductions obtainable by importance sampling, while protecting against the equally great variance increases that might take the practitioner by surprise. The second improvement is to show how importance sampling from two or more densities can be used to approach a zero sampling variance even for integrands that take both positive and negative values. © 2000 Taylor and Francis Group, LLC.}}, 
pages = {135--143}, 
number = {449}, 
volume = {95}
}
@article{10.1016/j.insmatheco.2020.06.006, 
year = {2020}, 
title = {{Relative bound and asymptotic comparison of expectile with respect to expected shortfall}}, 
author = {Tadese, Mekonnen and Drapeau, Samuel}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.06.006}, 
abstract = {{Expectile bears some interesting properties in comparison to the industry wide expected shortfall in terms of assessment of tail risk. We study the relationship between expectile and expected shortfall using duality results and the link to optimized certainty equivalent. Lower and upper bounds of expectile are derived in terms of expected shortfall as well as a characterization of expectile in terms of expected shortfall. Further, we study the asymptotic behavior of expectile with respect to expected shortfall as the confidence level goes to 1 in terms of extreme value distributions. We use concentration inequalities to illustrate that the estimation of value at risk requires larger sample size than expected shortfall and expectile for heavy tail distributions when α is close to 1. Illustrating the formulation of expectile in terms of expected shortfall, we also provide explicit or semi-explicit expressions of expectile and some simulation results for some classical distributions. © 2020 Elsevier B.V.}}, 
pages = {387--399}, 
number = {NA}, 
volume = {93}
}
@article{10.1016/j.eneco.2009.06.018, 
year = {2010}, 
title = {{The performance of composite forecast models of value-at-risk in the energy market}}, 
author = {Chiu, Yen-Chen and Chuang, I-Yuan and Lai, Jing-Yi}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2009.06.018}, 
abstract = {{This paper examines a comparative evaluation of the predictive performance of various Value-at-Risk (VaR) models in the energy market. This study extends the conventional research in literature, by proposing composite forecast models for applying to Brent and WTI crude oil prices. Forecasting techniques considered here include the EWMA, stable density, Kernel density, Hull and White, GARCH-GPD, plus composite forecasts from linearly combining two or more of the competing models above. Findings show Hull and White to be the most powerful approach for capturing downside risk in the energy market. Reasonable results are also available from carefully combining VaR forecasts. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {423--431}, 
number = {2}, 
volume = {32}
}
@article{10.1016/j.ribaf.2020.101259, 
year = {2020}, 
title = {{Forecasting Value-at-Risk of Cryptocurrencies with RiskMetrics type models}}, 
author = {Liu, Wei and Semeyutin, Artur and Lau, Chi Keung Marco and Gozgor, Giray}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2020.101259}, 
abstract = {{Since the financial crisis, risk management has been of growing interest to investors and the approach of Value-at-Risk has gained wide acceptance. Investing in Cryptocurrencies brings not only huge rewards but also huge risks. For this purpose, this paper investigates whether Cryptocurrencies investors’ decisions can rely on the pragmatic and parsimonious approaches for Value-at-Risk forecasting. Specifically, we suggest a parsimonious reflected gamma specification under the GAS framework, consider other GAS special cases and the Exponential Weights driven nonparametric methods, which fall into the same modelling category as the well-known and widely recognised original RiskMetrics™ approach. We focus on the returns for BTC, LTC and ETH and find that progress upon RiskMetricks™ may provide valuable gains in exposure modelling of Cryptocurrencies under the rough and primary backtesting conditions, though not all of the considered approaches demonstrate consistency at the selected risk confidence levels. In our setting, Laplace GAS specification, which controls for time-variation both in scale (volatility) and skewness (asymmetric responses to positive and negative volatility) parameters, performs the best at the most of the levels. We also find that controlling for time-variation in the degrees of freedom (tails) of the Student's t may be a worthwhile consideration, though such approach may still yield more conservative investors’ strategies than its Laplace asymmetric alternative. Reflected gamma and Extreme Value Theory linked Double Pareto specifications also demonstrate a modest performance, but likely suffer from the lack of asymmetry in their parameters, as our Reflected Gamma parametrisation accounts for time-variation in the tails, unlike Pareto specifications and does not outperform asymmetric Laplace specification. Data- driven nonparametric methods seem to struggle the most in approximating downside tail risks due to the sharp corrections in Cryptocurrencies’ value. © 2020 Elsevier B.V.}}, 
pages = {101259}, 
number = {NA}, 
volume = {54}
}
@article{10.21314/jop.2015.157, 
year = {2015}, 
title = {{Modeling correlated frequencies with application in operational risk management}}, 
author = {Badescu, Andrei L and Gong, Lan and Lin, X Sheldon and Tang, Dameng}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2015.157}, 
abstract = {{In this paper, we propose a copula-free approach for modeling correlated frequency distributions using an Erlang-based multivariate mixed Poisson distribution.We investigate some of the properties possessed by this class of distributions and derive a tailormade expectation-maximization algorithm for fitting purposes. The applicability of the proposed distribution is illustrated in an operational risk management context, where this class is used to model the operational loss frequencies and their complex dependence structure in a high-dimensional setting. Furthermore, by assuming that operational loss severities follow the mixture of Erlang distributions, our approach leads to a closed-form expression for the total aggregate loss distribution and its valueat- risk can be calculated easily by any numerical method. The efficiency and accuracy of the proposed approach are analyzed using a modified real operational loss data set. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--43}, 
number = {1}, 
volume = {10}
}
@article{10.18488/journal.aefr.2020.104.427.438, 
year = {2020}, 
title = {{The Shanghai- Hong Kong stock connect: An application of the Semi-CGARCH and SEMI-EGARCH}}, 
author = {Peitz, Christian and Feng, Yuanhua and Gilroy, Bernard M and Stoeckmann, Nico}, 
journal = {Asian Economic and Financial Review}, 
issn = {23052147}, 
doi = {10.18488/journal.aefr.2020.104.427.438}, 
abstract = {{This paper examines the impact on volatility on the Shanghai Stock Exchange and the Hong Kong Stock Exchange before and after the connection on November 17, 2014. We test whether this event led to a structural break. For this purpose, the volatility series are shown and analysed in more detail using two new models. We test both semiparametric GARCH extensions, the Semi-EGARCH based on the EGARCH model (Nelson, 1991) and the Semi-CGARCH model, based on the CGARCH (Engle \& Lee, 1999). Both univariate models work with a constrained local linear estimator for the scale function and a fully data-driven algorithm developed under weak moment conditions. The proposed method is applied to the two major Asian financial indices and to stocks from the banking sector. Furthermore, our focus is to improve the quantitative risk management analysis. When a parametric GARCH model provides satisfactory results for the calculation of e.g the Value at Risk (VaR), semiparametric can also be applied to improve the quality of measurement. This article compares the results of various parametric and semiparametric approaches regarding the VaR to show how the extensions increase the performance. © 2020 AESS Publications. All Rights Reserved.}}, 
pages = {427--438}, 
number = {4}, 
volume = {10}
}
@article{10.1016/j.csda.2013.09.019, 
year = {2014}, 
title = {{Bayesian estimation of smoothly mixing time-varying parameter GARCH models}}, 
author = {Chen, Cathy W.S. and Gerlach, Richard and Lin, Edward M.H.}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2013.09.019}, 
abstract = {{Smoothly time-varying (TV) GARCH models via an asymmetric logistic function mechanism are proposed, which are incorporated into the conditional volatility equation for capturing smooth structural breaks in the volatility of financial time series. The proposed models allow smooth transitions of varying "speed" between multiple, persistent regimes. A Bayesian computational method is employed to identify the locations of smooth structural transitions, and for estimation and inference, simultaneously accounting for heteroskedasticity. An informative prior is proposed to help ensure identification and allow accurate inference. The proposed methods are illustrated using simulated data, and an empirical study provides evidence for significant improvements in fit for the proposed smooth asymmetric time-varying volatility TV-GARCH models in two international stock market return series. A forecast study shows the proposed models significantly add to forecast accuracy for both volatility and Value-at-Risk. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {194--209}, 
number = {NA}, 
volume = {76}
}
@article{10.1109/cifer.2012.6327787, 
year = {2012}, 
title = {{Event-based historical Value-at-Risk}}, 
author = {Hogenboom, Frederik and Winter, Michael de and Jansen, Milan and Hogenboom, Alexander and Frasincar, Flavius and Kaymak, Uzay}, 
journal = {2012 IEEE Conference on Computational Intelligence for Financial Engineering \& Economics (CIFEr)}, 
issn = {NA}, 
doi = {10.1109/cifer.2012.6327787}, 
abstract = {{Value-at-Risk (VaR) is an important tool to assess portfolio risk. When calculating VaR based on historical stock return data, we hypothesize that this historical data is sensitive to outliers caused by news events in the sampled period. In this paper, we research whether the VaR accuracy can be improved by considering news events as additional input in the calculation. This involves processing the historical data in order to reflect the impact of news on the stock returns. Our experiments show that when an event occurs, removing the noise (that is caused by an event) from the measured stock prices for a small time window can improve VaR predictions. © 2012 IEEE.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icmlc.2009.5212342, 
year = {2009}, 
title = {{Fuzzy supply chain problem with var criteria}}, 
author = {Wang, Guo-Li and Liu, Yan-Kui and Qin, Rui}, 
journal = {2009 International Conference on Machine Learning and Cybernetics}, 
issn = {NA}, 
doi = {10.1109/icmlc.2009.5212342}, 
abstract = {{This paper attempts to present a new class of fuzzy two-stage supply chain problem with minimum risk criteria in the sense of Value-at-Risk (VaR), in which the transportation cost coefficients and the demands are characterized by fuzzy variables with known possibility distributions. Since the fuzzy parameters has infinite supports, the conventional optimization algorithms cannot be used directly to solve the proposed fuzzy supply chain problem. To overcome this difficulty, an approximation method is developed to turn the original supply chain problem into a finite dimensional one. Since approximating is a time-consuming process, we design a hybrid algorithm by integrating approximation method, neural network (NN) and particle swarm optimization (PSO) to solve it. Finally, one numerical example is presented to demonstrate the effectiveness of the designed algorithm. © 2009 IEEE.}}, 
pages = {668--673}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.physa.2004.02.034, 
year = {2004}, 
title = {{Dynamics of a financial market index after a crash}}, 
author = {Lillo, Fabrizio and Mantegna, Rosario N}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2004.02.034}, 
abstract = {{We discuss the statistical properties of index returns in a financial market just after a major market crash. The observed non-stationary behavior of index returns is characterized in terms of the exceedances over a given threshold. This characterization is analogous to the Omori law originally observed in geophysics. By performing numerical simulations and theoretical modelling, we show that the non-linear behavior observed in real market crashes cannot be described by a GARCH(1,1) model. We also show that the time evolution of the Value at Risk observed just after a major crash is described by a power-law function lacking a typical scale. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {125--134}, 
number = {1-2 SPEC. ISS.}, 
volume = {338}
}
@article{10.1016/j.irfa.2013.07.011, 
year = {2013}, 
title = {{Risk prediction management and weak form market efficiency in Eurozone financial crisis}}, 
author = {Righi, Marcelo Brutti and Ceretta, Paulo Sergio}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2013.07.011}, 
abstract = {{This paper aims to determine if during the recent European financial crisis European markets are efficient in the weak form, as well to introduce an approach to properly predict daily risk of portfolios composed by these market assets, considering their dependence structure. We use daily data from German, English, French, Greek, Dutch and Belgian markets. We perform variance ratio tests to verify the random walk hypothesis. In a general form European capital markets are efficient referent to past information during current crisis. Moreover, through marginal and Pair Copula Construction models, we predict daily Value at Risk for each market and for the portfolio composed by them. Individual risk predictions are correctly simulated. Simulations performed through PCC model properly predict the composed portfolio risk, highlighting that in this crisis period it is crucial to use a tool enable to make correct predictions about risk. The proposed approach emerges as a solution to this task. © 2013 Elsevier Inc.}}, 
pages = {384--393}, 
number = {NA}, 
volume = {30}
}
@article{10.1016/j.jbusres.2019.02.037, 
year = {2019}, 
title = {{Mind the tail, or risk to fail}}, 
author = {Gupta, Jairaj and Chaudhry, Sajid}, 
journal = {Journal of Business Research}, 
issn = {01482963}, 
doi = {10.1016/j.jbusres.2019.02.037}, 
abstract = {{In this study we hypothesise that more frequent extreme negative daily equity returns result in higher tail risk, and this subsequently increases firms’ likelihood of entering financial distress. Specifically, we investigate the role of Value-at-risk and Expected Shortfall in aggravating firms’ likelihood of experiencing financial distress. Our results show that longer horizon (three- and five-year) tail risk measures contributes positively toward firms’ likelihood of experiencing financial distress. Additionally, considering the declining number of bankruptcy filings, and increasing out-of-court negotiations and debt reorganisations, we argue in favour of penalising firms for becoming sufficiently close to bankruptcy that they have questionable going-concern status. Thus, we propose a definition of financial distress contingent upon firms’ earnings, financial expenses, market value and operating cash flow. © 2019 Elsevier Inc.}}, 
pages = {167--185}, 
number = {NA}, 
volume = {99}
}
@article{10.1007/s00181-011-0528-2, 
year = {2013}, 
title = {{Modeling different kinds of spatial dependence in stock returns}}, 
author = {Arnold, Matthias and Stahlberg, Sebastian and Wied, Dominik}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-011-0528-2}, 
abstract = {{The paper modifies previously suggested GMM approaches to spatial autoregression in stock returns. Our model incorporates global dependencies, dependencies inside industrial branches and local dependencies. As can be seen from Euro Stoxx 50 returns, this combination of spatial modeling and finance allows for superior risk forecasts in portfolio management. © 2011 Springer-Verlag.}}, 
pages = {761--774}, 
number = {2}, 
volume = {44}
}
@article{10.1002/9781118266588.ch38, 
year = {2011}, 
title = {{The Future of Risk Modeling}}, 
author = {Sheedy, Elizabeth and Kolb, Robert W.}, 
issn = {NA}, 
doi = {10.1002/9781118266588.ch38}, 
abstract = {{[No abstract available]}}, 
pages = {301--306}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icemms.2010.5563478, 
year = {2010}, 
title = {{Analyzing portfolios based on tail dependence coefficients}}, 
author = {Ou, Shide and Yi, Danhui}, 
journal = {2010 IEEE International Conference on Emergency Management and Management Sciences}, 
issn = {NA}, 
doi = {10.1109/icemms.2010.5563478}, 
abstract = {{For the sake of finding the portfolios with low risk and high return, copula function is used to compute the tail dependence coefficient. We present the investment ratio function of value-at-risk of portfolio, and use the tail dependence coefficient to research value-at-risk of portfolio. We propose to use the curve of portfolio value and the curve of portfolio value-at-risk to analyze investment ratio. Empirical research shows that according to the curve of portfolio value and the curve of portfolio value-at-risk to analyze investment ratio, the portfolio with low risk and high return can be found out. © 2010 IEEE.}}, 
pages = {152--156}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/135048699334591, 
year = {1999}, 
title = {{Numerical integration of mean reverting stochastic systems with applications to interest rate term structure simulation}}, 
author = {Morokoff, William J.}, 
journal = {Applied Mathematical Finance}, 
issn = {15226514}, 
doi = {10.1080/135048699334591}, 
abstract = {{A proof of convergence is presented for a simplified numerical integration method for solving systems of correlated stochastic differential equations describing mean reverting geometric Brownian motion. Such systems arise in modelling the time evolution of interest rate term structures. For time discretization of size Δt, the method leads to global error in time of O (Δt 2) and no error accumulation. The result is shown to extend to the case when principal components analysis is used to reduce the number of underlying stochastic factors. © 1999, Copyright Taylor \&amp; Francis Group, LLC.}}, 
pages = {19--28}, 
number = {1}, 
volume = {21}
}
@article{10.1109/cifer.2014.6924089, 
year = {2014}, 
title = {{Nonlinear filtering of asymmetric stochastic volatility models and Value-at-Risk estimation}}, 
author = {Nikolaev, Nikolay Y. and Menezes, Lilian M. de and Smirnov, Evgueni}, 
journal = {2014 IEEE Conference on Computational Intelligence for Financial Engineering \& Economics (CIFEr)}, 
issn = {NA}, 
doi = {10.1109/cifer.2014.6924089}, 
abstract = {{This paper develops an efficient approach to analytical learning of Asymmetric Stochastic Volatility (ASV) models through nonlinear filtering, and shows that they are useful for practical risk management. This involves the derivation of a Nonlinear Quadrature Filter (NQF) that operates directly on the nonlinear ASV model. The NQF filter makes Gaussian approximations to the prior and posterior density of the latent volatility, but not in the observation space which makes possible easy handling of heavy-tailed data. Experiments in Value-at-Risk (VaR) assessment via an original bootsrtapping methodology are conducted with NQF and several ASV learning algorithms. The results indicate that our approach yields models with better statistical characteristics than the considered competitors, and slightly improved VaR forecasts. © 2014 IEEE.}}, 
pages = {310--317}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/1540496x.2015.1011558, 
year = {2016}, 
title = {{Risk-Adjusted Performances of World Equity Indices}}, 
author = {Atilgan, Yigit and Demirtas, K. Ozgur}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2015.1011558}, 
abstract = {{This article investigates whether equity indices of twenty-four emerging and twenty-eight developed markets compensate their investors equally after adjusting for total or downside risk, and examines the predictive power of reward-to-risk ratios for expected market returns. We find that when all fifty-two markets are ranked based on their alternative reward-to-risk ratios, almost all of the countries in the top (bottom) quartile are emerging (developed) markets. The pooled means of the reward-to-risk ratios are also significantly higher for emerging markets. Both portfolio and regressions analysis reveal that there is a significantly positive relation between various reward-to-risk metrics and expected market returns. © 2016 Taylor \& Francis Group, LLC.}}, 
pages = {706--721}, 
number = {3}, 
volume = {52}
}
@article{10.1017/asb.2020.6, 
year = {2020}, 
title = {{WEIGHTED COMONOTONIC RISK SHARING under HETEROGENEOUS BELIEFS}}, 
author = {Liu, Haiyan}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2020.6}, 
abstract = {{We study a weighted comonotonic risk-sharing problem among multiple agents with distortion risk measures under heterogeneous beliefs. The explicit forms of optimal allocations are obtained, which are Pareto-optimal. A necessary and sufficient condition is given to ensure the uniqueness of the optimal allocation, and sufficient conditions are given to obtain an optimal allocation of the form of excess of loss or full insurance. The optimal allocation may satisfy individual rationality depending on the choice of the weight. When the distortion risk measure is value at risk or tail value at risk, an optimal allocation is generally of the excess-of-loss form. The numerical examples suggest that a risk is more likely to be shared among agents with heterogeneous beliefs, and the introduction of the weight enables us to prioritize some agents as part of a group sharing a risk. © 2020 Astin Bulletin.}}, 
pages = {647--673}, 
number = {2}, 
volume = {50}
}
@article{10.1016/j.ijar.2013.01.003, 
year = {2013}, 
title = {{Granularity adjustment for risk measures: Systematic vs unsystematic risks}}, 
author = {Gagliardini, Patrick and Gouriéroux, Christian}, 
journal = {International Journal of Approximate Reasoning}, 
issn = {0888613X}, 
doi = {10.1016/j.ijar.2013.01.003}, 
abstract = {{The granularity principle (Gordy, 2003) 17 allows for closed form expressions of the risk measures of a large portfolio at order 1/n, where n is the portfolio size. The granularity principle yields a decomposition of such risk measures that highlights the different effects of systematic and unsystematic risks. This paper derives the granularity adjustment of the Value-at-Risk (VaR), the Expected Shortfall and the other distortion risk measures for both static and dynamic risk factor models. The systematic factor can be multidimensional. The methodology is illustrated by several examples, such as the stochastic drift and volatility model, or the dynamic factor model for joint analysis of default and loss given default. © 2012 Elsevier Inc. All rights reserved.}}, 
pages = {717--747}, 
number = {6}, 
volume = {54}
}
@article{10.1061/(asce)me.1943-5479.0000585, 
year = {2018}, 
title = {{Value-at-Risk Model Analysis of Taiwanese High-Tech Facility Construction}}, 
author = {Chen, Po-Han and Peng, Tsu-Te}, 
journal = {Journal of Management in Engineering}, 
issn = {0742597X}, 
doi = {10.1061/(asce)me.1943-5479.0000585}, 
abstract = {{Taiwan is among the few countries that have a high density of high-tech facilities in the world, and the high-tech industry accounts for a large proportion of the annual output value of the global high-tech industry. During the construction period, the property loss of high-tech facilities is the issue of most concerned for the owners, investors, and contractors. This paper aims to study the risks of high-tech facility construction and identify the probability distributions suitable for the simulation of loss frequency, loss amount, and loss claim, respectively. The risks with the highest impact are attributable to negligence, fire, leakage, falling, and collision. Also, using the normalized loss factor as an indicator of the value at risk (VaR), this paper provides a way to estimate the maximum loss that might occur during a target time frame by means of the variance-covariance, historical simulation, and Monte Carlo methods. © 2017 American Society of Civil Engineers.}}, 
pages = {04017057}, 
number = {2}, 
volume = {34}
}
@article{10.1016/s0378-4266(99)00077-1, 
year = {2000}, 
title = {{From value at risk to stress testing: The extreme value approach}}, 
author = {Longin, François M}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(99)00077-1}, 
abstract = {{This article presents an application of extreme value theory to compute the value at risk of a market position. In statistics, extremes of a random process refer to the lowest observation (the minimum) and to the highest observation (the maximum) over a given time-period. Extreme value theory gives some interesting results about the distribution of extreme returns. In particular, the limiting distribution of extreme returns observed over a long time-period is largely independent of the distribution of returns itself. In financial markets, extreme price movements correspond to market corrections during ordinary periods, and also to stock market crashes, bond market collapses or foreign exchange crises during extraordinary periods. An approach based on extreme values to compute the VaR thus covers market conditions ranging from the usual environment considered by the existing VaR methods to the financial crises which are the focus of stress testing. Univariate extreme value theory is used to compute the VaR of a fully aggregated position while multivariate extreme value theory is used to compute the VaR of a position decomposed on risk factors. © 2000 Elsevier Science B.V.}}, 
pages = {1097--1130}, 
number = {7}, 
volume = {24}
}
@article{10.1016/j.physa.2011.12.046, 
year = {2012}, 
title = {{A dynamical model for forecasting operational losses}}, 
author = {Bardoscia, M. and Bellotti, R.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2011.12.046}, 
eprint = {1007.0026}, 
abstract = {{A novel dynamical model for the study of operational risk in banks and suitable for the calculation of the Value at Risk (VaR) is proposed. The equation of motion takes into account the interactions among different bank's processes, the spontaneous generation of losses via a noise term and the efforts made by the bank to avoid their occurrence. Since the model is very general, it can be tailored on the internal organizational structure of a specific bank by estimating some of its parameters from historical operational losses. The model is exactly solved in the case in which there are no causal loops in the matrix of couplings and it is shown how the solution can be exploited to estimate also the parameters of the noise. The forecasting power of the model is investigated by using a fraction f of simulated data to estimate the parameters, showing that for f=0.75 the VaR can be forecast with an error ≃10-3. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {2641--2655}, 
number = {8}, 
volume = {391}
}
@article{10.3390/risks6030084, 
year = {2018}, 
title = {{The impact of sovereign yield curve differentials on value-at-risk forecasts for foreign exchange rates}}, 
author = {Fink, Holger and Fuest, Andreas and Port, Henry}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks6030084}, 
abstract = {{A functional ARMA-GARCH model for predicting the value-at-risk of the EURUSD exchange rate is introduced. The model implements the yield curve differentials between EUR and the US as exogenous factors. Functional principal component analysis allows us to use the information of basically the whole yield curve in a parsimonious way for exchange rate risk prediction. The data analyzed in our empirical study consist of the EURUSD exchange rate and the EUR-and US-yield curves from 15 August 2005–30 September 2016. As a benchmark, we take an ARMA-GARCH and an ARMAX-GARCHX with the 2y-yield difference as the exogenous variable and compare the forecasting performance via likelihood ratio tests. However, while our model performs better in one situation, it does not seem to improve the performance in other setups compared to its competitors. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {84}, 
number = {3}, 
volume = {6}
}
@article{10.1016/j.physa.2019.123524, 
year = {2020}, 
title = {{Mixed value-at-risk and its numerical investigation}}, 
author = {Goel, Anubha and Sharma, Amita}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.123524}, 
abstract = {{We study an extension of value-at-risk (VaR) measure, named as Mixed VaR, a weighted sum of multiple VaRs quantified at different confidence levels. Classical VaR or single VaR computed at a fixed confidence level corresponds to a single percentile of distribution and therefore, is unable to reveal much information of risk involved in it. As a remedy to this, we propose to investigate the role of Mixed VaR and its deviation version in risk management of extreme events in the portfolio selection problem. We analyze the computational performance of portfolios from optimization models minimizing single VaR and Mixed VaR (and their deviation variants) for different combinations of confidence levels over historical as well as on simulated data in various financial performance parameters including mean value, risk measures (VaR and CVaR values quantified at multiple confidence levels), and risk-reward measures (Sharpe ratio, Sortino ratio, Sharpe with VaR, and Sharpe with CVaR). We also study the numerical comparison between Mixed VaR with its most crucial counterpart, the Mixed conditional value-at-risk (Mixed CVaR). We find that the performance of portfolios from Mixed VaR model is lying between the performance of its single VaR counterparts and rarely yield any worst value in any of the considered performance parameters. A similar observation is concluded for the deviation version of the models. Further, we find that Mixed VaR outperforms Mixed CVaR with respect to risk-reward measures considered in the study on both of the data sets, historical as well as on simulated. © 2019}}, 
pages = {123524}, 
number = {NA}, 
volume = {541}
}
@article{10.13335/j.1000-3673.pst.2018.2344, 
year = {2019}, 
title = {{High Risk Cascading Outage Assessment in Power Systems With Large-scale Wind Power Based on Stochastic Power Flow and Value at Risk [基于随机潮流和风险价值的含大规模风电系统高风险连锁故障评估]}}, 
author = {}, 
issn = {10003673}, 
doi = {10.13335/j.1000-3673.pst.2018.2344}, 
abstract = {{With increase of wind power penetration rate in power system, the influence of wind power output uncertainty on evolution path of power system cascading outage cannot be ignored. Based on this, this paper proposes a high risk fault chain model, and uses stochastic power flow and value at risk theory to analyze the risk level of cascading outages. Firstly, during fault chain forecasting, the follow-up fault search indicators combining real-time failure probability and fault consequence are constructed based on stochastic power flow considering correlation of wind power. Subsequent faults are selected according to the values at risk of these indicators. Secondly, in order to reflect the impact of wind power randomness on risk indicators, fuzzy clustering is used to establish wind power scenarios, then system loss of a load risk indicator is calculated. Based on the indicator, it can be analyzed whether current operating point is in a safe or a high-risk area, thereby guiding the system intraday scheduling. Finally, the improved IEEE 39-node system is taken as an example to illustrate rationality of the proposed model and algorithm. And the influence of wind power penetration rate and wind power forecasting error on the risk of cascading outages are analyzed. © 2019, Power System Technology Press. All right reserved.}}, 
number = {2}, 
volume = {43}
}
@article{10.1002/asmb.2246, 
year = {2017}, 
title = {{Maximum likelihood estimation for stochastic volatility in mean models with heavy-tailed distributions}}, 
author = {Abanto‐Valle, Carlos A. and Langrock, Roland and Chen, Ming‐Hui and Cardoso, Michel V.}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2246}, 
pmid = {28970740}, 
abstract = {{In this article, we introduce a likelihood-based estimation method for the stochastic volatility in mean (SVM) model with scale mixtures of normal (SMN) distributions. Our estimation method is based on the fact that the powerful hidden Markov model (HMM) machinery can be applied in order to evaluate an arbitrarily accurate approximation of the likelihood of an SVM model with SMN distributions. Likelihood-based estimation of the parameters of stochastic volatility models, in general, and SVM models with SMN distributions, in particular, is usually regarded as challenging as the likelihood is a high-dimensional multiple integral. However, the HMM approximation, which is very easy to implement, makes numerical maximum of the likelihood feasible and leads to simple formulae for forecast distributions, for computing appropriately defined residuals, and for decoding, that is, estimating the volatility of the process. Copyright © 2017 John Wiley \& Sons, Ltd. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {394--408}, 
number = {4}, 
volume = {33}
}
@article{10.1080/00949655.2015.1077387, 
year = {2016}, 
title = {{A robust closed-form estimator for the GARCH(1,1) model}}, 
author = {Bahamonde, Natalia and Veiga, Helena}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2015.1077387}, 
abstract = {{In this paper we extend the closed-form estimator for the generalized autoregressive conditional heteroscedastic (GARCH(1,1)) proposed by Kristensen and Linton [A closed-form estimator for the GARCH(1,1) model. Econom Theory. 2006; 22:323-337] to deal with additive outliers. It has the advantage that is per se more robust that the maximum likelihood estimator (ML) often used to estimate this model, it is easy to implement and does not require the use of any numerical optimization procedure. The robustification of the closed-form estimator is done by replacing the sample autocorrelations by a robust estimator of these correlations and by estimating the volatility using robust filters. The performance of our proposal in estimating the parameters and the volatility of the GARCH(1,1) model is compared with the proposals existing in the literature via intensive Monte Carlo experiments and the results of these experiments show that our proposal outperforms the ML and quasi-maximum likelihood estimators-based procedures. Finally, we fit the robust closed-form estimator and the benchmarks to one series of financial returns and analyse their performances in estimating and forecasting the volatility and the value-at-risk. © 2015 Taylor \& Francis.}}, 
pages = {1605--1619}, 
number = {8}, 
volume = {86}
}
@article{10.1109/ieem.2015.7385916, 
year = {2016}, 
title = {{A model to evaluate risk propagation considering effect of dynamic risk information sharing and multi-sourcing in supply chain networks}}, 
author = {Xu, Hai-Yan and Fu, Xiuiu and Ponnambalam, Lozanathan and Namarame=, Akira and Yin, Xiao Feng and Goh, Rick Siow Mong}, 
journal = {2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
issn = {21573611}, 
doi = {10.1109/ieem.2015.7385916}, 
abstract = {{The cost-optimal trend in global competitive environment has led to lean, complex and consequently vulnerable supply chains. This vulnerability is high due to the presence of increasing dependency in lean supply chain networks. Hence, it is essential for companies to regulate the supply chain network structure and information sharing strategies so as to be more resilient to disruptions/risks. In order to achieve this objective, it is imminent to analyze as to how best to quantify and examine the robustness of their supply chain networks under disruptions. Here, we propose an analytical model to assist companies to evaluate dynamic risk propagation and quantify likely disruption loss in supply chain networks with the consideration of the effect of multiple sourcing and information sharing on risk propagation. Mean loss and Value at risk (VaR) is employed to quantify the potential loss due to disruptions. © 2015 IEEE.}}, 
pages = {1593--1597}, 
number = {NA}, 
volume = {2016-January}
}
@article{10.1016/j.csda.2009.05.027, 
year = {2010}, 
title = {{Intradaily dynamic portfolio selection}}, 
author = {Bauwens, Luc and Omrane, Walid Ben and Rengifo, Erick}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2009.05.027}, 
abstract = {{A portfolio selection model which allocates a portfolio of currencies by maximizing the expected return subject to Value-at-Risk (VaR) constraint is designed and implemented. Based on an econometric implementation using intradaily data, the optimal portfolio allocation is forecasted at regular time intervals. For the estimation of the conditional variance from which the VaR is computed, univariate and multivariate GARCH models are used. Model evaluation is done using two economic criteria and two statistical tests. The result for each model is given by the best forecasted intradaily investment recommendations in terms of the optimal weights of the currencies in the risky portfolio. The results show that estimating the VaR from multivariate GARCH models improves the results of the forecasted optimal portfolio allocation, compared to using a univariate model. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {2400--2418}, 
number = {11}, 
volume = {54}
}
@article{10.4018/ijaeis.2020100104, 
year = {2020}, 
title = {{A delta normal approach for modelling risk forecasting of currency portfolio: The case of albanian agro exporters}}, 
author = {Todri, Ardita and Scalera, Francesco Roberto}, 
journal = {International Journal of Agricultural and Environmental Information Systems (IJAEIS)}, 
issn = {19473192}, 
doi = {10.4018/ijaeis.2020100104}, 
abstract = {{This research explores the benefits of a proactive model developed through delta normal approach implementation for the forecasting of currency portfolio volatility. The latter becomes a necessity for the Albanian agro exporters as they act in an international trading environment and face the de-Euroization process effects in domestic market. The forecasting of value at risk (VaR) at 99\% confidence level is obtained through the implementation of a moving window containing 251 daily currency exchange rates logarithmic returns calculated by the exponentially weighted moving average method (EWMA). A decay factor of 0.94 is used in the simulated currency portfolios database (composed from six different currency positions) pertaining to 30 agro exporters in reference of 2018 year data. The analysis of incremental VaR decomposed in risk per currency unit and VaR contribution concludes that the implementation of this mechanism offers hedge opportunities and enables the agro exporters to undertake even speculative interventions. Copyright © 2020, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.}}, 
pages = {55--68}, 
number = {4}, 
volume = {11}
}
@article{10.1016/j.najef.2014.06.013, 
year = {2014}, 
title = {{Statistics of extreme events in risk management: The impact of the subprime and global financial crisis on the German stock market}}, 
author = {Herrera, Rodrigo and Schipp, Bernhard}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2014.06.013}, 
abstract = {{Given the growing need for managing financial risk and the recent global crisis, risk prediction is a crucial issue in banking and finance. In this paper, we show how recent advances in the statistical analysis of extreme events can provide solid methodological fundamentals for modeling extreme events. Our approach uses self-exciting marked point processes for estimating the tail of loss distributions. The main result is that the time between extreme events plays an important role in the statistical analysis of these events and could therefore be useful to forecast the size and intensity of future extreme events in financial markets. We illustrate this point by measuring the impact of the subprime and global financial crisis on the German stock market in extenso, and briefly as a benchmark in the US stock market. With the help of our fitted models, we backtest the Value at Risk at various quantiles to assess the likeliness of different extreme movements on the DAX, S\&P 500 and Nasdaq stock market indices during the crisis. The results show that the proposed models provide accurate risk measures according to the Basel Committee and make better use of the available information. © 2014 Elsevier Inc.}}, 
pages = {218--238}, 
number = {NA}, 
volume = {29}
}
@article{10.1007/3-540-26993-2_4, 
year = {2005}, 
title = {{The new basel capital accord}}, 
author = {Holtorf, Claudia and Muck, Matthias and Rudolf, Markus}, 
issn = {NA}, 
doi = {10.1007/3-540-26993-2\_4}, 
abstract = {{This paper addresses the capital requirements based on the RiskMetrics™ framework and the BIS standard model. A case study is developed which shows that the capital requirements can be reduced by applying the more accurate RiskMetrics™ framework. Furthermore it gives an overview of the capital requirement rules for credit risk and operational risk in the Basel II Accord. © 2005 Springer Berlin · Heidelberg.}}, 
pages = {79--98}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/ijfe.2280, 
year = {2020}, 
title = {{Vines climbing higher: Risk management for commodity futures markets using a regular vine copula approach}}, 
author = {Li, Hemei and Liu, Zhenya and Wang, Shixuan}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.2280}, 
abstract = {{The volume of trading activity relating to China's commodity futures has grown rapidly over the course of the last decade. To improve risk management in China's commodity futures markets, this paper employs a regular vine (R-vine) copula model to study the dependence structure of commodity futures and to enhance Value-at-Risk (VaR) forecast. In doing so, we find that China's commodity futures market is not centred on one category of commodity futures and the tail dependence between different categories of commodity futures varies significantly. Based on the dependence structure analysed using the R-vine copula model, we forecast the VaR of individual indices, which are formed of several commodity futures, as well as forecasting the VaR of an equally-weighted portfolio. Our method can outperform the standard GARCH-VaR method in terms of VaR backtesting. The tool developed within this study will enable those involved in commodity futures markets to improve their risk management. © 2020 John Wiley \& Sons Ltd}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/cise.2010.5677066, 
year = {2010}, 
title = {{Empirical research based on the VaR model in risk measurement of stock fund}}, 
author = {Xianming, WEN and Ye, TAN and Ying, XIONG}, 
journal = {2010 International Conference on Computational Intelligence and Software Engineering}, 
issn = {NA}, 
doi = {10.1109/cise.2010.5677066}, 
abstract = {{Open-end funds and Closed-end funds are two different operating mode for the fund. In this paper, it calculate the risk of the stock fund and compare the average VaR between the Open-end funds and Closed-end funds. As the Fund's day yield is non-normal state of "spike-thick tail" of the distribution characteristics, we use GARCH models to calculate the value of the fund's VaR. Empirical results showed that the VaR value of open-end funds and closed-end funds is different. ©2010 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.2298/eka1085063a, 
year = {2010}, 
title = {{Extreme value theory in emerging markets}}, 
author = {Anđelić, Goran and Milošev, Ivana and Đaković, Vladimir}, 
journal = {Economic Annals}, 
issn = {00133264}, 
doi = {10.2298/eka1085063a}, 
abstract = {{This paper investigates the performance of extreme value theory (EVT) with the daily stock index returns of four different emerging markets. The research covers the sample representing the Serbian (BELEXline), Croatian (CROBEX), Slovenian (SBI20), and Hungarian (BUX) stock indexes using the data from January 2006 - September 2009. In the paper a performance test was carried out for the success of application of the extreme value theory in estimating and forecasting of the tails of daily return distribution of the analyzed stock indexes. Therefore the main goal is to determine whether EVT adequately estimates and forecasts the tails (2.5\% and 5\% at the tail) of daily stock index return distribution in the emerging markets of Serbia, Croatia, Slovenia, and Hungary. The applied methodology during the research includes analysi s, synthesis and statistical/mathematical methods. Research results according to estimated Generalized Pareto Distribution (GPD) parameters indicate the necessity of applying market risk estimation methods, i.e. extreme value theory (EVT) in the framework of a broader analysis of investment processes in emerging markets.}}, 
pages = {63--105}, 
number = {185}, 
volume = {54}
}
@article{10.1109/cifer.2012.6327788, 
year = {2012}, 
title = {{Online estimation of stochastic volatility for asset returns}}, 
author = {Luna, Ivette and Ballini, Rosangela}, 
journal = {2012 IEEE Conference on Computational Intelligence for Financial Engineering \& Economics (CIFEr)}, 
issn = {NA}, 
doi = {10.1109/cifer.2012.6327788}, 
abstract = {{An important application of financial institutions is quantifying the risk involved in investing in an asset. These are various measures of risk like volatility or value-at-risk. To estimate them from data, a model for underlying financial time series has to be specified and parameters have to be estimated. In the following, we propose a framework for estimation of stochastic volatility of asset returns based on adaptive fuzzy rule based system. The model is based on Takagi-Sugeno fuzzy systems, and it is built in two phases. In the first phase, the model uses the Subtractive Clustering algorithm to determine group structures in a reduced data set for initialization purpose. In the second phase, the system is modified dynamically via adding and pruning operators and a recursive learning algorithm determines automatically the number of fuzzy rules necessary at each step, whereas one step ahead predictions are estimated and parameters are updated as well. The model is applied for forecasting financial time series volatility, considering daily values the REAL/USD exchange rate. The model suggested is compared against generalized autoregressive conditional heteroskedaticity models. Experimental results show the adequacy of the adaptative fuzzy approach for volatility forecasting purposes. © 2012 IEEE.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {NA}
}
@article{10.2495/sssit20130431, 
year = {2014}, 
title = {{Research on stock index optimization replication in genetic algorithm}}, 
author = {}, 
issn = {17433517}, 
doi = {10.2495/sssit20130431}, 
abstract = {{In quest of replicating CSI300 index which is used the stratified sampling methodto construct a portfolio to optimization replicate CSI300 index. The first step is sampling the group of CSI300 index, choosing the largest 30 stocks in index, then using genetic algorithm and limiting the portfolio's Value-at-risk (VaR), calculating the weight of every stock in portfolio. So we can construct an underlying index tracking. The results show that no matter in the sample testing period or in the out sample examining period through the genetic algorithm can effectively reduce the tracing error by using index tracking portfolio, and can reduce the portfolio's VaR through constraint conditions. In addition, portfolio's accumulation rate of return is much better than the same period the accumulation of CSI300 index return in the out sample examining period. © 2014 WIT Press.}}, 
number = {NA}, 
volume = {52}
}
@article{10.1088/1742-6596/1725/1/012028, 
year = {2021}, 
title = {{Application of credible value at risk in predicting Indonesia's stock market return}}, 
author = {Pangestika, R and Novita, M and Nurrohmah, S}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1725/1/012028}, 
abstract = {{Risk is a probability of loss. In financial terms, loss can be interpreted as a possibility that an actual return on an investment will be lower than expected returns. Recent studies develop a new type of risk measures called credible value at risk (CrVaR). Credible value at risk is a model obtained by combining credibility theory and one of the most used risk measures, value at risk (VaR). Credibility theory is a model which gives a proper weight for both information and VaR is used to calculate maximum loss with the specific level of certainty and specific time frame. The combination of credibility theory and VaR is required to get a better value at risk estimation based on individual and group experiences. This paper discusses the model of credible value at risk, its parameter estimation, and focuses on the implementation of credible value at risk to predict the future rate of return from Indonesia's stock market data. © 2021 Journal of Physics: Conference Series.}}, 
pages = {012028}, 
number = {1}, 
volume = {1725}
}
@article{10.1109/icssbe.2012.6396561, 
year = {2012}, 
title = {{Value-at-risk and conditional value-at-risk estimation: A comparative study of risk performance for selected malaysian sectoral indices}}, 
author = {Thim, Chan Kok and Nourani, Mohammad and Choong, Yap Voon}, 
journal = {2012 International Conference on Statistics in Science, Business and Engineering (ICSSBE)}, 
issn = {NA}, 
doi = {10.1109/icssbe.2012.6396561}, 
abstract = {{Value-at-risk (VaR) has taken an important place in risk management since its acceptance as the main risk metric by Basel Committee on Banking Supervision (BCBS). Recently, BCBS announced the emphasis of implementing Conditional VaR (CVaR) in market risk assessment. While VaR measures the maximum loss in a given confidence level and period, CVaR gauges the amount of loss that exceed VaR in a given confidence level. Measuring industry risk is one of the crucial tasks for banks and other investors in order to manage their risks. This paper analyzes VaR and CVaR in the context of Malaysian industries. We compare industries with a selected benchmark. The results show that Technology has the highest risk and Consumer Product has the lowest risk. © 2012 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/agec.12366, 
year = {2017}, 
title = {{Modeling regime-dependent agricultural commodity price volatilities}}, 
author = {Li, Na and Ker, Alan and Sam, Abdoul G. and Aradhyula, Satheesh}, 
journal = {Agricultural Economics}, 
issn = {01695150}, 
doi = {10.1111/agec.12366}, 
abstract = {{In stark contrast to financial markets, relatively little attention has been given to modeling agricultural commodity price volatility. In recent years, numerous methodologies with various strengths have been proposed for modeling price volatility in financial markets. We propose using a mixture of normals with unique GARCH processes in each component for modeling agricultural commodity prices. While a normal mixture model is quite flexible and allows for time varying skewness and kurtosis, its biggest strength is that each component can be viewed as a different market regime and thus estimated parameters are more readily interpreted. We apply the proposed model to ten different agricultural commodity weekly cash prices. Both in-sample fit and out-of-sample forecasting tests confirm that the two-state NM-GARCH approach performs better than the traditional normal GARCH model. A significant and state-dependent inverse leverage effect is detected only for pork in the regime where the price is expected to drop, indicating the volatility in this regime tends to increase more following a realized price rise than a realized price drop. © 2017 International Association of Agricultural Economists}}, 
pages = {683--691}, 
number = {6}, 
volume = {48}
}
@article{10.1016/j.enpol.2009.12.020, 
year = {2010}, 
title = {{Value-at-risk estimations of energy commodities via long-memory, asymmetry and fat-tailed GARCH models}}, 
author = {Aloui, Chaker and Mabrouk, Samir}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2009.12.020}, 
abstract = {{In this paper, we evaluate the value-at-risk (VaR) and the expected shortfalls for some major crude oil and gas commodities for both short and long trading positions. Classical VaR estimations as well as RiskMetrics and other extensions to cases considering for long-range memory, asymmetry and fat-tail in energy markets volatility were conducted. We computed the VaR for three ARCH/GARCH-type models including FIGARCH, FIAPARCH and HYGARCH. These models were estimated in the presence of three alternative innovation's distributions: normal, Student and skewed Student. Our results show that considering for long-range memory, fat-tails and asymmetry performs better in predicting a one-day-ahead VaR for both short and long trading positions. Moreover, the FIAPARCH model outperforms the other models in the VaR's prediction. These results present several potential implications for energy markets risk quantifications and hedging strategies. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {2326--2339}, 
number = {5}, 
volume = {38}
}
@article{10.1016/j.eneco.2021.105221, 
year = {2021}, 
title = {{Systemic risk and financial contagion across top global energy companies}}, 
author = {Wu, Fei and Zhang, Dayong and Ji, Qiang}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2021.105221}, 
abstract = {{This study seeks to explore the firm-level interconnections in the fast changing and integrating global energy market. We investigate the risk connectedness using a Value-at-Risk (VaR) measure within a network composed of the top 20 global energy companies recognized by the Platts global energy company rankings, using the Diebold and Yilmaz's (2014) approach. We manage to identify the top risk contributors to the system across companies, regions and industries, and construct a total systemic risk index (TSRI) of the energy system. We further investigate which risk factors play a role in driving the evolution of the TSRI, again from a network perspective. The results show that its dynamics are mainly driven by the US stock market volatility and investors' sentiment in the financial market over the full sample, while energy market risks and exchange rate movements exert significant but short-term influences. © 2021 Elsevier B.V.}}, 
pages = {105221}, 
number = {NA}, 
volume = {97}
}
@article{10.1093/rof/rfw040, 
year = {2017}, 
title = {{A Simple Skewed Distribution with Asset Pricing Applications}}, 
author = {Roon, Frans de and Karehnke, Paul}, 
journal = {Review of Finance}, 
issn = {15723097}, 
doi = {10.1093/rof/rfw040}, 
abstract = {{Recent research has identified skewness and downside risk as one of the most important features of risk. We present a new distribution which makes modeling skewed risks no more difficult than normally distributed (symmetric) risks. Our distribution is a combination of the "downside" and "upside" half of two normal distributions, and its parameters can be calculated in closed form to match a given mean, variance, and skewness. Value at risk, expected shortfall, portfolio weights, and risk premia have simple expressions for our distribution and show economically meaningful deviations from the normal case already for very modest levels of skewness. An empirical application suggests that our distribution fits the data well. © The Authors 2016. Published by Oxford University Press on behalf of the European Finance Association. All rights reserved. For Permissions, please email: journals.permissions@oup.com.}}, 
pages = {rfw040}, 
number = {6}, 
volume = {21}
}
@article{10.21314/jor.2018.387, 
year = {2018}, 
title = {{Multifactor granularity adjustments for market and counterparty risks}}, 
author = {Fermanian, Jean-David and Florentin, Clement}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.387}, 
abstract = {{Approximated analytical calculations of loss distributions and risk measures are often accurate with factor models when portfolios become more fine-grained. Such calculations can be improved by granularity adjustment (GA) techniques if there remains a significant amount of undiversified idiosyncratic risk. We explain why it is so difficult to obtain analytic approximations of risk measures through granularity adjustment, when the underlying portfolio losses depend on several systematic factors. We propose several flexible families of models to manage the market and/or the counterparty risk of portfolios of financial assets. Explicit closed-form formulas based on GA techniques are provided, to approximate value-at-risks. We take into account random exposures, random recoveries and default risk simultaneously. Such models can be applied to portfolios of bonds, loans, stocks or even derivatives. We prove the accuracy of such analytic approximations through simulations, when the vectors of systematic factors are Gaussian or elliptical, more generally. © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {6}, 
volume = {20}
}
@article{10.21314/jcr.2017.223, 
year = {2017}, 
title = {{Primary-firm-driven portfolio loss}}, 
author = {Turnbull, Stuart}, 
journal = {The Journal of Credit Risk}, 
issn = {17446619}, 
doi = {10.21314/jcr.2017.223}, 
abstract = {{Many financial institutions provide loans to secondary firms, whose economic survival depends on the economic condition of primary firms. Even if loans from primary firms are not held in the loan portfolio, the financial distress of primary firms can adversely affect the loan portfolio of a financial institution. This paper describes a simple model that can be used for risk management. Our model directly incorporates the dependence of the conditional probability of default and loss given default of secondary firms on primary firms. Two simple examples show that failure to account for such dependence can result in the value-at-risk and the expected shortfall being greatly underestimated. © 2017 Incisive Risk Information (IP) Limited.}}, 
pages = {33--52}, 
number = {2}, 
volume = {13}
}
@article{10.1016/j.jbusres.2021.04.070, 
year = {2021}, 
title = {{How to conduct a bibliometric analysis: An overview and guidelines}}, 
author = {Donthu, Naveen and Kumar, Satish and Mukherjee, Debmalya and Pandey, Nitesh and Lim, Weng Marc}, 
journal = {Journal of Business Research}, 
issn = {0148-2963}, 
doi = {10.1016/j.jbusres.2021.04.070}, 
abstract = {{Bibliometric analysis is a popular and rigorous method for exploring and analyzing large volumes of scientific data. It enables us to unpack the evolutionary nuances of a specific field, while shedding light on the emerging areas in that field. Yet, its application in business research is relatively new, and in many instances, underdeveloped. Accordingly, we endeavor to present an overview of the bibliometric methodology, with a particular focus on its different techniques, while offering step-by-step guidelines that can be relied upon to rigorously perform bibliometric analysis with confidence. To this end, we also shed light on when and how bibliometric analysis should be used vis-à-vis other similar techniques such as meta-analysis and systematic literature reviews. As a whole, this paper should be a useful resource for gaining insights on the available techniques and procedures for carrying out studies using bibliometric analysis.}}, 
pages = {285--296}, 
volume = {133}, 
keywords = {}, 
local-url = {file://localhost/Users/barry/Documents/Papers%20Library/Donthu-How%20to%20conduct%20a%20bibliometric%20analysis-%20An%20overview%20and%20guidelines-2021-Journal%20of%20Business%20Research.pdf}
}
@article{10.1016/j.camwa.2008.10.099, 
year = {2009}, 
title = {{A high-order Markov-switching model for risk measurement}}, 
author = {Siu, T.K. and Ching, W.K. and Fung, E. and Ng, M. and Li, X.}, 
journal = {Computers \& Mathematics with Applications}, 
issn = {08981221}, 
doi = {10.1016/j.camwa.2008.10.099}, 
abstract = {{In this paper, we introduce a High-order Markov-Switching (HMS) model for measuring the risk of a portfolio. We suppose that the rate of return from a risky portfolio follows an HMS model with the drift and the volatility modulated by a discrete-time weak Markov chain. The states of the weak Markov chain are interpreted as observable states of an economy. We adopt the Value-at-Risk (VaR) as a metric for market risk quantification and examine the high-order effect of the underlying Markov chain on the risk measures via backtesting. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {1--10}, 
number = {1}, 
volume = {58}
}
@article{10.1016/j.ijforecast.2005.10.002, 
year = {2006}, 
title = {{Using extreme value theory to measure value-at-risk for daily electricity spot prices}}, 
author = {Chan, Kam Fong and Gray, Philip}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2005.10.002}, 
abstract = {{The recent deregulation in electricity markets worldwide has heightened the importance of risk management in energy markets. Assessing Value-at-Risk (VaR) in electricity markets is arguably more difficult than in traditional financial markets because the distinctive features of the former result in a highly unusual distribution of returns-electricity returns are highly volatile, display seasonalities in both their mean and volatility, exhibit leverage effects and clustering in volatility, and feature extreme levels of skewness and kurtosis. With electricity applications in mind, this paper proposes a model that accommodates autoregression and weekly seasonals in both the conditional mean and conditional volatility of returns, as well as leverage effects via an EGARCH specification. In addition, extreme value theory (EVT) is adopted to explicitly model the tails of the return distribution. Compared to a number of other parametric models and simple historical simulation based approaches, the proposed EVT-based model performs well in forecasting out-of-sample VaR. In addition, statistical tests show that the proposed model provides appropriate interval coverage in both unconditional and, more importantly, conditional contexts. Overall, the results are encouraging in suggesting that the proposed EVT-based model is a useful technique in forecasting VaR in electricity markets. © 2005 International Institute of Forecasters.}}, 
pages = {283--300}, 
number = {2}, 
volume = {22}
}
@article{10.1016/j.frl.2016.11.013, 
year = {2017}, 
title = {{Value-at-Risk estimation with stochastic interest rate models for option-bond portfolios}}, 
author = {Wang, Xiaoyu and Xie, Dejun and Jiang, Jingjing and Wu, Xiaoxia and He, Jia}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2016.11.013}, 
abstract = {{This article proposes a Monte Carlo simulation based approach for measuring Value-at-Risk of a portfolio consisting of options and bonds. The approach allows for jump-diffusions in underlying assets and affords to fit a variety of model layout, including both non-parametric and semi-parametric structures. Backtesting was conducted to assess the effectiveness of the method. The algorithm was tested against various trading positions, time horizons, and correlations between asset prices and market return rates. A prominent advantage of our approach is that its implementation does not require prior knowledge of the joint distribution or other statistical features of the related risk factors. © 2016 Elsevier Inc.}}, 
pages = {10--20}, 
number = {NA}, 
volume = {21}
}
@article{10.1007/978-3-642-25661-5_41, 
year = {2011}, 
title = {{Modeling supply chain network design problem with joint service level constraint}}, 
author = {Yang, Guoqing and Liu, Yankui and Yang, Kai}, 
journal = {Advances in Intelligent and Soft Computing}, 
issn = {18675662}, 
doi = {10.1007/978-3-642-25661-5\_41}, 
abstract = {{This paper studies a supply chain network design problem with stochastic parameters. A Value-at-Risk (VaR) based stochastic supply chain network design (VaR-SSCND) problem is built, in which both the transportation costs and customer demand are assumed to be random variables. The objective of the problem is to minimize the allowable invested capital. For general discrete distributions, the proposed problem is equivalent to a deterministic mixed-integer programming problem. So, we can employ conventional optimization algorithms such as branch-and-bound method to solve the deterministic programming problem. Finally, one numerical example is presented to demonstrate the validity of the proposed model and the effectiveness of the solution method. © 2011 Springer-Verlag Berlin Heidelberg.}}, 
pages = {311--318}, 
number = {NA}, 
volume = {123}
}
@article{10.1109/iccsee.2012.46, 
year = {2012}, 
title = {{Measuring the over-dispersed data in operational risk with the negative binomial process}}, 
author = {Lu, Zhaoyang}, 
journal = {2012 International Conference on Computer Science and Electronics Engineering}, 
issn = {NA}, 
doi = {10.1109/iccsee.2012.46}, 
abstract = {{In this paper, the negative binomial process is used to account for the over-dispersion in operational risk data. We estimate operational risk by means of the non-convex and convex risk measure, such as Value at Risk and Expected Shortfall, and provide a simple approximation to operational risk in a single risk cell. Moreover this approach is extended to the multivariate case, where the dependence structure between different risk cells is modeled by the Frank copula. In the final, we discuss almost all the limit cases when the dependence parameter differs. A practical example is presented to demonstrate the efficiency of approximation results. © 2012 IEEE.}}, 
pages = {463--467}, 
number = {NA}, 
volume = {3}
}
@article{10.1016/j.jfineco.2009.01.003, 
year = {2010}, 
title = {{First-passage probability, jump models, and intra-horizon risk}}, 
author = {Bakshi, Gurdip and Panayotov, George}, 
journal = {Journal of Financial Economics}, 
issn = {0304405X}, 
doi = {10.1016/j.jfineco.2009.01.003}, 
abstract = {{This paper proposes a risk measure, based on first-passage probability, which reflects intra-horizon risk in jump models with finite or infinite jump activity. Our empirical investigation shows, first, that the proposed risk measure consistently exceeds the benchmark value-at-risk (VaR). Second, jump risk tends to amplify intra-horizon risk. Third, we find large variation in our risk measure across jump models, indicative of model risk. Fourth, among the jump models we consider, the finite-moment log-stable model provides the most conservative risk estimates. Fifth, imposing more stringent VaR levels accentuates the impact of intra-horizon risk in jump models. Finally, using an alternative benchmark VaR does not dilute the role of intra-horizon risk. Overall, we contribute by showing that ignoring intra-horizon risk can lead to underestimation of risk exposures. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {20--40}, 
number = {1}, 
volume = {95}
}
@article{10.1007/978-3-540-87732-5_17, 
year = {2008}, 
title = {{Estimation of value-at-risk for exchange risk via kernel based nonlinear ensembled multi scale model}}, 
author = {He, Kaijian and Xie, Chi and Lai, Kinkeung}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-540-87732-5\_17}, 
abstract = {{Risk level in the exchange rate market is dynamically evolving with complicated structures. To further refine the analysis process and achieve more accurate measurement, this paper proposes a novel kernel based nonlinear ensembled multi scale Value at Risk methodology for evaluating the risk level in the exchange rate market. In the proposed algorithm, wavelet analysis is introduced to analyze the multi scale heterogeneous risk structures across different time scales. The Principle Component Analysis is used to extract principle components from the redundant forecast matrixes. Then the support vector regression technique is integrated into the modeling process to nonlinearly ensemble forecast matrixes and produce more stable and accurate results. Taking Euro market as a typical test case, empirical studies employing the proposed algorithm shows the superior performance than benchmark ARMA-GARCH and realized volatility based approaches. © 2008 Springer-Verlag Berlin Heidelberg.}}, 
pages = {148--157}, 
number = {PART 1}, 
volume = {5263 LNCS}
}
@article{10.1016/j.econlet.2015.10.028, 
year = {2015}, 
title = {{A simple and focused backtest of value at risk}}, 
author = {Krämer, Walter and Wied, Dominik}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2015.10.028}, 
abstract = {{We suggest a simple improvement of recent VaR-backtesting procedures based on time intervals between VaR-violations and show via Monte Carlo that our test has more power than its competitors against various empirically relevant clustering alternatives. © 2015 Elsevier B.V.}}, 
pages = {29--31}, 
number = {NA}, 
volume = {137}
}
@article{10.1016/j.rfe.2010.03.001, 
year = {2010}, 
title = {{An optimization process in Value-at-Risk estimation}}, 
author = {Huang, Alex YiHou}, 
journal = {Review of Financial Economics}, 
issn = {10583300}, 
doi = {10.1016/j.rfe.2010.03.001}, 
abstract = {{A new method is proposed to estimate Value-at-Risk (VaR) by Monte Carlo simulation with optimal back-testing results. The Monte Carlo simulation is adjusted through an iterative process to accommodate recent shocks, thereby taking into account the latest market conditions. Empirical validation covering the current financial crisis shows that VaR estimation via the optimization process is relatively reliable and consistent, and generally outperforms the VaR generated by a simple Monte Carlo simulation. This is particularly true in cases when the out-of-sample evaluation sample spans a lengthy period, as the traditional method tends to underestimate the number of extreme shocks. © 2010 Elsevier Inc.}}, 
pages = {109--116}, 
number = {3}, 
volume = {19}
}
@article{10.1016/s1135-2523(12)70002-3, 
year = {2012}, 
title = {{A naïve approach to speed up portfolio optimization problem using a multiobjective genetic algorithm}}, 
author = {Baixauli-Soler, J. Samuel and Alfaro-Cid, Eva and Fernandez-Blanco, Matilde O.}, 
journal = {Investigaciones Europeas de Dirección y Economía de la Empresa}, 
issn = {11352523}, 
doi = {10.1016/s1135-2523(12)70002-3}, 
abstract = {{Genetic algorithms (GAs) are appropriate when investors have the objective of obtaining mean-variance (VaR) efficient frontier as minimising VaR leads to non-convex and non-differential risk-return optimisation problems. However GAs are a time-consuming optimisation technique. In this paper, we propose to use a naïve approach consisting of using samples split by quartile of risk to obtain complete efficient frontiers in a reasonable computation time. Our results show that using reduced problems which only consider a quartile of the assets allow us to explore the efficient frontier for a large range of risk values. In particular, the third quartile allows us to obtain efficient frontiers from the 1.8\% to 2.5\% level of VaR quickly, while that of the first quartile of assets is from 1\% to 1.3\% level of VaR. © 2012 AEDEM.}}, 
pages = {126--131}, 
number = {2}, 
volume = {18}
}
@article{10.1007/s11408-012-0199-9, 
year = {2012}, 
title = {{Portfolio risk management in a data-rich environment}}, 
author = {Bouaddi, Mohammed and Taamouti, Abderrahim}, 
journal = {Financial Markets and Portfolio Management}, 
issn = {15554961}, 
doi = {10.1007/s11408-012-0199-9}, 
abstract = {{We study risk assessment using an optimal portfolio in which the weights are functions of latent factors and firm-specific characteristics (hereafter, diffusion index portfolio). The factors are used to summarize the information contained in a large set of economic data and thus reflect the state of the economy. First, we evaluate the performance of the diffusion index portfolio and compare it to both that of a portfolio in which the weights depend only on firm-specific characteristics and an equally weighted portfolio. We then use value-at-risk, expected shortfall, and downside probability to investigate whether the weights-modeling approach, which is based on factor analysis, helps reduce market risk. Our empirical results clearly indicate that using economic factors together with firm-specific characteristics helps protect investors against market risk. © 2012 Swiss Society for Financial Market Research.}}, 
pages = {469--494}, 
number = {4}, 
volume = {26}
}
@article{10.1016/j.physa.2016.11.131, 
year = {2017}, 
title = {{European option pricing under the Student's t noise with jumps}}, 
author = {Wang, Xiao-Tian and Li, Zhe and Zhuang, Le}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2016.11.131}, 
abstract = {{In this paper we present a new approach to price European options under the Student's t noise with jumps. Through the conditional delta hedging strategy and the minimal mean-square-error hedging, a closed-form solution of the European option value is obtained under the incomplete information case. In particular, we propose a Value-at-Risk-type procedure to estimate the volatility parameter σ such that the pricing error is in accord with the risk preferences of investors. In addition, the numerical results of us show that options are not priced in some cases in an incomplete information market. © 2016 Elsevier B.V.}}, 
pages = {848--858}, 
number = {NA}, 
volume = {469}
}
@article{10.1109/tfuzz.2018.2842752, 
year = {2018}, 
title = {{A Multi-Objective Portfolio Selection Model with Fuzzy Value-At-Risk Ratio}}, 
author = {Wang, Bo and Li, You and Wang, Shuming and Watada, Junzo}, 
journal = {IEEE Transactions on Fuzzy Systems}, 
issn = {10636706}, 
doi = {10.1109/tfuzz.2018.2842752}, 
abstract = {{Considering nonstatistical uncertainties and/or insufficient historical data in security return forecasts, fuzzy set theory has been applied in the past decades to build portfolio selection models. Meanwhile, various risk measurements such as variance, entropy, and Value-At-Risk have been proposed in fuzzy environments to evaluate investment risks from different perspectives. Sharpe ratio, also known as the reward-To-variability ratio, which measures the risk premium per unit of the nonsystematic risk (asset deviation), has received great attention in modern portfolio theory. In this study, the Sharpe ratio in fuzzy environments is introduced, whereafter, a fuzzy Value-At-Risk ratio is proposed. Compared with Sharpe ratio, Value-At-Risk ratio is an index with dimensional knowledge that reflects the risk premium per unit of the systematic risk (the greatest loss under a given confidence level). On the basis of the two ratios, a multi-objective model is built to evaluate their joint impact on portfolio selection. Then, the proposed model is solved by a fuzzy simulation based multi-objective particle swarm optimization algorithm, where the global best of each iteration is determined by an improved dominance times based method. Finally, the algorithm superiority is justified via comparing with existing solvers on benchmark problems, and the model effectiveness is exemplified by using three case studies on portfolio selection. © 1993-2012 IEEE.}}, 
pages = {3673--3687}, 
number = {6}, 
volume = {26}
}
@article{10.2202/1558-3708.1800, 
year = {2011}, 
title = {{Early detection techniques for market risk failure}}, 
author = {Olmo, Jose and Pouliot, William}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.2202/1558-3708.1800}, 
abstract = {{The implementation of appropriate statistical techniques (backtesting) for monitoring conditional VaR models is the mechanism used by financial institutions to determine the severity of departures of the VaR model from market results and subsequently, the tool used by regulators to determine the penalties imposed for inadequate risk models. So far, however, there has been no attempt to determine the timing of this rejection and with it to obtain some guidance regarding the cause of failure in reporting an appropriate VaR. This paper corrects this by proposing U-statistic type processes that extend standard CUSUM statistics widely employed for change-point detection. In contrast to CUSUM statistics these new tests are indexed by certain weight functions that enhance their statistical power to detect the timing of the market risk model failure. These tests are robust to estimation risk and can be devised to be very sensitive to detection of market failure produced early in the out-of-sample evaluation period, in which standard methods usually fail due to the absence of data. © 2011 The Berkeley Electronic Press. All rights reserved.}}, 
number = {4}, 
volume = {15}
}
@article{10.1504/ijmef.2008.019223, 
year = {2008}, 
title = {{On the role of volatility for modelling risk exposure}}, 
author = {Olmo, Jose}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2008.019223}, 
abstract = {{We show in this paper that volatility measures can be misleading indicators of risk if returns do not follow a Gaussian distribution. A more reliable measure of risk is the probability distribution of the return on an asset. Estimators for these measures are usually challenging and need of nonparametric and semi-parametric techniques. The aim of this paper is twofold. First, it proposes the use of semi-parametric estimators of the distribution function of the return on an asset based on extreme value theory for computing Value-at-Risk; and second, it discusses the validity of different volatility models in this semi-parametric framework. The conclusion is that different volatility models can yield different valid risk measures if coupled with the appropriate distribution function. Hence the puzzle in the choice of volatility measures. This is shown in an empirical exercise for data of financial indexes from USA, UK, Germany, Japan and Spain. © 2008 Inderscience Enterprises Ltd.}}, 
pages = {219}, 
number = {2}, 
volume = {1}
}
@article{10.3390/risks7020058, 
year = {2019}, 
title = {{Revisiting calibration of the solvency ii standard formula for mortality risk: Does the standard stress scenario provide an adequate approximation of value-at-risk?}}, 
author = {Gylys, Rokas and Šiaulys, Jonas}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks7020058}, 
abstract = {{The primary objective of this work is to analyze model based Value-at-Risk associated with mortality risk arising from issued term life assurance contracts and to compare the results with the capital requirements for mortality risk as determined using Solvency II Standard Formula. In particular, two approaches to calculate Value-at-Risk are analyzed: one-year VaR and run-off VaR. The calculations of Value-at-Risk are performed using stochastic mortality rates which are calibrated using the Lee-Carter model fitted using mortality data of selected European countries. Results indicate that, depending on the approach taken to calculate Value-at-Risk, the key factors driving its relative size are: sensitivity of technical provisions to the latest mortality experience, volatility of mortality rates in a country, policy term and benefit formula. Overall, we found that Solvency II Standard Formula on average delivers an adequate capital requirement, however, we also highlight particular situations where it could understate or overstate portfolio specific model based Value-at-Risk for mortality risk. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {58}, 
number = {2}, 
volume = {7}
}
@article{10.1007/s10203-014-0160-7, 
year = {2015}, 
title = {{Using Value-at-Risk to reconcile limited liability and the moral-hazard problem}}, 
author = {Tulli, Vanda and Weinrich, Gerd}, 
journal = {Decisions in Economics and Finance}, 
issn = {15938883}, 
doi = {10.1007/s10203-014-0160-7}, 
abstract = {{In the present paper uncertainty over the market price of a risk-neutral competitive firm’s output and limited liability imply the possibility of bankruptcy, give rise to moral hazard and entail that the firm’s output decision depends on its equity holding. Subjecting the firm to a Value-at-Risk (VaR) constraint induces it to behave in an as-if risk-averse manner, but in a static context moral hazard persists for a certain interval of values of equity. In a dynamic setting, the size of equity holding becomes a choice variable and the VaR constraint guides the firm to select equity values outside the moral-hazard interval. Thus it achieves to reconcile two apparently conflicting goals: encourage entrepreneurial activity by means of limited liability and avoid irresponsible gambling due to the incentives provided by it. © 2014, Springer-Verlag Italia.}}, 
pages = {93--118}, 
number = {1}, 
volume = {38}
}
@article{10.1109/cso.2012.105, 
year = {2012}, 
title = {{Robust alpha-reliable network design problem under distribution-free demand}}, 
author = {Sun, Hua and Gao, Ziyou}, 
journal = {2012 Fifth International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2012.105}, 
abstract = {{A general assumption in the reliable network design problem is that probability distributions of the sources of uncertainty are known. However, in reality, this distribution may be unavailable as we may have no (insufficient) data to calibrate the distribution. In this paper, we relax this assumption and present two robust alpha reliable network design models under distribution-free demand by adopting Worst-case Value-at-Risk (WVaR) and Worst-case conditional Value-at-risk (WCVR) risk measures, where only requires that the first m moments (m is a positive integer and associated with the form of link cost function) of demand to be known. We prove that the two models are equivalent to the same model under general distribution. The equivalent NDP model is formulated as mathematical programs with complementarity constraint (MPCC). A manifold suboptimization algorithm is developed to solve this alpha robust reliable network design problem (NDP). Numerical example is presented to illustrate the features of th proposed NDP model. © 2012 IEEE.}}, 
pages = {453--457}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ribaf.2020.101347, 
year = {2021}, 
title = {{Value at risk and returns of cryptocurrencies before and after the crash: long-run relations and fractional cointegration}}, 
author = {Tan, Zhengxun and Huang, Yilong and Xiao, Binuo}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2020.101347}, 
abstract = {{Cryptocurrency has become an increasingly important investment vehicle, thus the long-run relationship between risk and return of cryptocurrency is vital for both investors and policy-makers. We apply the Fractionally Cointegrated Vector Autoregression (FCVAR) model and investigate the risk-return relationship. This has not been studied previously, much less disintegrating the series into periods of pre-crash, post-crash, and the full sample. Empirical results indicate that risk series in all eight cryptocurrency markets exhibit long-memory property, and there is a long-run fractional cointegration relationship between the risk of altcoins and Bitcoin. Most importantly, though a positive risk-return tradeoff is found in the full sample, there are big differences between results of pre-crash and post-crash. The time horizon set to 14 days, all the eight currencies exhibit traditional risk-return tradeoff after the crisis, whereas the effect doesn't exist before the crisis, with the exception of Dash and Doge. In the same vein, there is no leverage effect in all eight currencies before the crisis but this effect is present in three cryptocurrencies after the crisis. The differences indicate that investors are more cautious, increasing risk awareness and demanding higher compensation for risk after the crash. The above results are robust when time horizon is 30 days. © 2020 Elsevier B.V.}}, 
pages = {101347}, 
number = {NA}, 
volume = {56}
}
@article{10.1088/1742-6596/1053/1/012113, 
year = {2018}, 
title = {{Could Bitcoin enhance the portfolio performance?}}, 
author = {Pinudom, Bundit and Tungpisansampun, Worathan and Tansuchat, Roengchai and Maneejuk, Paravee}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1053/1/012113}, 
abstract = {{This study analyses the effect of adding bitcoin into the portfolio by exploiting the Long Only investment strategy. The Portfolio consists of five assets: bitcoin, crude oil price index, stock exchange of Thailand (SET) price index, the exchange rate between Thai and USD and Thai government bond compound with treasurer bill. The model used for modelling the return of all asset is Multivariate t-copula based on GARCH and also measure the risk of the portfolio using the Value-at-risk (VaR) under the condition of minimizing the variance of return. We find that when adding more bitcoin into the portfolio, the return and risk of asset increased. If we only invest in bitcoin, we will face the risk at 16.90\% and gain 6.27\%. When comparing the effectiveness of portfolio by using Return-risk ratio, it found that portfolio with bitcoin shows the higher return rate than portfolios without bitcoin. Therefore, it can conclude that bitcoin could indeed increase the effectiveness of portfolio. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012113}, 
number = {1}, 
volume = {1053}
}
@article{10.4028/www.scientific.net/amm.345.368, 
year = {2013}, 
title = {{GARCHSK based risk assessment in electric power industry}}, 
author = {Wang, Rui Qing and Xiao, Zi Qian}, 
journal = {Applied Mechanics and Materials}, 
issn = {16609336}, 
doi = {10.4028/www.scientific.net/amm.345.368}, 
abstract = {{The restructuring/deregulation in electric power industry has heightened the importance of risk assessment. A model to estimate value-at-risk via GARCHSK specification is proposed, in which the seasonalities, heteroscedasticities, skewnesses, kurtosises and relationship to system loads are jointly addressed. The impacts of probability distribution assumption for innovations on value-at-risk estimate validation are analyzed for three distributions: normal, student-t and Gram-Charlier series expansion of the normal density function. The numerical example shows that the proposed model performs better in predicting one-period-ahead VaR. © (2013) Trans Tech Publications, Switzerland.}}, 
pages = {368--371}, 
number = {NA}, 
volume = {345}
}
@article{10.17485/ijst/2015/v8i36/88629, 
year = {2015}, 
title = {{A computationally more efficient distance based VaR methodology for real time market risk measurement}}, 
author = {Jammalamadaka, Sastry K R and Ramesh, K V N M and Murthy, J V R}, 
journal = {Indian Journal of Science and Technology}, 
issn = {09746846}, 
doi = {10.17485/ijst/2015/v8i36/88629}, 
abstract = {{Measurement of market risk requires lots of computational resources when the Value-at-Risk (VaR) is computed using the historical simulation approach as it involves full revaluation of the portfolio for the considered data points. Although approximations can be done using the delta-normal, delta-gamma and delta-gamma-theta approaches, historical simulation approach alone is straight forward method that uses past data to generate future values without assuming any distribution for the underlying returns. The requirement of intensive computational effort in case of historical simulation hinders it's usage for applying to real time VaR calculation. In this work we propose a methodology that doesn't forego the benefits of historical simulation approach but can be applied to calculate market risk VaR in real time. The VaR calculated using the proposed methodology converges as the range of the portfolio returns is increased. The proposed methodology is also superior to the historical simulation approach in terms of usage of the computational resources and applicability to real time without sacrificing accuracy obtained using historical simulation approach.}}, 
number = {36}, 
volume = {8}
}
@article{10.1016/j.insmatheco.2017.03.001, 
year = {2017}, 
title = {{Bootstrap consistency and bias correction in the nonparametric estimation of risk measures of collective risks}}, 
author = {Lauer, Alexandra and Zähle, Henryk}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2017.03.001}, 
abstract = {{We consider two nonparametric estimators for the risk measure of the sum of n i.i.d. individual insurance risks divided by n, where the number of historical single claims that are used for the statistical estimation is of order n. This framework matches the situation that nonlife insurance companies are faced with within the scope of premium calculation. Indeed, the risk measure of the collective risk divided by n can be seen as a suitable premium for each of the individual risks. For both estimators asymptotic normality has been obtained recently. Here we derive almost sure bootstrap consistency for both estimators, where we allow for the weighted exchangeable bootstrap and rather general law-invariant risk measures. Both estimators are subject to a relevant negative bias for small to moderate n. For one of them we investigate by means of numerical experiments the benefit of a bootstrap-based bias correction. The numerical experiments are performed for the Value at Risk and the Average Value at Risk, and the results are comparable to those of Kim and Hardy (2007) who did analogous experiments for classical nonparametric plug-in estimators. For the other estimator the benefit of a bootstrap-based bias correction can be ruled out by theoretical arguments. © 2017 Elsevier B.V.}}, 
pages = {99--108}, 
number = {NA}, 
volume = {74}
}
@article{10.1016/j.cam.2009.10.007, 
year = {2010}, 
title = {{How to estimate the Value at Risk under incomplete information}}, 
author = {Schepper, Ann De and Heijnen, Bart}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2009.10.007}, 
abstract = {{A key problem in financial and actuarial research, and particularly in the field of risk management, is the choice of models so as to avoid systematic biases in the measurement of risk. An alternative consists of relaxing the assumption that the probability distribution is completely known, leading to interval estimates instead of point estimates. In the present contribution, we show how this is possible for the Value at Risk, by fixing only a small number of parameters of the underlying probability distribution. We start by deriving bounds on tail probabilities, and we show how a conversion leads to bounds for the Value at Risk. It will turn out that with a maximum of three given parameters, the best estimates are always realized in the case of a unimodal random variable for which two moments and the mode are given. It will also be shown that a lognormal model results in estimates for the Value at Risk that are much closer to the upper bound than to the lower bound. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {2213--2226}, 
number = {9}, 
volume = {233}
}
@article{10.3390/risks8020048, 
year = {2020}, 
title = {{Testing the least-squares Monte Carlo method for the evaluation of capital requirements in life insurance}}, 
author = {Costabile, Massimo and Viviano, Fabio}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks8020048}, 
abstract = {{In this paper, we test the efficiency of least-squares Monte Carlo method to estimate capital requirements in life insurance. We choose a simplified Gaussian evaluation framework where closed-form formulas are available and allow us to obtain solid benchmarks. Extensive numerical experiments were conducted by considering different combinations of simulation runs and basis functions, and the corresponding results are illustrated. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {48}, 
number = {2}, 
volume = {8}
}
@article{10.1017/asb.2017.4, 
year = {2017}, 
title = {{Collective risk models with dependence uncertainty}}, 
author = {Liu, Haiyan and Wang, Ruodu}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2017.4}, 
abstract = {{We bring the recently developed framework of dependence uncertainty into collective risk models, one of the most classic models in actuarial science. We study the worst-case values of the Value-at-Risk (VaR) and the Expected Shortfall (ES) of the aggregate loss in collective risk models, under two settings of dependence uncertainty: (i) the counting random variable (claim frequency) and the individual losses (claim sizes) are independent, and the dependence of the individual losses is unknown; (ii) the dependence of the counting random variable and the individual losses is unknown. Analytical results for the worst-case values of ES are obtained. For the loss from a large portfolio of insurance policies, an asymptotic equivalence of VaR and ES is established. Our results can be used to provide approximations for VaR and ES in collective risk models with unknown dependence. Approximation errors are obtained in both cases. © 2017 Astin Bulletin.}}, 
pages = {361--389}, 
number = {2}, 
volume = {47}
}
@article{10.21314/jor.2015.314, 
year = {2015}, 
title = {{Improved estimation methods for value-at-risk, expected shortfall and risk contributions with high precision}}, 
author = {Muromachi, Yukio}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.314}, 
abstract = {{The (marginal) risk contribution is very useful for analyzing the concentration risk in a portfolio. However, it is difficult to estimate the risk contributions for value-at-risk (VaR) and expected shortfall (ES) precisely, especially using a Monte Carlo simulation. We applied a saddlepoint approximation to estimate the distribution function, so that the difficulty of estimating the risk contributions for VaR was dissolved. In this paper, we propose new estimation methods for ES and the risk contributions for ES based on the conditional independence and a saddlepoint approximation. Numerical studies confirm that these new methods are much better than existing ones. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--27}, 
number = {5}, 
volume = {17}
}
@article{10.1109/wsc.2010.5678972, 
year = {2010}, 
title = {{Importance sampling for risk contributions of credit portfolios}}, 
author = {Liu, Guangwu}, 
journal = {Proceedings of the 2010 Winter Simulation Conference}, 
issn = {08917736}, 
doi = {10.1109/wsc.2010.5678972}, 
abstract = {{Value-at-Risk is often used as a risk measure of credit portfolios, and it can be decomposed into a sum of risk contributions associated with individual obligors. These risk contributions play an important role in risk management of credit portfolios. They can be used to measure risk-adjusted performances of subportfolios and to allocate risk capital. Mathematically, risk contributions can be represented as conditional expectations, which are conditioned on rare events. In this paper, we develop a restricted importance sampling (IS) method for simulating risk contributions, and devise estimators whose mean square errors converge in a rate of n-1. Furthermore, we combine our method with the IS method in the literature to improve the efficiency of the estimators. Numerical examples show that the proposed method works quite well. ©2010 IEEE.}}, 
pages = {2771--2781}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.physa.2019.121798, 
year = {2019}, 
title = {{Analysis of shares frequency components on daily value-at-risk in emerging and developed markets}}, 
author = {Biage, Milton}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.121798}, 
abstract = {{Value-at-Risk was estimated using the technique of wavelet decomposition with the goal to analyze the frequency components’ impacts on variances of daily stock returns, and VaR forecasts. Daily returns of twenty-one shares of the Ibovespa and daily returns of twenty-two shares of the DJIA were used. The FIGARCH(1,d,1) model was applied to the reconstructed returns to model and establish the prediction of conditional variance, applying the rolling window technique. The Value-at-Risk was then estimated, and the results showed that the DJIA shares showed more efficient market behavior than those of Ibovespa. The differences in behavior induce to affirm that VaRs, used in the analysis of financial assets from different markets with different governance premises, should be estimated by series of returns reconstructed by aggregations of components of different frequencies. A set of back-testing was applied to confront the estimated VaRs, which demonstrated that the estimations of VaRs models are consistent. © 2019}}, 
pages = {121798}, 
number = {NA}, 
volume = {532}
}
@article{10.1111/poms.12688, 
year = {2017}, 
title = {{Risk Mitigation of Production Hedging}}, 
author = {Park, John H. and Kazaz, Burak and Webster, Scott}, 
journal = {Production and Operations Management}, 
issn = {10591478}, 
doi = {10.1111/poms.12688}, 
abstract = {{This study examines how a firm can mitigate global economic risk through production hedging, defined as producing less than the total demand. We investigate a firm's production planning, pricing, and financial hedging decisions under exchange rate and demand uncertainty with the objective of maximizing expected profit while complying with a value-at-risk (VaR) constraint that limits the firm's losses in amount and probability. The study makes three contributions. First, we show that production hedging, when compared to matching demand with production, can substantially reduce risk both from VaR and conditional-VaR perspectives while increasing expected profit. Our second contribution relates to the optimal pricing decisions. When a firm has pricing flexibility, it is commonly expected that the optimal price would increase under production hedging. Our study, however, shows that production hedging causes the firm to decrease the optimal price below the riskless price in order to benefit from exchange rate fluctuations. The pressure from risk aversion on the optimal price decision is not one directional, and can lead to both an increase and a decrease in price. Third, our work examines the interactions between financial hedging and production hedging. It identifies when financial hedging serves as a complement, and when as a substitute, to production hedging. Our work shows that financial hedging cannot always eliminate production hedging from being an optimal solution. © 2017 Production and Operations Management Society}}, 
pages = {1299--1314}, 
number = {7}, 
volume = {26}
}
@article{10.1016/j.csda.2014.08.011, 
year = {2016}, 
title = {{Confidence intervals for ARMA–GARCH Value-at-Risk: The case of heavy tails and skewness}}, 
author = {Spierdijk, Laura}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2014.08.011}, 
abstract = {{It is a well-known result that, when the ARMA–GARCH model errors lack a finite fourth moment, the asymptotic distribution of the quasi-maximum likelihood estimator may not be Normal. In such a scenario the conventional bootstrap turns out inconsistent. Surprisingly, simulations show that the conventional bootstrap, despite its inconsistency, provides accurate confidence intervals for ARMA–GARCH Value-at-Risk (VaR) in case of various symmetric error distributions without finite fourth moment. The usual bootstrap does fail, however, in the presence of skewed error distributions without finite fourth moment. In this case several other methods for estimating confidence intervals fail as well. A residual subsample bootstrap is proposed to obtain confidence intervals for ARMA–GARCH VaR. According to theory, this ‘omnibus’ method produces confidence intervals with asymptotically correct coverage rates under very mild conditions. By means of a simulation study the favorable finite-sample properties of the residual subsample bootstrap are illustrated. Confidence intervals for ARMA–GARCH VaR with good coverage rates are established, even when other methods fail in the presence of skewed model errors without finite fourth moment. The estimation of confidence intervals by means of the residual subsample bootstrap is illustrated in an empirical application to daily stock returns. © 2014 Elsevier B.V.}}, 
pages = {545--559}, 
number = {NA}, 
volume = {100}
}
@article{10.1016/j.insmatheco.2016.05.014, 
year = {2016}, 
title = {{Optimal management of DC pension plan under loss aversion and Value-at-Risk constraints}}, 
author = {Guan, Guohui and Liang, Zongxia}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2016.05.014}, 
abstract = {{This paper studies the risk management in a defined contribution (DC)pension plan. The financial market consists of cash, bond and stock. The interest rate in our model is assumed to follow an Ornstein-Uhlenbeck process while the contribution rate follows a geometric Brownian Motion. Thus, the pension manager has to hedge the risks of interest rate, stock and contribution rate. Different from most works in DC pension plan, the pension manger has to obtain the optimal allocations under loss aversion and Value-at-Risk(VaR) constraints. The loss aversion pension manager is sensitive to losses while the VaR pension manager has to ensure the quality of wealth at retirement. Since these problems are not standard concave optimization problems, martingale method is applied to derive the optimal investment strategies. Explicit solutions are obtained under these two optimization criterions. Moreover, sensitivity analysis is presented in the end to show the economic behaviors under these two criterions. © 2016 Elsevier B.V.}}, 
pages = {224--237}, 
number = {NA}, 
volume = {69}
}
@article{10.1080/03610918.2016.1208234, 
year = {2017}, 
title = {{Heterogenous market hypothesis evaluation using multipower variation volatility}}, 
author = {Chin, Wen Cheong and Lee, Min Cherng and Tan, Pei Pei}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2016.1208234}, 
abstract = {{High-frequency trading activities are one of the common phenomena in nowadays financial markets. Enormous amounts of high-frequency trading data are generated by huge numbers of market participants in every trading day. The availability of this information allows researchers to further examine the statistical properties of informationally efficient market hypothesis (EMH). Heterogenous market hypothesis (HMH) is one of the important extensions of EMH literature. HMH introduced nonlinear trading behaviors of heterogenous market participants instead of normality assumption under the EMH homogenous market participants. In this study, we attempt to explore more high-frequency volatility estimators in the HMH examination. These include the bipower, tripower, and quadpower variation integrated volatility estimates using Heterogenous AutoRegressive (HAR) models. The empirical findings show that these alternatives multipower variation (MPV) estimators provide better estimation and out-of-sample forecast evaluations as compared to the standard realized volatility. In other words, the usage of MPV estimators is able to better explain the HMH statistically. At last, a market risk determination is illustrated using value-at-risk approach. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {00--00}, 
number = {8}, 
volume = {46}
}
@article{10.1007/978-3-642-40728-4_69, 
year = {2013}, 
title = {{Independent component analysis filtration for value at risk modelling}}, 
author = {Szupiluk, Ryszard and Wojewnik, Piotr and Ząbkowski, Tomasz}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-40728-4\_69}, 
abstract = {{In this article we present independent component analysis (ICA) applied to the concept of value at risk (VaR) modelling. The use of ICA decomposition enables to extract components with particular statistical properties that can be interpreted in economic terms. However, the characteristic of financial time series, in particular the nonstationarity in terms of higher order statistics, makes it difficult to apply ICA to VaR right away. This requires using adequate ICA algorithms or their modification taking into account the statistical characteristics of financial data. © 2013 Springer-Verlag Berlin Heidelberg.}}, 
pages = {553--562}, 
number = {NA}, 
volume = {8131 LNCS}
}
@article{10.1016/j.ijpe.2018.11.025, 
year = {2019}, 
title = {{Effective management of performance-based contracts for sustainment dominant systems}}, 
author = {Patra, Pradipta and Kumar, U. Dinesh and Nowicki, David R. and Randalld, Wesley S.}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2018.11.025}, 
abstract = {{Performance-based contracting (PBC) is becoming the favoured procurement strategy among suppliers and customers of sustainment dominant systems (SDS) such as aircraft, weapon systems, mining equipment, etc. Under a PBC, supplier profit is linked to how well the SDS meets customer expectations in terms of relevant performance metrics. System availability is a common, contractually agreed upon performance metric that measures customer expectations. In this paper, we analyse performance contracts from the perspective of the supplier, where system availability serves as the performance metric. We develop single-period and multi-period supplier performance models and demonstrate how these models are used to maximise a supplier's profit in the context of a PBC. We provide empirical evidence from a mining and construction equipment industry and demonstrate how PBC overcomes the information asymmetry and moral hazard that is common in SDS principal agent models. Practically, our models can be used to increase the likelihood of success for both the supplier and the customer when they embrace PBC. Our main contributions include establishing the optimal availability that an original equipment manufacturer (OEM) can provide by considering their own probability of loss; analysing the properties of an OEM's future net profit function using first order autoregressive moving average (ARMA) process; and performing marginal analysis and providing bounds for the net profit function for linear and non-linear revenue functions. We derive a relationship between macro-level availability and micro-level parameters such as failure rate, fleet size and base stock levels. © 2018 Elsevier B.V.}}, 
pages = {369--382}, 
number = {NA}, 
volume = {208}
}
@article{10.1007/s11579-016-0165-9, 
year = {2016}, 
title = {{Natural risk measures}}, 
author = {Assa, Hirbod}, 
journal = {Mathematics and Financial Economics}, 
issn = {18629679}, 
doi = {10.1007/s11579-016-0165-9}, 
abstract = {{A coherent risk measure with a proper continuity condition cannot be defined on a large set of random variables. However, if one relaxes the sub-additivity condition and replaces it with co-monotone sub-additivity, the proper domain of risk measures can contain the set of all random variables. In this study, by replacing the sub-additivity axiom of law invariant coherent risk measures with co-monotone sub-additivity, we introduce the class of natural risk measures on the space of all bounded-below random variables. We characterize the class of natural risk measures by providing a dual representation of its members. © 2016, Springer-Verlag Berlin Heidelberg.}}, 
pages = {441--456}, 
number = {4}, 
volume = {10}
}
@article{10.1109/cifer.2012.6327765, 
year = {2012}, 
title = {{A multi-covariate semi-parametric conditional volatility model using probabilistic fuzzy systems}}, 
author = {Almeida, Rui Jorge and Basturk, Nalan and Kaymak, Uzay and Milea, Viorel}, 
journal = {2012 IEEE Conference on Computational Intelligence for Financial Engineering \& Economics (CIFEr)}, 
issn = {NA}, 
doi = {10.1109/cifer.2012.6327765}, 
abstract = {{Value at Risk (VaR) has been successfully estimated using single covariate probabilistic fuzzy systems (PFS), a method which combines a linguistic description of the system behaviour with statistical properties of data. In this paper, we consider VaR estimation based on a PFS model for density forecast of a continuous response variable conditional on a high-dimensional set of covariates. The PFS model parameters are estimated by a novel two-step process. The performance of the proposed model is compared to the performance of a GARCH model for VaR estimation of the S\&P 500 index. Furthermore, the additional information and process understanding provided by the different interpretations of the PFS models are illustrated. Our findings show that the validity of GARCH models are sometimes rejected, while those of PFS models of VaR are never rejected. Additionally, the PFS model captures both instant and periods of high volatility, and leads to less conservative models. © 2012 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ejor.2011.05.055, 
year = {2011}, 
title = {{Sudden changes in variance and time varying hedge ratios}}, 
author = {Aragó, Vicent and Salvador, Enrique}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2011.05.055}, 
abstract = {{This paper analyzes the influence of sudden changes in the unconditional volatility on the estimation and forecast of volatility and its impact on futures hedging strategies. We employ several multivariate GARCH models to estimate the optimal hedge ratios for the Spanish stock market including in each one some well-known patterns that may affect volatility forecasts (asymmetry and sudden changes). The main empirical results show that more complex models including sudden changes in volatility outperform the simpler models in hedging effectiveness both with in-sample and out-of-sample analysis. However, the evidence is stronger when the loss distribution tail is used as a measure for the effectiveness (Value at Risk (VaR) and Expected Shortfall (ES)) suggesting that traditional measures based on the variance of the hedged portfolio should be used with caution. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {393--403}, 
number = {2}, 
volume = {215}
}
@article{10.1109/cifer.2012.6327832, 
year = {2012}, 
title = {{Stressed Value-at-Risk}}, 
author = {Dash, Jan}, 
journal = {2012 IEEE Conference on Computational Intelligence for Financial Engineering \& Economics (CIFEr)}, 
issn = {NA}, 
doi = {10.1109/cifer.2012.6327832}, 
abstract = {{Stressed Value at Risk (Stressed VAR) in its advanced framework provides a realistic measure of market risk tailored for stressed market environments. The simpler regulatory version of Stressed VAR is a special case. Stressed VAR corrects various deficits of ordinary VAR in times of market stress. Stressed VAR incorporates scenario analysis in a VAR setting in a sophisticated and consistent fashion. The mathematical framework is familiar and simple, designed to be understandable in a practical way by risk managers, traders, and regulators. Specifically, the familiar Gaussian (normal) probability formalism is employed, but in a completely different way than for ordinary VAR, designed to account for tail risk and collective behavior. The two main ingredients for Stressed VAR are "fat-tail volatilities" that account for outlier events in the risk factors, and stressed correlations between risk factors that account for collective market participant behavior in stressed markets. This information is provided as input to standard Monte Carlo simulation to determine stressed market risks for a given portfolio. The VAR with the inputs of the fat-tail volatilities and the stressed correlations is the Stressed VAR. Bloomberg LP is implementing Stressed VAR in the PORT portfolio system. © 2012 IEEE.}}, 
pages = {1--1}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/10807039.2011.571103, 
year = {2011}, 
title = {{Risk value of emerging technology products and their diffusion through use of a caswn-ce and real options approach}}, 
author = {He, Yinglong and Zhou, Zongfang and Shi, Yong}, 
journal = {Human and Ecological Risk Assessment: An International Journal}, 
issn = {10807039}, 
doi = {10.1080/10807039.2011.571103}, 
abstract = {{The rapid development of emerging technology products (ETPs) in China'smarket makes it important to evaluate and forecast ETPs diffusion. Due to the high uncertainty of ETP marketing and inadequate marketing data of China ETPs markets, the traditional forecasting methods cannot be applied to effectively measure it. We use cellular automata embedded small world network (CASWN) and crossentropy (CE) divergence to measure the differences of the ETP adopters' spatial distribution. The empirical analysis indicates the success probabilities of ETPs can be predicted effectively though CASWN a d CE divergence, which can make a precise judgment for the market prospect of an ETP in the shortest time after an ETP launch. Based on this, we can calculate the expected cash flow accurately and the option value of an ETP project for sound decision-making. In the meantime, the CE threshold value of success of ETPs in China market is obtained only by the spatial data of early-stage after an ETP launch, not rely on time series data. So a new tool is provided for the prediction and venture capital policy of an ETP launch in this article. © Taylor \& Francis Group, LLC.}}, 
pages = {678--687}, 
number = {3}, 
volume = {17}
}
@article{10.1016/j.frl.2020.101570, 
year = {2021}, 
title = {{Modeling dynamic higher moments of crude oil futures}}, 
author = {Huang, Zhuo and Liang, Fang and Wang, Tianyi and Li, Chao}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2020.101570}, 
abstract = {{This paper investigates the time-varying conditional higher moments of the daily returns on WTI crude oil futures, using the GJR-GARCH model with Gram-Charlier expansion (GCE) of normal density. The empirical results suggest significant time-variations in the conditional skewness and kurtosis. The out-of-sample value-at-risk (VaR) forecasting results show the advantage of models with dynamic higher moments over those with constant higher moments. © 2020 Elsevier Inc.}}, 
pages = {101570}, 
number = {NA}, 
volume = {39}
}
@article{10.1109/upec.2014.6934617, 
year = {2014}, 
title = {{Risk-based asset replacement in EHV transmission networks}}, 
author = {Fleckenstein, Marco and Balzer, Gerd and Neumann, Claus and Wasserrab, Andreas}, 
journal = {2014 49th International Universities Power Engineering Conference (UPEC)}, 
issn = {NA}, 
doi = {10.1109/upec.2014.6934617}, 
abstract = {{The developed method of risk-based asset replacement consists of the three major parts: availability \& reliability calculations, asset risk determination with a Value at Risk (VaR) method and the multiple choice knapsack optimized replacement strategy developer. The transmission network availability \& reliability calculations are carried out with multiple load flow scenarios which cover all typical load states during a year of the investigated transmission system. The developed VaR-method includes a Monte Carlo simulation to generate and concentrate an amount of input values to one key figure per asset. The individual outage rates and costs which depend on the time of failure occurrence are taken into account by the VaR determination. The multiple choice knapsack optimized risk-based replacement strategy developer determines the decision which activity is chosen for the individual asset and the reliability \& availability of the overall transmission network and minimum investment costs as target figures. This two-dimensional optimum is found with the branch-and-bound algorithm of Sinha and Zolters. The three different opportunities are immediately replacement, refurbishment or inspection with a change to a corrective replacement. The transition from a standard to a risk-based replacement strategy allows a significant budget reduction while keeping transmission system reliability \& availability. The assets providing an increased risk in the network due to their age-related behavior and location in the transmission network are determined and get a higher priority for exchange. Thus, the major assets are replaced more quickly and due to the corrective replacement strategy the costs are reduced for the less important assets. © 2014 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10690-018-9260-7, 
year = {2019}, 
title = {{In search of robust methods for multi-currency portfolio construction by value at risk}}, 
author = {Tang, Mei-Ling and Do, Trung K.}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-018-9260-7}, 
abstract = {{The main purpose of this paper is to select the most appropriate technique predicting precisely the exchange rate risk from three main approaches, namely, the Historical Simulation approach, the Variance–Covariance approach and the Monte Carlo Simulation approach. Our main finding shows that the historical simulation approach with exponentially weighted moving average, which exhibits the lowest out-of-sample loss, is the most appropriate method for value at risk estimation with regard to a multi-currency portfolio construction in the Taiwan foreign exchange market. Moreover, results in backtesting lend support to the accuracy of our proposed strategies at the 99\% confidence level. © 2018, Springer Japan KK, part of Springer Nature.}}, 
pages = {107--126}, 
number = {1}, 
volume = {26}
}
@article{10.1007/s10922-016-9370-3, 
year = {2016}, 
title = {{Effective Risk Assessment in Resilient Communication Networks}}, 
author = {Rusek, Krzysztof and Guzik, Piotr and Chołda, Piotr}, 
journal = {Journal of Network and Systems Management}, 
issn = {10647570}, 
doi = {10.1007/s10922-016-9370-3}, 
abstract = {{The paper discusses business impact analysis in the context of resilient communication networks. It is based on the total (aggregated) penalty that may be paid by an operator when the services (identified with transport demands) provided are interrupted due to network failures. The level of penalty is expressed as a commonly accepted business risk measure, Value-at-Risk (VaR). First, the main concern over VaR, namely the theoretical lack of subadditivity, is discussed. The study shows that, in practice, disadvantages do not appear in resilient network design, and VaR can be used without the need to apply more complex and less informative measures. Second, a method for calculating the upper bound of the total penalty is presented. The assessment is performed for unprotected and protected services with a broad variety of compensation policies used to translate technical loss to monetarily expressed penalty. The proposed bounds are experimentally shown to be effective in comparison with alternative calculation methods, and also in the case when some of the assumptions taken during the modelling stage are not met. © 2016, The Author(s).}}, 
pages = {491--515}, 
number = {3}, 
volume = {24}
}
@article{10.1016/j.jfs.2014.08.009, 
year = {2014}, 
title = {{Geographic diversification in banking}}, 
author = {Fang, Yiwei and Lelyveld, Iman van}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2014.08.009}, 
abstract = {{In the aftermath of the 2007-2009 crisis, banks claiming positive diversification benefits are being met with skepticism. Nevertheless, diversification might be important and sizable for some large internationally active banking groups. We use a universally applicable correlation matrix approach to calculate international diversification effects, in which bank subsidiaries are treated as individual assets of the banking group portfolio. We apply the framework to 49 of the world's largest banking groups with significant foreign business units over the 1992-2009 period. Focusing on the most important risk in banking, credit risk, we find that allowing for geographical diversification could reduce banks' credit risk by 1.1\% on average, with risk reduction ranging from negligible up to 8\%. © 2014 Elsevier B.V.}}, 
pages = {172--181}, 
number = {NA}, 
volume = {15}
}
@article{10.1016/j.physa.2015.08.052, 
year = {2016}, 
title = {{Value-at-Risk forecasts by a spatiotemporal model in Chinese stock market}}, 
author = {Gong, Pu and Weng, Yingliang}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2015.08.052}, 
abstract = {{This paper generalizes a recently proposed spatial autoregressive model and introduces a spatiotemporal model for forecasting stock returns. We support the view that stock returns are affected not only by the absolute values of factors such as firm size, book-to-market ratio and momentum but also by the relative values of factors like trading volume ranking and market capitalization ranking in each period. This article studies a new method for constructing stocks' reference groups; the method is called quartile method. Applying the method empirically to the Shanghai Stock Exchange 50 Index, we compare the daily volatility forecasting performance and the out-of-sample forecasting performance of Value-at-Risk (VaR) estimated by different models. The empirical results show that the spatiotemporal model performs surprisingly well in terms of capturing spatial dependences among individual stocks, and it produces more accurate VaR forecasts than the other three models introduced in the previous literature. Moreover, the findings indicate that both allowing for serial correlation in the disturbances and using time-varying spatial weight matrices can greatly improve the predictive accuracy of a spatial autoregressive model. © 2015 Elsevier B.V.}}, 
pages = {173--191}, 
number = {NA}, 
volume = {441}
}
@article{10.1016/j.qref.2018.03.001, 
year = {2019}, 
title = {{Measurement of the displaced commercial risk in Islamic Banks}}, 
author = {Toumi, Kaouther and Viviani, Jean-Laurent and Chayeh, Zeinab}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2018.03.001}, 
abstract = {{The objective of the research is to quantify the displaced commercial risk (DCR) based on quantitative finance techniques. We develop an internal model based on the Value-at-risk (VaR) measure of risk to assess the DCR-VaR and the alpha coefficient αCAR in the capital adequacy ratio of Islamic banks. We identify first the scenarios of exposure of Islamic banks to DCR that depend on the actual return on unrestricted profit sharing investment accounts (PSIAU), the benchmark return as well as the level of the existing profit equalization reserve (PER) and investment risk reserve (IRR). Second, we quantify the DCR-VaR and the alpha coefficient αCAR−VaR for a given holding period and for given confidence level. We illustrate the DCR-VaR model on selected Islamic banks from Bahrain. Our model helps to better assess the needed equity to cover the DCR and an accurate capital adequacy ratio for Islamic banks. The model has also policy implications for regulators and the IFSB to develop better guidance on good practices in managing this risk. © 2018 Board of Trustees of the University of Illinois}}, 
pages = {18--31}, 
number = {NA}, 
volume = {74}
}
@article{10.1080/03610926.2020.1764036, 
year = {2020}, 
title = {{Truncated skewed type III generalized logistic distribution: risk measurement applications}}, 
author = {Theodossiou, Panayiotis}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2020.1764036}, 
abstract = {{This article derives the moment functions of the truncated skewed type III generalized logistic (SGL). These are then applied in finance for the development of value at risk (VaR), expected shortfall (ES), and downside risk measures for investment returns and values. The SGL distribution provides and good fit to the empirical distribution of a representative set of long series of financial data. Moreover, the SGL generates accurate VaR measures. © 2020, © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--24}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2007.11.012, 
year = {2008}, 
title = {{Backtesting trading risk of commercial banks using expected shortfall}}, 
author = {Wong, Woon K.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2007.11.012}, 
abstract = {{This paper uses saddlepoint technique to backtest the trading risk of commercial banks using expected shortfall. It is found that four out of six US commercial banks have excessive trading risks. Monte Carlo simulation studies show that the proposed backtest is very accurate and powerful even for small test samples. More importantly, risk managers can carry out the proposed backtest based on any number of exceptions, so that incorrect risk models can be promptly detected before any further huge losses are realized. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {1404--1415}, 
number = {7}, 
volume = {32}
}
@article{10.2298/yjor1002275c, 
year = {2010}, 
title = {{Evaluating total operational value and associated risks of financial holding companies in Taiwan}}, 
author = {Chen, Li-Hui}, 
journal = {Yugoslav Journal of Operations Research}, 
issn = {03540243}, 
doi = {10.2298/yjor1002275c}, 
abstract = {{This study comprises several different parts. The first part applies a normal benchmark valuation model established by Penman to assess the potential whole operational values of FHCs. The second part applies the concept of measuring financial risk as earnings variance to establish a financial risk measurement model. This model can be used to examine the degrees of financial risk before and after FHC's establishment, and to distinguish different combinations of FHC based on risk diversion efficiency. The final part of this research constructs a new value-risk relation model that can be applied to cross-analysis for measuring total operation value of FHCs with different degrees of financial risk. Through completion of the above steps this study will demonstrate what combination of FHC offers the co-benefits of risk diversion and high whole operational value.}}, 
pages = {275--292}, 
number = {2}, 
volume = {20}
}
@article{10.3390/risks6040133, 
year = {2018}, 
title = {{The value-at-risk estimate of stock and currency-stock portfolios’ returns}}, 
author = {Su, Jung-Bin and Hung, Jui-Cheng}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks6040133}, 
abstract = {{This study utilizes the seven bivariate generalized autoregressive conditional heteroskedasticity (GARCH) models to forecast the out-of-sample value-at-risk (VaR) of 21 stock portfolios and seven currency-stock portfolios with three weight combinations, and then employs three accuracy tests and one efficiency test to evaluate the VaR forecast performance for the above models. The seven models are constructed by four types of bivariate variance-covariance specifications and two approaches of parameters estimates. The four types of bivariate variance-covariance specifications are the constant conditional correlation (CCC), asymmetric and symmetric dynamic conditional correlation (ADCC and DCC), and the BEKK, whereas the two types of approach include the standard and non-standard approaches. Empirical results show that, regarding the accuracy tests, the VaR forecast performance of stock portfolios varies with the variance-covariance specifications and the approaches of parameters estimate, whereas it does not vary with the weight combinations of portfolios. Conversely, the VaR forecast performance of currency-stock portfolios is almost the same for all models and still does not vary with the weight combinations of portfolios. Regarding the efficiency test via market risk capital, the NS-BEKK model is the most suitable model to be used in the stock and currency-stock portfolios for bank risk managers irrespective of the weight combination of portfolios. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {133}, 
number = {4}, 
volume = {6}
}
@article{10.1016/j.insmatheco.2019.02.006, 
year = {2019}, 
title = {{Risk-adjusted Bowley reinsurance under distorted probabilities}}, 
author = {Cheung, Ka Chun and Yam, Sheung Chi Phillip and Zhang, Yiying}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2019.02.006}, 
abstract = {{In the seminal work of Chan and Gerber (1985), one of the earliest game theoretical approaches was proposed to model the interaction between the reinsurer and insurer; in particular, the optimal pricing density for the reinsurer and optimal ceded loss for the insurer were determined so that their corresponding expected utilities could be maximized. Over decades, their advocated Bowley solution (could be understood as Stackelberg equilibria) concept of equilibrium reinsurance strategy has not been revisited in the modern risk management framework. In this article, we attempt to fill this gap by extending their work to the setting of general premium principle for the reinsurer and distortion risk measure for the insurer. © 2019 Elsevier B.V.}}, 
pages = {64--72}, 
number = {NA}, 
volume = {86}
}
@article{10.17654/ms101040909, 
year = {2017}, 
title = {{Multivariate risks modeling for financial portfolio management and climate applications}}, 
author = {Loyara, Yves Bernadin Vini and Bagré, Remi Guillaume and Barro, Diakarya}, 
journal = {Far East Journal of Mathematical Sciences (FJMS)}, 
issn = {09720871}, 
doi = {10.17654/ms101040909}, 
abstract = {{This paper investigates some properties of derivative measures of the Value at Risk (VaR) of random variables modeling the stochastic behavior of a portfolio asset. Specifically, coherentness and convex properties of the conditional, the tail VaR and the standard deviation are established. Moreover, a new version of high risk scenario is characterized and bivariate densities are modeled via copula approach. © 2017 Pushpa Publishing House, Allahabad, India.}}, 
pages = {909--929}, 
number = {4}, 
volume = {101}
}
@article{10.1093/jjfinec/nbu029, 
year = {2015}, 
title = {{Semi-Parametric conditional quantile models for financial returns and realized volatility}}, 
author = {Žikeš, Filip and Baruník, Jozef}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbu029}, 
abstract = {{This paper investigates how the conditional quantiles of future returns and volatility of financial assets vary with various measures of ex post variation in asset prices as well as option-implied volatility. We work in the flexible quantile regression framework and rely on recently developed model-free measures of integrated variance, upside and downside semivariance, and jump variation. Our results for the S \& P 500 andWTI Crude Oil futures contracts show that simple linear quantile regressions for returns and heterogenous quantile autoregressions for realized volatility perform very well in capturing the dynamics of the respective conditional distributions, both in absolute terms as well as relative to a couple of well-established benchmark models. The models can therefore serve as useful risk management tools for investors trading the futures contracts themselves or various derivative contracts written on realized volatility. © The Author, 2014.}}, 
pages = {nbu029}, 
number = {1}, 
volume = {14}
}
@article{10.5424/fs/2017263-10445, 
year = {2017}, 
title = {{Incorporating stand level risk management options into forest decision support systems}}, 
author = {Eyvindson, Kyle and Saad, Rami and Eriksson, Ljusk Ola}, 
journal = {Forest Systems}, 
issn = {21715068}, 
doi = {10.5424/fs/2017263-10445}, 
abstract = {{Aim of study: To examine methods of incorporating risk and uncertainty to stand level forest decisions. Area of study: A case study examines a small forest holding from Jönköping, Sweden. Material and methods: We incorporate empirically estimated uncertainty into the simulation through a Monte Carlo approach when simulating the forest stands for the next 100 years. For the iterations of the Monte Carlo approach, errors were incorporated into the input data which was simulated according to the Heureka decision support system. Both the Value at Risk and the Conditional Value at Risk of the net present value are evaluated for each simulated stand. Main results: Visual representation of the errors can be used to highlight which decision would be most beneficial dependent on the decision maker’s opinion of the forest inventory results. At a stand level, risk preferences can be rather easily incorporated into the current forest decision support software. Research highlights: Forest management operates under uncertainty and risk. Methods are available to describe this risk in an understandable fashion for the decision maker. © 2017 INIA.}}, 
pages = {013}, 
number = {3}, 
volume = {26}
}
@article{10.1016/j.eneco.2020.104886, 
year = {2020}, 
title = {{Tail risk of electricity futures}}, 
author = {Peña, Juan Ignacio and Rodríguez, Rosa and Mayoral, Silvia}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2020.104886}, 
abstract = {{This paper compares the in-sample and out-of-sample performance of several models for computing the tail risk of one-month and one-year electricity futures contracts traded in the NordPool, French, German, and Spanish markets in 2008–2017. As measures of tail risk, we use the one-day-ahead Value-at-Risk (VaR) and the Expected Shortfall (ES). With VaR, the AR (1)-GARCH (1,1) model with Student-t distribution is the best-performing specification with 88\% cases in which the Fisher test accepts the model, with a success rate of 94\% in the left tail and of 81\% in the right tail. The model passes the test of model adequacy in the 100\% of the cases in the NordPool and German markets, but only in the 88\% and 63\% of the cases in the Spanish and French markets. With ES, this model passes the test of model adequacy in 100\% of cases in all markets. Historical Simulation and Quantile Regression-based approaches misestimate tail risks. The right-hand tail of the returns is more difficult to model than the left-hand tail and therefore financial regulators and the administrators of futures markets should take these results into account when setting additional regulatory capital requirements and margin account regulations to short positions. © 2020 Elsevier B.V.}}, 
pages = {104886}, 
number = {NA}, 
volume = {91}
}
@article{10.1109/ares.2014.86, 
year = {2014}, 
title = {{Risk-aware design and management of resilient networks}}, 
author = {Cholda, Piotr}, 
journal = {2014 Ninth International Conference on Availability, Reliability and Security}, 
issn = {NA}, 
doi = {10.1109/ares.2014.86}, 
abstract = {{A current view on the design of networks resilient to non-malicious failures supported by risk engineering is presented in this keynote. The aspect of risk response is emphasized. © 2014 IEEE.}}, 
pages = {468--475}, 
number = {NA}, 
volume = {NA}
}
@article{10.1198/jbes.2009.0016, 
year = {2009}, 
title = {{Inhomogeneous dependence modeling with time-varying copulae}}, 
author = {Giacomini, Enzo and Härdle, Wolfgang and Spokoiny, Vladimir}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/jbes.2009.0016}, 
abstract = {{Measuring dependence in multivariate time series is tantamount to modeling its dynamic structure in space and time. In risk management, the nonnormal behavior of most financial time series calls for non-Gaussian dependences. The correct modeling of non-Gaussian dependences is, therefore, a key issue in the analysis of multivariate time series. In this article we use copula functions with adaptively estimated time-varying parameters for modeling the distribution of returns. Furthermore, we apply copulae to the estimation of Value-at-Risk of portfolios and show their better performance over the RiskMetrics approach. © 2009 American Statistical Association.}}, 
pages = {224--234}, 
number = {2}, 
volume = {27}
}
@article{10.1007/s11222-009-9114-2, 
year = {2010}, 
title = {{Beta kernel quantile estimators of heavy-tailed loss distributions}}, 
author = {Charpentier, Arthur and Oulidi, Abder}, 
journal = {Statistics and Computing}, 
issn = {09603174}, 
doi = {10.1007/s11222-009-9114-2}, 
abstract = {{In this paper we suggest several nonparametric quantile estimators based on Beta kernel. They are applied to transformed data by the generalized Champernowne distribution initially fitted to the data. A Monte Carlo based study has shown that those estimators improve the efficiency of the traditional ones, not only for light tailed distributions, but also for heavy tailed, when the probability level is close to 1. We also compare these estimators with the Extreme Value Theory Quantile applied to Danish data on large fire insurance losses. © Springer Science+Business Media, LLC 2009.}}, 
pages = {35--55}, 
number = {1}, 
volume = {20}
}
@article{10.1088/1742-6596/1324/1/012085, 
year = {2019}, 
title = {{The extreme risk spillovers between the US and China's agricultural commodity futures markets}}, 
author = {Zhu, Qiujing and Tansuchat, Roengchai}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1324/1/012085}, 
abstract = {{In this study, we investigate the downside and upside risk spillover effects between the same kind of agricultural futures in the Chinese and US markets, taking the value-at-risk (VaR) and conditional value-at-risk (CVaR) as a risk measure, characterized and computed using copula-GARCH approach. We find evidence of a significant positive dependence between the US and Chinese agricultural commodity futures markets. And the empirical results also show that the existence of downside and upside risk spillover effects between variables, especially significant during financial turmoil periods. Market regulators and traders of agricultural futures will benefit by identifying tail dependence and extreme risk spillovers. © 2019 IOP Publishing Ltd. All rights reserved.}}, 
pages = {012085}, 
number = {1}, 
volume = {1324}
}
@article{10.1007/3-540-26993-2_20, 
year = {2005}, 
title = {{Asset/liability management of German life insurance companies: A value-at-risk approach in the presence of interest rate guarantees}}, 
author = {Albrecht, Peter and Weber, Carsten}, 
issn = {NA}, 
doi = {10.1007/3-540-26993-2\_20}, 
abstract = {{This contribution analyzes the implications of two major determinants influencing the asset allocation decision of German life insurers, which are the capital market development on the one hand and the interest rate guarantees of the traditional life insurance policies on the other hand. The adverse development of the stock prices between 2000 and 2002 asks for a consideration of not only the normal volatility but also the worst-case developments in an asset/liability management. In order to meet the latter requirement, we technically apply the risk measures of Value-at-Risk and Conditional Value-at-Risk. German life insurance policies incorporate interest rate guarantees, which are granted on an annual basis. This specific myopic nature of guarantees creates - beyond the control of the shortfall risk in general - the necessity to manage the asset allocation on an annual basis to match the time horizon of assets and liabilities. A quantitative approach analyzes the impacts on the asset allocation decision. In our research we do not only consider market valuation, but also institutional peculiarities (such as hidden reserves and accounting norms) of German life insurers. We reveal the possibility of a riskless one-year investment, either based on market values or on book values, to be crucial for guaranteeing interest rates on an annual basis. © 2005 Springer Berlin · Heidelberg.}}, 
pages = {407--419}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/j0r.2016.332, 
year = {2016}, 
title = {{Evaluating the performance of the skewed distributions to forecast value-at-risk in the global financial crisis}}, 
author = {Abad, Pilar and Benito, Sonia and Martín, Carmen López and Sánchez-Granero, Miguel Ángel}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/j0r.2016.332}, 
abstract = {{This paper evaluates the performance of several skewed and symmetric distributions by modeling the tail behavior of daily returns and forecasting value-at-risk (VaR). First, we use some goodness-of-fit tests to analyze which distribution best fits the data. The comparisons in terms of VaR are carried out by examining the accuracy of the VaR estimate and minimizing the loss function from the points of view of the regulator and the firm. The results show that the skewed distributions outperform the normal and Student t (ST) distributions in fitting portfolio returns. Following a two-stage selection process, whereby we initially ensure that the distributions provide accurateVaR estimates, and focusing on the firm’s loss function, we can conclude that skewed distributions outperform the normal and ST distributions in forecasting VaR. From the point of view of the regulator, the superiority of the skewed distributions related to ST is not evident. As the firms are free to choose the model they use to forecast VaR, in practice, skewed distributions will be used more frequently. © 2016 Incisive Risk Information (IP) Limited.}}, 
number = {5}, 
volume = {18}
}
@article{10.1016/j.jbankfin.2011.03.016, 
year = {2011}, 
title = {{Extreme returns: The case of currencies}}, 
author = {Osler, Carol and Savaser, Tanseli}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2011.03.016}, 
abstract = {{Financial market crashes can occur even in the absence of news. This paper highlights four properties of price-contingent trading that increase the frequency of such events. Price-contingent trading is common across financial market, since it includes algorithmic trading, technical trading, and dynamic option hedging. The four properties we consider are: (1) high kurtosis in the distribution of order sizes; (2) clustering of trades within the day; (3) clustering of trades at certain prices; and (4) feedback between trading and returns. The paper estimates the relative importance of these factors using data from the foreign exchange market. Calibrated simulations indicate that interactions among these factors are at least as important as any single one. Among individual factors, the orders' size distribution and feedback effects have the strongest influence. Overall, price-contingent trading could account for half of realized excess kurtosis. The paper suggests that extreme returns unaccompanied by news are statistically inevitable in the presence of price-contingent trading. © 2011 Elsevier B.V.}}, 
pages = {2868--2880}, 
number = {11}, 
volume = {35}
}
@article{10.1109/emeit.2011.6023741, 
year = {2011}, 
title = {{The research of software reliability measure based on conditional value at risk}}, 
author = {Xiao-mei, Zhu and Qun-yan, Zhang and xin, Ren}, 
journal = {Proceedings of 2011 International Conference on Electronic \& Mechanical Engineering and Information Technology}, 
issn = {NA}, 
doi = {10.1109/emeit.2011.6023741}, 
abstract = {{The research on software reliability is one of the main research contents in the software engineering. At present, the research on software reliability mostly focuses on the software reliability analysis in the conventional condition, and the study on software reliability problems at extreme conditions is less. This article introduces the theory of conditional value at risk in the financial risk management to study the problems of software reliability at extreme conditions. The data simulation illustrates the application process of this theory, the conclusions show that this method has a certain application value, which can play a supplementary function for the study of software reliability. © 2011 IEEE.}}, 
pages = {3092--3094}, 
number = {NA}, 
volume = {6}
}
@article{10.1016/j.ijforecast.2018.05.004, 
year = {2018}, 
title = {{Forecasting risk with Markov-switching GARCH models: A large-scale performance study}}, 
author = {Ardia, David and Bluteau, Keven and Boudt, Kris and Catania, Leopoldo}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2018.05.004}, 
abstract = {{We perform a large-scale empirical study in order to compare the forecasting performances of single-regime and Markov-switching GARCH (MSGARCH) models from a risk management perspective. We find that MSGARCH models yield more accurate Value-at-Risk, expected shortfall, and left-tail distribution forecasts than their single-regime counterparts for daily, weekly, and ten-day equity log-returns. Also, our results indicate that accounting for parameter uncertainty improves the left-tail predictions, independently of the inclusion of the Markov-switching mechanism. © 2018 The Author(s)}}, 
pages = {733--747}, 
number = {4}, 
volume = {34}
}
@article{10.23919/chicc.2017.8028254, 
year = {2017}, 
title = {{A semi-parametric quantile regression random forest approach for evaluating muti-period value at risk}}, 
author = {Jiang, Feng and Wu, Wenjun and Peng, Zijun}, 
journal = {2017 36th Chinese Control Conference (CCC)}, 
issn = {19341768}, 
doi = {10.23919/chicc.2017.8028254}, 
abstract = {{In this paper, we propose a hybrid semi-parametric quantile regression random forest approach to evaluate value at risk (VaR). A Quantile regression random forest is introduced to explain the non-linear relationship in multi-period VaR measurement. Moreover, the essential algorithms and distributions of various holding periods are given. The results show that the generalized autoregressive conditional heteroskedasticity-quantile regression random forest with t distribution and generalized error distribution have the best performance, compared with other distributions. At the same time, the semi-parametric approach outperforms the benchmarked parametric technique. © 2017 Technical Committee on Control Theory, CAA.}}, 
pages = {5642--5646}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ribaf.2018.08.003, 
year = {2019}, 
title = {{Empirical analysis of intertemporal relations between downside risks and expected returns—Evidence from Asian markets}}, 
author = {Chiang, Thomas C.}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2018.08.003}, 
abstract = {{This paper tests the risk-return relations for Asian stock markets by employing conditional volatility, local downside risk, regional downside risk, and world/U.S. downside risk. We find positive and significant intertemporal relations between excess stock returns and various risks. The evidence supports the risk-return tradeoff not only from local risk but also from external risk. The model is robust as it pertains to the risk of small variations as well as big shocks. The evidence supports positive risk-return relations across 10 Asian markets after controlling for the lagged dividend yield, higher moments of stock returns, and exchange rate variations. © 2018 Elsevier B.V.}}, 
pages = {264--278}, 
number = {NA}, 
volume = {47}
}
@article{10.1007/s10713-005-4677-0, 
year = {2005}, 
title = {{Optimal insurance design under a value-at-risk framework}}, 
author = {Wang, Ching-Ping and Shyu, David and Huang, Hung-Hsi}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1007/s10713-005-4677-0}, 
abstract = {{This study designs an optimal insurance policy form endogenously, assuming the objective of the insured is to maximize expected final wealth under the Value-at-Risk (VaR) constraint. The optimal insurance policy can be replicated using three options, including a long call option with a small strike price, a short call option with a large strike price, and a short cash-or-nothing call option. Additionally, this study also calculates the optimal insurance levels for these models when we restrict the indemnity to be one of three common forms: a deductible policy, an upper-limit policy, or a policy with proportional coinsurance. © 2005 The Geneva Association.}}, 
pages = {161--179}, 
number = {2}, 
volume = {30}
}
@article{10.1016/j.actaastro.2010.07.011, 
year = {2011}, 
title = {{Comparative cost and utility analysis of monolith and fractionated spacecraft using failure and replacement Markov models}}, 
author = {Dubos, Gregory F. and Saleh, Joseph H.}, 
journal = {Acta Astronautica}, 
issn = {00945765}, 
doi = {10.1016/j.actaastro.2010.07.011}, 
abstract = {{Failure of a single component on-board a spacecraft can compromise the integrity of the whole system and put its entire capability and value at risk. Part of this fragility is intrinsic to the current dominant design of space systems, which is mainly a single, large, monolithic system. The space industry has therefore recently proposed a new architectural concept termed fractionation, or co-located space-based network (SBN). By physically distributing functions in multiple orbiting modules wirelessly connected, this architecture allows the sharing of resources on-orbit (e.g., data processing, downlinks). It has been argued that SBNs could offer significant advantages over the traditional monolithic architecture as a result of the network structure and the separation of sources of risk in the spacecraft. Careful quantitative analyses are still required to identify the conditions under which SBNs can "outperform" monolithic spacecraft. In this work, we develop Markov models of module failures and replacement to quantitatively compare the lifecycle cost and utility of both architectures. We run Monte-Carlo simulations of the models, and discuss important trends and invariants. We then investigate the impact of our model parameters on the existence of regions in the design space in which SBNs "outperform" the monolith spacecraft on a cost, utility, and utility per unit cost basis. Beyond the life of one single spacecraft, this paper compares the cost and utility implications of maintaining each architecture type through successive replacements. © 2010 Elsevier Ltd. All rights reserved.}}, 
pages = {172--184}, 
number = {1-2}, 
volume = {68}
}
@article{10.1016/j.jimonfin.2007.06.011, 
year = {2007}, 
title = {{Varying the VaR for unconditional and conditional environments}}, 
author = {Cotter, John}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2007.06.011}, 
abstract = {{Accurate forecasting of risk is the key to successful risk management techniques. Using the largest stock index futures from 12 European bourses, this paper presents VaR measures based on their unconditional and conditional distributions for single and multi-period settings. These measures underpinned by extreme value theory are statistically robust explicitly allowing for fat-tailed densities. Conditional tail estimates accounting for volatility clustering are obtained by adjusting the unconditional extreme value procedure with GARCH filtered returns. The conditional modelling results in iid returns allowing for the use of a simple and efficient multi-period extreme value scaling law. The paper examines the properties of these distinct conditional and unconditional trading models. The paper finds that the biases inherent in unconditional single and multi-period estimates assuming normality extend to the conditional setting. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {1338--1354}, 
number = {8}, 
volume = {26}
}
@article{10.1016/j.ememar.2014.05.001, 
year = {2014}, 
title = {{VaR performance during the subprime and sovereign debt crises: An application to emerging markets}}, 
author = {Brio, Esther B. Del and Mora-Valencia, Andrés and Perote, Javier}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2014.05.001}, 
abstract = {{Highly volatile scenarios, such as those provoked by the recent subprime and sovereign debt crises, have questioned the accuracy of current risk forecasting methods. This paper adds fuel to this debate by comparing the performance of alternative specifications for modeling the returns filtered by an ARMA-GARCH: Parametric distributions (Student's t and skewed-t), the extreme value theory (EVT), semi-nonparametric methods based on the Gram-Charlier (GC) expansion and the normal (benchmark). We implement backtesting techniques for the pre-crisis and crisis periods for stock index returns and a hedge fund of emerging markets. Our results show that the Student's t fails to forecast VaR during the crisis, while the EVT and GC accurately capture market risk, the latter representing important savings in terms of efficient regulatory capital provisions. © 2014 Elsevier B.V.}}, 
pages = {23--41}, 
number = {NA}, 
volume = {20}
}
@article{10.1016/j.srfe.2012.06.001, 
year = {2012}, 
title = {{Building good deals with arbitrage-free discrete time pricing models}}, 
author = {Balbás, Beatriz and Balbás, Raquel}, 
journal = {The Spanish Review of Financial Economics}, 
issn = {21731268}, 
doi = {10.1016/j.srfe.2012.06.001}, 
abstract = {{Recent literature has proved that many classical very important pricing models of Financial Economics (Black and Scholes, Heston, etc.) and risk measures (VaR, CVaR, etc.) may lead to "pathological meaningless situations", since there exist sequences of portfolios whose negative risk and positive expected return are unbounded. Such a sequence of strategies will be called "good deal". This paper focuses on a discrete time arbitrage-free and complete pricing model and goes beyond existence properties. It deals with the effective construction of good deals, i.e., sequences (ym)m=1∞ of portfolios such that. (VaR(ym),CVaR(ym),Expected\_return(ym))tends to (-∞, -∞, +∞). Under quite general conditions the explicit expression of a good deal is given, and practical algorithms are provided. The sensitivity of our results with respect to measurement errors or dynamic changes of the parameters is analyzed, and numerical experiments are presented with the binomial model. © 2012 Asociación Española de Finanzas.}}, 
pages = {53--61}, 
number = {2}, 
volume = {10}
}
@article{10.1109/lcsys.2020.2998543, 
year = {2020}, 
title = {{Control Design for Risk-Based Signal Temporal Logic Specifications}}, 
author = {Safaoui, Sleiman and Lindemann, Lars and Dimarogonas, Dimos V. and Shames, Iman and Summers, Tyler H.}, 
journal = {IEEE Control Systems Letters}, 
issn = {24751456}, 
doi = {10.1109/lcsys.2020.2998543}, 
eprint = {2006.00317}, 
abstract = {{We present a general framework for risk semantics on Signal Temporal Logic (STL) specifications for stochastic dynamical systems using axiomatic risk theory. We show that under our recursive risk semantics, risk constraints on STL formulas can be expressed in terms of risk constraints on atomic predicates. We then show how this allows a (stochastic) STL risk constraint to be transformed into a risk-tightened deterministic STL constraint on a related deterministic nominal system, enabling the application of existing STL methods. For affine predicate functions and a (coherent) Distributionally Robust Value at Risk measure, we show how risk constraints on atomic predicates can be reformulated as tightened deterministic affine constraints. We demonstrate the framework using a Model Predictive Control (MPC) design with an STL risk constraint. © 2017 IEEE.}}, 
pages = {1000--1005}, 
number = {4}, 
volume = {4}
}
@article{10.1016/j.insmatheco.2016.06.002, 
year = {2016}, 
title = {{Market risk forecasting for high dimensional portfolios via factor copulas with GAS dynamics}}, 
author = {Bartels, Mariana and Ziegelmann, Flavio A.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2016.06.002}, 
abstract = {{In this paper we propose forecasting market risk measures, such as Value at Risk (VaR) and Expected Shortfall (ES), for large dimensional portfolios via copula modeling. For that we compare several high dimensional copula models, from naive ones to complex factor copulas, which are able to simultaneously tackle the curse of dimensionality and introduce a high level of complexity into the model. We explore both static and dynamic copula fitting. In the dynamic case we allow different levels of flexibility for the dependence parameters which are driven by a GAS (Generalized Autoregressive Scores) model, in the spirit of Oh and Patton (2015). Our empirical results, for assets negotiated at Brazilian BOVESPA stock market from January, 2008 to December, 2014, suggest that, compared to the other copula models, the GAS dynamic factor copula approach has a superior performance in terms of AIC (Akaike Information Criterion) and a non-inferior performance with respect to VaR and ES forecasting. © 2016 Elsevier B.V.}}, 
pages = {66--79}, 
number = {NA}, 
volume = {70}
}
@article{10.1111/j.0960-1627.2004.00207.x, 
year = {2004}, 
title = {{Dynamic minimization of worst conditional expectation of shortfall}}, 
author = {Sekine, Jun}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/j.0960-1627.2004.00207.x}, 
abstract = {{In a complete financial market model, the shortfall-risk minimization problem at the terminal date is treated for the seller of a derivative security F. The worst conditional expectation of the shortfall is adopted as the measure of this risk, ensuring that the minimized risk satisfies certain desirable properties as the dynamic measure of risk, as proposed by Cvitanić and Karatzas (1999). The terminal value of the optimized portfolio is a binary functional dependent on F and the Radon-Nikodym density of the equivalent local martingale measure. In particular, it is observed that there exists a positive number x* that is less than the replicating cost X F of F, and that the strategy minimizing the expectation of the shortfall is optimal if the hedger's capital is in the range [x*, x f].}}, 
pages = {605--618}, 
number = {4}, 
volume = {14}
}
@article{10.1287/ijoc.2013.0572, 
year = {2014}, 
title = {{Conditional value-at-risk approximation to value-at-risk constrained programs: A remedy via Monte Carlo}}, 
author = {Hong, L Jeff and Hu, Zhaolin and Zhang, Liwei}, 
journal = {INFORMS Journal on Computing}, 
issn = {10919856}, 
doi = {10.1287/ijoc.2013.0572}, 
abstract = {{We study optimization problems with value-at-risk (VaR) constraints. Because it lacks subadditivity, VaR is not a coherent risk measure and does not necessarily preserve the convexity. Thus, the problems we consider are typically not provably convex. As such, the conditional value-at-risk (CVaR) approximation is often used to handle such problems. Even though the CVaR approximation is known as the best convex conservative approximation, it sometimes leads to solutions with poor performance. In this paper, we investigate the CVaR approximation from a different perspective and demonstrate what is lost in this approximation. We then show that the lost part of this approximation can be remedied using a sequential convex approximation approach, in which each iteration only requires solving a CVaR-like approximation via certain Monte Carlo techniques. We show that the solution found by this approach generally makes the VaR constraints binding and is guaranteed to be better than the solution found by the CVaR approximation and moreover is empirically often globally optimal for the target problem. The numerical experiments show the effectiveness of our approach. © 2014 INFORMS.}}, 
pages = {385--400}, 
number = {2}, 
volume = {26}
}
@article{10.1016/j.econmod.2012.05.036, 
year = {2012}, 
title = {{Markets liquidity risk under extremal dependence: Analysis with VaRs methods}}, 
author = {Ourir, Awatef and Snoussi, Wafa}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2012.05.036}, 
abstract = {{Value-at-Risk (VaR) is a widely used tool for assessing financial market risk. In practice, the estimation of liquidity extreme risk by VaR generally uses models assuming independence of bid-ask spreads. However, bid-ask spreads tend to occur in clusters with time dependency, particularly during crisis period. Our paper attempts to fill this gap by studying the impact of negligence of dependency in liquidity extreme risk assessment of Tunisian stock market. The main methods which take into account returns dependency to assess market risk is Time series-Extreme Value Theory combination. Therefore we compare VaRs estimated under independency (Variance-Covariance Approach, Historical Simulation and the VaR adjusted to extreme values) relatively to the VaR when dependence is considered. The efficiency of those methods was tested and compared using the backtesting tests. The results confirm the adequacy of the recent extensions of liquidity risk in the VaR estimation. Therefore, we prove a performance improvement of VaR estimates under the assumption of dependency across a significant reduction of the estimation error, particularly with AR (1)-GARCH (1,1)-GPD model. © 2012 Elsevier B.V..}}, 
pages = {1830--1836}, 
number = {5}, 
volume = {29}
}
@article{10.1051/ps/2018015, 
year = {2018}, 
title = {{A consistent estimator to the orthant-based tail value-at-risk}}, 
author = {Beck, Nicholas and Mailhot, Mélina}, 
journal = {ESAIM: Probability and Statistics}, 
issn = {12928100}, 
doi = {10.1051/ps/2018015}, 
abstract = {{In this paper, we address the estimation of multivariate value-at-risk (VaR) and tail valueat-risk (TVaR). We recall definitions for the bivariate lower and upper orthant VaR and bivariate lower and upper orthant TVaR, presented in Cossette et al. [Eur. Actuar. J. 3 (2013) 321\{357 or Methodol. Comput. Appl. Probab. (2014) 1\{22]. Here, we present estimators for both these measures extended to an arbitrary dimension d = 2 and establish the consistency of our estimator for the lower and upper orthant TVaR in any dimension. We demonstrate these results by providing numerical examples that compare our estimator to theoretical results for both simulated and real data. © 2018 EDP Sciences, SMAI.}}, 
pages = {163--177}, 
number = {NA}, 
volume = {22}
}
@article{10.21314/jrmv.2019.213, 
year = {2019}, 
title = {{Value-at-risk in the European energy market: a comparison of parametric, historical simulation and quantile regression value-at-risk}}, 
author = {Westgaard, Sjur}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2019.213}, 
abstract = {{This paper examines a set of value-at-risk (VaR) models and their ability to appropriately describe and capture price-change risk in the European energy market. We make in-sample, one-day-ahead VaR forecasts using one simple parametric model, one historical simulation model and one quantile regression (QR) model. We apply our models to nine different energy futures: Brent crude oil, API2 coal, UK natural gas, and three German and Nordic power futures in the period 2007–17. The models are tested at both long and short positions. Our research suggests that the QR model is easy to implement and offers accurate VaR forecasts in the European energy market. © 2019, Infopro Digital Risk (IP) Limited. All rights reserved.}}, 
pages = {43--69}, 
number = {4}, 
volume = {13}
}
@article{10.1080/1540496x.2016.1142218, 
year = {2016}, 
title = {{Value-at-Risk Forecasting of Chinese Stock Index and Index Future under Jumps, Permanent Component, and Asymmetric Information}}, 
author = {Li, Shaoyu and Wei, Lijia and Huang, Zehua}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2016.1142218}, 
abstract = {{This article investigates the performance of time series models considering the jumps, permanent component of volatility, and asymmetric information in predicting value-at-risk (VaR). We use evaluation statistics including size and variability, accuracy, and efficiency to determine some suitable VaR measures for the Chinese stock index and its futures. The results reveal that models with jumps can provide VaR series that are less average conservative and have higher variability. Furthermore, additional considering the permanent component of volatility and asymmetric effect can induce more accurate and efficient risk measure in the long and short positions of the stock index and its futures. © 2016 Taylor \& Francis Group, LLC.}}, 
pages = {1072--1091}, 
number = {5}, 
volume = {52}
}
@article{10.1007/s10479-019-03366-0, 
year = {2019}, 
title = {{Partially ordered data sets and a new efficient method for calculating multivariate conditional value-at-risk}}, 
author = {Lee, Jinwook and Kim, Jongpil}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-019-03366-0}, 
abstract = {{Recent studies in Lee and Prékopa (Oper Res Lett 45:19–24, 2017) and Lee (Oper Res Lett 45:1204–1220, 2017) showed that a union of partially ordered orthants in Rn can be decomposed only into the largest and the second largest chains. This allows us to calculate the probability of the union of such events in a recursive manner. If the vertices of such orthants designate p-level efficient points, i.e., the multivariate quantile or the multivariate value-at-risk (MVaR) in Rn, then the number of them, say N, is typically very large, which makes it almost impossible to calculate the multivariate conditional value-at-risk (MCVaR) introduced by Prékopa (Ann Oper Res 193(1):49–69, 2012). This is because it takes O(2 N) in case of N MVaRs in Rn to find the exact value of MCVaR. In this paper, upon the basis of ideas in Lee and Prékopa (Oper Res Lett 45:19–24, 2017) and Lee (Oper Res Lett 45:1204–1220, 2017), together with proper adjustments, we study efficient methods for the calculation of the MCVaR without resorting to an approximation. In fact, the proposed methods not only have polynomial time complexity but also computes the exact value of MCVaR. We also discuss additional benefits MCVaR has to offer over its univariate counter part, the conditional value-at-risk, by providing numerical results. Numerical examples are presented with computing time in both cases of given population and sample data sets. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1--27}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0378-4266(02)00265-0, 
year = {2002}, 
title = {{VaR and expected shortfall in portfolios of dependent credit risks: Conceptual and practical insights}}, 
author = {Frey, Rüdiger and McNeil, Alexander J.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00265-0}, 
abstract = {{In the first part of this paper we address the non-coherence of value-at-risk (VaR) as a risk measure in the context of portfolio credit risk, and highlight some problems which follow from this theoretical deficiency. In particular, a realistic demonstration of the non-subadditivity of VaR is given and the possibly nonsensical consequences of VaR-based portfolio optimisation are shown. The second part of the paper discusses VaR and expected shortfall estimation for large balanced credit portfolios. All standard industry models (Creditmetrics, KMV, Credit-Risk+) are presented as Bernoulli mixture models to facilitate their direct comparison. For homogeneous groups it is shown that measures of tail risk for the loss distribution may be approximated in large portfolios by analysing the tail of the mixture distribution in the Bernoulli representation. An example is given showing that, for portfolios of lower quality, choice of model has some impact on measures of extreme risk. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {1317--1334}, 
number = {7}, 
volume = {26}
}
@article{10.1002/for.1036, 
year = {2007}, 
title = {{Evaluation of correlation forecasting models for risk management}}, 
author = {Skintzi, Vasiliki D. and Xanthopoulos‐Sisinis, Spyros}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1036}, 
abstract = {{Reliable correlation forecasts are of paramount importance in modern risk management systems. A plethora of correlation forecasting models have been proposed in the open literature, yet their impact on the accuracy of value-atrisk calculations has not been explicitly investigated. In this paper, traditional and modern correlation forecasting techniques are compared using standard statistical and risk management loss functions. Three portfolios consisting of stocks, bonds and currencies are considered. We find that GARCH models can better account for the correlation's dynamic structure in the stock and bond portfolios. On the other hand, simpler specifications such as the historical mean model or simple moving average models are better suited for the currency portfolio. Copyright © 2007 John Wiley \& Sons, Ltd.}}, 
pages = {497--526}, 
number = {7}, 
volume = {26}
}
@article{10.1016/j.insmatheco.2014.06.008, 
year = {2014}, 
title = {{Quantifying the risk using copulae with nonparametric marginals}}, 
author = {Bolancé, Catalina and Bahraoui, Zuhair and Artís, Manuel}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.06.008}, 
abstract = {{We show that copulae and kernel estimation can be mixed to estimate the risk of an economic loss. We analyze the properties of the Sarmanov copula. We find that the maximum pseudo-likelihood estimation of the dependence parameter associated with the copula with double transformed kernel estimation to estimate marginal cumulative distribution functions is a useful method for approximating the risk of extreme dependent losses when we have large data sets. We use a bivariate sample of losses from a real database of auto insurance claims. © 2014 Elsevier B.V.}}, 
pages = {46--56}, 
number = {1}, 
volume = {58}
}
@article{10.1016/j.frl.2021.102024, 
year = {2021}, 
title = {{Backtesting VaR under the COVID-19 sudden changes in volatility}}, 
author = {Castillo, Brenda and León, Ángel and Ñíguez, Trino-Manuel}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2021.102024}, 
abstract = {{We analyze the impact of the COVID-19 pandemic on the conditional variance of stock returns. We look at this effect from a global perspective, so we employ series of major stock market and sector indices. We use the Hansen's Skewed-t distribution with EGARCH extended to control for sudden changes in volatility. We oversee the COVID-19 effect on measures of downside risk such as the Value-at-Risk. Our results show that there is a significant sudden shift up in the return distribution variance post the announcement of the pandemic, which must be explained properly to obtain reliable measures for financial risk management. © 2021}}, 
pages = {102024}, 
number = {NA}, 
volume = {43}
}
@article{10.1016/j.jfi.2019.100844, 
year = {2020}, 
title = {{Bank capital allocation under multiple constraints}}, 
author = {Goel, Tirupam and Lewrick, Ulf and Tarashev, Nikola}, 
journal = {Journal of Financial Intermediation}, 
issn = {10429573}, 
doi = {10.1016/j.jfi.2019.100844}, 
abstract = {{We study how a bank allocates capital across its business units when facing multiple constraints over several periods. If a constraint tightens – be it because of stricter regulation or higher risk – capital flows to the more efficient unit, i.e. the unit offering a higher marginal return on required capital. Relative efficiency helps explain how a policy measure targeting a specific business unit – e.g. imposing requirements for market risk, or ring-fencing lending – spills over to another, seemingly unrelated unit. It also helps explain the bank's response to the tightening of a constraint that is contemporaneously slack but likely to bind later on. © 2019 Elsevier Inc.}}, 
pages = {100844}, 
number = {NA}, 
volume = {44}
}
@article{10.1016/j.jeconom.2018.09.017, 
year = {2019}, 
title = {{Mark to market value at risk}}, 
author = {Chen, Yu and Wang, Zhicheng and Zhang, Zhengjun}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2018.09.017}, 
abstract = {{Financial risk management has been overwhelmed by applications and research of value at risk (VaR) in daily practice mainly due to its simple form and easily interpretable feature. Yet, its serious drawback of underestimating an asset's market risk has been noticed in numerous applications, and many alternative risk measures have been proposed in the literature. Among all existing alternative risk measures, it is hard to find one that a financial institution whose portfolio has multiple settlements before the end of holding period uses to internally perform risk assessment. We propose a new risk measure termed mark to market value at risk (MMVaR) for settlement being taken daily during the holding period. MMVaR is a natural alternative risk measure to VaR as it is a direct generalization of VaR. It not only maintains easily interpretable feature held by VaR, but also better computes an asset's market risk in a financial institution having daily account settlements. We show that MMVaR is superior to VaR using simulation examples and real data. In real data analysis, we find that risks calculated using MMVaR are about 20\% higher than risks calculated using classical VaR, which provides an evidence proof of Basel III's new capital adequacy ratio requirement, and hence it can become an implementable daily risk measure. © 2018 Elsevier B.V.}}, 
pages = {299--321}, 
number = {1}, 
volume = {208}
}
@article{10.1109/icmse.2011.6070096, 
year = {2011}, 
title = {{Real estate bubble in China: An empirical study based on VaR model}}, 
author = {Lei, Feng and Lu-Kui, Jia}, 
journal = {2011 International Conference on Management Science \& Engineering 18th Annual Conference Proceedings}, 
issn = {21551847}, 
doi = {10.1109/icmse.2011.6070096}, 
abstract = {{Real estate market in China has developed rapidly these years and enjoys brilliant achievements, but at the same time certain problems such as high mortgage level and soaring housing price come out, arousing public concern of bubble in Chinese real estate market. In this paper, we intensively discussed statistical models including VaR models and heavy tail models to explain the phenomenon of high frequency of market bubble occurrence. We also use historical simulation VaR model, which is a widely used statistical model and has been proved successful in financial market bubble analysis, to calculate the Value at Risk for Chinese Housing Price Index January 2011. Based on analysis, certain suggestions such as the construction of real estate industry database, the creation of sound bubble risk monitoring and administration system are given, in order to prevent dangerous property bubble from happening again. © 2011 IEEE.}}, 
pages = {1117--1122}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icebeg.2011.5882790, 
year = {2011}, 
title = {{RETRACTED ARTICLE: Budget at risk and its application in program investing \& decision-making}}, 
author = {Baofa, Liu and Zhaoju, Zou}, 
journal = {2011 International Conference on E-Business and E-Government (ICEE)}, 
issn = {NA}, 
doi = {10.1109/icebeg.2011.5882790}, 
abstract = {{Beause there are some limitations in the traditional budget and decision-making of program investment, the article adopts the idea of Value at Risk, and analyes the Budget at Risk (BaR). Through introduce the BaR into the program investing \& decision-making, we find that the BaR method can decrease the 20-30\% of all budget and effectively reduces its risk © 2011 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.cie.2020.106674, 
year = {2020}, 
title = {{Pricing and two-dimensional warranty policy of multi-products with online and offline channels using a value-at-risk approach}}, 
author = {Taleizadeh, Ata Allah and Mokhtarzadeh, Mahdi}, 
journal = {Computers \& Industrial Engineering}, 
issn = {03608352}, 
doi = {10.1016/j.cie.2020.106674}, 
abstract = {{Nowadays, manufacturers often sell different products (models of a product) through both online and offline channels. To face challenges such as legislation or competition, manufacturers often need to provide warranties for their products. On the one hand, customers are more attracted to goods covered with a warranty; on the other hand, warranties generate additional costs for manufacturers. This study investigates a case that a manufacturer sells through online and offline channels and offers a two-dimensional warranty policy including a warranty-age and a warranty-usage package for sold products through online channel. Because different models share several components, failures of all manufacturer's portfolio products are statistically dependent. A model to optimize both the pricing and warranty policies is proposed. Given that the claim rates for warranties are stochastic, the value-at-risk approach is implemented to solve the optimization problem. Furthermore, the covariance between warranty claims associated with different products is computed through a copula. The findings indicate the importance of considering the covariance among different models claims when optimal warranty policy is offered to customers along with the proper pricing strategy in an online channel. © 2020 Elsevier Ltd}}, 
pages = {106674}, 
number = {NA}, 
volume = {148}
}
@article{10.1016/j.bir.2016.09.002, 
year = {2017}, 
title = {{Multivariate market risk evaluation between Malaysian Islamic stock index and sectoral indices}}, 
author = {Ng, Sew Lai and Chin, Wen Cheong and Chong, Lee Lee}, 
journal = {Borsa Istanbul Review}, 
issn = {22148450}, 
doi = {10.1016/j.bir.2016.09.002}, 
abstract = {{Without an efficient financial risk management, it may cause massive consequences to a financial institution as well as individual. Therefore, developing a methodology which gives precise estimates to reduce the exposure of risk to a minimum is of great importance. This paper uses an asymmetric BEKK-GARCH model to examine the return and volatility linkages between the FTSE Bursa Malaysia Emas Shariah (FBMS) index and the sectoral indices under a normal market. The findings suggest that the FBMS plays a leading role in the mean return spillover effect. There is a strong evidence of significant transmission of past shocks, volatilities and leverage effects are observed on the current conditional variance-covariance in all the pair-wise models. These empirical results are helpful in quantifying the cross-market risk evaluation, risk minimizing weight and cross-market hedge ratio for strategizing appropriate portfolio selection. © 2016 Borsa İstanbul Anonim Şirketi}}, 
pages = {49--61}, 
number = {1}, 
volume = {17}
}
@article{10.1109/icmss.2009.5305121, 
year = {2009}, 
title = {{An empirical analysis on the portfolio of QDII fund and greenhouse gas emission permits}}, 
author = {LU, Wei and WANG, Wen-jun and MIAO, Bai-qi}, 
journal = {2009 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2009.5305121}, 
abstract = {{The European Union Allowance (EUA) futures traded on European Climate Exchange (ECX) is selected to represent the Greenhouse Gas Emission Permits (carbon assets) in this paper. The Copula function is applied to get the joint distribution function of the return rates of EUA futures and a Qualified Domestic Institutional Investor (QDII) fund in China. Based on the joint distribution function of the two assets, the distribution function of any portfolio of the two assets is presented. Then, at different significance levels optimal portfolios of the two assets with the minimal Value at Risk (VaR) are shown. Through analysis of the optimal portfolios, it is found that all of the optimal portfolios showed a higher rate of return than the original QDII fund, and had lower VaRs at different significance levels. Meanwhile, the optimal investment proportional coefficient is found to have a low sensitivity to the variation of VaR required by potential investors, thus the feasibility of the proposed portfolios is further proved. ©2009 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1061/(asce)he.1943-5584.0001494, 
year = {2017}, 
title = {{Copula-based Markov process for forecasting and analyzing risk of water quality time series}}, 
author = {Arya, Farid Khalil and Zhang, Lan}, 
journal = {Journal of Hydrologic Engineering}, 
issn = {10840699}, 
doi = {10.1061/(asce)he.1943-5584.0001494}, 
abstract = {{This study applies the copula-based Markov process to model water quality time series. The bivariate copula is applied to investigate the first-order Markov processes. The D-Vine copula is applied to investigate the more complicated higher-order (k≥2) Markov processes. The Value-at-Risk (VaR), computed using the best-fitted copula-based Markov process, is applied for the risk analysis. Using water quality time series at the Snohomish River watershed (Washington) and the Chattahoochee River watershed (Georgia), the results show that the copula-based Markov processes (1) are able to properly model the temporal dependence for dissolved oxygen (DO) series [i.e., forecast root-mean-square error (RMSE) <1 mg/L at both watersheds] and temperature (T) series (i.e., forecast RMSE = 1.5°C at the Chattahoochee watershed); and (2) can only predict the overall trend for nitrate and conductivity series, due to the fact that these two series also depend heavily on other factors (e.g., runoff). Overall, the study indicates that the copula-based Markov process may be an efficient tool in the assessment of water quality and the associated risks with the following advantages: (1) constructing the transitional probability explicitly and properly; (2) studying the temporal dependence independently from the marginal distributions; (3) avoiding the strict assumptions of the classic time series modeling approach (e.g., the time series belonging to the Gaussian process); and (4) providing a reasonable risk measure through the VaR. © 2016 American Society of Civil Engineers.}}, 
pages = {04017005}, 
number = {6}, 
volume = {22}
}
@article{10.1016/j.ijpe.2021.108154, 
year = {2021}, 
title = {{“Maximizing the probability of realizing profit targets versus maximizing expected profits: A reconciliation to resolve an agency problem”}}, 
author = {Kamrad, Bardia and Ord, Keith and Schmidt, Glen M.}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2021.108154}, 
abstract = {{It is generally accepted in the operations literature that a firm should strive to maximize its expected profit. However, in practice it is not uncommon for a firm to offer a bonus to managers for achieving some pre-established target profit, possibly yielding managerial actions that differ from the profit-maximizing approach (given a profit target, we assume managers will maximize the probability of reaching that target). We use the Newsvendor framework to illustrate how the firm's shareholders (e.g., through its board of directors) can align these two seemingly different decision approaches: maximizing expected profit versus maximizing the probability of reaching a target profit. Alignment is achieved by setting what we call an “aligned profit target” (APT) – a target profit that yields the same managerial action namely, the same stocking quantity, across both decision approaches. We find that the APT should typically be an aggressive profit target, one that is significantly higher than the maximum expected profit, with a corresponding low probability of achievement – this result is consistent across demand distributions with light tails (uniform), moderate tails (normal) and heavy tails (lognormal). Notably, the aggressive APT target should be distinguished from any target that the firm might set to signal future profit expectations to financial analysts. © 2021 Elsevier B.V.}}, 
pages = {108154}, 
number = {NA}, 
volume = {238}
}
@article{10.1007/s10479-008-0509-9, 
year = {2009}, 
title = {{Dynamic asset allocation under VaR constraint with stochastic interest rates}}, 
author = {Hainaut, Donatien}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-008-0509-9}, 
abstract = {{This paper addresses the problem of dynamic asset allocation under a bounded shortfall risk in a market composed of three assets: cash, stocks and a zero coupon bond. The dynamics of the instantaneous short rates is driven by a Hull and White model. In this setting, we determine and compare optimal investment strategies maximizing the CRRA utility of terminal wealth with and without value at risk constraint. © Springer Science+Business Media, LLC 2008.}}, 
pages = {97}, 
number = {1}, 
volume = {172}
}
@article{10.1080/02331930701617080, 
year = {2007}, 
title = {{A note on conditional value at risk (CVaR)}}, 
author = {Howlett, P. and Piantadosi, J.}, 
journal = {Optimization}, 
issn = {02331934}, 
doi = {10.1080/02331930701617080}, 
abstract = {{We give a direct proof of the fundamental minimization theorem for CVaR presented by Rockafellar and Uryasev in their definitive paper (Conditional value-at-risk for general loss distributions. Journal of Banking and Finance, 26, 1443-1471, 2002).}}, 
pages = {629--632}, 
number = {5-6}, 
volume = {56}
}
@article{10.2478/amns.2020.2.00025, 
year = {2020}, 
title = {{Measurement of Risk Based on QR-GARCH-EVT Model}}, 
author = {Duan, Jun and Zhang, Baoshuai}, 
journal = {Applied Mathematics and Nonlinear Sciences}, 
issn = {24448656}, 
doi = {10.2478/amns.2020.2.00025}, 
abstract = {{This paper described the volatility characteristic of the rate of return of financial asset by using QR-GARCH model, through introducing EVT model and constructing the extreme risk measure model based on QR-GARCH-EVT. In this paper, HS300 index data test was applied to show that under 5\% significance level, and QR-GARCH-EVT model can effectively measure the risk value of the sample, but under 1\% significance level. QR-GARCH-EVT model will underestimate the risk value of the sample to a certain degree, but generally speaking, compared with other models, the risk value measured by QR-GARCH-EVT model has a higher accuracy to enhance effectiveness. © 2020 Jun Duan et al., published by Sciendo 2020.}}, 
pages = {473--482}, 
number = {2}, 
volume = {5}
}
@article{10.1016/j.jeconom.2012.08.012, 
year = {2013}, 
title = {{Stable mixture GARCH models}}, 
author = {Broda, Simon A. and Haas, Markus and Krause, Jochen and Paolella, Marc S. and Steude, Sven C.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2012.08.012}, 
abstract = {{A new model class for univariate asset returns is proposed which involves the use of mixtures of stable Paretian distributions, and readily lends itself to use in a multivariate context for portfolio selection. The model nests numerous ones currently in use, and is shown to outperform all its special cases. In particular, an extensive out-of-sample risk forecasting exercise for seven major FX and equity indices confirms the superiority of the general model compared to its special cases and other competitors. Estimation issues related to problems associated with mixture models are discussed, and a new, general, method is proposed to successfully circumvent these. The model is straightforwardly extended to the multivariate setting by using an independent component analysis framework. The tractability of the relevant characteristic function then facilitates portfolio optimization using expected shortfall as the downside risk measure. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {292--306}, 
number = {2}, 
volume = {172}
}
@article{10.1109/ciced.2018.8592096, 
year = {2018}, 
title = {{Energy storage sizing method considering V2G response capability}}, 
author = {Chen, Jia-Min and Xu, Yong-Hai}, 
journal = {2018 China International Conference on Electricity Distribution (CICED)}, 
issn = {21617481}, 
doi = {10.1109/ciced.2018.8592096}, 
abstract = {{Utilizing V2G technology, plug-in electric vehicles(PEVs) can behave as mobile energy storage units to participate in power grid operation. Based on the traffic behavior characteristics, the V2G response capability boundaries of PEVs considering time, energy and battery restraint is constructed. Then, the PEVs connected to the grid are divided into two types: responsive/non-responsive. Finally, the Monte Carlo method is used to establish the T-limit V2G response capability forecast models for individual PEV and PEV groups respectively. On the other side, the value-at-risk(VaR) method is introduced to energy storage capacity sizing, based on which, a reliability-at-risk(RaR) model is built to calculate the conventional storage capacity under a certain confidence level of power system reliability . Finally, the feasibility and effectiveness of the methods proposed above are proved in the examples. © 2018 IEEE}}, 
pages = {912--916}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/1351847032000143396, 
year = {2005}, 
title = {{Market risk models for intraday data}}, 
author = {Giot, Pierre}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847032000143396}, 
abstract = {{In this paper, market risk at an intraday time horizon is quantified using normal GARCH, Student GARCH, RiskMetrics and high-frequency duration (log-ACD) models set in the framework of the conditional VaR methodology. Because of the small time horizon of the intraday returns (15 and 30 minute returns in this paper), an evaluation of intraday market risk can be useful to market participants (traders, market makers) involved in frequent trading. As expected, the volatility features an important intraday seasonality, which must be removed prior to using the market risk models. The four models are applied to intraday returns data for three stocks traded on the New York Stock Exchange and it is shown that the Student GARCH model performs best. The use of price durations as a measure of risk on time is commented upon. © 2005 Taylor \& Francis.}}, 
pages = {309--324}, 
number = {4}, 
volume = {11}
}
@article{10.1007/978-3-030-49579-4_21, 
year = {2021}, 
title = {{A rational assessment procedure of long-term sustainable values for bank lending purposes}}, 
author = {Tajani, Francesco and Morano, Pierluigi and Giudice, Vincenzo Del and Paola, Pierfrancesco De}, 
journal = {Green Energy and Technology}, 
issn = {18653529}, 
doi = {10.1007/978-3-030-49579-4\_21}, 
abstract = {{Even if the mortgage lending value is not included among the bases of valuation recognized by the International Valuation Standards, the consequences of the global financial crisis have pointed out the peremptoriness for the banking institutions to adopt, in the transactions that involve properties as credit exposure guarantees, sustainable value judgments in the long term. In the present work, with reference to the Italian context, a rational method for estimating the mortgage lending value is proposed and tested. By borrowing the operating logic of the Value at Risk and introducing appropriate assumptions, the model is based on a time series analysis of the property values recorded in the Italian regional capitals, and assesses the abatement coefficients of the market value according to the location of the urban area (central, semi-central or peripheral) of the property. The coefficients obtained satisfy the need for a rational assessment of the property risk and for an appropriate spatial contextualization of the risk components as regards the local demand and supply. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.}}, 
pages = {315--325}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2008.11.009, 
year = {2009}, 
title = {{A claims persistence process and insurance}}, 
author = {Vallois, Pierre and Tapiero, Charles S.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2008.11.009}, 
abstract = {{The purpose of this paper is to introduce and construct a state dependent counting and persistent random walk. Persistence is imbedded in a Markov chain for predicting insured claims based on their current and past period claim. We calculate for such a process, the probability generating function of the number of claims over time and as a result are able to calculate their moments. Further, given the claims severity probability distribution, we provide both the claims process generating function as well as the mean and the claim variance that an insurance firm confronts over a given period of time and in such circumstances. A number of results and applictions are then outlined (such as a Compound Claim Persistence Process). © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {367--373}, 
number = {3}, 
volume = {44}
}
@article{10.1007/978-3-319-11179-7_98, 
year = {2014}, 
title = {{Financial self-organizing Maps}}, 
author = {Resta, Marina}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-11179-7\_98}, 
abstract = {{This paper introduces Financial Self-Organizing Maps (FinSOM) as a SOM sub-class where the mapping of inputs on the neural space takes place using functions with economic soundness, that makes them particularly well-suited to analyze financial data. The visualization capabilities as well as the explicative power of both the standard SOM and the FinSOM variants is tested on data from the German Stock Exchange. The results suggest that, dealing with financial data, the FinSOM seem to offer superior representation capabilities of the observed phenomena. © 2014 Springer International Publishing Switzerland.}}, 
pages = {781--788}, 
number = {NA}, 
volume = {8681 LNCS}
}
@article{10.1109/ai4i46381.2019.00034, 
year = {2019}, 
title = {{On estimation of value-at-risk with recurrent neural network}}, 
author = {Du, Zhichao and Wang, Menghan and Xu, Zhewen}, 
journal = {2019 Second International Conference on Artificial Intelligence for Industries (AI4I)}, 
issn = {NA}, 
doi = {10.1109/ai4i46381.2019.00034}, 
abstract = {{In this paper, we propose a new method for predicting Value-at-Risk using recurrent neural network. We show that our new approach provide us a more flexible semi-parametric framework for forecasting VaR, and we obtain improved results comparing with other forecasting methods. © 2019 IEEE.}}, 
pages = {103--106}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2008.05.011, 
year = {2008}, 
title = {{Optimal reinsurance under VaR and CTE risk measures}}, 
author = {Cai, Jun and Tan, Ken Seng and Weng, Chengguo and Zhang, Yi}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2008.05.011}, 
abstract = {{Let X denote the loss initially assumed by an insurer. In a reinsurance design, the insurer cedes part of its loss, say f (X), to a reinsurer, and thus the insurer retains a loss If (X) = X - f (X). In return, the insurer is obligated to compensate the reinsurer for undertaking the risk by paying the reinsurance premium. Hence, the sum of the retained loss and the reinsurance premium can be interpreted as the total cost of managing the risk in the presence of reinsurance. Based on a technique used in [Müller, A., Stoyan, D., 2002. Comparison Methods for Stochastic Models and Risks. In: Willey Series in Probability and Statistics] and motivated by [Cai J., Tan K.S., 2007. Optimal retention for a stop-loss reinsurance under the VaR and CTE risk measure. Astin Bull. 37 (1), 93-112] on using the value-at-risk (VaR) and the conditional tail expectation (CTE) of an insurer's total cost as the criteria for determining the optimal reinsurance, this paper derives the optimal ceded loss functions in a class of increasing convex ceded loss functions. The results indicate that depending on the risk measure's level of confidence and the safety loading for the reinsurance premium, the optimal reinsurance can be in the forms of stop-loss, quota-share, or change-loss. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {185--196}, 
number = {1}, 
volume = {43}
}
@article{10.1016/j.intfin.2015.05.012, 
year = {2015}, 
title = {{Is risk higher during non-trading periods? The risk trade-off for intraday versus overnight market returns}}, 
author = {Riedel, Christoph and Wagner, Niklas}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2015.05.012}, 
abstract = {{We study the magnitude of tail risk - particularly lower tail downside risk - that is present in intraday versus overnight market returns and thereby examine the nature of the respective market risk borne by market participants. Using the Generalized Pareto Distribution for the return innovations, we use a GARCH model for the conditional market return components of major stock markets covering the U.S., France, Germany and Japan. Testing for fat-tails and tail index equality, we find that overnight return innovations exhibit significant tail risk, while intraday innovations do not. We illustrate this volatility versus tail risk trade-off based on conditional Value-at-Risk calculations. Our results show that overnight downside market risk is composed of a moderate volatility risk component and a significant tail risk component. We conclude that market participants face different intraday versus overnight risk profiles and that a risk assessment based on volatility only will severely underestimate overnight downside risk. © 2015.}}, 
pages = {53--64}, 
number = {NA}, 
volume = {39}
}
@article{10.21314/jrmv.2019.211, 
year = {2019}, 
title = {{A study on window-size selection for threshold and bootstrap value-at-risk models}}, 
author = {Smith, Anri and Huang, Chun-Kai}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2019.211}, 
abstract = {{This paper investigates the effects of window-size selection on various models for value-at-risk (VaR) forecasting using high-performance computing. Subsequently, automated procedures using change-point analysis for optimal window-size selection are proposed. In particular, stationary bootstrapping and the peaks-over-threshold method are utilized for a rolling daily VaR estimation and are contrasted with the classical conditional Gaussian model. It is evidenced that change-point procedures can, on average, result in more adequate risk predictions than a predetermined, fixed window size. The data sets analyzed include indexes across five continents, ie, the Dow Jones Industrial Average Index (DJI), the Financial Times Stock Exchange 100 Index (UKX), the Nikkei Top 225 Index (NKY), the Johannesburg Stock Exchange Top 40 Index (JSE Top 40), the Ibovespa Brazil Sao Paulo Stock Exchange All Index (IBOV) and the Bombay Stock Exchange Top 500 Index (BSE 500). © 2019 Infopro Digital Risk (IP) Limited}}, 
number = {4}, 
volume = {13}
}
@article{10.1007/978-981-10-4502-8_12, 
year = {2018}, 
title = {{Is Socially Responsible Investing More Risky? Australian Evidence}}, 
author = {Mackie, Ewan and Palit, Imon and Veeraraghavan, Madhu and Watson, John}, 
journal = {Accounting, Finance, Sustainability, Governance \& Fraud: Theory and Application}, 
issn = {25097873}, 
doi = {10.1007/978-981-10-4502-8\_12}, 
abstract = {{Prior studies, which analyse the performance of socially responsible investments (SRIs) compared to conventional funds, have thus far ignored the assessment of risk. In response to this identified lack of research, we make a major attempt to fill the void by investigating whether daily returns of Australian equity socially responsible investment funds have different tail risk exposure in the return distribution compared to matched conventional equity funds. The Australian funds management industry provides a natural setting within which to study the risk exposure of SRI funds. The Australian funds management industry has one of the largest and fastest growing funds management sectors in the world. This growth is underpinned by Australia’s government-mandated retirement scheme. In addition, Australia is the first country to introduce regulations that require issuers of financial products and financial advisors to disclose and advise on ethical, social, and governance (ESG) considerations. Using a sample of 26 funds spanning the period 1998–2013, we establish several new findings. First, in assessing tail risk exposure we observe no evidence of significant difference in riskiness amongst socially responsible investment compared to that of conventional funds with similar investment styles. Second, when comparing two downside risk measures across socially responsible and matched conventional funds, namely Value-at-Risk and expected shortfall, we find that return distributions amongst Australian funds do not exhibit particularly heavy tails. Taken together, we show that investors do not pay a penalty (in terms of higher risk) to invest ethically. © 2018, Springer Nature Singapore Pte Ltd.}}, 
pages = {261--305}, 
number = {NA}, 
volume = {NA}
}
@article{10.1515/snde-2012-0004, 
year = {2013}, 
title = {{Computational aspects of portfolio risk estimation in volatile markets: A survey}}, 
author = {Fabozzi, Frank J. and Stoyanov, Stoyan V. and Rachev, Svetlozar T.}, 
journal = {Studies in Nonlinear Dynamics and Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2012-0004}, 
abstract = {{Portfolio risk estimation requires appropriate modeling of fat-tails and asymmetries in dependence in combination with a true downside risk measure. In this survey, we discuss computational aspects of a Monte Carlo based framework for risk estimation and risk capital allocation. We review different probabilistic approaches focusing on practical aspects of statistical estimation and scenario generation. We discuss value-at-risk and conditional value-at-risk and comment on the implications of using a fat-tailed Monte Carlo framework for the reliability of risk estimates including model risk and Monte Carlo variability.}}, 
pages = {103--120}, 
number = {1}, 
volume = {17}
}
@article{10.1002/for.2503, 
year = {2018}, 
title = {{Value-at-risk under market shifts through highly flexible models}}, 
author = {BenSaïda, Ahmed and Boubaker, Sabri and Nguyen, Duc Khuong and Slim, Skander}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2503}, 
abstract = {{Managing market risk under unknown future shocks is a critical issue for policymakers, investors, and professional risk managers. Despite important developments in market risk modeling and forecasting over recent years, market participants are still skeptical about the ability of existing econometric designs to accurately predict potential losses, particularly in the presence of hidden structural changes. In this paper, we introduce Markov-switching APARCH models under the skewed generalized t and the generalized hyperbolic distributions to fully capture the fuzzy dynamics and stylized features of financial market returns and to generate value-at-risk (VaR) forecasts. Our empirical analysis of six major stock market indexes shows the superiority of the proposed models in detecting and forecasting unobservable shocks on market volatility, and in calculating daily capital charges based on VaR forecasts. Copyright © 2018 John Wiley \& Sons, Ltd.}}, 
pages = {790--804}, 
number = {8}, 
volume = {37}
}
@article{10.1109/ccgrid.2009.82, 
year = {2009}, 
title = {{Online risk analytics on the cloud}}, 
author = {Kim, Hyunjoo and Chaudhari, Shivangi and Parashar, Manish and Marty, Christopher}, 
journal = {2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid}, 
issn = {NA}, 
doi = {10.1109/ccgrid.2009.82}, 
abstract = {{In todays turbulent market conditions, the ability to generate accurate and timely risk measures has become critical to operating successfully, and necessary for survival. Value-at- Risk (VaR) is a market standard risk measure used by senior management and regulators to quantify the risk level of a firm's holdings. However, the time-critical nature and dynamic computational workloads of VaR applications, make it essential for computing infrastructures to handle bursts in computing and storage resources needs. This requires on-demand scalability, dynamic provisioning, and the integration of distributed resources. While emerging utility computing services and clouds have the potential for cost-effectively supporting such spikes in resource requirements, integrating clouds with computing platforms and data centers, as well as developing and managing applications to utilize the platform remains a challenge. In this paper, we focus on the dynamic resource requirements of online risk analytics applications and how they can be addressed by cloud environments. Specifically, we demonstrate how the CometCloud autonomic computing engine can support online multi-resolution VaR analytics using and integration of private and Internet cloud resources. © 2009 IEEE.}}, 
pages = {484--489}, 
number = {NA}, 
volume = {NA}
}
@article{10.1057/jam.2015.30, 
year = {2015}, 
title = {{Are the log-returns of Italian open-end mutual funds normally distributed? A risk assessment perspective}}, 
author = {Bianchi, Michele Leonardo}, 
journal = {Journal of Asset Management}, 
issn = {14708272}, 
doi = {10.1057/jam.2015.30}, 
abstract = {{In this article we conduct an empirical analysis of daily log-returns of Italian open-end mutual funds and their respective benchmarks in the period from February 2007 to May 2015. First, we estimate the classical normal-based model on the log-returns of a large set of funds. Then we compare it with five models allowing for asymmetry and (or) heavy tails. We empirically assess that both the value at risk and the average value at risk are model-dependent and we show that the difference between models should be taken into consideration in the evaluation of risk measures. © 2015 Macmillan Publishers Ltd.}}, 
pages = {437--449}, 
number = {7}, 
volume = {16}
}
@article{10.1007/s00180-011-0283-z, 
year = {2012}, 
title = {{Estimating value at risk with semiparametric support vector quantile regression}}, 
author = {Shim, Jooyong and Kim, Yongtae and Lee, Jangtaek and Hwang, Changha}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/s00180-011-0283-z}, 
abstract = {{Value at Risk (VaR) has been used as an important tool to measure the market risk under normal market. Usually the VaR of log returns is calculated by assuming a normal distribution. However, log returns are frequently found not normally distributed. This paper proposes the estimation approach of VaR using semiparametric support vector quantile regression (SSVQR) models which are functions of the one-step-ahead volatility forecast and the length of the holding period, and can be used regardless of the distribution. We find that the proposed models perform better overall than the variance-covariance and linear quantile regression approaches for return data on S\&P 500, NIKEI 225 and KOSPI 200 indices. © 2011 Springer-Verlag.}}, 
pages = {685--700}, 
number = {4}, 
volume = {27}
}
@article{10.1007/s10203-011-0123-1, 
year = {2013}, 
title = {{Option-based risk management of a bond portfolio under regime switching interest rates}}, 
author = {Antonelli, Fabio and Ramponi, Alessandro and Scarlatti, Sergio}, 
journal = {Decisions in Economics and Finance}, 
issn = {15938883}, 
doi = {10.1007/s10203-011-0123-1}, 
abstract = {{In the present paper, we assume an economy with regime switching short rates and show how the Value at Risk of a financial position on zero-coupon bonds, hedged by buying protective put options under budget constraints, can be minimized by selecting optimal (regime-dependent) strike prices. © 2011 Springer-Verlag.}}, 
pages = {47--70}, 
number = {1}, 
volume = {36}
}
@article{10.1016/j.irfa.2013.07.009, 
year = {2014}, 
title = {{Extreme downside risk spillover from the United States and Japan to Asia-Pacific stock markets}}, 
author = {Liu, Lu}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2013.07.009}, 
abstract = {{This paper proposes a binary response model approach to measure and forecast extreme downside risks in Asia-Pacific markets given information on extreme downside risks in the U.S. and Japanese markets. The extreme downside risk of a market is measured as the occurrence of extreme downside movement-market returns falling below left-tail Value at Risk in a Markov switching framework. The empirical findings are consistent with the following notions. First, extreme downside movements of the S\&P 500 and Nikkei 225 are significantly predictive for the likelihood of extreme downside movements in all the investigated Asia-Pacific markets. Second, the majority of Asia-Pacific markets become more sensitive to Japan's extreme downside risk when the Japanese market switches into high volatility periods, whereas the U.S. spillover effect is intensified only on Taiwan during high volatility periods in the U.S. Third, mainland China is the least sensitive to extreme downside risk in the U.S. and Japan, Australia is the most sensitive to the U.S., and Singapore is the most sensitive to Japan. © 2013 Elsevier Inc.}}, 
pages = {39--48}, 
number = {NA}, 
volume = {33}
}
@article{10.1287/opre.1080.0685, 
year = {2010}, 
title = {{Percentile optimization for Markov decision processes with parameter uncertainty}}, 
author = {Delage, Erick and Mannor, Shie}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1080.0685}, 
abstract = {{Markov decision processes are an effective tool in modeling decision making in uncertain dynamic environments. Because the parameters of these models typically are estimated from data or learned from experience, it is not surprising that the actual performance of a chosen strategy often differs significantly from the designer's initial expectations due to unavoidable modeling ambiguity. In this paper, we present a set of percentile criteria that are conceptually natural and representative of the trade-off between optimistic and pessimistic views of the question. We study the use of these criteria under different forms of uncertainty for both the rewards and the transitions. Some forms are shown to be efficiently solvable and others highly intractable. In each case, we outline solution concepts that take parametric uncertainty into account in the process of decision making. © 2010 INFORMS.}}, 
pages = {203--213}, 
number = {1}, 
volume = {58}
}
@article{10.1080/23737484.2017.1298482, 
year = {2016}, 
title = {{On the flexibility of GARCH-family models with an application to the BRICS stock indices}}, 
author = {Afuecheta, Emmanuel and Ruiz, Diego Andrés Pérez and Utazi, Chigozie and Nwosu, Chinwe}, 
journal = {Communications in Statistics: Case Studies, Data Analysis and Applications}, 
issn = {23737484}, 
doi = {10.1080/23737484.2017.1298482}, 
abstract = {{Financial asset returns are known to exhibit fatter tails with substantial skewness and high kurtosis due to volatility. In this article, the dynamics of implied volatility of BRICS indices are described using 10 GARCH-type models. The innovation processes of these GARCH models are characterized using 13 parametric distributions, including these 4 uncommon, flexible ones: Student's t-gamma mixture, normal-gamma mixture, asymmetric exponential power, and generalized asymmetric student's t distributions. The performance and predictive ability of these GARCH models are evaluated in terms of value at risk and expected shortfall with some loss functions. The EST-GARCH with error distribution based on the asymmetric exponential power distribution gives the best fit. © 2016, © 2016 Taylor \& Francis.}}, 
pages = {44--77}, 
number = {1-2}, 
volume = {2}
}
@article{10.2143/ast.40.1.2049226, 
year = {2010}, 
title = {{Optimal reinsurance revisited - A geometric approach}}, 
author = {Cheung, Ka Chun}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.40.1.2049226}, 
abstract = {{In this paper, we reexamine the two optimal reinsurance problems studied in Cai et al. (2008), in which the objectives are to find the optimal reinsurance contracts that minimize the value-at-risk (VaR) and the conditional tail expectation (CTE) of the total risk exposure under the expectation premium principle. We provide a simpler and more transparent approach to solve these problems by using intuitive geometric arguments. The usefulness of this approach is further demonstrated by solving the VaR-minimization problem when the expectation premium principle is replaced by Wang's premium principle. © 2010 by Astin Bulletin. All rights reserved.}}, 
pages = {221--239}, 
number = {1}, 
volume = {40}
}
@article{10.1002/asmb.1915, 
year = {2013}, 
title = {{Intensity-based estimation of extreme loss event probability and value at risk}}, 
author = {Hamidieh, Kamal and Stoev, Stilian and Michailidis, George}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.1915}, 
abstract = {{We develop a methodology for the estimation of extreme loss event probability and the value at risk, which takes into account both the magnitudes and the intensity of the extreme losses. Specifically, the extreme loss magnitudes are modeled with a generalized Pareto distribution, whereas their intensity is captured by an autoregressive conditional duration model, a type of self-exciting point process. This allows for an explicit interaction between the magnitude of the past losses and the intensity of future extreme losses. The intensity is further used in the estimation of extreme loss event probability. The method is illustrated and backtested on 10 assets and compared with the established and baseline methods. The results show that our method outperforms the baseline methods, competes with an established method, and provides additional insight and interpretation into the prediction of extreme loss event probability. Copyright © 2012 John Wiley \& Sons, Ltd.}}, 
pages = {171--186}, 
number = {3}, 
volume = {29}
}
@article{10.1016/j.jbankfin.2005.04.002, 
year = {2006}, 
title = {{Dynamic portfolio selection with process control}}, 
author = {MacLean, Leonard and Zhao, Yonggan and Ziemba, William}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.04.002}, 
abstract = {{The risk inherent in the accumulation of investment capital depends on the true return distributions of the risky assets, the accuracy of estimated returns, and the investment strategy. This paper considers risk control with Value-at-Risk and Conditional Value-at-Risk, using control limits to determine times for portfolio rebalancing. Optimal strategies and control limits are determined for a geometric Brownian motion asset pricing model with random parameters. The approaches to risk control are applied to the fundamental problem of investment in stocks, bonds, and cash over time. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {317--339}, 
number = {2}, 
volume = {30}
}
@article{10.18564/jasss.3927, 
year = {2019}, 
title = {{Impact of Basel III countercyclical measures on financial stability: An agent-based model}}, 
author = {Llacay, Barbara and Peffer, Gilbert}, 
journal = {Journal of Artificial Societies and Social Simulation}, 
issn = {14607425}, 
doi = {10.18564/jasss.3927}, 
abstract = {{The financial system is inherently procyclical, as it amplifies the course of economic cycles, and precisely one of the factors that has been suggested to exacerbate this procyclicality is the Basel regulation on capital requirements. After the recent credit crisis, international regulators have turned their eyes to countercyclical regulation as a solution to avoid similar episodes in the future. Countercyclical regulation aims at preventing excessive risk taking during booms to reduce the impact of losses suffered during recessions, for example increasing the capital requirements during the good times to improve the resilience of financial institutions at the downturn. The Basel Committee has already moved forward towards the adoption of countercyclical measures on a global scale: the Basel III Accord, published in December 2010, revises considerably the capital requirement rules to reduce their procyclicality. These new countercyclical measures will not be completely implemented until 2019, so their impact cannot be evaluated yet, and it is a crucial question whether they will be effective in reducing procyclicality and the appearance of crisis episodes such as the one experienced in 2007-08. For this reason, we present in this article an agent-based model aimed at analysing the effect of two countercyclical mechanisms introduced in Basel III: the countercyclical buffer and the stressed VaR. In particular, we focus on the impact of these mechanisms on the procyclicality induced by market risk requirements and, more specifically, by value-at-risk models, as it is a issue of crucial importance that has received scant attention in the modeling literature. The simulation results suggest that the adoption of both of these countercyclical measures improves market stability and reduces the emergence of crisis episodes. © 2019, University of Surrey. All rights reserved.}}, 
number = {1}, 
volume = {22}
}
@article{10.1109/cinc.2009.81, 
year = {2009}, 
title = {{Credibilistic risk optimization models and algorithms}}, 
author = {Dong, Wen and Peng, Jin}, 
journal = {2009 International Conference on Computational Intelligence and Natural Computing}, 
issn = {NA}, 
doi = {10.1109/cinc.2009.81}, 
abstract = {{Risk optimization is a very important issue in decision making. In this paper, some credibilistic risk optimization models and algorithms are presented. Firstly, we recall some definitions and results of value-at-risk in credibilistic risk analysis. Secondly, we propose some risk optimization models by means of fuzzy programming, or more precisely, credibilistic programming. Thirdly, hybrid intelligent algorithms are designed to solve the proposed risk optimization models. Finally, some numerical examples are illustrated. © 2009 IEEE.}}, 
pages = {378--381}, 
number = {1}, 
volume = {NA}
}
@article{10.1093/jjfinec/nbz014, 
year = {2020}, 
title = {{The Threshold GARCH Model: Estimation and Density Forecasting for Financial Returns}}, 
author = {Cai, Yuzhi and Stander, Julian}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbz014}, 
abstract = {{We consider multiple threshold value-at-risk (VaRt) estimation and density forecasting for financial data following a threshold GARCH model. We develop an α-quantile quasi-maximum likelihood estimation (QMLE) method for VaRt by showing that the associated density function is an α-quantile density and belongs to the tick-exponential family. This establishes that our estimator is consistent for the parameters of VaRt. We propose a density forecasting method for quantile models based on VaRt at a single nonextreme level, which overcomes some limitations of existing forecasting methods with quantile models. We find that for heavy-tailed financial data our α-quantile QMLE method for VaRt outperforms the Gaussian QMLE method for volatility. We also find that density forecasts based on VaRt outperform those based on the volatility of financial data. Empirical work on market returns shows that our approach also outperforms some benchmark models for density forecasting of financial returns. © 2019 The Author(s) 2019. Published by Oxford University Press. All rights reserved. For permissions, please email: journals.permissions@oup.com.}}, 
pages = {395--424}, 
number = {2}, 
volume = {18}
}
@article{10.1080/13518470701322284, 
year = {2007}, 
title = {{Factor-based, non-parametric risk measurement framework for hedge funds and fund-of-funds}}, 
author = {Goodworth, T. R. J. and Jones, C. M.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/13518470701322284}, 
abstract = {{A factor-decomposition based framework is presented that facilitates non-parametric risk analysis for complex hedge fund portfolios in the absence of portfolio level transparency. This approach has been designed specifically for use within the hedge fund-of-funds environment, but is equally relevant to those who seek to construct risk-managed portfolios of hedge funds under less than perfect underlying portfolio transparency. Using dynamic multivariate regression analysis coupled with a qualitative understanding of hedge fund return drivers, one is able to perform a robust factor decomposition to attribute risk within any hedge fund portfolio with an identifiable strategy. Furthermore, through use of Monte Carlo simulation techniques, these factors can be employed to generate implied risk profiles at either the constituent fund or aggregate fund-of-funds level. As well as being pertinent to risk forecasting and monitoring, such methods also have application to style analysis, profit attribution, portfolio stress testing and diversification studies. This paper outlines such a framework and presents sample results in each of these areas.}}, 
pages = {645--655}, 
number = {7}, 
volume = {13}
}
@article{10.1109/access.2020.3001589, 
year = {2020}, 
title = {{A Note on Two-Stage Fuzzy Location Problems under VaR Criterion with Irregular Fuzzy Variables}}, 
author = {Wang, Ke and Wang, Yuanyuan and Yang, Yan and Goh, Mark}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2020.3001589}, 
abstract = {{Recently, Yang et al. (Computers Industrial Engineering, 131: 157-171, 2019) proposed a solution approach to the two-stage fuzzy location problem under the Value-at-Risk (VaR) criterion, which significantly reduced the computational complexity compared with the other approximation treatments. However, in their work, the approach was developed for cases with regular fuzzy variables, which renders it useless when dealing with other types of fuzzy parameters such as discrete fuzzy variables and trapezoidal fuzzy numbers. In this note, for the general case involving arbitrary types of fuzzy parameters, we show that the VaR of a location decision can be determined exactly by solving a corresponding deterministic linear programming. Consequently, a similar approach is developed for the generalized case. Utilizing the extended approach, we show that the discrete case presented by Yang et al. can be solved directly rather than by enumerating all possible scenarios. Furthermore, a numerical example with regular fuzzy variables studied in their work is extended to the continuous but irregular case to illustrate our extension. © 2013 IEEE.}}, 
pages = {110306--110315}, 
number = {NA}, 
volume = {8}
}
@article{10.1007/s00500-017-2979-7, 
year = {2018}, 
title = {{Value-at-risk forecasts by dynamic spatial panel GJR-GARCH model for international stock indices portfolio}}, 
author = {Zhang, Wei-Guo and Mo, Guo-Li and Liu, Fang and Liu, Yong-Jun}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-017-2979-7}, 
abstract = {{To provide accurate value-at-risk (VaR) forecasts for the returns of international stock indices portfolio, this paper proposes a dynamic spatial panel with generalized autoregressive conditional heteroscedastic model (DSP-GJR-GARCH). The proposed model considers the spatiotemporal dependence as well as asymmetric volatility of returns, with the theories of spatial econometrics. We construct an economic spatial weight matrix and set part of the initial estimated values as unknown parameters to get more acute of parameter estimations. After that, we compare the proposed model with three closely related models including GARCH, spatiotemporal-AR, dynamic spatial panel GARCH models, with respect to the performances of daily volatility and VaR forecasting. The empirically comparative data involve six composite indices of major countries, namely USA (DJI), German (DAX), France (FCHI), U.K. (ISEQ), Japan (N225) and China (SSE). The comparative computational results show that, since the proposed model considers spatial dependence and time series correlation simultaneously, it could get more accurate prediction of VaR than the three ones. Moreover, the findings reveal that the predictive accuracy of a spatial regressive model can be improved by considering asymmetric volatility in the disturbances. Thus, we can conclude that DSP-GJR-GARCH model performs better than the other three compared models. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {5279--5297}, 
number = {16}, 
volume = {22}
}
@article{10.1108/s1571-0386(2012)0000022020, 
year = {2012}, 
title = {{Evaluation of hedge fund returns value at risk using GARCH models}}, 
author = {Khanniche, Sabrina}, 
journal = {International Symposia in Economic Theory and Econometrics}, 
issn = {15710386}, 
doi = {10.1108/s1571-0386(2012)0000022020}, 
abstract = {{Purpose - This chapter aimed to investigate hedge funds market risk. One aims to go further the traditional measures of risk that underestimates it by introducing a more appropriate method to hedge funds. One demonstrates that daily hedge fund return distributions are asymmetric and leptokurtic. Furthermore, volatility clustering phenomenon and the existence of ARCH effects demonstrate that hedge funds volatility varies through time. These features suggest the modelisation of their volatility using symmetric (GARCH) and asymmetric (EGARCH and TGARCH) models used to evaluate a 1-day-ahead value at risk (VaR). Methodology/Approach - The conditional variances were estimated under the assumption that residuals et follow the normal and the student law. The knowledge of the conditional variance was used to forecast 1-day-ahead VaR. The estimations are compared with the Gaussian, the student and the modified VaR. To sum up, 12 VaRs are computed; those based on standard deviation and computed with normal, student and cornish fisher quantile and those based on conditional volatility models (GARCH, TGARCH and EGARCH) computed with the same quantiles. Findings - The results demonstrate that VaR models based on normal quantile underestimate risk while those based on student and cornish fisher quantiles seem to be more relevant measurements. GARCH-type VaRs are very sensitive to changes in the return process. Back-testing results show that the choice of the model used to forecast volatility has an importance. Indeed, the VaR based on standard deviation is not relevant to measure hedge funds risks as it fails the appropriate tests. On the opposite side, GARCH-, TGARCH- and EGARCH-type VaRs are accurate as they pass most of the time successfully the back-testing tests. But, the quantile used has a more significant impact on the relevance of the VaR models considered. GARCH-type VaR computed with the student and especially cornish fisher quantiles lead to better results, which is consistent with Monteiro (2004) and Pochon and Teïletche (2006). Originality/Value of chapter - A large set of GARCH-type models are considered to estimate hedge funds volatility leading to numerous evaluation of VaRs. These estimations are very helpful. Indeed, public savings under institutional investors management then delegate to hedge funds are concerned. Therefore, an adequate risk management is required. Another contribution of this chapter is the use of daily data to measure all hedge fund strategies risks. Copyright © 2012 by Emerald Group Publishing Limited. All rights reserved.}}, 
pages = {315--342}, 
number = {NA}, 
volume = {22}
}
@article{10.1007/978-3-642-38279-6_11, 
year = {2013}, 
title = {{Implications of unisex assumptions in the analysis of longevity for insurance portfolios}}, 
author = {Ornelas, Arelly and Guillén, Montserrat and Alcañiz, Manuela}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-642-38279-6\_11}, 
abstract = {{Since the Court of Justice of the European Union (EU) imposed a rule that insurance prices cannot be different for men and women, it is relevant to look into insights of mortality rate calculation and its implications. We already know that mortality curves are distinct between men and women, mainly in youth ages. However we are interested in the longevity risk, therefore we studied elderly ages. We start the analysis describing mortality rates for each sex and age. The data used come from INEGI and CONAPO in Mexico. For each 5-year period, the number of deaths and the number of living people by age and sex are available. We assume possible scenarios of gender proportion in mortality tables. Using survival analysis we estimated the parameters of a Weibull distribution, moreover we calculated the value at risk at different confidence levels. We emphasize the limitations of using unisex mortality tables to estimate longevity risk if the proportion of male and female is not equilibrated in the insurance portfolio. The actuarial modeling is necessary even when regulatory circumstances impose rules on the use of risk factors such as gender for tarification. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {99--107}, 
number = {NA}, 
volume = {145}
}
@article{10.1007/s10479-012-1142-1, 
year = {2013}, 
title = {{Sensitivity of portfolio VaR and CVaR to portfolio return characteristics}}, 
author = {Stoyanov, Stoyan V. and Rachev, Svetlozar T. and Fabozzi, Frank J.}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-012-1142-1}, 
abstract = {{Risk management through marginal rebalancing is important for institutional investors due to the size of their portfolios. We consider the problem of improving marginally portfolio VaR and CVaR through a marginal change in the portfolio return characteristics. We study the relative significance of standard deviation, mean, tail thickness, and skewness in a parametric setting assuming a Student's t or a stable distribution for portfolio returns. We also carry out an empirical study with the constituents of DAX30, CAC40, and SMI. Our analysis leads to practical implications for institutional investors and regulators. © 2012 Springer Science+Business Media, LLC.}}, 
pages = {169--187}, 
number = {1}, 
volume = {205}
}
@article{10.1109/icmss.2009.5305445, 
year = {2009}, 
title = {{The empirical study of dynamic VaR on the index of Shanghai security market based GARCH-EVT}}, 
author = {Ma, Yulin and Ye, Fei and Bi, Xiaoxia}, 
journal = {2009 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2009.5305445}, 
abstract = {{This paper propose a method for estimating VaR and related risk measures describing the tail of the conditional distribution of a heteroscedastic financial return series. Our approach combines GARCH models to estimate the current volatility and extreme value theory for estimating the tail of the innovation distribution of the GARCH model. We use our method to estimate VaR and conditional expected shortfalls. Using backtesting of Shanghai stock market we find that there is serious ARCH effect on return rate of the stock market and GARCH-EVT model gives better estimates than GARCH-Normal and GARCH-t model. ©2009 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/jrf-03-2016-0029, 
year = {2017}, 
title = {{Back-testing extreme value and Lévy value-at-risk models: Evidence from international futures markets}}, 
author = {Mozumder, Sharif and Dempsey, Michael and Kabir, M. Humayun}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-03-2016-0029}, 
abstract = {{Purpose: The purpose of the paper is to back-test value-at-risk (VaR) models for conditional distributions belonging to a Generalized Hyperbolic (GH) family of Lévy processes – Variance Gamma, Normal Inverse Gaussian, Hyperbolic distribution and GH – and compare their risk-management features with a traditional unconditional extreme value (EV) approach using data from future contracts return data of S\&P500, FTSE100, DAX, HangSeng and Nikkei 225 indices. Design/methodology/approach: The authors apply tail-based and Lévy-based calibration to estimate the parameters of the models as part of the initial data analysis. While the authors utilize the peaks-over-threshold approach for generalized Pareto distribution, the conditional maximum likelihood method is followed in case of Lévy models. As the Lévy models do not have closed form expressions for VaR, the authors follow a bootstrap method to determine the VaR and the confidence intervals. Finally, for back-testing, they use both static calibration (on the entire data) and dynamic calibration (on a four-year rolling window) to test the unconditional, independence and conditional coverage hypotheses implemented with 95 and 99 per cent VaRs. Findings: Both EV and Lévy models provide the authors with a conservative proportion of violation for VaR forecasts. A model targeting tail or fitting the entire distribution has little effect on either VaR calculation or a VaR model’s back-testing performance. Originality/value: To the best of the authors’ knowledge, this is the first study to explore the back-testing performance of Lévy-based VaR models. The authors conduct various calibration and bootstrap techniques to test the unconditional, independence and conditional coverage hypotheses for the VaRs. © 2017, © Emerald Publishing Limited.}}, 
pages = {88--118}, 
number = {1}, 
volume = {18}
}
@article{10.1109/icbdie50010.2020.00116, 
year = {2020}, 
title = {{Parallel Monte Carlo Simulation of VaR Calculation Based on Intel MIC Architecture}}, 
author = {Wen, Tiansheng and Mao, Rui and Tan, Cheng}, 
journal = {2020 International Conference on Big Data and Informatization Education (ICBDIE)}, 
issn = {NA}, 
doi = {10.1109/icbdie50010.2020.00116}, 
abstract = {{With regard to the research on financial risk management, Value-at-Risk(VaR) has been widely accepted as a standard approach to financial risk management. There are various ways applicable to calculate VaR, of which Monte Carlo Simulation is regarded as the most effective one to deal with VaR calculation. Nevertheless, the calculation accuracy of the Monte Carlo Simulation method tends to be affected by the scale of simulation. As the scale of simulation increases, the calculation accuracy improves. In the meantime, however, the time cost rises on a continued basis. Therefore, it is difficult for the Monte Carlo Simulation method to be applied in practice. In order to solve this problem, a parallel Monte Carlo Simulation calculation method based on Intel MIC architecture is proposed in this paper to calculate VaR. The simulation process consists of a group of simulation tasks at a time. Due to the absence of data dependency between each simulation task, it is capable of excellent parallelism. In this paper, the OpenMP programming model was applied to assign each simulation task to a different thread for execution. Then, a comparison was performed between the CPU sequential execution scheme, the scheme based on MIC offload mode and the scheme based on MIC native mode. According to the comparative results, the solution based on MIC native mode is superior to that based on MIC offload mode. When the number of times for simulation to be conducted reaches 50,000, the parallel algorithm in MIC native mode achieves a maximum speed-up of 93.2 times compared to CPU sequential calculations, which could make the Monte Carlo Simulation of VaR calculations suitable for practical application. © 2020 IEEE.}}, 
pages = {470--473}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icams.2010.5553227, 
year = {2010}, 
title = {{RETRACTED ARTICLE: Estimation of value at risk for Chinese financial market: ARMA-FIAPARCH-SKST model}}, 
author = {Lin, Yu and Chen, Yanxiang and Luo, Jianying}, 
journal = {2010 IEEE International Conference on Advanced Management Science(ICAMS 2010)}, 
issn = {NA}, 
doi = {10.1109/icams.2010.5553227}, 
abstract = {{Asymmetry and long memory of return and volatility are two important stylized facts in financial market, an effective financial risk management must be consider them. Therefore, this paper uses ARMA (1,1)-FIAPARCH (1,d,1)-SKST to estimate dynamic Value at Risk(VaR), and apply Kupiec's LRT technique to test risk measurement accuracy of different risk model. Our results show that ARMA(1,1)-FIAPARCH(1,d,1)-SKST model is best excellent model which was used in this paper; SKST fit distribution of financial return; Risk Metrics model can not measure risk of finance market accurately. © 2010 IEEE.}}, 
pages = {329--332}, 
number = {NA}, 
volume = {3}
}
@article{10.1142/s0218488516500264, 
year = {2016}, 
title = {{Uncertain Distribution-Minimum Spanning Tree Problem}}, 
author = {Zhou, Jian and Yi, Xiajie and Wang, Ke and Liu, Jing}, 
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems}, 
issn = {02184885}, 
doi = {10.1142/s0218488516500264}, 
abstract = {{This paper studies the minimum spanning tree problem on a graph with uncertain edge weights, which are formulated as uncertain variables. The concept of ideal uncertain minimum spanning tree (ideal UMST) is initiated by extending the definition of the uncertain α-minimum spanning tree to reect the overall properties of the α-minimum spanning tree weights at any confidence level α [0,1]. On the basis of this new concept, the definition of uncertain distribution-minimum spanning tree is proposed in three ways. Particularly, by considering the tail value at risk from the perspective of risk management, the notion of uncertain β-distribution-minimum spanning tree (β-distribution-UMST) is suggested. It is shown that the β-distribution-UMST is just the uncertain expected minimum spanning tree when β = 0. For any β [0,1], this problem can be effectively solved via the proposed deterministic graph transformation-based approach with the aid of the β-distribution-path optimality condition. Furthermore, the proposed definitions and solutions are illustrated by some numerical examples. © 2016 World Scientific Publishing Company.}}, 
pages = {537--560}, 
number = {4}, 
volume = {24}
}
@article{10.1080/03610918.2012.698779, 
year = {2014}, 
title = {{Robust eligible own funds and value at risk under solvency ii system}}, 
author = {Pitselis, Georgios}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2012.698779}, 
abstract = {{In the present article, we focus on the estimation of the eligible capital (own funds) an insurance company should hold in order to manage losses and avoid insolvencies under Solvency II system. For the supervision of solvency and prediction of insurer insolvency random coefficient regression (RCR) models are applied. Robust RCR models are also applied to remedy the effect of outliers and produce a more reliable parameter estimation. Based on the above-mentioned, models confidence intervals for the value at risk (VaR) are also provided. The techniques are implemented using data provided by private insurance companies. © 2014 Taylor and Francis Group, LLC.}}, 
pages = {161--182}, 
number = {1}, 
volume = {43}
}
@article{10.1016/j.jbankfin.2006.11.014, 
year = {2007}, 
title = {{The limits of diversification when losses may be large}}, 
author = {Ibragimov, Rustam and Walden, Johan}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2006.11.014}, 
abstract = {{Recent results in value at risk analysis show that, for extremely heavy-tailed risks with unbounded distribution support, diversification may increase value at risk, and that generally it is difficult to construct an appropriate risk measure for such distributions. We further analyze the limitations of diversification for heavy-tailed risks. We provide additional insight in two ways. First, we show that similar non-diversification results are valid for a large class of risks with bounded support, as long as the risks are concentrated on a sufficiently large interval. The required length of the support depends on the number of risks available and on the degree of heavy-tailedness. Second, we relate the value at risk approach to more general risk frameworks. We argue that in markets for risky assets where the number of assets is limited compared with the (bounded) distribution support of the risks, unbounded heavy-tailed risks may provide a reasonable approximation. We suggest that this type of analysis may have a role in explaining various types of market failures in markets for assets with possibly large negative outcomes. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {2551--2569}, 
number = {8}, 
volume = {31}
}
@article{10.1007/978-3-642-38279-6_8, 
year = {2013}, 
title = {{Generalizing some usual risk measures in financial and insurance applications}}, 
author = {Belles-Sampera, Jaume and Guillén, Montserrat and Santolino, Miguel}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-642-38279-6\_8}, 
abstract = {{We illustrate a family of risk measures called GlueVaR that combine Value-at-Risk and Tail Value-at-Risk at different tolerance levels and have analytical closed-form expressions for the most frequently used distribution functions in financial and insurance applications, i.e. Normal, Log-normal, Student t and Generalized Pareto distributions. Tail-subadditivity is a remarkable property of a subfamily of GlueVaR risk measures. An implementation to the analysis of risk in an insurance portfolio is investigated. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {75--82}, 
number = {NA}, 
volume = {145}
}
@article{10.1016/j.spl.2014.12.016, 
year = {2015}, 
title = {{Subadditivity of Value-at-Risk for Bernoulli random variables}}, 
author = {Hofert, Marius and McNeil, Alexander J.}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2014.12.016}, 
abstract = {{Necessary and sufficient conditions for the subadditivity of Value-at-Risk (V aRα) for portfolios of bonds are presented under various dependence assumptions. For sufficiently large α, V aRα is subadditive. However, for any α one can construct portfolios for which V aRα is superadditive. © 2014 Elsevier B.V.}}, 
pages = {79--88}, 
number = {NA}, 
volume = {98}
}
@article{10.1016/j.econmod.2016.12.014, 
year = {2017}, 
title = {{Testing the Gaussian and Student's t copulas in a risk management framework}}, 
author = {Lourme, Alexandre and Maurer, Frantz}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2016.12.014}, 
abstract = {{This paper introduces a semiparametric framework for selecting either a Gaussian or a Student's t copula in a d-dimensional setting. We compare the two models using four different approaches: (i) four goodness-of-fit graphical plots, (ii) a bootstrapped correlation matrix generated in each scenario with the empirical correlation matrix used as a benchmark, (iii) Value-at-Risk (VaR) and Expected Shortfall (ES) as risk measures, and (iv) co-Value-at-Risk (CoVaR) and Marginal Expected Shortfall (MES) as co-risk measures. We illustrate this four-step procedure using a portfolio of daily returns of six international stock indices. The VaR results confirm that the t-based copula model is an attractive alternative to the Gaussian. The ES analysis is less conclusive, and indicates that risk managers should jointly use the risk measure as well as the copula model. The results highlight the importance of promoting stress testing rather than ES in the risk management industry, particularly in the aftermath of a financial crisis. © 2016 Elsevier Ltd}}, 
pages = {203--214}, 
number = {NA}, 
volume = {67}
}
@article{10.1007/978-3-540-87477-5_46, 
year = {2008}, 
title = {{A Wavelet Based Multi Scale VaR Model for Agricultural Market}}, 
author = {He, Kaijian and Lai, Kin Keung and Guu, Sy-Ming and Zhang, Jinlong}, 
journal = {Communications in Computer and Information Science}, 
issn = {18650929}, 
doi = {10.1007/978-3-540-87477-5\_46}, 
abstract = {{Participants in the agricultural industries are subject to significant market risks due to long production lags. Traditional methodology analyzes the risk evolution following a time invariant approach. However, this paper analyzes and proposes wavelet analysis to track risk evolution in a time variant fashion. A wavelet-econometric hybrid model is further proposed for VaR estimates. The proposed wavelet decomposed VaR (WDVaR) is ex-ante in nature and is capable of estimating risks that are multi-scale structured. Empirical studies in major agricultural markets are conducted for both the hybrid ARMA-GARCH VaR and the proposed WDVaR. Experiment results confirm significant performance improvement. Besides, incorporation of time variant risks tracking capability offers additional flexibility for adaptability of the proposed hybrid algorithm to different market environments. WDVaR can be tailored to specific market characteristics to capture unique investment styles, time horizons, etc. © Springer-Verlag Berlin Heidelberg 2008.}}, 
pages = {429--438}, 
number = {NA}, 
volume = {14}
}
@article{10.2298/yjor1101103d, 
year = {2011}, 
title = {{Return distribution and value at risk estimation for BELEX15}}, 
author = {Đorić, Dragan and Nikolić-Đorić, Emilija}, 
journal = {Yugoslav Journal of Operations Research}, 
issn = {03540243}, 
doi = {10.2298/yjor1101103d}, 
abstract = {{The aim of this paper is to find distributions that adequately describe returns of the Belgrade Stock Exchange index BELEX15. The sample period covers 1067 trading days from 4 October 2005 to 25 December 2009. The obtained models were considered in estimating Value at Risk ( VaR ) at various confidence levels. Evaluation of VaR model accuracy was based on Kupiec likelihood ratio test.}}, 
pages = {103--118}, 
number = {1}, 
volume = {21}
}
@article{10.1142/s0219622019500445, 
year = {2020}, 
title = {{Dependence structure analysis and VaR estimation based on China's and international gold price: A copula approach}}, 
author = {Liang, Zhicheng and Wang, Junwei and Lai, Kin Keung}, 
journal = {International Journal of Information Technology \& Decision Making}, 
issn = {02196220}, 
doi = {10.1142/s0219622019500445}, 
abstract = {{Since 2013, China has become the world's largest gold producer and consumer. To gain the corresponding global pricing power in gold, many actions have been taken by China in recent years, including the International Board at Shanghai Gold Exchange, Shanghai-Hong Kong Gold Connect and Shanghai Gold Fix. Our work studies the dependence structure between China's and international gold price and examines whether these moves are changing the dependence structure. We use GARCH-copula models to detect the dynamic dependence and tail dependence. The research period is set to contain the Financial Crisis in 2008, the dramatical plunge of gold price in 2013 and a series of black swan events in 2016. The empirical study shows that some event driven dependence structure breaks are statistically insignificant. And the time-varying Symmetrized Joe-Clayton copula is the best copula to model the dependence structure based on AIC value. Finally, an example of applications of this dependence structure is given by estimating the VaR of an equally weighted portfolio with a simulation-based method. © 2020 World Scientific Publishing Company.}}, 
pages = {169--193}, 
number = {1}, 
volume = {19}
}
@article{10.1016/j.jbankfin.2020.105855, 
year = {2020}, 
title = {{Modelling extremal dependence for operational risk by a bipartite graph}}, 
author = {Kley, Oliver and Klüppelberg, Claudia and Paterlini, Sandra}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2020.105855}, 
abstract = {{We introduce a statistical model for operational losses based on heavy-tailed distributions and bipartite graphs, which captures the event type and business line structure of operational risk data. The model explicitly takes into account the Pareto tails of losses and the heterogeneous dependence structures between them. We then derive estimators and provide estimation methods for individual as well as aggregated tail risk, measured in terms of Value-at-Risk and Conditional-Tail-Expectation for very high confidence levels, and introduce also an asymptotically full capital allocation method for portfolio risk. Having access to real-world operational risk losses from the Italian banking system, we apply our model to these data, and carry out risk estimation in terms of the previously derived quantities. Simulation studies further reveal first that even with a small number of observations, the proposed estimation methods produce estimates that converge to the true asymptotic values, and second, that quantifying dependence by means of the empirical network has a big impact on estimates at both individual and aggregate level, as well as for capital allocations. © 2020}}, 
pages = {105855}, 
number = {NA}, 
volume = {117}
}
@article{10.1002/for.2641, 
year = {2020}, 
title = {{Forecasting of dependence, market, and investment risks of a global index portfolio}}, 
author = {Hernandez, Jose Arreola and Janabi, Mazin A.M. Al}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2641}, 
abstract = {{This paper undertakes an in-sample and rolling-window comparative analysis of dependence, market, and portfolio investment risks on a 10-year global index portfolio of developed, emerging, and commodity markets. We draw our empirical results by fitting vine copulas (e.g., r-vines, c-vines, d-vines), IGARCH(1,1) RiskMetrics value-at-risk (VaR), and portfolio optimization methods based on risk measures such as the variance, conditional value-at-risk, conditional drawdown-at-risk, minimizing regret (Minimax), and mean absolute deviation. The empirical results indicate that all international indices tend to correlate strongly in the negative tail of the return distribution; however, emerging markets, relative to developed and commodity markets, exhibit greater dependence, market, and portfolio investment risks. The portfolio optimization shows a clear preference towards the gold commodity for investment, while Japan and Canada are found to have the highest and lowest market risk, respectively. The vine copula analysis identifies symmetry in the dependence dynamics of the global index portfolio modeled. Large VaR diversification benefits are produced at the 95\% and 99\% confidence levels by the modeled international index portfolio. The empirical results may appeal to international portfolio investors and risk managers for advanced portfolio management, hedging, and risk forecasting. © 2019 John Wiley \& Sons, Ltd.}}, 
pages = {512--532}, 
number = {3}, 
volume = {39}
}
@article{10.1109/access.2020.3028124, 
year = {2020}, 
title = {{Google index-driven oil price value-at-risk forecasting: A decomposition ensemble approach}}, 
author = {Zhao, Lu-Tao and Zheng, Zhi-Yi and Fu, Ying and Liu, Ze-Xi and Li, Ming-Fang}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2020.3028124}, 
abstract = {{The oil price is influenced not only by the fundamentals of supply and demand but also by unpredictable political conflicts, climate emergencies, and investor intentions, which cause enormous short-term fluctuations in the oil price. The proposition of the Google index-driven decomposition ensemble model to forecast crude oil price risk uses big data technology and a time series decomposition method. First, by constructing an index of investor attention for the market and emergencies combined with a bivariate empirical mode decomposition, we analyze the impact of investor attention on oil price fluctuations. Second, we establish a vector autoregression model, and the impulse responses define the impact of emergencies on the crude oil price. Finally, with the help of machine learning and historical simulation methods, the risk of crude oil price shocks from unexpected events is predicted. Empirical research demonstrates that concerns related to the oil market and emergencies that appear in Google search data are closely related to changes in oil prices. Based on the Google index, our model’s prediction of crude oil prices is more accurate than other models, and the prediction of value-at-risk is closer to the theoretical value than the historical simulation with the ARMA forecasts method. Considering the impact of emergencies in the prediction of crude oil price risk can help provide technical guidance for investors and risk managers and avoid economic risks caused by climate disasters or political conflicts. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.}}, 
pages = {183351--183366}, 
number = {NA}, 
volume = {8}
}
@article{10.1109/gucon.2018.8675083, 
year = {2019}, 
title = {{Application of swarm intelligence to portfolio optimisation}}, 
author = {Sethia, Akhil Mukesh}, 
journal = {2018 International Conference on Computing, Power and Communication Technologies (GUCON)}, 
issn = {NA}, 
doi = {10.1109/gucon.2018.8675083}, 
abstract = {{Portfolio optimisation is a thoroughly research domain, and with developments in asset allocation objective functions, current optimisation methods are proving insufficient. This study compares the performance and convergence time of different swarm-intelligence based methods for 2 types of objective functions, each consisting of 12, 24, 48 and 96 stocks each. The objective functions are Sharpe ratio maximisation and maximization of Value at Risk weighted return of the portfolio. Cuckoo search, Firefly, Micro-Bat Echolocation, Elephant Herd, Harmony Search, Flower Pollination, Differential Evolution and Particle Swarm Optimisation algorithms are used for optimisation and analysis. The performance of each algorithm is measured by the resultant return, variance measured risk, sharpe ratio and time per iteration. The portfolios are sampled from and the return and risk is calibrated from Nifty100 historical price data. © 2018 IEEE.}}, 
pages = {1029--1033}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11222-008-9063-1, 
year = {2008}, 
title = {{Bayesian inference and model comparison for asymmetric smooth transition heteroskedastic models}}, 
author = {Gerlach, Richard and Chen, Cathy W. S.}, 
journal = {Statistics and Computing}, 
issn = {09603174}, 
doi = {10.1007/s11222-008-9063-1}, 
abstract = {{Inference, quantile forecasting and model comparison for an asymmetric double smooth transition heteroskedastic model is investigated. A Bayesian framework in employed and an adaptive Markov chain Monte Carlo scheme is designed. A mixture prior is proposed that alleviates the usual identifiability problem as the speed of transition parameter tends to zero, and an informative prior for this parameter is suggested, that allows for reliable inference and a proper posterior, despite the non-integrability of the likelihood function. A formal Bayesian posterior model comparison procedure is employed to compare the proposed model with its two limiting cases: the double threshold GARCH and symmetric ARX GARCH models. The proposed methods are illustrated using both simulated and international stock market return series. Some illustrations of the advantages of an adaptive sampling scheme for these models are also provided. Finally, Bayesian forecasting methods are employed in a Value-at-Risk study of the international return series. The results generally favour the proposed smooth transition model and highlight explosive and smooth nonlinear behaviour in financial markets. © 2008 Springer Science+Business Media, LLC.}}, 
pages = {391}, 
number = {4}, 
volume = {18}
}
@article{10.1109/ccdc.2015.7162249, 
year = {2015}, 
title = {{Analyzing project tardiness risk based on value at risk}}, 
author = {Jiang, Guanje and Min, Huang and Hong, Wang}, 
journal = {The 27th Chinese Control and Decision Conference (2015 CCDC)}, 
issn = {NA}, 
doi = {10.1109/ccdc.2015.7162249}, 
abstract = {{A project consists of many subprojects with precedence. Subprojects' completion times are often uncertain due to many factors from inside and outside environment, which makes the project's completion time uncertain. In such case, the project cannot be delivered on time leading to tardiness risk. To address this uncertainty, we provide a risk measure-value at risk (VaR). A model with the objective to minimizing VaR is built to describe and quantify the tardiness risk. The proposed model is compared to Monte Carlo simulation. Comparison results show that this proposed method can replace Monte Carlo simulation in measuring tardiness risk. © 2015 IEEE.}}, 
pages = {1999--2003}, 
number = {NA}, 
volume = {NA}
}
@article{10.13306/j.1672-3813.2018.03.008, 
year = {2018}, 
title = {{Model and Solution of Routing Optimization Problem in the Fourth Party Logistics with Tardiness Risk [考虑拖期风险的第四方物流路径优化问题模型与求解]}}, 
author = {}, 
issn = {16723813}, 
doi = {10.13306/j.1672-3813.2018.03.008}, 
abstract = {{As to the practical complex logistics distribution situation that the distribution task cannot be completed in time, which brings tardiness risk and loss for company, the routing optimization problem in the fourth party logistics with consideration of tardiness risk is studied. A mathematical model minimizing tardiness risk and taking the distribution costs as constraint is set up, using Value-at-Risk to measure time risk. A deletion algorithm embedded harmony search is proposed considering nonlinear and NP-Hard characteristic of the problem. The effectiveness of model and algorithm is verified through solving the different scales of cases. © 2018, The Journal of Agency of Complex Systems and Complexity Science. All right reserved.}}, 
number = {3}, 
volume = {15}
}
@article{10.29220/csam.2021.28.1.059, 
year = {2021}, 
title = {{Value at Risk of portfolios using copulas}}, 
author = {Byun, Kiwoong and Song, Seongjoo}, 
journal = {Communications for Statistical Applications and Methods}, 
issn = {22877843}, 
doi = {10.29220/csam.2021.28.1.059}, 
abstract = {{Value at Risk (VaR) is one of the most common risk management tools in finance. Since a portfolio of several assets, rather than one asset portfolio, is advantageous in the risk diversification for investment, VaR for a portfolio of two or more assets is often used. In such cases, multivariate distributions of asset returns are considered to calculate VaR of the corresponding portfolio. Copulas are one way of generating a multivariate distribution by identifying the dependence structure of asset returns while allowing many different marginal distributions. However, they are used mainly for bivariate distributions and are not widely used in modeling joint distributions for many variables in finance. In this study, we would like to examine the performance of various copulas for high dimensional data and several different dependence structures. This paper compares copulas such as elliptical, vine, and hierarchical copulas in computing the VaR of portfolios to find appropriate copula functions in various dependence structures among asset return distributions. In the simulation studies under various dependence structures and real data analysis, the hierarchical Clayton copula shows the best performance in the VaR calculation using four assets. For marginal distributions of single asset returns, normal inverse Gaussian distribution was used to model asset return distributions, which are generally high-peaked and heavy-tailed. ©2021 The Korean Statistical Society, and Korean International Statistical Society. All rights reserved.}}, 
pages = {59--79}, 
number = {1}, 
volume = {28}
}
@article{10.1109/ptc.2003.1304701, 
year = {2003}, 
title = {{Provision of financial transmission rights}}, 
author = {Kristiansen, T.}, 
journal = {2003 IEEE Bologna Power Tech Conference Proceedings,}, 
issn = {NA}, 
doi = {10.1109/ptc.2003.1304701}, 
abstract = {{This paper studies the credit risks faced by the providers of financial transmission rights (FTRs) The introduction of FTRs in different systems in the USA must be viewed in relationship to the organization of the market Often, private players own the central grid, while an independent system operator (ISO) operates the grid. The revenues from transmission congestion collected in the day-ahead and balancing markets should give the ISO sufficient revenues to cover the costs associated with providing FTRs. This can be ensured if the issued FTRs fulfill the simultaneous feasibility test described by Hogan. We study this test on a three-node network under different assumptions and find the maximum volumes which can be sold, including contingency constraints. Next we analyze the feasibility test when taking into account the FTR prices, and demonstrate that a higher volume might be issued. We introduce uncertainty under different scenarios for locational prices and calculate the maximum provided volumes. As a tool for risk management the provider of the FTRs can use the Value at Risk approach. Finally we discuss provision of FTRs by private parties. © 2003 IEEE.}}, 
pages = {10--242}, 
number = {NA}, 
volume = {4}
}
@article{10.1109/bife.2012.74, 
year = {2012}, 
title = {{Mutual Information based copulas to aggregate banking risks}}, 
author = {Yi, Shanli and Li, Jianping and Zhu, Xiaoqian and Feng, Jichuang}, 
journal = {2012 Fifth International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2012.74}, 
abstract = {{This paper develops a methodology to aggregate the market, credit and operational risk and derive the economic capital. While the data is obtained by mapping the profit \& loss items of income statement into risk types, the dependence structure between the risks is modeled through the combination of mutual information and copulas, which enables to capture both linear and non-linear dependence. The results show that the non-linear dependence could have influences on the risk measure such as Value-at-Risk (VaR), and that ignoring it leads to risk underestimation. © 2012 IEEE.}}, 
pages = {323--327}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2012.06.006, 
year = {2012}, 
title = {{Probabilistic forecasts of volatility and its risk premia}}, 
author = {Maneesoonthorn, Worapree and Martin, Gael M. and Forbes, Catherine S. and Grose, Simone D.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2012.06.006}, 
abstract = {{The object of this paper is to produce distributional forecasts of asset price volatility and its associated risk premia using a non-linear state space approach. Option and spot market information on the latent variance process is captured by using dual 'model-free' variance measures to define a bivariate observation equation in the state space model. The premium for variance diffusive risk is defined as linear in the latent variance (in the usual fashion) whilst the premium for variance jump risk is specified as a conditionally deterministic dynamic process, driven by a function of past measurements. The inferential approach adopted is Bayesian, implemented via a Markov chain Monte Carlo algorithm that caters for the multiple sources of non-linearity in the model and for the bivariate measure. The method is applied to spot and option price data on the S\&P500 index from 1999 to 2008, with conclusions drawn about investors' required compensation for variance risk during the recent financial turmoil. The accuracy of the probabilistic forecasts of the observable variance measures is demonstrated, and compared with that of forecasts yielded by alternative methods. To illustrate the benefits of the approach, it is used to produce forecasts of prices of derivatives on volatility itself. In addition, the posterior distribution is augmented by information on daily returns to produce value at risk predictions. Linking the variance risk premia to the risk aversion parameter in a representative agent model, probabilistic forecasts of (approximate) relative risk aversion are also produced. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {217--236}, 
number = {2}, 
volume = {171}
}
@article{10.1016/j.automatica.2012.11.012, 
year = {2013}, 
title = {{Direct data-driven portfolio optimization with guaranteed shortfall probability}}, 
author = {Calafiore, Giuseppe Carlo}, 
journal = {Automatica}, 
issn = {00051098}, 
doi = {10.1016/j.automatica.2012.11.012}, 
abstract = {{This paper proposes a novel methodology for optimal allocation of a portfolio of risky financial assets. Most existing methods that aim at compromising between portfolio performance (e.g.; expected return) and its risk (e.g.; volatility or shortfall probability) need some statistical model of the asset returns. This means that: (i) one needs to make rather strong assumptions on the market for eliciting a return distribution, and (ii) the parameters of this distribution need be somehow estimated, which is quite a critical aspect, since optimal portfolios will then depend on the way parameters are estimated. Here we propose instead a direct, data-driven, route to portfolio optimization that avoids both of the mentioned issues: the optimal portfolios are computed directly from historical data, by solving a sequence of convex optimization problems (typically, linear programs). Much more importantly, the resulting portfolios are theoretically backed by a guarantee that their expected shortfall is no larger than an a-priori assigned level. This result is here obtained assuming efficiency of the market, under no hypotheses on the shape of the joint distribution of the asset returns, which can remain unknown and need not be estimated. © 2012 Elsevier Ltd. All rights reserved.}}, 
pages = {370--380}, 
number = {2}, 
volume = {49}
}
@article{10.1080/14697688.2014.926018, 
year = {2015}, 
title = {{Systematic scenario selection: stress testing and the nature of uncertainty}}, 
author = {Flood, Mark D. and Korenko, George G.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2014.926018}, 
abstract = {{We present a technique for selecting multidimensional shock scenarios for use in financial stress testing. The methodology systematically enforces internal consistency among the shock dimensions by sampling points of arbitrary severity from a plausible joint probability distribution. The approach involves a grid search of sparse, well distributed, stress-test scenarios, which we regard as a middle ground between traditional stress testing and reverse stress testing. Choosing scenarios in this way reduces the danger of ‘blind spots’ in stress testing. We suggest extensions to address the issues of non-monotonic loss functions and univariate shocks. We provide tested and commented source code in Matlab®.}}, 
pages = {43--59}, 
number = {1}, 
volume = {15}
}
@article{10.1017/asb.2016.42, 
year = {2017}, 
title = {{A neyman-pearson perspective on optimal reinsurance with constraints}}, 
author = {Lo, Ambrose}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2016.42}, 
abstract = {{The formulation of optimal reinsurance policies that take various practical constraints into account is a problem commonly encountered by practitioners. In the context of a distortion-risk-measure-based optimal reinsurance model without moral hazard, this article introduces and employs a variation of the Neyman-Pearson Lemma in statistical hypothesis testing theory to solve a wide class of constrained optimal reinsurance problems analytically and expeditiously. Such a Neyman-Pearson approach identifies the unit-valued derivative of each ceded loss function as the test function of an appropriate hypothesis test and transforms the problem of designing optimal reinsurance contracts to one that resembles the search of optimal test functions achieved by the classical Neyman-Pearson Lemma. As an illustration of the versatility and superiority of the proposed Neyman-Pearson formulation, we provide complete and transparent solutions of several specific constrained optimal reinsurance problems, many of which were only partially solved in the literature by substantially more difficult means and under extraneous technical assumptions. Examples of such problems include the construction of the optimal reinsurance treaties in the presence of premium budget constraints, counterparty risk constraints and the optimal insurer-reinsurer symbiotic reinsurance treaty considered recently in Cai et al. (2016). © 2017 Astin Bulletin.}}, 
pages = {467--499}, 
number = {2}, 
volume = {47}
}
@article{10.1007/s11156-010-0175-2, 
year = {2011}, 
title = {{An analysis of risk-based asset allocation and portfolio insurance strategies}}, 
author = {Ho, Lan-chih and Cadle, John and Theobald, Michael}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-010-0175-2}, 
abstract = {{This paper compares traditional portfolio insurance strategies with modern risk-based dynamic asset allocation strategies within a currency portfolio context for reserve management. Given the objective of preserving reserve value, the evaluation of the hedging performances of various strategies focuses on four perspectives regarding, in particular, the return distribution of the hedged portfolio. In terms of the Sharpe Ratio, the constant proportional portfolio insurance is the best performer due to having the lowest volatility, while the Value at Risk strategy based upon the normal distribution is the worst due to its having the smallest return. From the perspective that the return distribution of the hedged portfolio is shifted to the right, the synthetic put performs the best, with the expected shortfall strategy the second best. In terms of the cumulative portfolio return across years, the expected shortfall strategy using the historical distribution ranks first, as a result of its participation in upward markets. Furthermore, the expected shortfall-based strategy results in a lower turnover within the investment horizon, thereby saving transaction costs. © 2010 Springer Science+Business Media, LLC.}}, 
pages = {247--267}, 
number = {2}, 
volume = {36}
}
@article{10.1239/aap/1346955266, 
year = {2012}, 
title = {{The convergence rate and asymptotic distribution of the bootstrap quantile variance estimator for importance sampling}}, 
author = {Liu, Jingchen and Yang, Xuan}, 
journal = {Advances in Applied Probability}, 
issn = {00018678}, 
doi = {10.1239/aap/1346955266}, 
abstract = {{Importance sampling is a widely used variance reduction technique to compute sample quantiles such as value at risk. The variance of the weighted sample quantile estimator is usually a difficult quantity to compute. In this paper we present the exact convergence rate and asymptotic distributions of the bootstrap variance estimators for quantiles of weighted empirical distributions. Under regularity conditions, we show that the bootstrap variance estimator is asymptotically normal and has relative standard deviation of order O(n -1/4). © Applied Probability Trust 2012.}}, 
pages = {815--841}, 
number = {3}, 
volume = {44}
}
@article{10.1177/0972150920933857, 
year = {2020}, 
title = {{The Role of Contagion and Integration in Risk Management Measures}}, 
author = {Filho, Jaime de Jesus and Matos, Paulo and Fonseca, Ronald}, 
journal = {Global Business Review}, 
issn = {09721509}, 
doi = {10.1177/0972150920933857}, 
abstract = {{We add to the discussion on risk management by proposing an innovative measure of Value at Risk (VaR), which relaxes some statistical assumptions. We provide a VaR based on time-varying moments of the best-fitting probability distribution function. This risk measure can capture the cross-effects associated with contagion and integration through the estimation of a multivariate autoregressive moving average–generalized autoregressive heteroskedasticity (ARMA–GARCH). We implement an empirical exercise to account for the risk management of some of the main worldwide financial sector indices of G20 economies. According to Basel back-testing and back-testing that deals with the frequency and conditionality of losses exceeding VaR, this innovative VaR seems to perform better than Basel VaR. © 2020 International Management Institute, New Delhi.}}, 
pages = {097215092093385}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11408-006-0020-8, 
year = {2006}, 
title = {{Uncertainty in value-at-risk estimates under parametric and non-parametric modeling}}, 
author = {Aussenegg, Wolfgang and Miazhynskaia, Tatiana}, 
journal = {Financial Markets and Portfolio Management}, 
issn = {15554961}, 
doi = {10.1007/s11408-006-0020-8}, 
abstract = {{This study evaluates a set of parametric and non-parametric value-at-risk (VaR) models that quantify the uncertainty in VaR estimates in form of a VaR distribution. We propose a new VaR approach based on Bayesian statistics in a GARCH volatility modeling environment. This Bayesian approach is compared with other parametric VaR methods (quasi-maximum likelihood and bootstrap resampling on the basis of GARCH models) as well as with non-parametric historical simulation approaches (classical and volatility adjusted). All these methods are evaluated based on the frequency of failures and the uncertainty in VaR estimates. Within the parametric methods, the Bayesian approach is better able to produce adequate VaR estimates, and results mostly in a smaller VaR variability. The non-parametric methods imply more uncertain 99\%-VaR estimates, but show good performance with respect to 95\%-VaRs.}}, 
pages = {243--264}, 
number = {3}, 
volume = {20}
}
@article{10.1504/ijgei.2006.008385, 
year = {2006}, 
title = {{An improved historical simulation approach for estimating 'value at risk' of crude oil price}}, 
author = {Fan, Ying and Jiao, Jian Ling}, 
journal = {International Journal of Global Energy Issues}, 
issn = {09547118}, 
doi = {10.1504/ijgei.2006.008385}, 
abstract = {{Value at risk, an effective measurement of financial risk, can be used to forecast the risk associated with oil price movements. In this paper, we propose an improved Historical Simulation Approach, EDFAAF, which is based on a former approach, HSAF. By comparing it with the HSAF approach, we give evidence to show that EDFAAF has a more effective forecasting power in the field of oil risk management. Copyright © 2006 Inderscience Enterprises Ltd.}}, 
pages = {83}, 
number = {1-2}, 
volume = {25}
}
@article{10.1109/iswcs.2014.6933312, 
year = {2014}, 
title = {{MU-MIMO power control under statistical CSI and probabilistic constraints}}, 
author = {Krishna, Chitti and Sneidel, Joachim}, 
journal = {2014 11th International Symposium on Wireless Communications Systems (ISWCS)}, 
issn = {NA}, 
doi = {10.1109/iswcs.2014.6933312}, 
abstract = {{Uplink sum power minimization problem with required Quality of Service (QoS) under statistical Channel State Information (CSI) and probabilistic constraints in solved for a multicell multiuser scenario. To handle the probabilistic constraints and analyze the system performance, financial risk management measures Value at Risk (VaR) and Conditional Value at Risk (CVaR) are applied. The resulting expressions may involve functions that are non-invertible and a combination of higher order functions, so obtaining closed form solutions may not be possible. To find the optimal value of the optimization problem and also estimate the worst case performance for a given probability, Extreme Value Theory (EVT) is applied. Simulation results show that the EVT approach generates best bounds when compared with existing methods. © 2014 IEEE.}}, 
pages = {17--21}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.pacfin.2020.101454, 
year = {2020}, 
title = {{Sequential forecasting of downside extreme risk during overnight and daytime: Evidence from the Chinese Stock Market☆}}, 
author = {Jian, Zhihong and Li, Xupei and Zhu, Zhican}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2020.101454}, 
abstract = {{This paper proposes a sequential methodology to forecast the stock markets downside extreme risk during overnight and daytime periods. We jointly characterize Value at Risk (VaR) and Expected Shortfall (ES) dynamics during overnight and daytime using a sequential hybrid GAS model conditional on lagged information. We implement the Chernozhukov-Hong MCMC methods for parameter estimation and model evaluation. The results show that the sequential models in forecasting VaR and ES outperform the non-sequential ones, and accumulated information during non-trading overnight hours can help improve the forecasting of daytime downside extreme risk. Moreover, overnight and daytime extreme risks evidently increase during the economic downturn. © 2020 Elsevier B.V.}}, 
pages = {101454}, 
number = {NA}, 
volume = {64}
}
@article{10.1016/j.jeconom.2015.03.029, 
year = {2015}, 
title = {{Sample quantile analysis for long-memory stochastic volatility models}}, 
author = {Ho, Hwai-Chung}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2015.03.029}, 
abstract = {{This study investigates asymptotic properties of sample quantile estimates in the context of long-memory stochastic volatility models in which the latent volatility component is an exponential transformation of a linear long-memory time series. We focus on the least absolute deviation quantile estimator and show that while the underlying process is a sequence of stationary martingale differences, the estimation errors are asymptotically normal with the convergence rate which is slower than n and determined by the dependence parameter of the volatility sequence. A non-parametric resampling method is employed to estimate the normalizing constants by which the confidence intervals are constructed. To demonstrate the methodology, we conduct a simulation study as well as an empirical analysis of the Value-at-Risk estimate of the S\&P 500 daily returns. Both are consistent with the theoretical findings and provide clear evidence that the coverage probabilities of confidence intervals for the quantile estimate are severely biased if the strong dependence of the unobserved volatility sequence is ignored. © 2015 Elsevier B.V.}}, 
pages = {360--370}, 
number = {2}, 
volume = {189}
}
@article{10.1016/j.insmatheco.2014.11.006, 
year = {2015}, 
title = {{On multivariate extensions of the conditional Value-at-Risk measure}}, 
author = {Bernardino, E. Di and Fernández-Ponce, J.M. and Palacios-Rodríguez, F. and Rodríguez-Griñolo, M.R.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.11.006}, 
abstract = {{CoVaR is a systemic risk measure proposed by Adrian and Brunnermeier (2011) able to measure a financial institution's contribution to systemic risk and its contribution to the risk of other financial institutions. CoVaR stands for conditional Value-at-Risk, i.e.it indicates the Value at Risk for a financial institution that is conditional on a certain scenario. In this paper, two alternative extensions of the classic univariate Conditional Value-at-Risk are introduced in a multivariate setting. The two proposed multivariate CoVaRs are constructed from level sets of multivariate distribution functions (. resp. of multivariate survival distribution functions). These vector-valued measures have the same dimension as the underlying risk portfolio. Several characterizations of these new risk measures are provided in terms of the copula structure and stochastic orderings of the marginal distributions. Interestingly, these results are consistent with existing properties on univariate risk measures. Furthermore, comparisons between existent risk measures and the proposed multivariate CoVaR are developed. Illustrations are given in the class of Archimedean copulas. Estimation procedure for the multivariate proposed CoVaRs is illustrated in simulated studies and insurance real data. © 2014 Elsevier B.V.}}, 
pages = {1--16}, 
number = {NA}, 
volume = {61}
}
@article{10.1108/jrf-07-2017-0115, 
year = {2018}, 
title = {{Value-at-risk and related measures for the Bitcoin}}, 
author = {Stavroyiannis, Stavros}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-07-2017-0115}, 
abstract = {{Purpose: The purpose of this paper is to examine the value-at-risk and related measures for the Bitcoin and to compare the findings with Standard and Poor’s SP500 Index, and the gold spot price time series. Design/methodology/approach: A GJR-GARCH model has been implemented, in which the residuals follow the standardized Pearson type-IV distribution. A large variety of value-at-risk measures and backtesting criteria are implemented. Findings: Bitcoin is a highly volatile currency violating the value-at-risk measures more than the other assets. With respect to the Basel Committee on Banking Supervision Accords, a Bitcoin investor is subjected to higher capital requirements and capital allocation ratio. Practical implications: The risk of an investor holding Bitcoins is measured and quantified via the regulatory framework practices. Originality/value: This paper is the first comprehensive approach to the risk properties of Bitcoin. © 2018, Emerald Publishing Limited.}}, 
pages = {127--136}, 
number = {2}, 
volume = {19}
}
@article{10.1214/11-aoas457, 
year = {2011}, 
title = {{Lambert W random variables-a new family of generalized skewed distributions with applications to risk estimation}}, 
author = {Goerg, Georg M.}, 
journal = {The Annals of Applied Statistics}, 
issn = {19326157}, 
doi = {10.1214/11-aoas457}, 
eprint = {0912.4554}, 
abstract = {{Originating from a system theory and an input/output point of view, I introduce a new class of generalized distributions. A parametric nonlinear transformation converts a random variable X into a so-called Lambert W random variable Y, which allows a very flexible approach to model skewed data. Its shape depends on the shape of X and a skewness parameter γ. In particular, for symmetric X and nonzero γ the output Y is skewed. Its distribution and density function are particular variants of their input counterparts. Maximum likelihood and method of moments estimators are presented, and simulations show that in the symmetric case additional estimation of γ does not affect the quality of other parameter estimates. Applications in finance and biomedicine show the relevance of this class of distributions, which is particularly useful for slightly skewed data. A practical by-result of the Lambert W framework: data can be "unskewed." The R package LambertW developed by the author is publicly available (CRAN). © Institute of Mathematical Statistics, 2011.}}, 
pages = {2197--2230}, 
number = {3}, 
volume = {5}
}
@article{10.1080/13504851.2011.593496, 
year = {2012}, 
title = {{Beyond reasonable doubt: Multiple tail risk measures applied to European industries}}, 
author = {Allen, David Edmund and Powell, Robert John and Singh, Abhay Kumar}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2011.593496}, 
abstract = {{Using a comprehensive range of metrics, this article determines how relative market and credit risk change among European sectors during extreme market fluctuations. Differences are found between conditional and nonconditional outcomes, and sectors which were most risky prior to the Global Financial Crisis (GFC) are found to be different to the riskiest sectors during the GFC. These findings are consistent across the metrics used. The insights into extreme sectoral risk are important to investors in portfolio selection and to banks in setting sectoral concentration limits. © 2012 Taylor \& Francis.}}, 
pages = {671--676}, 
number = {7}, 
volume = {19}
}
@article{10.1109/wicom.2008.2405, 
year = {2008}, 
title = {{The extreme value copulas analysis of the risk dependence for the foreign exchange data}}, 
author = {LU, Jin and TIAN, Wen-ju and ZHANG, Pu}, 
journal = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing}, 
issn = {NA}, 
doi = {10.1109/wicom.2008.2405}, 
abstract = {{The aim of this paper is to analyze the dependence structure between the asset returns using the extreme value copulas. We first focus on the use of extreme value theory, more specifically, the generalized extreme distribution (GEV) to model the tail behavior of the financial return series based on monthly maxima and minima of daily USD/UK, USD/EUR foreign exchange data. A parameter estimation based on maximum likelihood method is proposed for the parameter estimation of the GEV model, on the basis of which, we conduct the statistical estimation of the copula parameters using Inference for Margins (IFM) method. We thereafter identify the suitable copulas based on the parametric and nonparametric estimation of the dependence function. The procedure is proposed for the calibration of the copula functions to recover the joint tail distribution and quantify the magnitude of tail dependence by comparing three different extreme value copulas. The results show that three members we concerned are all suitable copulas that have the desired property to measure the joint tail risk and tail dependence for our empirical market data. © 2008 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eneco.2016.06.004, 
year = {2016}, 
title = {{Oil price volatility forecast with mixture memory GARCH}}, 
author = {Klein, Tony and Walther, Thomas}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2016.06.004}, 
abstract = {{We expand the literature of volatility and Value-at-Risk forecasting of oil price returns by comparing the recently proposed Mixture Memory GARCH (MMGARCH) model to other discrete volatility models (GARCH, RiskMetrics, EGARCH, APARCH, FIGARCH, HYGARCH, and FIAPARCH). We incorporate an Expectation-Maximization algorithm for parameter estimation of the MMGARCH and find different structures in volatility level as well as shock persistence. MMGARCH is also able to cover asymmetric and long memory effects. Furthermore, a dissimilar memory structure in variance of WTI and Brent crude oil prices is observed which is supported by additional tests. Parameter estimation and comparison of the models reveal significant long memory and asymmetry in oil price returns. In regard of variance forecasting and Value-at-Risk prediction, it is shown that MMGARCH outperforms the aforementioned models due to its dynamic approach in varying the volatility level and memory of the process. We find MMGARCH superior for application in risk management as a result of its flexibility in adjusting to variance shifts and shocks. © 2016 Elsevier B.V.}}, 
pages = {46--58}, 
number = {NA}, 
volume = {58}
}
@article{10.1016/j.ememar.2018.11.010, 
year = {2019}, 
title = {{On the failure of mutual fund industry regulation}}, 
author = {Mugerman, Yevgeny and Hecht, Yoel and Wiener, Zvi}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2018.11.010}, 
abstract = {{Mutual funds grant retail investors access to professional asset management and facilitate exposure to financial markets. The academic literature and regulators have traditionally focused on issues such as portfolio diversification, performance, liquidity, and management fees in attempts to analyze and improve market efficiency. Scarce attention has been paid to market risk management. There is unanimity on this issue throughout the world. The lack of regulatory attention creates a gap, which is partially covered by mutual fund rating agents and asset management analysts. Those agents base their ratings on various rating methodologies — which engenders a wide array of difficulties, especially for retail investors. We employ proprietary data on historic mutual fund ratings in Israel and show that retail investors do not necessarily benefit from this diversity of opinions. Furthermore, we find that the voluntary implementation of quantitative risk measurement techniques by certain mutual funds tends to be associated with fewer outflows and greater inflows in these funds. Interestingly, the application of (backward-looking) value-at-risk analysis is associated with fewer outflows, while (forward-looking) stress-testing techniques are associated with greater inflows. Given the similarity of mutual fund industry environments across the globe, our results have worldwide applicability. © 2018 Elsevier B.V.}}, 
pages = {51--72}, 
number = {NA}, 
volume = {38}
}
@article{10.1016/j.physa.2017.08.064, 
year = {2018}, 
title = {{Measuring Value-at-Risk and Expected Shortfall of crude oil portfolio using extreme value theory and vine copula}}, 
author = {Yu, Wenhua and Yang, Kun and Wei, Yu and Lei, Likun}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2017.08.064}, 
abstract = {{Volatilities of crude oil price have important impacts on the steady and sustainable development of world real economy. Thus it is of great academic and practical significance to model and measure the volatility and risk of crude oil markets accurately. This paper aims to measure the Value-at-Risk (VaR) and Expected Shortfall (ES) of a portfolio consists of four crude oil assets by using GARCH-type models, extreme value theory (EVT) and vine copulas. The backtesting results show that the combination of GARCH-type-EVT models and vine copula methods can produce accurate risk measures of the oil portfolio. Mixed R-vine copula is more flexible and superior to other vine copulas. Different GARCH-type models, which can depict the long-memory and/or leverage effect of oil price volatilities, however offer similar marginal distributions of the oil returns. © 2017 Elsevier B.V.}}, 
pages = {1423--1433}, 
number = {NA}, 
volume = {490}
}
@article{10.1016/j.srfe.2013.06.001, 
year = {2014}, 
title = {{A comprehensive review of Value at Risk methodologies}}, 
author = {Abad, Pilar and Benito, Sonia and López, Carmen}, 
journal = {The Spanish Review of Financial Economics}, 
issn = {21731268}, 
doi = {10.1016/j.srfe.2013.06.001}, 
abstract = {{In this article we present a theoretical review of the existing literature on Value at Risk (VaR) specifically focussing on the development of new approaches for its estimation. We effect a deep analysis of the State of the Art, from standard approaches for measuring VaR to the more evolved, while highlighting their relative strengths and weaknesses. We will also review the backtesting procedures used to evaluate VaR approach performance. From a practical perspective, empirical literature shows that approaches based on the Extreme Value Theory and the Filtered Historical Simulation are the best methods for forecasting VaR. The Parametric method under skewed and fat-tail distributions also provides promising results especially when the assumption that standardised returns are independent and identically distributed is set aside and when time variations are considered in conditional high-order moments. Lastly, it appears that some asymmetric extensions of the CaViaR method provide results that are also promising. © 2012 Asociación Española de Finanzas.}}, 
pages = {15--32}, 
number = {1}, 
volume = {12}
}
@article{10.1016/j.econmod.2013.12.023, 
year = {2014}, 
title = {{Energy portfolio risk management using time-varying extreme value copula methods}}, 
author = {Ghorbel, Ahmed and Trabelsi, Abdelwahed}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2013.12.023}, 
abstract = {{This work is concerned with the statistical modeling of the dependence structure between three energy commodity markets (WTI crude oil, natural gas and heating oil) using the concept of copulas and proposes a method for estimating the Value at risk (VaR) of energy portfolio based on the combination of time series models with models of the extreme value theory before fitting a copula. Each return series is modeled by AR-(FI) GARCH univariate model. Then, we fit the GPD distribution to the tails of the residuals to model marginal residuals distributions. The extreme value copula to the iid residuals is fitted and we simulate from it to construct N portfolios and estimate VaR. As a first step, the method is applied to a two-dimensional energy portfolio. In second step, we extend method in trivariate context to measure VaR of three-dimensional energy portfolio. Dependences between residuals are modeled using a trivariate nested Gumbel copulas. Methods proposed are compared with various univariate and multivariate conventional VaR methods. The reported results demonstrate that GARCH-t, conditional EVT and FIGARCH extreme value copula methods produce acceptable estimates of risk both for standard and more extreme VaR quantiles. Generally, copula methods are less accurate compared with their predictive performances in the case of portfolio composed of exchange market indices. © 2014 Elsevier B.V.}}, 
pages = {470--485}, 
number = {NA}, 
volume = {38}
}
@article{10.1016/j.ribaf.2013.05.004, 
year = {2014}, 
title = {{Dynamic characteristics of the daily yen-dollar exchange rate}}, 
author = {Kurita, Takamitsu}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2013.05.004}, 
abstract = {{This paper explores various dynamic properties of daily data for the yen-dollar exchange rate. This empirical study shows that quantitative information articulated with technical trading acts as market-based indicators, thus contributing to the modelling of daily fluctuations in the exchange rate. Value-at-Risk analysis is also performed to demonstrate that allowing for data properties such as skewness is essential for representing the underlying volatility of the yen-dollar rate. © 2013 Elsevier B.V.}}, 
pages = {72--82}, 
number = {1}, 
volume = {30}
}
@article{10.1002/asmb.2212, 
year = {2016}, 
title = {{Nonparametric conditional autoregressive expectile model via neural network with applications to estimating financial risk}}, 
author = {Xu, Qifa and Liu, Xi and Jiang, Cuixia and Yu, Keming}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2212}, 
abstract = {{The parametric conditional autoregressive expectiles (CARE) models have been developed to estimate expectiles, which can be used to assess value at risk and expected shortfall. The challenge lies in parametric CARE modeling is the specification of a parametric form. To avoid any model misspecification, we propose a nonparametric CARE model via neural network. The nonparametric CARE model can be estimated by a classical gradient based nonlinear optimization algorithm, and the consistency of nonparametric conditional expectile estimators is established. We then apply the nonparametric CARE model to estimating value at risk and expected shortfall of six stock indices. Empirical results for the new model is competitive with those classical models and parametric CARE models. Copyright © 2016 John Wiley \& Sons, Ltd. Copyright © 2016 John Wiley \& Sons, Ltd.}}, 
pages = {882--908}, 
number = {6}, 
volume = {32}
}
@article{10.1016/j.econlet.2018.01.020, 
year = {2018}, 
title = {{An application of extreme value theory to cryptocurrencies}}, 
author = {Gkillas, Konstantinos and Katsiampa, Paraskevi}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2018.01.020}, 
abstract = {{We study the tail behaviour of the returns of five major cryptocurrencies. By employing an extreme value analysis and estimating Value-at-Risk and Expected Shortfall as tail risk measures, we find that Bitcoin Cash is the riskiest, while Bitcoin and Litecoin are the least risky cryptocurrencies. © 2018 Elsevier B.V.}}, 
pages = {109--111}, 
number = {NA}, 
volume = {164}
}
@article{10.1080/03610918.2016.1235188, 
year = {2017}, 
title = {{Value at risk estimation under stochastic volatility models using adaptive PMCMC methods}}, 
author = {Yang, Xinxia and Chatpatanasiri, Ratthachat and Sattayatham, Pairote}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2016.1235188}, 
abstract = {{In this paper, we propose a value-at-risk (VaR) estimation technique based on a new stochastic volatility model with leverage effect, non-constant conditional mean and jump. In order to estimate the model parameters and latent state variables, we integrate the particle filter and adaptive Markov Chain Monte Carlo (MCMC) algorithms to develop a novel adaptive particle MCMC (A-PMCMC) algorithm. Comprehensive simulation experiments based on three stock indices and two foreign exchange time series show effectiveness of the proposed A-PMCMC algorithm and the VaR estimation technique. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {00--00}, 
number = {9}, 
volume = {46}
}
@article{10.1088/1742-6596/855/1/012026, 
year = {2017}, 
title = {{On the Value at Risk Using Bayesian Mixture Laplace Autoregressive Approach for Modelling the Islamic Stock Risk Investment}}, 
author = {Miftahurrohmah, Brina and Iriawan, Nur and Fithriasari, Kartika}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/855/1/012026}, 
abstract = {{Stocks are known as the financial instruments traded in the capital market which have a high level of risk. Their risks are indicated by their uncertainty of their return which have to be accepted by investors in the future. The higher the risk to be faced, the higher the return would be gained. Therefore, the measurements need to be made against the risk. Value at Risk (VaR) as the most popular risk measurement method, is frequently ignore when the pattern of return is not uni-modal Normal. The calculation of the risks using VaR method with the Normal Mixture Autoregressive (MNAR) approach has been considered. This paper proposes VaR method couple with the Mixture Laplace Autoregressive (MLAR) that would be implemented for analysing the first three biggest capitalization Islamic stock return in JII, namely PT. Astra International Tbk (ASII), PT. Telekomunikasi Indonesia Tbk (TLMK), and PT. Unilever Indonesia Tbk (UNVR). Parameter estimation is performed by employing Bayesian Markov Chain Monte Carlo (MCMC) approaches. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012026}, 
number = {1}, 
volume = {855}
}
@article{10.3233/af-140034, 
year = {2014}, 
title = {{Linear-time accurate lattice algorithms for tail conditional expectation}}, 
author = {Chen, Bryant and Hsu, William W.Y. and Ho, Jan-Ming and Kao, Ming-Yang}, 
journal = {Algorithmic Finance}, 
issn = {21585571}, 
doi = {10.3233/af-140034}, 
abstract = {{This paper proposes novel lattice algorithms to compute tail conditional expectation of European calls and puts in linear time. We incorporate the technique of prefix-sum into tilting, trinomial, and extrapolation algorithms as well as some syntheses of these algorithms. Furthermore, we introduce fractional-step lattices to help reduce interpolation error in the extrapolation algorithms. We demonstrate the efficiency and accuracy of these algorithms with numerical results. A key finding is that combining the techniques of tilting lattice, extrapolation, and fractional steps substantially increases speed and accuracy.}}, 
pages = {87--140}, 
number = {1-2}, 
volume = {3}
}
@article{10.1007/s00477-011-0479-3, 
year = {2011}, 
title = {{Ecological risk assessment for Water scarcity in Chinas Yellow River Delta Wetland}}, 
author = {Qin, Yan and Yang, Zhifeng and Yang, Wei}, 
journal = {Stochastic Environmental Research and Risk Assessment}, 
issn = {14363240}, 
doi = {10.1007/s00477-011-0479-3}, 
abstract = {{Wetlands are ecologically important due to their hydrologic attributes and their role as ecotones between terrestrial and aquatic ecosystems. Based on a 2-year study in the Yellow River Delta Wetland and a Markov-chain Monte Carlo (MCMC) simulation, we discovered temporal and spatial relationships between soil water content and three representative plant species (Phragmites australis (Cav.) Trin. ex Steud., Suaeda salsa (Linn.) Pall, and Tamarix chinensis Lour.). We selected eight indices (biodiversity, biomass, and the uptake of TN, TP, K, Ca, Mg, and Na) at three scales (community, single plant, and micro-scale) to assess ecological risk. We used the ecological value at risk (EVR) model, based on the three scales and eight indices, to calculate EVR and generate a three-level classification of ecological risk using MCMC simulation. The high-risk areas at a community scale were near the Bohai Sea. The high-risk areas at a single-plant scale were near the Bohai Sea and along the northern bank of the Yellow River. At a micro-scale, we found no concentration of high-risk areas. The results will provide a foundation on which the watershed's planners can allocate environmental flows and guide wetland restoration. © 2011 Springer-Verlag.}}, 
pages = {697}, 
number = {5}, 
volume = {25}
}
@article{10.1007/s10957-012-0014-9, 
year = {2012}, 
title = {{Addendum to: Entropic Value-at-Risk: A New Coherent Risk Measure}}, 
author = {Ahmadi-Javid, A.}, 
journal = {Journal of Optimization Theory and Applications}, 
issn = {00223239}, 
doi = {10.1007/s10957-012-0014-9}, 
abstract = {{This short addendum consists of two sections. The first provides proofs that were omitted in Ahmadi-Javid (J. Optim. Theory Appl., 2012) for the sake of brevity, and also demonstrates that the dual representation of the entropic value-at-risk, which is given in Ahmadi-Javid (J. Optim. Theory Appl., 2012) for the case of bounded random variables, holds for all random variables whose moment-generating functions exist everywhere. The second section provides a few corrections. © 2012 Springer Science+Business Media, LLC.}}, 
pages = {1124--1128}, 
number = {3}, 
volume = {155}
}
@article{10.1016/j.egypro.2019.01.027, 
year = {2019}, 
title = {{Risk implemented simultaneous game-theoretic approach for energy trading in residential microgrids}}, 
author = {Zhang, Zhenyuan and Tang, Haoyue and Huang, Qi}, 
journal = {Energy Procedia}, 
issn = {18766102}, 
doi = {10.1016/j.egypro.2019.01.027}, 
abstract = {{With the promoting of residential microgrids, peer-to-peer (P2P) energy trading market among small household entities have emerged. In order to improve the flexibility of transactions and financial benefits for each participants, game-theoretic approaches have been widely investigated in recent studies. However, in current studies, these approaches still have some problems, such as the limitation of the number of participants and some unfair constraints on trade. To solve these problems, this paper proposes a simultaneous game-theoretic approach in the P2P energy trading. Proposed approach overcame the limits on participants' number in this approach, and gives all participants equal rights to decide their trading prices for maximizing their own benefits. Besides, aiming to mitigate a potential risk of transaction failure in the simultaneous game-theoretic approach, a tool for risk analysis, "Value at Risk" (VaR), is also implemented into this approach. Through the VaR, participants could control their trading prices in a reasonable range to ensure successful energy trading. Moreover, case studies with multiple households involved in a residential microgrid confirm the effectiveness of the risk implemented simultaneous game-theoretic approach. © 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of ICAE2018 - The 10th International Conference on Applied Energy.}}, 
pages = {6679--6686}, 
number = {NA}, 
volume = {158}
}
@article{10.1080/1351847x.2021.1908391, 
year = {2021}, 
title = {{On the statistics of scaling exponents and the multiscaling value at risk}}, 
author = {Brandi, Giuseppe and Matteo, T. Di}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2021.1908391}, 
abstract = {{Research on scaling analysis in finance is vast and still flourishing. We introduce a novel statistical procedure based on the generalized Hurst exponent, the Relative Normalized and Standardized Generalized Hurst Exponent (RNSGHE), to robustly estimate and test the multiscaling property. Furthermore, we introduce a new tool to estimate the optimal aggregation time used in our methodology which we name Autocororrelation Segmented Regression. We numerically validate this procedure on simulated time series by using the Multifractal Random Walk and we then apply it to real financial data. We present results for times series with and without anomalies and we compute the bias that such anomalies introduce in the measurement of the scaling exponents. We also show how the use of proper scaling and multiscaling can ameliorate the estimation of risk measures such as Value at Risk (VaR). Finally, we propose a methodology based on Monte Carlo simulation, which we name Multiscaling Value at Risk (MSVaR), that takes into account the statistical properties of multiscaling time series. We mainly show that by using this statistical procedure in combination with the robustly estimated multiscaling exponents, the one year forecasted MSVaR mimics the VaR on the annual data for the majority of the stocks. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--22}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2010.09.028, 
year = {2011}, 
title = {{How accurate is the square-root-of-time rule in scaling tail risk: A global study}}, 
author = {Wang, Jying-Nan and Yeh, Jin-Huei and Cheng, Nick Ying-Pin}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2010.09.028}, 
abstract = {{The square-root-of-time rule (SRTR) is popular in assessing multi-period VaR; however, it makes several unrealistic assumptions. We examine and reconcile different stylized factors in returns that contribute to the SRTR scaling distortions. In complementing the use of the variance ratio test, we propose a new intuitive subsampling-based test for the overall validity of the SRTR. The results indicate that serial dependence and heavy-tailedness may severely bias the applicability of SRTR, while jumps or volatility clustering may be less relevant. To mitigate the first-order effect from time dependence, we suggest a simple modified-SRTR for scaling tail risks. By examining 47 markets globally, we find the SRTR to be lenient, in that it generally yields downward-biased 10-day and 30-day VaRs, particularly in Eastern Europe, Central-South America, and the Asia Pacific. Nevertheless, accommodating the dependence correction is a notable improvement over the traditional SRTR. © 2010 Elsevier B.V.}}, 
pages = {1158--1169}, 
number = {5}, 
volume = {35}
}
@article{10.1016/j.frl.2004.07.001, 
year = {2004}, 
title = {{Bias of a Value-at-Risk estimator}}, 
author = {Bao, Yong and Ullah, Aman}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2004.07.001}, 
abstract = {{We develop the analytical second-order bias of a Value-at-Risk estimator based on an ARCH(1) volatility specification when the parameters are estimated by the method of quasi maximum likelihood. We show that the bias results from two sources: assumption on the distribution of the standardized residuals and the parameter estimation error. © 2004 Elsevier Inc. All rights reserved.}}, 
pages = {241--249}, 
number = {4}, 
volume = {1}
}
@article{10.1088/1755-1315/473/1/012073, 
year = {2020}, 
title = {{Analysis of government development financing between Sukuk and Bonds}}, 
author = {Alimuddin and Putri, N A and Nurleni}, 
journal = {IOP Conference Series: Earth and Environmental Science}, 
issn = {17551307}, 
doi = {10.1088/1755-1315/473/1/012073}, 
abstract = {{This study aims to examine the performance of government development financing sukuk/SBSN compared with bonds/SUN period 2014-2017. The method used is descriptive quantitative with secondary data. The data obtained were analyzed and concluded based on a predetermined framework. The results of the analysis based on the method of efficiency ratio, Data Envelopment Analysis (DEA) and Value at Risk (VaR) shown that SUN has high efficiency and high risk, while SBSN has low efficiency and low risk. Based on the analysis of the Efficient Portfolio Frontier shown that an efficient portfolio exists at risk level to 0.14\% and profit rate up to 369.31\% exist in the proportion of 60\% SBSN financing and 40\% SUN financing proportion, it means that the Sukuk financing portfolio was better rather than Bonds financing portfolio. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012073}, 
number = {1}, 
volume = {473}
}
@article{10.21314/jor.2018.376, 
year = {2018}, 
title = {{Estimation risk for value-at-risk and expected shortfall}}, 
author = {Kabaila, Paul and Mainzer, Rheanna}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.376}, 
abstract = {{For a given time series of daily losses that display volatility clustering, the exact next-day and ten-day value-at-risk (VaR) and expected shortfall (ES) are unknown. The usual procedure is to approximate these values by replacing true parameter values with estimates in the formulas for VaR and ES. Parameter estimation errors for a GARCH(1,1) model for this time series lead to approximate VaR and ES that differ from the exact VaR and ES, respectively. Accurate estimation of the VaR and ES is very important for the proper management of financial risks. In this paper, we find linear regression models in which the response variable is the approximate VaR (ES) and the explanatory variable is the exact VaR (ES). We use these linear regression models to determine the properties of the approximate VaR (ES), conditional on the corresponding exact value. For a given value of the exact VaR (ES), the approximate VaR (ES) is close to being an unbiased estimator of the corresponding exact value, but it may differ from this exact value by more than 10\% of the exact value with substantial probability. © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {3}, 
volume = {20}
}
@article{10.1108/afr-04-2017-0025, 
year = {2019}, 
title = {{The Basel accords, capital reserves, and agricultural lending}}, 
author = {Brester, Gary W. and Watts, Myles J.}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/afr-04-2017-0025}, 
abstract = {{Purpose: The safety and soundness of financial institutions has become a leading worldwide issue because of the recent global financial crisis. Historically, financial crises have occurred approximately every 20 years. The worst financial crisis in the last 75 years occurred in 2008–2009. US regulatory efforts with respect to capital reserve requirements are likely to have several unintended consequences for the agricultural lending sector—especially for smaller, less-diversified (and often, rural agricultural) lenders. The paper discusses these issues. Design/methodology/approach: Simulation models and value-at-risk (VaR) criteria are used to evaluate the impact of capital reserve requirements on lending return on equity. In addition, simulations are used to calculate the effects of loan numbers and portfolio diversification on capital reserve requirements. Findings: This paper illustrates that increasing capital reserve requirements reduces lending return on equity. Furthermore, increases in the number of loans and portfolio diversification reduce capital reserve requirements. Research limitations/implications: The simulation methods are a simplification of complex lending practices and VaR calculations. Lenders use these and other procedures for managing capital reserves than those modeled in this paper. Practical implications: Smaller lending institutions will be pressured to increase loan sector diversification. In addition, traditional agricultural lenders will likely be under increased pressure to diversify portfolios. Because agricultural loan losses have relatively low correlations with other sectors, traditional agricultural lenders can expect increased competition for agricultural loans from non-traditional agricultural lenders. Originality/value: This paper is novel in that the authors illustrate how lender capital requirements change in response to loan payment correlations both within and across lending sectors. © 2018, Emerald Publishing Limited.}}, 
pages = {27--47}, 
number = {1}, 
volume = {79}
}
@article{10.1016/j.irfa.2015.11.008, 
year = {2016}, 
title = {{Intraday risk management in International stock markets: A conditional EVT approach}}, 
author = {Karmakar, Madhusudan and Paul, Samit}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2015.11.008}, 
abstract = {{The study compares the predictive ability of various models in estimating intraday Value-at-Risk (VaR) and Expected Shortfall (ES) using high frequency share price index data from sixteen different countries across the world for a period of seven and half months from September 20, 2013 to May 07, 2014. The main emphasis of the study has been given to Extreme Value Theory (EVT) and to evaluate how well Conditional EVT model performs in modeling tails of distributions and in estimating and forecasting intraday VaR and ES measures. We have followed McNeil and Frey's (2000) two stage approach called Conditional EVT to estimate dynamic intraday VaR and ES. We have compared the accuracy of Conditional EVT approach to intraday VaR and ES estimation with other competing models. The best performing model is found to be the Conditional EVT in estimating both the quantiles for the entire sample. The study is useful for market participants (such as intraday traders and market makers) involved in frequent intraday trading in such equity markets. © 2015 Elsevier Inc..}}, 
pages = {34--55}, 
number = {NA}, 
volume = {44}
}
@article{10.1111/j.1937-5956.2005.tb00011.x, 
year = {2005}, 
title = {{Channel coordination with a risk-neutral supplier and a downside-risk-averse retailer}}, 
author = {Gan, Xianghua and Sethi, Suresh P. and Yan, Houmin}, 
journal = {Production and Operations Management}, 
issn = {10591478}, 
doi = {10.1111/j.1937-5956.2005.tb00011.x}, 
abstract = {{We investigate how a supply chain involving a risk-neutral supplier and a downside-risk-averse retailer can be coordinated with a supply contract. We show that the standard buy-back or revenue-sharing contracts may not coordinate such a channel. Using a definition of coordination of supply chains proposed earlier by the authors, we design a risk-sharing contract that offers the desired downside protection to the retailer, provides respective reservation profits to the agents, and accomplishes channel coordination. © 2005 Production and Operations Management Society.}}, 
pages = {80--89}, 
number = {1}, 
volume = {14}
}
@article{10.1109/wicom.2007.1125, 
year = {2007}, 
title = {{Risk management under extreme loss}}, 
author = {Lai, Li-Hua and Wu, Pei-Hsuan}, 
journal = {2007 International Conference on Wireless Communications, Networking and Mobile Computing}, 
issn = {NA}, 
doi = {10.1109/wicom.2007.1125}, 
abstract = {{This paper empirically analyzes tail behavior of rice loss due to typhoon. One of the issues of risk management is the choice of the distribution of loss data. Using peak-over-threshold approach to extreme value modeling, we fitted generalized Pareto distribution to agriculture natural disaster loss data of Taiwan from 1971 to 2005. By comparing with standard parametric loss modeling based on lognormal and gamma distributions, the appropriateness of the upper tail fitting to loss data and And generalized Pareto distribution outperforms classical parametric fits was evaluated. Finally, we computed tail risk measures and draw some implication of risk management. © 2007 IEEE.}}, 
pages = {4577--4580}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10614-010-9226-y, 
year = {2010}, 
title = {{Partially adaptive econometric methods for regression and classification}}, 
author = {Hansen, James V. and McDonald, James B. and Theodossiou, Panayiotis and Larsen, Brad J.}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-010-9226-y}, 
abstract = {{Assumptions about the distributions of domain variables are important for much of statistical learning, including both regression and classification problems. However, it is important that the assumed models are consistent with the stylized facts. For example selecting a normal distribution permits modeling two data characteristics-the mean and the variance, but it is not appropriate for data which are skewed or have thick tails. The adaptive methods developed here offer the flexibility found in many machine learning models, but lend themselves to statistical interpretation, as well. This paper contributes to the development of partially adaptive estimation methods that derive their adaptability from membership in families of distributions, which are distinguished by modifications of simple parameters. In particular, we have extended the methods to include recently proposed distributions, including example applications and computational details. © 2010 Springer Science+Business Media, LLC.}}, 
pages = {153--169}, 
number = {2}, 
volume = {36}
}
@article{10.1007/978-3-319-89824-7_65, 
year = {2018}, 
title = {{The rearrangement algorithm of Puccetti and Rüschendorf: Proving the Convergence}}, 
author = {Galeotti, Marcello and Rabitti, Giovanni and Vannucci, Emanuele}, 
issn = {NA}, 
doi = {10.1007/978-3-319-89824-7\_65}, 
abstract = {{In 2012 Puccetti and Rüschendorf [J. Comp. Appl. Math., 236 (2012)] proposed a new algorithm to compute the upper Value-at-Risk (VaR), at a given level of confidence, of a portfolio of risky positions, whose mutual dependence is unknown. The algorithm was called Rearrangement, as it consists precisely in rearranging the columns of a matrix, whose entries are quantiles of the marginal distributions. In the following years the algorithm has performed quite well in several practical situations, but the convergence has remained an open problem. In the present paper we show that the rearrangement algorithm converges, once the deterministic procedure has been precisely defined and an initial optimality condition is satisfied. © Springer International Publishing AG.}}, 
pages = {363--367}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/isgt-la.2015.7381199, 
year = {2016}, 
title = {{A methodology to determine the firm capacity of distributed generation units}}, 
author = {Fiorotti, Rodrigo and Fardin, Jussara Farias and Encarnação, Lucas Frizera and Donadel, Clainer Bravin}, 
journal = {2015 IEEE PES Innovative Smart Grid Technologies Latin America (ISGT LATAM)}, 
issn = {NA}, 
doi = {10.1109/isgt-la.2015.7381199}, 
abstract = {{This paper presents a new methodology to determine the firm capacity of distributed generation units (DGs) connected to electrical distribution networks, using the risk analysis tool Value at Risk (VaR). The stochastic nature of wind and photovoltaic generation can be represented through Monte Carlo simulation. These results were used to analyze the impact of DGs on the feeders' peak demand value. © 2015 IEEE.}}, 
pages = {461--466}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/for.2408, 
year = {2016}, 
title = {{Bayesian Assessment of Dynamic Quantile Forecasts}}, 
author = {Gerlach, Richard and Chen, Cathy W. S. and Lin, Edward M. H.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2408}, 
abstract = {{Bayesian methods for assessing the accuracy of dynamic financial value-at-risk (VaR) forecasts have not been considered in the literature. Such methods are proposed in this paper. Specifically, Bayes factor analogues of popular frequentist tests for independence of violations from, and for correct coverage of a time series of, dynamic quantile forecasts are developed. To evaluate the relevant marginal likelihoods, analytic integration methods are utilized when possible; otherwise multivariate adaptive quadrature methods are employed to estimate the required quantities. The usual Bayesian interval estimate for a proportion is also examined in this context. The size and power properties of the proposed methods are examined via a simulation study, illustrating favourable comparisons both overall and with their frequentist counterparts. An empirical study employs the proposed methods, in comparison with standard tests, to assess the adequacy of a range of forecasting models for VaR in several financial market data series. Copyright © 2016 John Wiley \& Sons, Ltd. Copyright © 2016 John Wiley \& Sons, Ltd.}}, 
pages = {751--764}, 
number = {8}, 
volume = {35}
}
@article{10.1088/1755-1315/564/1/012072, 
year = {2020}, 
title = {{Input and output market risk Vaname Shrimp hatchery business (Litopenaeus vannamei)}}, 
author = {Adhawati, S S and Sumarauw, R L and Fakhriyyah, S and Amiluddin and Tahang, H and Gosari, B A J}, 
journal = {IOP Conference Series: Earth and Environmental Science}, 
issn = {17551307}, 
doi = {10.1088/1755-1315/564/1/012072}, 
abstract = {{The research aims to identify and analyze the sources of risk from the input and output markets of the vaname shrimp hatchery business, and to find strategies to overcome the risks. The study was conducted in June-August 2019 in the Barru Regency, South Sulawesi Province. Survey research type. Descriptive analysis methods are used to explain the source of risk. Value at Risk (VaR) is used to analyze the source of risk, and the analysis of preventive strategies is used to find strategies to minimize risk. The results of the study: (1) sources of input market risk consist of feed prices with a probability of 36.53\% and a parent price with a probability of 25.93\%. Sources of output market risk; consist of fry prices with a probability of 44.74\%. and fry sales with a probability of 78.52\%. (2) sources of feed price risk are in quadrant II, and sources of risk for parent prices, fry prices, and fry sales are in quadrant III. (3) strategies for overcoming risks, namely making feed purchase contracts for the stability of feed and parent prices, market control, management control, and distribution supervision to the consumer level. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012072}, 
number = {1}, 
volume = {564}
}
@article{10.1080/09603107.2011.617694, 
year = {2012}, 
title = {{Forecasting volatility using range data: Analysis for emerging equity markets in Latin America}}, 
author = {Asai, Manabu and Brugal, Iván}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2011.617694}, 
abstract = {{The article suggests a simple but effective approach for estimating value-at-risk thresholds using range data, working with the filtered historical simulation. For this purpose, we consider asymmetric heterogeneous Autoregressive Moving Average (ARMA) model for log-range, which captures the leverage effects and the effects from daily, weekly and monthly horizons. The empirical analysis on stock market indices on the US, Mexico, Brazil and Argentina shows that 1\% and 5\% Value at Risk (VaR) thresholds based on one-step-ahead forecasts of log-range are satisfactory for the period includes the global financial crisis. © 2011 Copyright Taylor and Francis Group, LLC.}}, 
pages = {461--470}, 
number = {6}, 
volume = {22}
}
@article{10.1080/1351847x.2010.495484, 
year = {2011}, 
title = {{On the performance of the minimum VaR portfolio}}, 
author = {Durand, Robert B. and Gould, John and Maller, Ross}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2010.495484}, 
abstract = {{Alexander and Baptista [2002. Economic implications of using a mean-value-at-risk (VaR) model for portfolio selection: A comparison with mean-variance analysis. Journal of Economic Dynamics and Control 26: 1159-93] develop the concept of mean-VaR efficiency for portfolios and demonstrate its very close connection with mean-variance efficiency. In particular, they identify the minimum VaR portfolio as a special type of mean-variance efficient portfolio. Our empirical analysis finds that, for commonly used VaR breach probabilities, minimum VaR portfolios yield ex post returns that conform well with the specified VaR breach probabilities and with return/risk expectations. These results provide a considerable extension of evidence supporting the empirical validity and tractability of the mean-VaR efficiency concept. © 2011 Taylor \& Francis.}}, 
pages = {553--576}, 
number = {7}, 
volume = {17}
}
@article{10.1016/j.orl.2020.06.004, 
year = {2020}, 
title = {{Risk quantification and validation for Bitcoin}}, 
author = {Jiménez, Ma Inés and Mora-Valencia, Andrés and Perote, Javier}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2020.06.004}, 
abstract = {{This paper introduces a semi-nonparametric approach for modeling Bitcoin risk relatively to other parametric distributions and volatility models. Model performance is assessed through different backtesting techniques, including multinomial test, for three risk measures: Value-at-Risk, Expected Shortfall and Median Shortfall. Our results show that the ‘large’ semi-nonparametric expansion is a good alternative to measure Bitcoin risk according to recommendations of Basel Committee on Banking Supervision, but also that 99\%-Median Shortfall seems to be an accurate and robust risk measure for Bitcoin. © 2020 Elsevier B.V.}}, 
pages = {534--541}, 
number = {4}, 
volume = {48}
}
@article{10.5547/01956574.38.3.jhan, 
year = {2017}, 
title = {{Managing energy price risk using futures contracts: A comparative analysis}}, 
author = {Hanly, Jim}, 
journal = {The Energy Journal}, 
issn = {01956574}, 
doi = {10.5547/01956574.38.3.jhan}, 
abstract = {{This paper carries out a comparative analysis of managing energy risk through futures hedging, for energy market participants across a broad dataset that encompasses the largest and most actively traded energy products. Uniquely, we carry out a hedge comparison using a variety of risk measures including Variance, Value at risk (VaR), and Expected Shortfall as well as a utility based performance metric for two different investor horizons; weekly and monthly. We find that hedging is effective across the spectrum of risk measures we employ. We also find significant differences in both the hedging strategies and the hedging effectiveness of different energy assets. Better performance is found for West Texas Intermediate Oil and Heating Oil while the poorest performer in hedging terms is Natural Gas. © 2017 by the IAEE.}}, 
number = {3}, 
volume = {38}
}
@article{10.1080/1351847x.2013.862173, 
year = {2015}, 
title = {{Risk management in the energy markets and value-at-risk modelling: A hybrid approach}}, 
author = {Andriosopoulos, Kostas and Nomikos, Nikos}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2013.862173}, 
abstract = {{This paper proposes a set of Value-at-Risk (VaR) models appropriate to capture the dynamics of energy prices and subsequently quantify energy price risk by calculating VaR and expected shortfall measures. Amongst the competing VaR methodologies evaluated in this paper, besides the commonly used benchmark models, a Monte Carlo (MC) simulation approach and a hybrid MC with historical simulation approach, both assuming various processes for the underlying spot prices, are also being employed. All VaR models are empirically tested on eight spot energy commodities that trade futures contracts on the New York Mercantile Exchange (NYMEX) and the constructed Spot Energy Index. A two-stage evaluation and selection process is applied, combining statistical and economic measures, to choose amongst the competing VaR models. Finally, both long and short trading positions are considered as it is of utmost importance for energy traders and risk managers to be able to capture efficiently the characteristics of both tails of the distributions. © 2013, Taylor \& Francis.}}, 
pages = {548--574}, 
number = {7}, 
volume = {21}
}
@article{10.1108/cfri-12-2016-0130, 
year = {2018}, 
title = {{Does a unique “T+1 trading rule” in China incur return difference between daytime and overnight periods?}}, 
author = {Diao, Xundi and Qiu, Hongyang and Tong, Bin}, 
journal = {China Finance Review International}, 
issn = {20441398}, 
doi = {10.1108/cfri-12-2016-0130}, 
abstract = {{Purpose: The purpose of this paper is to examine the difference between the daytime (open-to-close) and overnight (close-to-open) returns of CSI 300 index and its derivative futures. Design/methodology/approach: The paper explores the difference between the daytime and overnight time returns by using nonparametric techniques. Moreover, investigation on some factors such as short selling, trading rules, risks are made to seek the sources of the day and night effects based on a large number of empirical analysis. In the end, further analyses on daytime and overnight returns are given by the use of high-frequency data and linear regression technique. Findings: The authors show that the daytime returns of CSI 300 index are no less than its overnight returns, while the daytime returns of CSI 300 index futures are no more than its overnight returns, even after removing the heteroscedasticity of the researched time series. Specifically, the PM returns (13:05 to close) play a quite important role in the intra-day time. The findings also suggest that the unique “T+1 trading rule” in China may be a reason that incurs the lower opening price in the morning and the higher closing price in the afternoon, resulting in the statistically significant differences between the daytime and overnight returns. Practical implications: The findings are of great importance for investors to decide when to buy and sell stock and futures portfolios in Chinese financial markets. Originality/value: This study empirically analyzes why there the higher daytime returns and the lower overnight returns exist in the Chinese stock markets from different aspects and contributes the existing literature on day and night effects because of periodic market closures. © 2018, Emerald Publishing Limited.}}, 
pages = {2--20}, 
number = {1}, 
volume = {8}
}
@article{10.1080/02664763.2018.1436701, 
year = {2018}, 
title = {{Modeling multivariate cybersecurity risks}}, 
author = {Peng, Chen and Xu, Maochao and Xu, Shouhuai and Hu, Taizhong}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664763.2018.1436701}, 
abstract = {{Modeling cybersecurity risks is an important, yet challenging, problem. In this paper, we initiate the study of modeling multivariate cybersecurity risks. We develop the first statistical approach, which is centered at a Copula-GARCH model that uses vine copulas to model the multivariate dependence exhibited by real-world cyber attack data. We find that ignoring the due multivariate dependence causes a severe underestimation of cybersecurity risks. Both simulation and empirical studies show that the proposed approach leads to accurate predictions of multivariate cybersecurity risks. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--23}, 
number = {15}, 
volume = {45}
}
@article{10.1016/j.cie.2019.03.039, 
year = {2019}, 
title = {{A new solution approach to two-stage fuzzy location problems with risk control}}, 
author = {Yang, Yan and Zhou, Jian and Wang, Ke and Pantelous, Athanasios A.}, 
journal = {Computers \& Industrial Engineering}, 
issn = {03608352}, 
doi = {10.1016/j.cie.2019.03.039}, 
abstract = {{In the present paper, a two-stage fuzzy facility location problem under the Value-at-Risk (VaR) criterion is considered for controlling the risk in location decisions. Because the fuzzy parameters involved are represented in the form of regular fuzzy numbers (e.g., triangular, Gaussian, and Cauchy fuzzy numbers), it is shown that the VaR of a location decision can be determined exactly by solving the corresponding linear programming problem. This new solution approach has a significantly lower computation complexity compared with the already known approximation treatment of the problem. In this regard, the VaR-based two-stage fuzzy location model is transformed into a one-stage mixed-integer linear programming model, and is then solved using some standard programming techniques. Furthermore, the VaR-based solutions are shown to be linked to the robust optimization counterparts, and new results for the location decisions and the loss distribution under perfect information are deduced. Finally, numerical examples illustrate the effectiveness of our treatment. © 2019 Elsevier Ltd}}, 
pages = {157--171}, 
number = {NA}, 
volume = {131}
}
@article{10.1002/for.2384, 
year = {2016}, 
title = {{Forecasting Based on Decomposed Financial Return Series: A Wavelet Analysis}}, 
author = {Berger, Theo}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2384}, 
abstract = {{We transform financial return series into its frequency and time domain via wavelet decomposition to separate short-run noise from long-run trends and assess the relevance of each frequency to value-at-risk (VaR) forecast. Furthermore, we analyze financial assets in calm and turmoil market times and show that daily 95\% VaR forecasts are mainly driven by the volatility that is captured by the first scales comprising the short-run information, whereas more timescales are needed to adequately forecast 99\% VaR. As a result, individual timescales linked via copulas outperform classical parametric VaR approaches that incorporate all information available. Copyright © 2015 John Wiley \& Sons, Ltd. Copyright © 2015 John Wiley \& Sons, Ltd.}}, 
pages = {419--433}, 
number = {5}, 
volume = {35}
}
@article{10.1016/j.econmod.2010.11.016, 
year = {2011}, 
title = {{Empirical analysis of jump dynamics, heavy-tails and skewness on value-at-risk estimation}}, 
author = {Su, Jung-Bin and Hung, Jui-Cheng}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2010.11.016}, 
abstract = {{This study provides a comprehensive analysis of the possible influences of jump dynamics, heavy-tails, and skewness with regard to VaR estimates through the assessment of both accuracy and efficiency. To this end, the ARJI model, and its degenerative GARCH model with normal, GED, and skewed normal (SN) distributions were adopted to capture the properties of time-varying volatility, time-varying jump intensity, heavy-tails and skewness, for a range of stock indices across international stock markets during the period of the U.S. subprime mortgage crisis. Empirical results show that, with regard to the evaluation of accuracy, the role of jump dynamics is more substantial than heavy-tails or skewness as it pertains to VaR accuracy at the 90\% and 95\% levels, while heavy-tails become more important at the 99\% level for a long position. However, the influence of the abovementioned properties on VaR estimation does not appear substantial for a short position. In addition, the properties of jump dynamics and skewness appear to be beneficial for the improvement of efficiency. © 2010.}}, 
pages = {1117--1130}, 
number = {3}, 
volume = {28}
}
@article{10.1080/03610926.2020.1860223, 
year = {2020}, 
title = {{Optimal insurance design under Vajda condition and exclusion clauses}}, 
author = {Chen, Yanhong and Hu, Yijun}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2020.1860223}, 
abstract = {{In this paper, we explore the optimal insurance problem where the exclusion clause is taken into account. Assume that the insurable loss is mutually exclusive from another loss that is denied in the insurance coverage. Our objective is to characterize the optimal insurance strategy by minimizing the risk-adjusted value of a policyholder’s liability, where the unexpected loss is calculated by either the value at risk (VaR) or the tail value at risk (TVaR). To prevent moral hazard and to reflect the spirit of insurance, we analyze the optimal solutions over the class of ceded loss functions such that the policyholder’s retained loss and the proportion paid by an insurer are both increasing. We show that every admissible insurance contract is suboptimal to a ceded loss function composed of three interconnected line segments if the insurance premium principles satisfy risk loading and convex order preserving. The form of optimal insurance can be further simplified if the premium principles satisfies an additional weak property. Finally, we derive the optimal insurance explicitly for the expected value principle and Wang’s principle. © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--27}, 
number = {NA}, 
volume = {NA}
}
@article{10.1177/0958305x18802777, 
year = {2019}, 
title = {{Weather risk assessment of Indian power sector: A conditional value-at-risk approach}}, 
author = {Basu, Mahuya and Chakraborty, Tanupa}, 
journal = {Energy \& Environment}, 
issn = {0958305X}, 
doi = {10.1177/0958305x18802777}, 
abstract = {{This paper aims to assess the weather risk exposure of Indian power sector from both generation and demand sides. The study considers two representative firms – firstly, Damodar Valley Corporation (DVC), a hydro-generator, to assess its rainfall exposure, and secondly, Calcutta Electric Supply Corporation (CESC), a retail power supplier, to assess the temperature sensitivity of power demand. The study opts for ‘Value at Risk’ approach, which combines both the sensitivity of power variables towards weather variable and the probability of weather change. The sensitivity is measured using regression analysis with autoregressive distributed lag (ARDL). Parametric distributions are fitted to weather data to assess probabilities. Due to the ‘fat-tail’ characteristic of the fitted distribution, a ‘conditional value-at-risk’ model is considered more effective. The study reveals that the hydroelectricity generation is highly exposed to monsoon rainfall fluctuation and hence the hydro-generator may experience substantial loss of revenue due to insufficient monsoon, whereas the revenue of retail power distributor is moderately exposed to fluctuation of daily surface temperature. © The Author(s) 2018.}}, 
pages = {641--661}, 
number = {4}, 
volume = {30}
}
@article{10.1017/s1748499518000039, 
year = {2019}, 
title = {{Real-time Bayesian non-parametric prediction of solvency risk}}, 
author = {Hong, Liang and Martin, Ryan}, 
journal = {Annals of Actuarial Science}, 
issn = {17484995}, 
doi = {10.1017/s1748499518000039}, 
abstract = {{Insurance regulation often dictates that insurers monitor their solvency risk in real time and take appropriate actions whenever the risk exceeds their tolerance level. Bayesian methods are appealing for prediction problems thanks to their ability to naturally incorporate both sample variability and parameter uncertainty into a predictive distribution. However, handling data arriving in real time requires a flexible non-parametric model, and the Monte Carlo methods necessary to evaluate the predictive distribution in such cases are not recursive and can be too expensive to rerun each time new data arrives. In this paper, we apply a recently developed alternative perspective on Bayesian prediction based on copulas. This approach facilitates recursive Bayesian prediction without computing a posterior, allowing insurers to perform real-time updating of risk measures to assess solvency risk, and providing them with a tool for carrying out dynamic risk management strategies in today's big data era. © Institute and Faculty of Actuaries 2018.}}, 
pages = {67--79}, 
number = {1}, 
volume = {13}
}
@article{10.1109/tdc.2005.1547037, 
year = {2005}, 
title = {{Risk assessment and management of portfolio optimization for power plants}}, 
author = {Wang, R. and Shang, J.C. and Zhou, X.Y. and Zhang, Y.C.}, 
journal = {2005 IEEE/PES Transmission \& Distribution Conference \& Exposition: Asia and Pacific}, 
issn = {NA}, 
doi = {10.1109/tdc.2005.1547037}, 
abstract = {{In electric power markets, power plants become one of the main players and they are facing different choices of supply markets. Different markets have unique fluctuating characters of market prices and revenue rate. The power producer will decide the proportional electricity supply in different markets for a bidding strategy of the annual total electricity supply. The power portfolio strategy is to maximize the annual profit under the lowest level of bidding risks and under the restriction of exceeding the minimum expected annual profit. Risks measurement and management techniques, Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR), proposed in financial research field recently are introduced in this paper to build a portfolio optimization model with CVaR risk minimization for power producers in electricity markets. The mathematical model is discussed, the solution is given out and then the model is applied into a simulation scenario to verify its efficiency. Simulation results indicate that the proposed CVaR based risk measurement and decision model for generators can be efficiently adopted to give out the optimal allocation of bidding generation in different markets. © 2005 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {2005}
}
@article{10.1016/j.asoc.2011.02.018, 
year = {2011}, 
title = {{Measuring financial risk with generalized asymmetric least squares regression}}, 
author = {Wang, Yongqiao and Wang, Shouyang and Lai, K.K.}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2011.02.018}, 
abstract = {{This paper proposes a generalized asymmetric least squares regression method to estimate Value-at-risk and expected shortfall. By solving an asymmetric least squares regression in a Reproducing Kernel Hilbert Space, the method achieves nonlinear prediction power, while making no assumption on the underlying probability distributions. Two toy datasets are used to demonstrate its nonlinear prediction power. The empirical results on the S\&P 500 stock index obviously show that the method is superior to other four benchmark methods. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {5793--5800}, 
number = {8}, 
volume = {11}
}
@article{10.1111/j.1468-5957.2011.02272.x, 
year = {2012}, 
title = {{Credit Rating Migration Risk and Business Cycles}}, 
author = {Fei, Fei and Fuertes, Ana‐Maria and Kalotychou, Elena}, 
journal = {Journal of Business Finance \& Accounting}, 
issn = {0306686X}, 
doi = {10.1111/j.1468-5957.2011.02272.x}, 
abstract = {{Basel III seeks to improve the financial sector's resilience to stress scenarios which calls for a reassessment of banks' credit risk models and, particularly, of their dependence on business cycles. This paper advocates a Mixture of Markov Chains (MMC) model to account for stochastic business cycle effects in credit rating migration risk. The MMC approach is more efficient and provides superior out-of-sample credit rating migration risk predictions at long horizons than a naïve approach that conditions deterministically on the business cycle phase. Banks using the MMC estimator would counter-cyclically increase capital by 6\% during economic expansion and free up to 17\% capital for lending during downturns relative to the naïve estimator. Thus, the MMC estimator is well aligned with the Basel III macroprudential initiative to dampen procyclicality by reducing the recession-versus-expansion gap in capital buffers. © 2012 Blackwell Publishing Ltd.}}, 
pages = {229--263}, 
number = {1-2}, 
volume = {39}
}
@article{10.21314/j0r.2016.324, 
year = {2016}, 
title = {{On optimal smoothing of density estimators obtained from orthogonal polynomial expansion methods}}, 
author = {Marumo, Kohei and Wolff, Rodney}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/j0r.2016.324}, 
abstract = {{We discuss the application of orthogonal polynomials to the estimation of probability density functions, particularly with regard to accessing features of a portfolio’s profit/loss distribution. Such expansions are given by the sum of known orthogonal polynomials multiplied by an associated weight function. However, naive applications of expansion methods are flawed. The shape of the estimator’s tail can undulate under the influence of the constituent polynomials in the expansion, and it can even exhibit regions of negative density. This paper presents techniques to remedy these flaws and improve the quality of risk estimation.We show that by targeting a smooth density that is sufficiently close to the target density, we can obtain expansion-based estimators that do not have the shortcomings of equivalent naive estimators. In particular, we apply optimization and smoothing techniques that place greater weight on the tails than on the body of the distribution. Numerical examples using both real and simulated data illustrate our approach. We further outline how our techniques can apply to a wide class of expansion methods and indicate opportunities to extend to the multivariate case, where distributions of individual component risk factors in a portfolio can be accessed for the purpose of risk management. © 2016 Incisive Risk Information (IP) Limited.}}, 
pages = {47--76}, 
number = {3}, 
volume = {18}
}
@article{10.1016/j.jmateco.2006.03.006, 
year = {2006}, 
title = {{Relevant coherent measures of risk}}, 
author = {Stoica, George}, 
journal = {Journal of Mathematical Economics}, 
issn = {03044068}, 
doi = {10.1016/j.jmateco.2006.03.006}, 
abstract = {{We introduce and study the f0-relevance property of a coherent measure of risk on a positions vector space with vector ordering. We show that it is equivalent to a special no arbitrage condition on bounded positions spaces. Continuity from below leads to representations of f0-relevant coherent measures of risk based on equivalent functionals in Banach subspaces of the order dual. We define and describe f0-martingales in a lattice, and present a solution to the hedging price problem: the asset price process is an order convergent f0-martingale. Under the f0-relevance hypothesis we study the relationship between worst conditional mean and value at risk. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {794--806}, 
number = {6}, 
volume = {42}
}
@article{10.1142/s0219024907004548, 
year = {2007}, 
title = {{A comparison of some univariate models for Value-at-Risk and expected shortfall}}, 
author = {MARINELLI, CARLO and D'ADDONA, STEFANO and RACHEV, SVETLOZAR T}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024907004548}, 
abstract = {{We compare in a backtesting study the performance of univariate models for Valueat-Risk (VaR) and expected shortfall based on stable laws and on extreme value theory (EVT). Analyzing these different approaches, we test whether the sum-stability assumption or the max-stability assumption, that respectively imply α-stable laws and Generalized Extreme Value (GEV) distributions, is more suitable for risk management based on VaR and expected shortfall. Our numerical results indicate that α-stable models tend to outperform pure EVT-based methods (especially those obtained by the so-called block maxima method) in the estimation of Value-at-Risk, while a peaks-over-threshold method turns out to be preferable for the estimation of expected shortfall. We also find empirical evidence that some simple semiparametric EVT-based methods perform well in the estimation of VaR. © World Scientific Publishing Company.}}, 
pages = {1043--1075}, 
number = {6}, 
volume = {10}
}
@article{10.1016/j.ifacol.2017.08.2471, 
year = {2017}, 
title = {{Control of Brokerage Margins}}, 
author = {Calafiore, Giuseppe C. and Massai, Leonardo}, 
journal = {IFAC-PapersOnLine}, 
issn = {24058963}, 
doi = {10.1016/j.ifacol.2017.08.2471}, 
abstract = {{In this paper we analyze financial risk from the point of view of a brokerage company, who exposes itself to risk by lending assets or money to its clients for allowing short-selling or leveraged operations (firm-wise risk). We develop analytical models for control of firm-wise risk, by defining both specific margin factors for single assets and a global margin factor that takes into account the overall riskiness of a complex portfolio. In the first part of this work we derive a model to evaluate leverage factors, by linking them with the probability for the client's portfolio value to go below a certain safety threshold, using Value-at-Risk and Expected Shortfall approaches. Further, we present optimization models based on these two approaches in order to determine the optimal leverage factors. In the second part, we present a model for margin control based on the concept of marginal availability. A global margin factor considering the overall riskiness of a complex portfolio is derived, and we show the effectiveness of the approach also when dealing with portfolios containing options. © 2017}}, 
pages = {12279--12284}, 
number = {1}, 
volume = {50}
}
@article{10.1155/2020/3061298, 
year = {2020}, 
title = {{Pareto-Optimal Reinsurance Revisited: A Two-Stage Optimisation Procedure Approach}}, 
author = {Fang, Ying and Wang, Lu and Qu, Zhongfeng and Yu, Wenguang}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2020/3061298}, 
abstract = {{In this paper, based on the Tail-Value-at-Risk (TVaR) measure, we revisit the Pareto-optimal reinsurance policies for the insurer and the reinsurer via a two-stage optimisation procedure. To reduce ex-post moral hazard, we assume that reinsurance contracts satisfy the principle of indemnity and the incentive compatible constraint which have been advocated by Huberman et al. (1983). We show that the Pareto-optimal reinsurance policy exists if the reinsurance premiums can be expressed as an integral form. The proposed class of premium principles encompasses the net premium principle, expected value premium principle, TVaR premium principle, generalized percentile premium principle, and so on. We further use the TVaR premium principle and the expected value premium principle as examples to illustrate the two-stage optimisation procedure by deriving explicitly the Pareto-optimal reinsurance policies. We extend the results by Cai et al. (2017) when the expected value premium principle is replaced by the TVaR premium principle. © 2020 Ying Fang et al.}}, 
pages = {1--16}, 
number = {NA}, 
volume = {2020}
}
@article{10.1016/j.ijforecast.2019.07.003, 
year = {2020}, 
title = {{Semi-parametric dynamic asymmetric Laplace models for tail risk forecasting, incorporating realized measures}}, 
author = {Gerlach, Richard and Wang, Chao}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2019.07.003}, 
abstract = {{This paper extends the joint Value-at-Risk (VaR) and expected shortfall (ES) quantile regression model of Taylor (2019), by incorporating a realized measure to drive the tail risk dynamics, as a potentially more efficient driver than daily returns. Furthermore, we propose and test a new model for the dynamics of the ES component. Both a maximum likelihood and an adaptive Bayesian Markov chain Monte Carlo method are employed for estimation, the properties of which are compared in a simulation study. The results favour the Bayesian approach, which is employed subsequently in a forecasting study of seven financial market indices. The proposed models are compared to a range of parametric, non-parametric and semi-parametric competitors, including GARCH, realized GARCH, the extreme value theory method and the joint VaR and ES models of Taylor (2019), in terms of the accuracy of one-day-ahead VaR and ES forecasts, over a long forecast sample period that includes the global financial crisis in 2007–2008. The results are favorable for the proposed models incorporating a realized measure, especially when employing the sub-sampled realized variance and the sub-sampled realized range. © 2019 International Institute of Forecasters}}, 
pages = {489--506}, 
number = {2}, 
volume = {36}
}
@article{10.1016/j.ejor.2011.06.023, 
year = {2011}, 
title = {{R\&D pipeline management: Task interdependencies and risk management}}, 
author = {Colvin, Matthew and Maravelias, Christos T.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2011.06.023}, 
abstract = {{Maintaining a rich research and development (R\&D) pipeline is the key to remaining competitive in many industrial sectors. Due to its nature, R\&D activities are subject to multiple sources of uncertainty, the modeling of which is compounded by the ability of the decision maker to alter the underlying process. In this paper, we present a multi-stage stochastic programming framework for R\&D pipeline management, which demonstrates how essential considerations can be modeled in an efficient manner including: (i) the selection and scheduling of R\&D tasks with general precedence constraints under pass/fail uncertainty, and (ii) resource planning decisions (expansion/contraction and outsourcing) for multiple resource types. Furthermore, we study interdependencies between tasks in terms of probability of success, resource usage and market impact. Finally, we explore risk management approaches, including novel formulations for value at risk and conditional value at risk. © 2011 Published by Elsevier B.V. All rights reserved.}}, 
pages = {616--628}, 
number = {3}, 
volume = {215}
}
@article{10.1080/03610918.2019.1699572, 
year = {2019}, 
title = {{Bayesian predictive analysis for Weibull-Pareto composite model with an application to insurance data}}, 
author = {Deng, M. and Aminzadeh, M. S.}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2019.1699572}, 
abstract = {{Aminzadeh and Deng, respectively provide Bayesian predictive models for Exponential-Pareto and Inverse Gamma-Pareto composite distributions which are one-parameter models. The purpose of this article is to develop an alternative Bayesian predictive model (two-parameter) which can be used to compute important risk measures that are not defined via the above predictive models. Bayesian predictive density for the Weibull-Pareto composite distribution is developed and is used to compute risk measures such as Value at Risk (VaR), Conditional Tail Expectation (CTE), Predictive Expectation (PE), Limited Predictive Expected value (LPE), Limited Predictive Variance (LPV), and Limited Predictive Tail-VaR (LPCTE). Accuracy of parameter estimates as well as the risk measures are assessed via simulation studies. It is shown that the informative Bayes estimates are consistently more accurate than ML and the non-informative Bayes estimates. Backtesting for the risk measures is performed and goodness-of-fit of Weibull-Pareto among other composite models to the Danish fire data is assessed. © 2019, © 2019 Taylor \& Francis Group, LLC.}}, 
pages = {1--27}, 
number = {NA}, 
volume = {NA}
}
@article{10.1515/snde-2013-0123, 
year = {2015}, 
title = {{Estimating dynamic copula dependence using intraday data}}, 
author = {Grossmass, Lidan and Poon, Ser-Huang}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2013-0123}, 
abstract = {{We estimate the dynamic daily dependence between assets by applying the Semiparametric Copula-Based Multivariate Dynamic (SCOMDY) model on intraday data. Using tick data of three stock returns of the period before and during the credit crisis, we find that our dependence estimator better captures the steep increase in dependence during the onset of the crisis as compared to other commonly used time-varying copula methods. Like other high-frequency estimators, we find that the dependence estimator exhibits long memory and forecast it using a HAR model. We show that for out-of-sample forecasts, our dependence estimator performs better than the constant estimator and other commonly used time-varying copula dependence estimators. © 2015 by De Gruyter.}}, 
pages = {501--529}, 
number = {4}, 
volume = {19}
}
@article{10.1504/aajfa.2015.069879, 
year = {2015}, 
title = {{Volatility forecasting and risk management in some MENA stock markets: A nonlinear framework}}, 
author = {Aloui, Chaker}, 
journal = {Afro-Asian J. of Finance and Accounting}, 
issn = {17516447}, 
doi = {10.1504/aajfa.2015.069879}, 
abstract = {{In this paper, we estimate the value-at-risk (VaR) for some Middle East and North African emerging stock markets (Egypt, Israel, Turkey and Morocco) for the short and the long trading positions. We check whether considering for LM, asymmetries and fat-tails in the stock return's behaviour offer more accurate VaR forecasts. We compute the VaR for two ARCH/GARCH-type models including FIGARCH and FIAPARCH under two density functions: student and skewed student. The obtained results point out that that accounting for long dependence in return and volatility, fat-tails and asymmetry provides better one-day-ahead VaR forecasts. Furthermore, the FIAPARCH model out-performs the other models in the VaR forecasts. Finally, the FIAPARCH model provides for all the stock market indexes the lowest number of violations under the Basel II rules, given a risk exposure at the 99\% confidence level. Our results offer potential implications for MENA stock markets risk quantifications, policy regulations and hedging strategies. Copyright © 2015 Inderscience Enterprises Ltd.}}, 
pages = {160}, 
number = {2}, 
volume = {5}
}
@article{10.1016/j.iimb.2021.03.011, 
year = {2021}, 
title = {{Forecasting gains by using extreme value theory with realised GARCH filter}}, 
author = {Paul, Samit and Sharma, Prateek}, 
journal = {IIMB Management Review}, 
issn = {09703896}, 
doi = {10.1016/j.iimb.2021.03.011}, 
abstract = {{Early empirical evidence suggests that the realised generalised autoregressive conditional heteroskedasticity (GARCH) model provides significant forecasting gains over the standard GARCH models in volatility forecasting. We extend this literature in quantile forecasting by implementing conditional extreme value theory (EVT) framework with realised GARCH. We generate one-step-ahead value-at-risk (VaR) and expected shortfall (ES) forecasts for the S\&P CNX NIFTY index using 14 standalone GARCH and GARCH-EVT models. In out-of-sample comparisons, the GARCH-EVT specification generally outperforms the standalone GARCH models. In general, the realised-GARCH EVT models provide the best forecasting performance. This finding is robust to the choice of different realised volatility estimators used to estimate realised GARCH. © 2021}}, 
pages = {64--70}, 
number = {1}, 
volume = {33}
}
@article{10.1504/ijmef.2008.019218, 
year = {2008}, 
title = {{Predictive performance of conditional Extreme Value Theory in Value-at-Risk estimation}}, 
author = {Ghorbel, Ahmed and Trabelsi, Abdelwahed}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2008.019218}, 
abstract = {{This paper conducts a comparative evaluation of the predictive performance of various Value-at-Risk (VaR) models. Special emphasis is paid to two methodologies related to the Extreme Value Theory (EVT): The Peaks Over Threshold (POT) and the Block Maxima (BM). We apply both unconditional and conditional EVT models to management of extreme market risks in stock markets. They are applied on daily returns of the BVMT and CAC 40 indices with the intention to compare the performance of various estimation methods on markets with different capitalisation and trading practices. The results we report demonstrate that conditional POT-EVT method produces the most accurate forecasts of extreme losses both for standard and more extreme VaR quantiles. The conditional block maxima EVT method is less accurate. © 2008 Inderscience Enterprises Ltd.}}, 
pages = {121}, 
number = {2}, 
volume = {1}
}
@article{10.1016/j.pacfin.2014.07.005, 
year = {2014}, 
title = {{Risk contributions of trading and non-trading hours: Evidence from Chinese commodity futures markets}}, 
author = {Liu, Qingfu and An, Yunbi}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2014.07.005}, 
abstract = {{This paper examines the overall risks in Chinese copper, rubber, and soybean futures markets using a copula-VaR (value at risk) and copula-ES (expected shortfall) framework that explicitly accounts for both trading and non-trading information. Our results show that information accumulating during non-trading hours contributes substantially to overall risks, with non-trading VaR weights exceeding 40\% in all these markets. In particular, the information during non-trading hours is more important than the information during trading hours in explaining the total risk of all three futures as measured by ESs and volatility weights. Moreover, the risk due to non-trading information increases with the length of non-trading periods, reflecting the fact that information accumulates continuously over time. © 2014 Elsevier B.V.}}, 
pages = {17--29}, 
number = {NA}, 
volume = {30}
}
@article{10.1007/s00791-007-0073-x, 
year = {2007}, 
title = {{VaR and ES for linear portfolios with mixture of elliptic distributions risk factors}}, 
author = {Kamdem, Jules Sadefo}, 
journal = {Computing and Visualization in Science}, 
issn = {14329360}, 
doi = {10.1007/s00791-007-0073-x}, 
abstract = {{In this paper, we generalize the Linear VaR method from portfolios with normally distributed risk factors to portfolios with mixture of elliptically distributed ones. We treat both the Expected Shortfall and the Value-at-Risk of such portfolios. Special attention is given to the particular case of a mixture of multivariate t-distributions. © 2007 Springer-Verlag.}}, 
pages = {197--210}, 
number = {4}, 
volume = {10}
}
@article{10.1007/s00181-012-0588-y, 
year = {2013}, 
title = {{Robust estimation of the simplified multivariate GARCH model}}, 
author = {Iqbal, Farhat}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-012-0588-y}, 
abstract = {{In this paper, robust M-estimation of multivariate GARCH models are considered. The simplified GARCH model is chosen that involves the estimation of only univariate GARCH models, and hence easy to estimate, and does not put additional constraints on the model. The results of Monte Carlo simulations showed that accurate estimates of conditional correlations can be obtained using these robust estimators when the errors are heavy-tailed. We also investigate the forecasting performance of the class of robust estimators in predicting value-at-risk using various evaluation measures and collect empirical evidences of the better predictive potential of estimators such as LAD and B-estimator over the widely-used quasi-maximum likelihood estimator for the estimation and prediction of multivariate GARCH models. Applications to real data sets are also presented. © 2012 Springer-Verlag.}}, 
pages = {1353--1372}, 
number = {3}, 
volume = {44}
}
@article{10.1007/978-3-319-27284-9_22, 
year = {2016}, 
title = {{Modeling co-movement and risk management of gold and silver spot prices}}, 
author = {Yang, Chen and Sriboonchitta, Songsak and Sirisrisakulchai, Jirakom and Liu, Jianxu}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-319-27284-9\_22}, 
abstract = {{This paper aims to model volatility and correlation dynamics in spot price returns of gold and silver, and examines the corresponding market risk management implications. VaR (value at risk) and ES (expected shortfall) are used to analyze the market risk associated with investments in gold and silver. Many GARCH family models are employed to describe the volatility. This work applied the copula based-GARCH model in the estimation of a portfolio VaR and ES composed of gold and silver spot prices. The empirical results exhibit that the NAGARCH and the TGARCH families performed better than other GARCH family members in describing the volatility of gold and silver returns, respectively. Furthermore, the time-varying T copula has the most appropriate performance in capturing the dependence structure between gold and silver returns. The out-of-sample forecast performance indicates that the time-varying T copula-based GARCH model can measure the VaR and ES with the accurate estimates of gold and silver. © Springer International Publishing Switzerland 2016.}}, 
pages = {347--362}, 
number = {NA}, 
volume = {622}
}
@article{10.1007/s10479-017-2547-7, 
year = {2018}, 
title = {{When is tail mean estimation more efficient than tail median? Answers and implications for quantitative risk management}}, 
author = {Barnard, Roger W. and Pearce, Kent and Trindade, A. Alexandre}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-017-2547-7}, 
abstract = {{We investigate the relative efficiency of the empirical “tail median” versus “tail mean” as estimators of location when the data can be modeled by an exponential power distribution (EPD), a flexible family of light-tailed densities. By considering appropriate probabilities so that the quantile of the untruncated EPD (tail median) and mean of the left-truncated EPD (tail mean) coincide, limiting results are established concerning the ratio of asymptotic variances of the corresponding estimators. The most remarkable finding is that in the limit of the right tail, the asymptotic variance of the tail median estimate is approximately 36\% larger than that of the tail mean, irrespective of the EPD shape parameter. This discovery has important repercussions for quantitative risk management practice, where the tail median and tail mean correspond to value-at-risk and expected shortfall, respectively. To this effect, a methodology for choosing between the two risk measures that maximizes the precision of the estimate is proposed. From an extreme value theory perspective, analogous results and procedures are discussed also for the case when the data appear to be heavy-tailed. © 2017, Springer Science+Business Media New York.}}, 
pages = {47--65}, 
number = {1}, 
volume = {262}
}
@article{10.1007/s40995-018-0484-1, 
year = {2019}, 
title = {{Conditional ASGT-GARCH Approach to Value-at-Risk}}, 
author = {Altun, Emrah and Tatlıdil, Hüseyin and Özel, Gamze}, 
journal = {Iranian Journal of Science and Technology, Transactions A: Science}, 
issn = {10286276}, 
doi = {10.1007/s40995-018-0484-1}, 
abstract = {{Most of the Value-at-Risk (VaR) models assume that asset returns are normally distributed, despite the fact that they are commonly known to be left skewed, fat-tailed and excess kurtosis. Forecasting VaR with misspecified model leads to the underestimation or overestimation of the true VaR. This paper proposes a new conditional model to forecast VaR by employing the alpha-skew generalized T (ASGT) distribution to GARCH models. ASGT distribution, introduced by Acitas et al. (Revista Colombiana de Estadistica 38(2):353–370, 2015), allows to model skewness, leptokurtosis and fat tail properties of conditional distribution of asset returns. ISE-100 index is used to examine the one-day-ahead VaR forecasting ability of the GARCH model under normal, Student’s t, generalized error, generalized T, skewed generalized T and ASGT innovation distributions. Empirical results show that the ASGT provides a superior fit to the conditional distribution of the log-returns followed by normal, Student’s t, generalized error, generalized T and skewed generalized T distributions. Moreover, for all confidence levels, all models tend to underestimate real market risk. Furthermore, the GARCH-based model, with ASGT error distribution, generates the most reliable VaR forecasts followed by other competitive models for a long position. As a result of this study, we conclude that the effects of skewness and fat-tails are more important in terms of forecasting true VaR than only the effect of fat-tails on VaR forecasts. © 2018, Shiraz University.}}, 
pages = {239--247}, 
number = {1}, 
volume = {43}
}
@article{10.1016/j.ejor.2011.07.029, 
year = {2012}, 
title = {{Gas storage valuation applying numerically constructed recombining trees}}, 
author = {Felix, Bastian Joachim and Weber, Christoph}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2011.07.029}, 
abstract = {{The liberalization of European natural gas markets forces market participants to base their decisions on market prices. For owners and operators of natural gas storage facilities it is therefore necessary to take market prices into account for their decisions. In this framework this paper provides a new approach for the valuation of natural gas storage facilities. Using stochastic dynamic programming on multinomial recombining trees, the optimal storage strategy and value are determined. For this we (i) estimate the deterministic and random impacts on natural gas prices, (ii) simulate gas prices considering the results of the first step, (iii) construct numerically the recombining tree using the simulation results, (iv) determine the optimal storage strategy and value. Besides the determination of the optimal storage value and operation schedule the value quantiles are calculated. Via the quantiles relevant risk measures like value at risk and conditional value at risk are determined. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {178--187}, 
number = {1}, 
volume = {216}
}
@article{10.1016/j.insmatheco.2004.04.002, 
year = {2004}, 
title = {{An optimization approach to the dynamic allocation of economic capital}}, 
author = {Laeven, Roger J.A. and Goovaerts, Marc J.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2004.04.002}, 
abstract = {{We propose an optimization approach to allocating economic capital, distinguishing between an allocation or raising principle and a measure for the risk residual. The approach is applied both at the aggregate (conglomerate) level and at the individual (subsidiary) level and yields an integrated solution to the capital allocation problem. In particular, we formalize a procedure to determine (i) the optimal amount of economic capital to be held by a financial conglomerate, (ii) the optimal allocation of this amount among the subsidiaries and (iii) a consistent distribution of the cost of risk-bearing borne by the conglomerate. Different degrees of information on the dependence structure between the subsidiaries are considered. The results provide a theoretical justification for the use of Value-at-Risk, not as a measure of risk, but as an optimal allocation or raising principle. Static solutions are generalized to a dynamic setting. The approach is illustrated using an example of a financial conglomerate represented by a multivariate Wiener process. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {299--319}, 
number = {2 SPEC. ISS.}, 
volume = {35}
}
@article{10.1016/j.intfin.2019.02.007, 
year = {2019}, 
title = {{Systematic extreme downside risk}}, 
author = {Harris, Richard D.F. and Nguyen, Linh H. and Stoja, Evarist}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2019.02.007}, 
abstract = {{We propose new systematic tail risk measures constructed using two different approaches. The first is a non-parametric measure that captures the tendency of a stock to crash at the same time as the market, while the second is based on the sensitivity of stock returns to innovations in market crash risk. Both tail risk measures are associated with a significantly positive risk premium after controlling for other measures of downside risk, including downside beta, coskewness and cokurtosis. Using the new measures, we examine the relevance for investors of the tail risk premium over different horizons. © 2019 Elsevier B.V.}}, 
pages = {128--142}, 
number = {NA}, 
volume = {61}
}
@article{10.1007/s00500-016-2326-4, 
year = {2018}, 
title = {{Application of efficient metaheuristics to solve a new bi-objective optimization model for hub facility location problem considering value at risk criterion}}, 
author = {Ghezavati, Vahidreza and Hosseinifar, Parisa}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-016-2326-4}, 
abstract = {{In this paper, a new bi-objective hub facility location problem is studied where the demands of customers are stochastic and follow a normal distribution function. The first objective is to minimize total amount of value at risk which has a new probabilistic criterion and optimizes amount of lost demand. In addition, the second objective minimizes total costs of the network. Furthermore, some constraints are nonlinear in this model and it is aimed to linearize them by efficient approximate procedures. This paper tries to apply three efficient solution procedures to solve the bi-objective model. Thus, three algorithms are ε -constraint, non-dominated sorting genetic algorithm-II (NSGA-II), and multi-objective particle swarm optimizers (MOPSO) which are creativity ways used in this paper to approximate the Pareto-optimal solutions. Taguchi experimental design is used to find the right parameter settings for metaheuristic algorithms. A comparative study of three proposed algorithms demonstrates the most effectiveness algorithm with respect to five existing performance measures for numerous test problems. Finally, the comparison within each two algorithms is completed by applying multiple statistical tests and diagrams. The obtained solutions by MOPSO are better than NSGA-II at 95 \% confidence level. © Springer-Verlag Berlin Heidelberg 2016.}}, 
pages = {195--212}, 
number = {1}, 
volume = {22}
}
@article{10.1016/j.mcm.2011.11.070, 
year = {2012}, 
title = {{Risk measures and behaviors for bonds under stochastic interest rate models}}, 
author = {Song, Na and Siu, Tak Kuen and Fard, Farzad Alavi and Ching, Wai-Ki and Fung, Eric S.}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/j.mcm.2011.11.070}, 
abstract = {{This paper develops a model for measuring the risk inherent from trading a bond position under some important stochastic interest rate models. We employ the value at risk (VaR) and expected shortfall (ES) as proxies for the extreme risk inherent from trading a bond position. In particular, we concern ourselves with the average tail behavior of the real-world profit/loss distribution for a bond position. We investigate the risk behaviors of a bond position under some stochastic interest rate models including the Merton model, the Vasicek model, and the Cox-Ingersoll-Ross (CIR) model. © 2011 Elsevier Ltd.}}, 
pages = {204--217}, 
number = {9-10}, 
volume = {56}
}
@article{10.1007/978-3-319-19857-6_57, 
year = {2015}, 
title = {{Building a sensitivity-based portfolio selection models}}, 
author = {Zhang, Huiming and Watada, Junzo and Li, Ye and Li, You and Wang, Bo}, 
journal = {Smart Innovation, Systems and Technologies}, 
issn = {21903018}, 
doi = {10.1007/978-3-319-19857-6\_57}, 
abstract = {{Sensitivity Analysis is a method to evaluate the influence of each variable change. In portfolio selection model, it is essential to evaluate the sensitivity of each stock or security return rate in investment decision making. Investors look for selecting stable stocks or securities. For this purpose, sensitivity analysis should play a pivotal role. It is important for the decision-making to get both sensitivity and stability of each selection. This paper proposes a new portfolio-selection model (PSM) called the sensitivity-based portfolio selection models (SPSM). The SPSM model will focus on the sensitivity of the selected portfolio. In order to analyze the sensitivity of portfolio selection models, a sensitivity analysis will be introduced for calculating out insensitive stocks or securities with maximum return and minimum risk. Abstract environment. © Springer International Publishing Switzerland 2015.}}, 
pages = {673--681}, 
number = {NA}, 
volume = {39}
}
@article{10.1007/s00500-018-3416-2, 
year = {2019}, 
title = {{Intuitionistic fuzzy inverse 1-median location problem on tree networks with value at risk objective}}, 
author = {Soltanpour, Akram and Baroughi, Fahimeh and Alizadeh, Behrooz}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-018-3416-2}, 
abstract = {{The inverse p-median location problem on networks is to modify the parameters of the original problem at minimum total cost with respect to given modification bounds such that a prespecified set of p vertices becomes a p-median with respect to new parameters. Therefore, the inverse p-median location problem is a decision-making problem in which decision-makers’ knowledge of the modification costs may be vague and imprecise. In this paper, we investigate the inverse 1-median location problem on tree networks with intuitionistic fuzzy weight modification costs. We first propose the new concepts of the credibilistic value at risk and conditional value at risk metrics in an intuitionistic fuzzy environment. Then we prove that these metrics satisfy in the harmonious risk metric properties. Finally, we solve the inverse 1-median location problem with intuitionistic fuzzy weight modification costs on tree networks and obtain its value at risk function in O(n2log n) time. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {7843--7852}, 
number = {17}, 
volume = {23}
}
@article{10.1287/mnsc.1060.0632, 
year = {2007}, 
title = {{Proper conditioning for coherent VaR in portfolio management}}, 
author = {Garcia, René and Renault, Éric and Tsafack, Georges}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1060.0632}, 
abstract = {{Value at risk (VaR) is a central concept in risk management. As stressed by Artzner et al. (1999, Coherent measures of risk, Math. Finance 9(3) 203-228), VaR may not possess the subadditivity property required to be a coherent measure of risk. The key idea of this paper is that, when tail thickness is responsible for violation of subadditivity, eliciting proper conditioning information may restore VaR rationale for decentralized risk management. The argument is threefold. First, since individual traders are hired because they possess a richer information on their specific market segment than senior management, they just have to follow consistently the prudential targets set by senior management to ensure that decentralized VaR control will work in a coherent way. The intuition is that if one could build a fictitious conditioning information set merging all individual pieces of information, it would be rich enough to restore VaR subadditivity. Second, in this decentralization context, we show that if senior management has access ex post to the portfolio shares of the individual traders, it amounts to recovering some of their private information. These shares can be used to improve backtesting to check that the prudential targets have been enforced by the traders. Finally, we stress that tail thickness required to violate subadditivity, even for small probabilities, remains an extreme situation because it corresponds to such poor conditioning information that expected loss appears to be infinite. We then conclude that lack of coherence of decentralized VaR management, that is VaR nonsubadditivity at the richest level of information, should be an exception rather than a rule. © 2007 INFORMS.}}, 
pages = {483--494}, 
number = {3}, 
volume = {53}
}
@article{10.1109/pct.2007.4538422, 
year = {2007}, 
title = {{Lattice method of real option analysis - Solving the curse of dimensionality and strategic planning}}, 
author = {Teoh, C. C. and Sheblè, G. B.}, 
journal = {2007 IEEE Lausanne Power Tech}, 
issn = {NA}, 
doi = {10.1109/pct.2007.4538422}, 
abstract = {{The deregulation policy introduces uncertainties into the power market. The power market uncertainties have increased the significance of two factors in decision analysis: financial risks and managerial flexibility. Real Option Analysis enables such flexibility to management. There are several major methods under Real Option Analysis: traditional Black-Scholes Option-Pricing Method, Lattice (Binomial and Trinomial) Methods, Monte Carlo Simulation Method, and Finite Element (Explicit, Implicit, and Crank-Nicolson) Method. This paper concentrates on the lattice method. Lattice model is easy to implement, appreciate and understand. However, when the investment duration is large (or the length of model period - step size is small), the lattice model becomes a massive bush of lattice, which is known as the curse of dimensionality. This paper proposes a new efficient methodology of solving the curse of dimensionality for the lattice model. The massive bush of lattice model can be reduced by analyzing the boundary of the lattice where the decision changes. This can be achieved via the implementation of value at risk into the lattice model. Besides reducing the degree of dimensionality, this new methodology also specifies "when" a decision changes. This is a very critical part in strategic budgeting planning. Timing and simplification yet maintaining high accuracy in analysis are essential in the new deregulated power economic uncertainties. ©2007 IEEE.}}, 
pages = {825--830}, 
number = {NA}, 
volume = {NA}
}
@article{10.3233/jifs-169216, 
year = {2017}, 
title = {{Uncertain random portfolio optimization models based on value-at-risk}}, 
author = {Qin, Zhongfeng and Dai, Yuanzhen and Zheng, Haitao}, 
journal = {Journal of Intelligent \& Fuzzy Systems}, 
issn = {10641246}, 
doi = {10.3233/jifs-169216}, 
abstract = {{This paper studies a portfolio optimization problem in which some candidate securities possess sufficient transaction data and the others are newly listed and lack enough data. Their corresponding returns are assumed to be random variables and uncertain variables, respectively. Accordingly, the total return on a portfolio becomes an uncertain random variable. In this paper, we first define value-at-risk of uncertain random variable and discuss its mathematical properties as well as numerical solution procedure. Then we employ it to measure the risk associated with uncertain random returns and formulate the corresponding portfolio optimization models with uncertain random returns. An active-set method is used to solve the proposed models and a numerical example is given to illustrate its application. © 2017-IOS Press and the authors. All rights reserved.}}, 
pages = {4523--4531}, 
number = {6}, 
volume = {32}
}
@article{10.1063/5.0059048, 
year = {2021}, 
title = {{Optimal reinsurance combination of quota-share and stop-loss reinsurance based on conditional-tail-expectation (CTE) optimization}}, 
author = {Orvin, Y. and Nurrohmah, S. and Fithriani, I.}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/5.0059048}, 
abstract = {{To maintain financial stability and to effectively manage risk, an insurer will partially reinsure the loss to a reinsurance company. Two of the most often used reinsurance contracts are quota-share and stop-loss. In quota-share, the loss will be split based on a fixed proportion and the reinsurance premium depends on the value of the proportion, while in stop-loss the loss will be split depending on the retention value. In the hope that these two types of reinsurance can cover each other weaknesses, this study combines both quota-share and stop-loss reinsurance. Subsequently, to get a good coverage for the insurer, it is necessary to find the optimal proportion and retention value. One way to do so is using risk measure optimization. The smaller the value of the risk measure, the smaller the loss borne by the insurer. The risk measure used in this paper is Conditional-Tail-Expectation (CTE), where it involves Value-at-Risk (VaR) in its calculation. Calculated using the expected value principle, the reinsurance premium is used as a constraint in the CTE optimization for each of the reinsurance combinations, which are stop-loss after quota-share and quota-share after stop-loss. By optimizing CTE, it is found that each combination produces the same minimal CTE, so both reinsurance combinations are optimal for use by the insurer. By using different distributions, it is seen that the minimal CTE depends on the distribution's tail behavior. Furthermore, in determining the minimal value, the conditions that are used in optimization using CTE are different from VaR. © 2021 Author(s).}}, 
pages = {030014}, 
number = {NA}, 
volume = {2374}
}
@article{10.1002/ijfe.2214, 
year = {2020}, 
title = {{A new risk measurement method for China's carbon market}}, 
author = {Yang, Xianzi and Zhang, Chen and Yang, Yu and Wang, Wenjun and Wagan, Zulfiqar Ali}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.2214}, 
abstract = {{Carbon markets were set up with the aim to achieve carbon reduction target and sustainable development. However, market risk has become one of the key factors influencing continuous development of carbon markets. Different from traditional financial asset price, carbon price has a heterogeneous characteristic in its tail distribution. The current value at risk (VaR) model with student t or generalized error distribution (GED) cannot describe the asymmetric tail distribution of carbon price. Therefore, this article propose to develop a combined model for China's carbon market risk measurement. First, extend generalized autoregressive conditional heteroscedasticity (GARCH) with standardized standard asymmetric exponential power distribution (SSAEPD) to reflect volatility clustering phenomenon and heterogeneous distribution character of China's carbon price. Then, genetic algorithm (GA) was innovatively used to solve GARCH-SSAEPD linear programming instead of interior-point algorithm. Finally, use VaR to measure the carbon market risk. The new model (GARCH-SSAEPD-GA-VaR) is implied to China's carbon market and compared with the traditional GARCH-VaR model, the empirical results show: (a) Compared with current VaR framework, the GARCH-SSAEPD-GA-VaR model we constructed can help describe the heterogeneous tail distribution of carbon price and help increase the precision of carbon market risk measurement. (b) SSAEPD can capture fat-tail, asymmetric effects of China's carbon price more entirely, which puts forward a new method to study the evolvement laws of carbon market risk. (c) GA is effective to achieve global optimum to some extent in parameter estimation. This study contribute to developing theory and methodology of describing particularity features of carbon price, increasing the accuracy of carbon market risk measurement and provide a new perspective for investigating the evolvement regularity of China's carbon market risks. © 2020 John Wiley \& Sons Ltd}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.trpro.2017.03.032, 
year = {2017}, 
title = {{Rapid transit network design: Considering recovery robustness and risk aversion measures}}, 
author = {Cadarso, Luis and Codina, Esteve and Escudero, Laureano F. and Marín, Angel}, 
journal = {Transportation Research Procedia}, 
issn = {23521457}, 
doi = {10.1016/j.trpro.2017.03.032}, 
abstract = {{Rapid transit network design is highly dependent on the future system usage. These spatially distributed systems are vulnerable to disruptions: during daily operations different incidents may occur. Despite the unpredictable nature of them, effective mitigation methods from an engineering perspective should be designed. In this paper, we present several risk averse measures for risk reduction in the rapid transit network design problem based on a set of finite scenarios to represent the disruptions' uncertainty. As a counter-parts of the typical risk neutral strategy, some measures that are presented are aiming to minimizing the impacts of the worst scenario in the network operation, and another additionally, takes into account different risk reduction profiles. Some computational experience is presented. © 2017 The Authors. Published by Elsevier B.V.}}, 
pages = {255--264}, 
number = {NA}, 
volume = {22}
}
@article{10.3969/j.issn.0253-2778.2019.08.010, 
year = {2019}, 
title = {{Dynamic correlation of quantile regression model based on smooth transition mechanism}}, 
author = {}, 
issn = {02532778}, 
doi = {10.3969/j.issn.0253-2778.2019.08.010}, 
abstract = {{A quantile regression model was built under the smooth transition mechanism with the market volatility index (VIX) as smooth transition variable to study the non-linear effects of the US stock market on most of the global stock markets. The transition position of the smoothing mechanism model can describe the sensitive point of global stock market to the US stock market, and the transition slope describes the conversion rate of interconnectedness. The empirical results show that there does exist nonlinear mechanism transformation in the correlation of international stock markets, and that almost all global stock markets are subject to the impact of the US stock market. Moreover, under different quantiles, the conversion rates between different mechanism are not identical. Evident difference is found under low quantiles, which suggests that the correlation between financial markets is mainly due to tail- risk conduction. Then the collected data was divided into three sub samples and were studied respectively using the proposed model. The results show that there is a significant difference between position parameters during and after the crisis. During the crisis the position parameters decreased, and the correlation improved significantly under low quantiles, indicating that the proposed model is feasible to study the dynamic correlation between financial markets and that the exogenous variable VIX has a considerable influence on the correlation between financial markets. This provides a new perspective for international investors and policy makers to consider the impact of the US economy on global equity markets with the help of the VIX. © 2019, Editorial Department of Journal of University of Science and Technology of China. All rights reserved.}}, 
number = {8}, 
volume = {49}
}
@article{10.1108/jiabr-01-2014-0004, 
year = {2017}, 
title = {{Analysis of value at risk of Sukuk and conventional bonds in Pakistan}}, 
author = {Nasir, Adeel and Farooq, Umar}, 
journal = {Journal of Islamic Accounting and Business Research}, 
issn = {17590817}, 
doi = {10.1108/jiabr-01-2014-0004}, 
abstract = {{Purpose: The purpose of this paper is to provide empirical evidence that Sukuk are different from conventional bonds from risk perspective. This study is about the comparative risk analysis of Sukuk and conventional bonds in Pakistan. Design/methodology/approach: Sample consists of 15 Sukuk and 30 Term Finance Certificates issued in Pakistan. Value at risk is deployed by using delta normal approach to calculate risk. Two portfolios are formed separately with equal investment of ₹3m to explore the maximum loss an investor would have in portfolio of Sukuk and conventional bonds separately. Findings: Results revealed that Sukuk are less risky and more stable instrument as compared to conventional bonds. Risk and stability of Sukuk are explained with diversification theory and liquidity perspective. It is found that correlation among most of Sukuk securities are less or negative, which help in diversifying their risk. However, the attribute of stability can be due to the few days of trading in case of Sukuk comparatively. Originality/value: Literature has explored the operational differences between conventional and Islamic bonds on theoretical basis. However, few studies explain their differences empirically especially with respect to risk in case of Pakistan where debt market is developing. Therefore, the originality of this research lies within its comparative investigation of risk for two securities that are different from their operational perspectives. © 2017, © Emerald Publishing Limited.}}, 
pages = {375--388}, 
number = {4}, 
volume = {8}
}
@article{10.21314/jop.2015.155, 
year = {2015}, 
title = {{Bayesian operational risk models}}, 
author = {Figini, Silvia and Gao, Lijun and Giudici, Paolo}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2015.155}, 
abstract = {{Operational risk is hard to quantify, due to the presence of heavy tailed loss distributions. Extreme value distributions, used in this context, are very sensitive to the data, and this is a problem in the presence of rare loss data. Self-assessment questionnaires for risk, if properly structured, may provide prior opinions that can be used, in a Bayesian analysis, to better estimate operational risks. In this paper we propose a methodology to frame risk self-assessment data into suitable prior distributions that, updated with the observed loss data, can produce posterior distributions from which accurate operational risk measures, such as value-at-risk (VaR), can be obtained. We test our proposed model on a real database, made up of internal loss data and risk selfassessment questionnaires of an anonymous commercial bank. Our results show that the proposed Bayesian models perform better with respect to classical extreme value models, leading to a smaller quantification of the VaR required to cover unexpected losses. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {45--60}, 
number = {2}, 
volume = {10}
}
@article{10.1177/8756972818810967, 
year = {2019}, 
title = {{Planning, Tracking, and Reducing a Complex Project’s Value at Risk}}, 
author = {Browning, Tyson R.}, 
journal = {Project Management Journal}, 
issn = {87569728}, 
doi = {10.1177/8756972818810967}, 
abstract = {{Uncertainty, risk, and rework make it extremely challenging to meet goals and deliver anticipated value in complex projects, and conventional techniques for planning and tracking earned value do not account for these phenomena. This article presents a methodology for planning and tracking cost, schedule, and technical performance (or quality) in terms of a project’s key value attributes and threats to them. It distinguishes four types of value and two general types of risks. The “high jumper” analogy helps to consider how high the “bar” is set for a project (its set goals) and therefore how challenging and risky it will be. A project’s capabilities as a “jumper” (to clear the bar and meet its goals) determine the portion of its value at risk (VaR). By understanding the amounts of value, risk, and opportunity in a project, project managers can design it for appropriate levels of each. Project progress occurs through reductions in its VaR: Activities “add value” by chipping away at the project’s “anti-value”—the risks that threaten value. This perspective on project management incentivizes generating results that eliminate these threats, rather than assuming that value exists until proven otherwise. © 2019 Project Management Institute, Inc.}}, 
pages = {71--85}, 
number = {1}, 
volume = {50}
}
@article{10.1108/jmlc-01-2018-0005, 
year = {2019}, 
title = {{The risk analysis of Bitcoin and major currencies: value at risk approach}}, 
author = {Uyar, Umut and Kahraman, Ibrahim Korkmaz}, 
journal = {Journal of Money Laundering Control}, 
issn = {13685201}, 
doi = {10.1108/jmlc-01-2018-0005}, 
abstract = {{Purpose: This study aims to compare investors of major conventional currencies and Bitcoin (BTC) investors by using the value at risk (VaR) method common risk measure. Design/methodology/approach: The paper used a risk analysis named as VaR. The analysis has various computations that Historical Simulation and Monte Carlo Simulation methods were used for this paper. Findings: Findings of the analysis are assessed in two different aspects of singular currency risk and portfolios built. First, BTC is found to be significantly risky with respect to the major currencies; and it is six times riskier than the singular most risky currency. Second, in terms of inclusion of BTC into a portfolio, which equally weights all currencies, it elevates overall portfolio risk by 98 per cent. Practical implications: In spite of the remarkable risk level, it could be considered that investors are desirous of making an investment on BTC could mitigate their overall exposed risk relatively by building a portfolio. Originality/value: The paper questions the risk level of Bitcoin, which is a digital currency. BTC, a matter of debate in the contemporary period, is seen as a digital currency free from control or supervision of a regulatory board. With the comparison of major currencies and BTC shows that how could be risky of a financial instrument without regulations. However, there is some advice for investors who would like to invest digital currencies despite the risk level in this study. © 2019, Emerald Publishing Limited.}}, 
pages = {38--52}, 
number = {1}, 
volume = {22}
}
@article{10.15446/cuad.econ.v37n76.57654, 
year = {2019}, 
title = {{Dynamic quantile regression for the measurement of a value at risk: An application to Colombian data [Régression quantile dynamique pour la mesure de la valeur en risque: Une application aux données colombiennes] [Regressão quantílica dinâmica para a medição do valor em risco: Uma aplicação a dados colombianos] [Regresión cuantílica dinámica para la medición del valor en riesgo: Una aplicación a datos colombianos]}}, 
author = {}, 
issn = {01214772}, 
doi = {10.15446/cuad.econ.v37n76.57654}, 
abstract = {{This document contains the results for the estimation of Value at Risk (VaR) based on linear and non-linear quantile regression techniques. In particular, several CAViaR (conditional autoregressive value at risk) models are implemented for this purpose. These models can replicate the empirical properties of asset returns without requiring distributional assumptions. In addition, these methods are compared with traditional VaR techniques for the Colombian peso exchange rate, a public debt market price index, and the Colombian stock price index, during the periods of December 2007 and November 2015. In general, the quantile regression-based techniques show a good performance with respect to the traditional models. © 2019 Universidad Nacional de Colombia.Dans ce document, on évalue la valeur en risque (VaR) en utilisant des méthodes semiparamétriques basées sur la régression quantile linéaire et non linéaire. En particulier, on utilise plusieurs caractérisations de la famille de modèles CAViaR (conditional autoregressive value at risk). Ces modèles permettent de saisir des faits stylisés des séries financières et évitent d'imposer des hypothèses en rapport avec la distribution des actifs financiers. En outre, ces méthodologies sont comparées avec des techniques de VaR traditionnelles pour le taux de change représentatif du marché, un indice de prix de bons de dette publique et l'indice de la bourse de valeurs de Colombie, pour la période comprise entre décembre 2007 et novembre 2015. En général, on trouve que les mesures de risque de marché selon ces méthodologies donnent un meilleur résultat que les traditionnelles. © 2019 Universidad Nacional de Colombia.Neste documento estima-se o valor em risco (VaR) utilizando métodos semi-paramétricos baseados em regressão quantílica linear e não linear. Particularmente, usam-se várias especificações da família de modelos CAViaR (conditional autoregressive value at risk). Estes modelos permitem capturar fatos estilizados das séries financeiras e evitam impor supostos relacionados com a distribuição dos ativos financeiros. Além do mais, estas metodologias são comparadas às técnicas de VaR tradicionais para a taxa de câmbio representativa do mercado, um índice de preços de bônus de dívida pública e o índice da bolsa de valores da Colômbia, durante o período compreendido entre dezembro de 2007 e novembro de 2015. De modo geral, constatou-se que as medidas de risco de mercado sob estas metodologias têm um melhor desempenho com relação às tradicionais. © 2019 Universidad Nacional de Colombia.En este documento se estima el valor en riesgo (VaR) utilizando métodos semiparamétricos basados en regresión cuantílica lineal y no lineal. En particular, se usan varias especificaciones de la familia de modelos CAViaR (conditional autoregressive value at risk). Estos modelos permiten capturar hechos estilizados de las series financieras y evitan imponer supuestos relacionados con la distribución de los activos financieros. Además, estas metodologías son comparadas con técnicas de VaR tradicionales para la tasa de cambio representativa del mercado, un índice de precios de bonos de deuda pública y el índice de la bolsa de valores de Colombia, durante el período comprendido entre diciembre de 2007 y noviembre de 2015. En general, se encontró que las medidas de riesgo de mercado bajo estas metodologías tienen un mejor desempeño respecto a las tradicionales. © 2019 Universidad Nacional de Colombia.}}, 
number = {76}, 
volume = {38}
}
@article{10.1541/ieejeiss.140.786, 
year = {2020}, 
title = {{Weekly unit commitment problem considering the risk of profit fluctuations [収益変動リスクを考慮した発電機の週間運転計画]}}, 
author = {健人, 内藤 and 聖一, 北村 and 一之, 森}, 
journal = {電気学会論文誌Ｃ（電子・情報・システム部門誌）}, 
issn = {03854221}, 
doi = {10.1541/ieejeiss.140.786}, 
abstract = {{In the deregulated electricity market, power producers are exposed to risks of generation imbalance and decreasing earnings. In order to stabilize earnings, they need to make generation plan of their own generators and bidding plan to the electricity market in consideration of these risks. These risks are due to uncertainties of forecasts of electricity price, renewable energy generation, electricity demand, and so on. Up to now, there are many studies about stochastic unit commitment methods considering these risks. However, conventional methods have problems that operators have difficulty in reflecting risk appetite intuitively. In this study, we present a stochastic unit commitment method which optimizes VaR (Value at Risk) of profit with a given confidence level. A confidence level means the percent of event which operators consider for risks to all events, so operators can reflect own risk appetite intuitively. The usefulness of the proposed method has been confirmed by numerical simulation. The numerical results show that when a power producer has low risk tolerance, the volume of forward contract increases. Studying efficient calculation methods and formulations and the evaluation using actual scenario are future tasks. © 2020 The Institute of Electrical Engineers of Japan.}}, 
pages = {786--793}, 
number = {7}, 
volume = {140}
}
@article{10.1016/j.jedc.2005.04.008, 
year = {2006}, 
title = {{Equilibrium impact of value-at-risk regulation}}, 
author = {Leippold, Markus and Trojani, Fabio and Vanini, Paolo}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/j.jedc.2005.04.008}, 
abstract = {{We study the asset-pricing implications of value-at-risk (VaR) regulation in incomplete continuous-time economies with intermediate expenditure, stochastic opportunity set, and heterogeneous attitudes to risk. Our findings show that because of an anticipatory effect of VaR constraints on the optimal hedging demand, the partial equilibrium incentives of VaR regulation can lead banks to increase their risk exposure in high-volatility states. In general equilibrium, VaR constraints can produce unambiguously lower interest rates and higher equity Sharpe ratios. The VaR impact on equity volatility and equity expected returns is ambiguous. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {1277--1313}, 
number = {8}, 
volume = {30}
}
@article{10.1016/j.irfa.2009.03.006, 
year = {2009}, 
title = {{Are RiskMetrics forecasts good enough? Evidence from 31 stock markets}}, 
author = {McMillan, David G. and Kambouroudis, Dimos}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2009.03.006}, 
abstract = {{Academic research has highlighted the inherent flaws within the RiskMetrics model and demonstrated the superiority of the GARCH approach in-sample. However, these results do not necessarily extend to forecasting performance. This paper seeks answer to the question of whether RiskMetrics volatility forecasts are adequate in comparison to those obtained from GARCH models. To answer the question stock index data is taken from 31 international markets and subjected to two exercises, a straightforward volatility forecasting exercise and a Value-at-Risk exceptions forecasting competition. Our results provide some simple answers to the above question. When forecasting volatility of the G7 stock markets the APARCH model, in particular, provides superior forecasts that are significantly different from the RiskMetrics models in over half the cases. This result also extends to the European markets with the APARCH model typically preferred. For the Asian markets the RiskMetrics model performs well, and is only significantly dominated by the GARCH models for one market, although there is evidence that the APARCH model provides a better forecast for the larger Asian markets. Regarding the Value-at-Risk exercise, when forecasting the 1\% VaR the RiskMetrics model does a poor job and is typically the worst performing model, again the APARCH model does well. However, forecasting the 5\% VaR then the RiskMetrics model does provide an adequate performance. In short, the RiskMetrics model only performs well in forecasting the volatility of small emerging markets and for broader VaR measures. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {117--124}, 
number = {3}, 
volume = {18}
}
@article{10.1287/mnsc.1100.1213, 
year = {2010}, 
title = {{Nested simulation in portfolio risk measurement}}, 
author = {Gordy, Michael B and Juneja, Sandeep}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1100.1213}, 
abstract = {{Risk measurement for derivative portfolios almost invariably calls for nested simulation. In the outer step, one draws realizations of all risk factors up to the horizon, and in the inner step, one reprices each instrument in the portfolio at the horizon conditional on the drawn risk factors. Practitioners may perceive the computational burden of such nested schemes to be unacceptable and adopt a variety of second-best pricing techniques to avoid the inner simulation. In this paper, we question whether such short cuts are necessary. We show that a relatively small number of trials in the inner step can yield accurate estimates, and we analyze how a fixed computational budget may be allocated to the inner and the outer step to minimize the mean square error of the resultant estimator. Finally, we introduce a jackknife procedure for bias reduction. © 2010 INFORMS.}}, 
pages = {1833--1848}, 
number = {10}, 
volume = {56}
}
@article{10.1080/1350486x.2020.1725582, 
year = {2019}, 
title = {{Structural Electricity Models and Asymptotically Normal Estimators to Quantify Parameter Risk}}, 
author = {Harms, Cord and Kiesel, Rüdiger}, 
journal = {Applied Mathematical Finance}, 
issn = {1350486X}, 
doi = {10.1080/1350486x.2020.1725582}, 
abstract = {{We estimate a structural electricity (multi-commodity) model based on historical spot and futures data (fuels and power prices, respectively) and quantify the inherent parameter risk using an average value at risk approach (‘expected shortfall’). The mathematical proofs use the theory of asymptotic statistics to derive a parameter risk measure. We use far in-the-money options to derive a confidence level and use it as a prudent present value adjustment when pricing a virtual power plant. Finally, we conduct a present value benchmarking to compare the approach of temperature-driven demand (based on load data) to an ‘implied demand approach’ (demand implied from observable power futures prices). We observe that the implied demand approach can easily capture observed electricity price volatility whereas the estimation against observable load data will lead to a gap, because–amongst others–the interplay of demand and supply is not captured in the data (i.e., unexpected mismatches). © 2020, © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--48}, 
number = {5}, 
volume = {26}
}
@article{10.1080/00207540903051684, 
year = {2010}, 
title = {{Enterprise risk management: A DEA VaR approach in vendor selection}}, 
author = {Wu, Desheng Dash and Olson, David}, 
journal = {International Journal of Production Research}, 
issn = {00207543}, 
doi = {10.1080/00207540903051684}, 
abstract = {{Enterprise risk management (ERM) has become an important topic in today's more complex, interrelated global business environment, replete with threats from natural, political, economic, and technical sources. The development and current status of ERM is presented, with a demonstration of how risk modelling can be applied in supply chain management. Within supply chain management, a major managerial decision is vendor selection. We start with discussion of the advanced ERM technology, i.e. value-at-risk (VaR) and develop DEA VaR model as a new tool to conduct risk management in enterprises. A vendor selection set of data is used to demonstrate how this model can be used to assess supply risks in ERM. Such models provide means to quantitatively improve decision making with respect to risk. © 2010 Taylor and Francis.}}, 
pages = {4919--4932}, 
number = {16}, 
volume = {48}
}
@article{10.1016/j.physa.2014.01.037, 
year = {2014}, 
title = {{Semi-nonparametric VaR forecasts for hedge funds during the recent crisis}}, 
author = {Brio, Esther B. Del and Mora-Valencia, Andrés and Perote, Javier}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2014.01.037}, 
abstract = {{The need to provide accurate value-at-risk (VaR) forecasting measures has triggered an important literature in econophysics. Although these accurate VaR models and methodologies are particularly demanded for hedge fund managers, there exist few articles specifically devoted to implement new techniques in hedge fund returns VaR forecasting. This article advances in these issues by comparing the performance of risk measures based on parametric distributions (the normal, Student's t and skewed-t), semi-nonparametric (SNP) methodologies based on Gram-Charlier (GC) series and the extreme value theory (EVT) approach. Our results show that normal-, Student's t- and Skewed t- based methodologies fail to forecast hedge fund VaR, whilst SNP and EVT approaches accurately success on it. We extend these results to the multivariate framework by providing an explicit formula for the GC copula and its density that encompasses the Gaussian copula and accounts for non-linear dependences. We show that the VaR obtained by the meta GC accurately captures portfolio risk and outperforms regulatory VaR estimates obtained through the meta Gaussian and Student's t distributions.©2014 Elsevier B.V. All rights reserved.}}, 
pages = {330--343}, 
number = {NA}, 
volume = {401}
}
@article{10.1016/j.jbankfin.2012.02.010, 
year = {2012}, 
title = {{Granularity adjustment for mark-to-market credit risk models}}, 
author = {Gordy, Michael B. and Marrone, James}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.02.010}, 
abstract = {{The impact of undiversified idiosyncratic risk on value-at-risk and expected shortfall can be approximated analytically via a methodology known as granularity adjustment (GA). In principle, the GA methodology can be applied to any risk-factor model of portfolio risk. Thus far, however, analytical results have been derived only for simple models of actuarial loss, i.e., credit loss due to default. We demonstrate that the GA is entirely tractable for single-factor versions of a large class of models that includes all the commonly used mark-to-market approaches. Our approach covers both finite ratings-based models and models with a continuum of obligor states. We apply our methodology to CreditMetrics and KMV Portfolio Manager, as these are benchmark models for the finite and continuous classes, respectively. Comparative statics of the GA reveal striking and counterintuitive patterns. We explain these relationships with a stylized model of portfolio risk. © 2012.}}, 
pages = {1896--1910}, 
number = {7}, 
volume = {36}
}
@article{10.24411/1993-7601-2019‑10008, 
year = {2019}, 
title = {{The importance of being informed: Forecasting market risk measures for the Russian RTS index future using online data and implied volatility over two decades}}, 
author = {}, 
issn = {19937601}, 
doi = {10.24411/1993-7601-2019‑10008}, 
abstract = {{This paper focuses on the forecasting of market risk measures for the Russian RTS index future, and examines whether augmenting a large class of volatility models with implied volatility and Google Trends data improves the quality of the estimated risk measures. We considered a time sample of daily data from 2006 till 2019, which includes several episodes of large-scale turbulence in the Russian future market. We found that the predictive power of several models did not increase if these two variables were added, but actually decreased. The worst results were obtained when these two variables were added jointly and during periods of high volatility, when parameters estimates became very unstable. Moreover, several models augmented with these variables did not reach numerical convergence. Our empirical evidence shows that, in the case of Russian future markets, TGARCH models with implied volatility and Student’s t errors are better choices if robust market risk measures are of concern. © 2019 Sinergia Press. All rights reserved.}}, 
number = {NA}, 
volume = {55}
}
@article{10.1504/gber.2014.065364, 
year = {2014}, 
title = {{Value-at-risk and expected shortfall: A dual long memory framework}}, 
author = {Mighri, Zouheir and Mansouri, Faysal and Hewings, Geoffrey J D}, 
journal = {Global Business and Economics Review}, 
issn = {10974954}, 
doi = {10.1504/gber.2014.065364}, 
abstract = {{In this article, we use the dual long memory properties to assess the value-at-risk and expected shortfall for the Argentinean stock market under both short and long daily trading positions. We attempt to show whether considering for long memory properties in both, the returns and volatility, volatility asymmetry and fat-tails could provide more accurate value-at-risk's and expected shortfall's estimations. For this purpose, the joint ARFIMA-FIGARCH, ARFIMA-HYGARCH and ARFIMA-FIAPARCH models are applied to the MERVAL stock price index under normal, student-t and skewed student-t distributed innovations. We show that the skewed student-t-ARFIMA-FIAPARCH model performs better in predicting the in-sample and out-of-sample one-step ahead value-at-risk and expected shortfall for both short and long trading positions. © 2014 Inderscience Enterprises Ltd.}}, 
pages = {416}, 
number = {4}, 
volume = {16}
}
@article{10.1080/03610926.2017.1361984, 
year = {2018}, 
title = {{A review of backtesting for value at risk}}, 
author = {Zhang, Y. and Nadarajah, S.}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2017.1361984}, 
abstract = {{There have been many backtesting methods proposed for value at risk. Yet they have rarely been applied in practice. Here, we provide a comprehensive review of all of the recent backtesting methods for VaR. This review could encourage applications and also the development of further backtesting methods. © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {3616--3639}, 
number = {15}, 
volume = {47}
}
@article{10.1016/j.najef.2013.02.024, 
year = {2013}, 
title = {{Time-varying mixture GARCH models and asymmetric volatility}}, 
author = {Haas, Markus and Krause, Jochen and Paolella, Marc S. and Steude, Sven C.}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2013.02.024}, 
abstract = {{The class of mixed normal conditional heteroskedastic (MixN-GARCH) models, which couples a mixed normal distributional structure with GARCH-type dynamics, has been shown to offer a plausible decomposition of the contributions to volatility, as well as excellent out-of-sample forecasting performance, for financial asset returns. In this paper, we generalize the MixN-GARCH model by relaxing the assumption of constant mixing weights. Two different specifications with time-varying mixing weights are considered. In particular, by relating current weights to past returns and realized (component-wise) likelihood values, an empirically reasonable representation of Engle and Ng's (1993) news impact curve with an asymmetric impact of unexpected return shocks on future volatility is obtained. An empirical out-of-sample study confirms the usefulness of the new approach and gives evidence that the leverage effect in financial returns data is closely connected, in a non-linear fashion, to the time-varying interplay of mixture components representing, for example, various groups of market participants. © 2013 Elsevier Inc.}}, 
pages = {602--623}, 
number = {NA}, 
volume = {26}
}
@article{10.1080/03461238.2019.1616323, 
year = {2019}, 
title = {{Reinsurance contract design with adverse selection}}, 
author = {Cheung, K. C. and Yam, S. C. P. and Yuen, F. L.}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2019.1616323}, 
abstract = {{In light of the richness of their structures in connection with practical implementation, we follow the seminal works in economics to use the principal–agent (multidimensional screening) models to study a monopolistic reinsurance market with adverse selection; instead of adopting the classical expected utility paradigm, the novelty of our present work is to model the risk assessment of each insurer (agent) by his value-at-risk at his own chosen risk tolerance level consistent with Solvency II. Under information asymmetry, the reinsurer (principal) aims to maximize his average profit by designing an optimal policy provision (menu) of ‘shirt-fit’ reinsurance contracts for every insurer from one of the two groups with hidden characteristics. Our results show that a quota-share component, on the top of simple stop-loss, is very crucial for mitigating asymmetric information from the insurers to the reinsurer. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--15}, 
number = {9}, 
volume = {2019}
}
@article{10.1002/for.1119, 
year = {2009}, 
title = {{Volatility forecasting with double Markov switching GARCH models}}, 
author = {Chen, Cathy W. S. and So, Mike K. P. and Lin, Edward M. H.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1119}, 
abstract = {{This paper investigates inference and volatility forecasting using a Markov switching heteroscedastic model with a fat-tailed error distribution to analyze asymmetric effects on both the conditional mean and conditional volatility of financial time series. The motivation for extending the Markov switching GARCH model, previously developed to capture mean asymmetry, is that the switching variable, assumed to be a first-order Markov process, is unobserved. The proposed model extends this work to incorporate Markov switching in the mean and variance simultaneously. Parameter estimation and inference are performed in a Bayesian framework via a Markov chain Monte Carlo scheme. We compare competing models using Bayesian forecasting in a comparative value-at-risk study. The proposed methods are illustrated using both simulations and eight international stock market return series. The results generally favor the proposed double Markov switching GARCH model with an exogenous variable. Copyright © 2008 John Wiley \& Sons, Ltd.}}, 
pages = {681--697}, 
number = {8}, 
volume = {28}
}
@article{10.1002/cjs.5550340406, 
year = {2006}, 
title = {{A robust prediction error criterion for Pareto modelling of upper tails}}, 
author = {Dupuis, Debbie J. and Victoria‐Feser, Maria‐Pia}, 
journal = {Canadian Journal of Statistics}, 
issn = {03195724}, 
doi = {10.1002/cjs.5550340406}, 
abstract = {{Estimation of the Pareto tail index from extreme order statistics is an important problem in many settings. The upper tail of the distribution, where data are sparse, is typically fitted with a model, such as the Pareto model, from which quantities such as probabilities associated with extreme events are deduced. The success of this procedure relies heavily not only on the choice of the estimator for the Pareto tail index but also on the procedure used to determine the number k of extreme order statistics that are used for the estimation. The authors develop a robust prediction error criterion for choosing k and estimating the Pareto index. A Monte Carlo study shows the good performance of the new estimator and the analysis of real data sets illustrates that a robust procedure for selection, and not just for estimation, is needed.}}, 
pages = {639--658}, 
number = {4}, 
volume = {34}
}
@article{10.1016/s0927-5398(00)00022-0, 
year = {2000}, 
title = {{Value-at-Risk: A multivariate switching regime approach}}, 
author = {Billio, Monica and Pelizzon, Loriana}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(00)00022-0}, 
abstract = {{This paper analyses the application of a switching volatility model to forecast the distribution of returns and to estimate the Value-at-Risk (VaR) of both single assets and portfolios. We calculate the VaR value for 10 Italian stocks and a number of portfolios based on these stocks. The calculated VaR values are also compared with the variance-covariance approach used by JP Morgan in RiskMetrics™ and GARCH(1,1) models. Under backtesting, the VaR values calculated using the switching regime beta model are preferred to both other methods. The Proportion of Failure and Time Until First Failure tests [The Journal of Derivatives (1995) 73-84] confirm this result. © 2000 Elsevier Science B.V.}}, 
pages = {531--554}, 
number = {5}, 
volume = {7}
}
@article{10.1016/j.physa.2004.02.039, 
year = {2004}, 
title = {{Statistical models for operational risk management}}, 
author = {Cornalba, Chiara and Giudici, Paolo}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2004.02.039}, 
abstract = {{The Basel Committee on Banking Supervision has released, in the last few years, recommendations for the correct determination of the risks to which a banking organization is subject. This concerns, in particular, operational risks, which are all those management events that may determine unexpected losses. It is necessary to develop valid statistical models to measure and, consequently, predict, such operational risks. In the paper we present the possible approaches, including our own proposal, which is based on Bayesian networks. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {166--172}, 
number = {1-2 SPEC. ISS.}, 
volume = {338}
}
@article{10.1016/j.ijforecast.2020.01.008, 
year = {2020}, 
title = {{Forecasting value at risk and expected shortfall with mixed data sampling}}, 
author = {Le, Trung H.}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2020.01.008}, 
abstract = {{I propose applying the Mixed Data Sampling (MIDAS) framework to forecast Value at Risk (VaR) and Expected shortfall (ES). The new methods exploit the serial dependence on short-horizon returns to directly forecast the tail dynamics of the desired horizon. I perform a comprehensive comparison of out-of-sample VaR and ES forecasts with established models for a wide range of financial assets and backtests. The MIDAS-based models significantly outperform traditional GARCH-based forecasts and alternative conditional quantile specifications, especially in terms of multi-day forecast horizons. My analysis advocates models that feature asymmetric conditional quantiles and the use of the Asymmetric Laplace density to jointly estimate VaR and ES. © 2020 International Institute of Forecasters}}, 
pages = {1362--1379}, 
number = {4}, 
volume = {36}
}
@article{10.1109/wsc.2009.5429348, 
year = {2009}, 
title = {{A general framework of importance sampling for value-at-risk and conditional value-at-risk}}, 
author = {Sun, Lihua and Hong, L. Jeff}, 
journal = {Proceedings of the 2009 Winter Simulation Conference (WSC)}, 
issn = {08917736}, 
doi = {10.1109/wsc.2009.5429348}, 
abstract = {{Value-at-risk (VaR) and conditional value-at-risk (CVaR) are important risk measures. Importance sampling (IS) is often used to estimate them. We derive the asymptotic representations for IS estimators of VaR and CVaR. Based on these representations, we are able to give simple conditions under which the IS estimators have smaller asymptotic variances than the ordinal estimators. We show that the exponential twisting can yield an IS distribution that satisfies the conditions for both the IS estimators of VaR and CVaR. Therefore, we may be able to estimate VaR and CVaR accurately at the same time. ©2009 IEEE.}}, 
pages = {415--421}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s42452-019-1715-6, 
year = {2019}, 
title = {{Application of mathematical modeling value-at-risk (VaR) to optimize decision making in distribution networks}}, 
author = {Khorshidi, E. and Ghezavati, V. R.}, 
journal = {SN Applied Sciences}, 
issn = {25233971}, 
doi = {10.1007/s42452-019-1715-6}, 
abstract = {{Managers and capital masters of companies and factories try to adopt methods that maximize their profit and minimize their costs. A way for increasing profit is risk management. The risk management and the type of risk are defined in the literature of financial management. For measurement of the risk, many methods are defined which are all created in the recent century and it means the risk and risk management are rather new concepts. Among the newest tools for measuring the risk is value-at-risk (VaR), which was modified by Morgan (Riskmetrics technical document, Morgan Guaranty Trust Company, New York, 1996). VaR represents the maximum expected loss over a certain period of time and at a given confidence level. Many parametric, semi-parametric and nonparametric methods for VaR estimation have been developed. In this article using the mean–variance method one of the parametric techniques of VaR, has been tried to minimize the cost arisen due to locating the supply chain and minimize the maximum level of capital losses, to optimize decision making in the distribution network. The calculated model is tested with numerical examples by MATLAB and Lingo software, and this example supported the resulting model. © 2019, Springer Nature Switzerland AG.}}, 
pages = {1671}, 
number = {12}, 
volume = {1}
}
@article{10.1111/j.1368-423x.2008.00277.x, 
year = {2009}, 
title = {{Value at Risk with time varying variance, skewness and kurtosis - the NIG-ACD model}}, 
author = {Wilhelmsson, Anders}, 
journal = {The Econometrics Journal}, 
issn = {13684221}, 
doi = {10.1111/j.1368-423x.2008.00277.x}, 
abstract = {{A new model for financial returns with time varying variance, skewness and kurtosis based on the Normal Inverse Gaussian (NIG) distribution is proposed. The new model and two previously suggested NIG models are evaluated by their Value at Risk (VaR) forecasts on a long series of daily Standard and Poor's 500 returns. All three models perform very well compared with extant models and clearly outperform a Gaussian GARCH model. Moreover, the results show that only the new model cannot be rejected as providing correct conditional VaR forecasts. © The Author(s). Journal compilation © Royal Economic Society 2009.}}, 
pages = {82--104}, 
number = {1}, 
volume = {12}
}
@article{10.1016/j.ijforecast.2016.06.003, 
year = {2016}, 
title = {{Variational Bayes for assessment of dynamic quantile forecasts}}, 
author = {Gerlach, Richard and Abeywardana, Sachin}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2016.06.003}, 
abstract = {{Recently, various Bayes factor analogues of frequentist tests for the accuracy of dynamic quantile forecasts have been developed. However, in evaluating the marginal likelihoods involved, either inappropriate assumptions have been made, or pre-packaged multivariate adaptive quadrature methods have been employed, without an accuracy assessment. This paper develops variational Bayes methods for estimating lower bounds for these marginal likelihoods efficiently. This facilitates a more accurate version of one existing Bayesian test, and allows for the development of a new test based on the probit regression model. The size and power properties of the proposed methods are examined via a simulation study, illustrating favourable comparisons with existing testing methods. The accuracy and speed of the VB methods are also assessed. An empirical study illustrates the sensible performance and applicability of the proposed methods, relative to standard tests, for assessing the adequacy of a range of forecast models for the Value at Risk (VaR) in several financial market data series. © 2016 International Institute of Forecasters}}, 
pages = {1385--1402}, 
number = {4}, 
volume = {32}
}
@article{10.14254/2071-8330.2015/8-2/5, 
year = {2015}, 
title = {{Compatibility of market risk measures}}, 
author = {Mentel, Grzegorz and Brożyna, Jacek}, 
journal = {JOURNAL OF INTERNATIONAL STUDIES}, 
issn = {20718330}, 
doi = {10.14254/2071-8330.2015/8-2/5}, 
abstract = {{An important element of everyday financial decisions is to assess the scale of the risk of investing in various financial products. Knowledge on the degree of risk of the activities undertaken in this field allows a certain predictability about the negative effects that may occur. This is of great importance in the context of aversion to risk, and thus a better allocation of resources. A multitude of market risk measures is substantial and in addition they provide information about the risk of investment considered in a different perspective and form. A very interesting issue is the scale compatibility of these measures. It is important whether such measures to the same extent define the scale of risk and whether any signals about the dangers overlap in time. The above considerations based on the value at risk and so-caled RiskGrade have become a contribution to the creation of this publication. © CSR, 2015.}}, 
pages = {52--62}, 
number = {2}, 
volume = {8}
}
@article{10.1016/j.econmod.2020.11.005, 
year = {2021}, 
title = {{Bayesian estimation for a semiparametric nonlinear volatility model}}, 
author = {Hu, Shuowen and Poskitt, D.S. and Zhang, Xibin}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2020.11.005}, 
abstract = {{This paper presents a new volatility model which extends the nonstationary nonparametric volatility model of Han and Zhang (2012) by including an ARCH(1) component. This model also allows the errors to be independent and follow an unknown distribution. A Bayesian sampling algorithm is presented to estimate the ARCH coefficient and smoothing parameters. Empirical results show that the proposed model outperforms its competitors under several evaluation criteria. © 2020 Elsevier B.V.}}, 
pages = {361--370}, 
number = {NA}, 
volume = {98}
}
@article{10.1504/ijram.2016.079617, 
year = {2016}, 
title = {{Practical implementation of scenario generation-based risk analysis of electrical grids investment projects}}, 
author = {}, 
issn = {14668297}, 
doi = {10.1504/ijram.2016.079617}, 
abstract = {{This paper proposes a risk analysis methodology to complement the traditional economical evaluation of investment projects in electrical grids. The methodology is meant to quantify the risk of projects at a given confidence level with minimum additional effort from the planning engineer. This requires a specific scenario generation approach to represent the main uncertainty drivers of risk and a sensible mapping of the represented uncertainty into suitable risk measures. In this paper we present the state-of-the-art methods used for scenario generation and risk assessment, apply such methods to the electrical grid investment context, and finally propose changes to such methods in order to make them practical enough to be implemented as part of the project evaluation process carried out in a daily basis by planning engineers. Copyright © 2016 Inderscience Enterprises Ltd.}}, 
number = {4}, 
volume = {19}
}
@article{10.1504/ijcat.2014.066722, 
year = {2014}, 
title = {{Evaluating electricity price volatility risk in competitive environment based on ARMAXGARCHSK-EVT model}}, 
author = {Wang, Jin and Wang, Ruiqing}, 
journal = {International Journal of Computer Applications in Technology}, 
issn = {09528091}, 
doi = {10.1504/ijcat.2014.066722}, 
abstract = {{Effectively evaluating volatility of price risk is the foundation of risk management in competitive environments. Considering the natures of electricity prices, a two-stage model for estimating value-at-risk (VaR) based on ARMAX-GARCHSK and extreme value theory (EVT) is proposed. First, an ARMAX-GARCHSK model, which can capture the most important characteristics of electricity price series, such as seasonalities, heteroscedasticities, skewnesses and lepkurtosises, is used to filter electricity price series. In this way, an approximately independently and identically distributed normalised residual series is acquired. Then EVT is adopted to explicitly model the tails of the standardised residuals of ARMAX-GARCHSK model, and accurate estimates of VaR in electricity market can be yielded. The empirical analysis suggests that the ARMAX-GARCHSK-EVT model can rapidly reflect the recent and relevant changes of electricity prices and produce more accurate forecasts of VaR at all confidence levels, showing better dynamic characteristics. These results present several potential implications for electricity market risk quantifications and hedging strategies. Copyright © 2014 Inderscience Enterprises Ltd.}}, 
pages = {174}, 
number = {3}, 
volume = {50}
}
@article{10.1109/fuzzy.2010.5584608, 
year = {2010}, 
title = {{Value of information and solution under VaR criterion for fuzzy random optimization problems}}, 
author = {Wang, Shuming and Watada, Junzo}, 
journal = {International Conference on Fuzzy Systems}, 
issn = {NA}, 
doi = {10.1109/fuzzy.2010.5584608}, 
abstract = {{Under the Value-at-Risk (VaR) criterion, this paper studies on the value of information and solution for two stage fuzzy random optimization problems. First, the value of perfect information (VPI) in VaR criterion is discussed by studying the difference of the wait-and-see (WS) solution and the here-and-now (HN) solution to the two-stage fuzzy random programming with VaR criterion. Then, the value of fuzzy random solution n (VFRS) in VaR is examined by investigating the difference of the HN solution and the random solution (RS), as well as the difference of HN solution and the expected value (EV) solution. Finally, a lower bound and an upper bound for the HN solution are derived. © 2010 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10260-007-0091-3, 
year = {2009}, 
title = {{Some developments on the log-Dagum distribution}}, 
author = {Domma, Filippo and Perri, Pier Francesco}, 
journal = {Statistical Methods and Applications}, 
issn = {16182510}, 
doi = {10.1007/s10260-007-0091-3}, 
abstract = {{Skewed and fat-tailed distributions frequently occur in many applications. Models proposed to deal with skewness and kurtosis may be difficult to treat because the density function cannot usually be written in a closed form and the moments might not exist. The log-Dagum distribution is a flexible and simple model obtained by a logarithmic transformation of the Dagum random variable. In this paper, some characteristics of the model are illustrated and the estimation of the parameters is considered. An application is given with the purpose of modeling kurtosis and skewness that mark the financial return distribution. © 2008 Springer-Verlag.}}, 
pages = {205--220}, 
number = {2}, 
volume = {18}
}
@article{10.1080/13675567.2021.1990872, 
year = {2021}, 
title = {{Quantifying supply chain disruption: a recovery time equivalent value at risk approach}}, 
author = {Zhang, Allan N and Wagner, Stephan M and Goh, Mark and Asian, Sobhan}, 
journal = {International Journal of Logistics Research and Applications}, 
issn = {13675567}, 
doi = {10.1080/13675567.2021.1990872}, 
abstract = {{The global pandemic COVID-19 has disrupted supply chains in many industries all over the world. If not well contained and managed at an early stage, such unprecedented disruptions may lead to even more serious consequences in the era of supply chain reglobalization. The attendant challenge for research and practice is on how to readily measure and quantify the disruption risk. To address the plethora of concerns, this article presents a recovery time equivalent (RTE) disruption risk measurement model using Value at Risk (VaR). We consider a disruption recovery model comprising abrupt, normal, fast, and slow modes. To demonstrate the practical relevance of our study, we establish a case study with a multinational corporation from the IT sector. Decision makers and supply chain managers can use the model to conduct ‘what-if' analyses on their supply chain vulnerabilities and risks for a more proactive business continuity planning and contingency management. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--21}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/13504851.2021.1912696, 
year = {2021}, 
title = {{Did COVID-19 increase equity market risk exposure? Evidence from China, the UK, and the US}}, 
author = {Li, Matthew C. and Lai, Catherine C. and Xiao, Ling}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2021.1912696}, 
abstract = {{By studying equity market returns to China, the UK, and the US, we explore the key question of whether the COVID-19 pandemic changes the risk exposure of equity markets, which is fundamental to market stability and investor confidence. Using data from the World Health Organization and Bloomberg, our full sample covers the period 3 July 2019 to 15 December 2020 which facilities a subsample (Normal, Shock, Endurance) analysis. Utilizing Value-at-Risk (VaR) metrics as our risk exposure measure, we find that 1) There exists a sharp increase in equity market risk exposure across the three equity markets. 2) A stronger pandemic impact is found in different market capitalization segments–China, large-cap; the UK, small-cap; the US, mid-cap. 3) Generally, investors consider the number of new cases as a more worrying factor than deaths while UK investors are sensitive to both. Our observations suggest that given limited resources but rising demands from both businesses and households for government assistance, a one-size-fits-all policy to support market recovery would be sub-optimal. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-9965.2005.00227.x, 
year = {2005}, 
title = {{An axiomatic approach to capital allocation}}, 
author = {Kalkbrener, Michael}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/j.1467-9965.2005.00227.x}, 
abstract = {{Capital allocation techniques are of central importance in portfolio management and risk-based performance measurement. In this paper we propose an axiom system for capital allocation and analyze its satisfiability and completeness: it is shown that for a given risk measure ρ there exists a capital allocation Λ ρ that satisfies the main axioms if and only if ρ is subadditive and positively homogeneous. Furthermore, it is proved that the axiom system uniquely specifies Λ ρ. We apply the axiomatization to the most popular risk measures in the finance industry in order to derive explicit capital allocation formulae for these measures. © 2005 Blackwell Publishing Inc.,.}}, 
pages = {425--437}, 
number = {3}, 
volume = {15}
}
@article{10.1002/9781118266588.ch39, 
year = {2011}, 
title = {{What Happened to Risk Management During the 2008-2009 Financial Crisis?}}, 
author = {Mcaleer, Michael and Pérez‐Amaral, Teodosio and Jiménez‐Martin, Juan‐Angel and Kolb, Robert W.}, 
issn = {NA}, 
doi = {10.1002/9781118266588.ch39}, 
abstract = {{[No abstract available]}}, 
pages = {307--316}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.cor.2018.03.011, 
year = {2018}, 
title = {{Chance-constrained stochastic programming under variable reliability levels with an application to humanitarian relief network design}}, 
author = {Elçi, Özgün and Noyan, Nilay and Bülbül, Kerem}, 
journal = {Computers \& Operations Research}, 
issn = {03050548}, 
doi = {10.1016/j.cor.2018.03.011}, 
abstract = {{We focus on optimization models involving individual chance constraints, in which only the right-hand side vector is random with a finite distribution. A recently introduced class of such models treats the reliability levels / risk tolerances associated with the chance constraints as decision variables and trades off the actual cost / return against the cost of the selected reliability levels in the objective function. Leveraging recent methodological advances for modeling and solving chance-constrained linear programs with fixed reliability levels, we develop strong mixed-integer programming formulations for this new variant with variable reliability levels. In addition, we introduce an alternate cost function type associated with the risk tolerances which requires capturing the value-at-risk (VaR) associated with a variable reliability level. We accomplish this task via a new integer linear programming representation of VaR. Our computational study illustrates the effectiveness of our mathematical programming formulations. We also apply the proposed modeling approach to a new stochastic last mile relief network design problem and provide numerical results for a case study based on the real-world data from the 2011 Van earthquake in Turkey. © 2018 Elsevier Ltd}}, 
pages = {91--107}, 
number = {NA}, 
volume = {96}
}
@article{10.1214/aos/1065705113, 
year = {2003}, 
title = {{Financial options and statistical prediction intervals}}, 
author = {Mykland, Per Aslak}, 
journal = {The Annals of Statistics}, 
issn = {00905364}, 
doi = {10.1214/aos/1065705113}, 
abstract = {{The paper shows how to convert statistical prediction sets into worst case hedging strategies for derivative securities. The prediction sets can, in particular, be ones for volatilities and correlations of the underlying securities, and for interest rates. This permits a transfer of statistical conclusions into prices for options and similar financial instruments. A prime feature of our results is that one can construct the trading strategy as if the prediction set had a 100\% probability. If, in fact, the set has probability 1 - α, the hedging strategy will work with at least the same probability. Different types of prediction regions are considered. The starting value A 0 for the trading strategy corresponding to the 1 - α prediction region is a form of long term value at risk. At the same time, A 0 is coherent.}}, 
pages = {1413--1438}, 
number = {5}, 
volume = {31}
}
@article{10.1007/s10288-015-0296-5, 
year = {2016}, 
title = {{Mean-value at risk portfolio efficiency: approaches based on data envelopment analysis models with negative data and their empirical behaviour}}, 
author = {Branda, Martin}, 
journal = {4OR}, 
issn = {16194500}, 
doi = {10.1007/s10288-015-0296-5}, 
abstract = {{We deal with the problem of an investor who is using a mean-risk model for accessing efficiency of investment opportunities. Our investor employs value at risk on several risk levels at the same time which corresponds to the approach called risk shaping. We review several data envelopment analysis (DEA) models which can deal with negative data. We show that a diversification–consistent extension of the DEA models based on a directional distance measure can be used to identify the Pareto–Koopmans efficient investment opportunities. We derive reformulations as chance constrained, nonlinear and mixed-integer problems under particular assumptions. In the numerical study, we access efficiency of US industry representative portfolios based on empirical distribution of random returns. We employ bootstrap and jackknife to investigate the empirical properties of the efficiency estimators. © 2015, Springer-Verlag Berlin Heidelberg.}}, 
pages = {77--99}, 
number = {1}, 
volume = {14}
}
@article{10.1145/2661631, 
year = {2014}, 
title = {{Monte carlo methods for value-at-risk and conditional value-at-risk: A review}}, 
author = {Hong, L. Jeff and Hu, Zhaolin and Liu, Guangwu}, 
journal = {ACM Transactions on Modeling and Computer Simulation (TOMACS)}, 
issn = {10493301}, 
doi = {10.1145/2661631}, 
abstract = {{Value-at-risk (VaR) and conditional value-at-risk (CVaR) are two widely used risk measures of large losses and are employed in the financial industry for risk management purposes. In practice, loss distributions typically do not have closed-form expressions, but they can often be simulated (i.e., random observations of the loss distribution may be obtained by running a computer program). Therefore, Monte Carlo methods that design simulation experiments and utilize simulated observations are often employed in estimation, sensitivity analysis, and optimization of VaRs and CVaRs. In this article, we review some of the recent developments in these methods, provide a unified framework to understand them, and discuss their applications in financial risk management. © 2014 ACM 1049-3301/2014/08-ART21 \$15.00.}}, 
pages = {22}, 
number = {4}, 
volume = {24}
}
@article{10.1007/s10687-014-0198-5, 
year = {2014}, 
title = {{Upper bounds on value-at-risk for the maximum portfolio loss}}, 
author = {Yuen, Robert and Stoev, Stilian}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-014-0198-5}, 
abstract = {{Extremal dependence of the losses in a portfolio is one of the most important features that should be accounted for when estimating Value-at-Risk (VaR) at high levels. Multivariate extreme value theory provides a principled framework for the modeling and estimation of extremal dependence. In practice, however, this involves dealing with a challenging infinite dimensional parameter such as the spectral measure. Here, following recent developments in Schlather and Tawn (Extremes, 5(1) 87–102; 2002), Molchanov (Extremes, 11, 235–259; 2008), and Strokorb and Schlather (2013), we propose to represent extremal dependence of a multivariate portfolio via the so–called Tawn–Molchanov (TM) model, which is finite dimensional. Every max–stable random vector X can be associated with a TM max-stable vector Y = TM(X) so that the extremal coefficients of X and Ymatch and at the same time Y stochastically dominates X in the lower orthant order. This result readily yields an optimal upper bound on the value-at-risk VaRα(X∨ of the maximum portfolio loss X∨:=maxj=1,...,dXj. We develop a statistical methodology for estimating TM models from data and illustrate the resulting upper bounds on VaRα(X∨)with simulations and real data. Fitting TM models to portfolio data may be of independent practical interest, since their coefficients provide a qualitative picture of the degree and nature of diversification to extreme shocks. © 2014, Springer Science+Business Media New York.}}, 
pages = {585--614}, 
number = {4}, 
volume = {17}
}
@article{10.1016/j.jeconom.2005.07.018, 
year = {2006}, 
title = {{Validating forecasts of the joint probability density of bond yields: Can affine models beat random walk?}}, 
author = {Egorov, Alexei V. and Hong, Yongmiao and Li, Haitao}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2005.07.018}, 
abstract = {{Most existing empirical studies on affine term structure models (ATSMs) have mainly focused on in-sample goodness-of-fit of historical bond yields and ignored out-of-sample forecast of future bond yields. Using an omnibus nonparametric procedure for density forecast evaluation in a continuous-time framework, we provide probably the first comprehensive empirical analysis of the out-of-sample performance of ATSMs in forecasting the joint conditional probability density of bond yields. We find that although the random walk models tend to have better forecasts for the conditional mean dynamics of bond yields, some ATSMs provide better forecasts for the joint probability density of bond yields. However, all ATSMs considered are still overwhelmingly rejected by our tests and fail to provide satisfactory density forecasts. There exists room for further improving density forecasts for bond yields by extending ATSMs. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {255--284}, 
number = {1-2}, 
volume = {135}
}
@article{10.1137/s0040585x97t988526, 
year = {2018}, 
title = {{Ordering results for aggregate claim amounts from two heterogeneous marshall–olkin extended exponential portfolios and their applications in insurance analysis}}, 
author = {Barmalzan, G. and Najafabadi, A. T. Payandeh and Balakrishnan, N.}, 
journal = {Theory of Probability \& Its Applications}, 
issn = {0040585X}, 
doi = {10.1137/s0040585x97t988526}, 
abstract = {{In this work, we discuss the stochastic comparison of two classical surplus processes in a one-year insurance period. Under the Marshall–Olkin extended exponential random aggregate claim amounts, we extend one result of Khaledi and Ahmadi [J. Statist. Plann. Inference, 138 (2008), pp. 2243–2251]. Applications of our results to the value-at-risk and ruin probability are also given. Our results show that the heterogeneity of the risks in a given insurance portfolio tends to make the portfolio volatile, which in turn leads to requiring more capital. © 2018 Society for Industrial and Applied Mathematics.}}, 
pages = {117--131}, 
number = {1}, 
volume = {62}
}
@article{10.1080/14697688.2016.1184303, 
year = {2017}, 
title = {{Risk forecasting in (T)GARCH models with uncorrelated dependent innovations}}, 
author = {Beckers, Benjamin and Herwartz, Helmut and Seidel, Moritz}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2016.1184303}, 
abstract = {{(G)ARCH-type models are frequently used for the dynamic modelling and forecasting of risk attached to speculative asset returns. While the symmetric and conditionally Gaussian GARCH model has been generalized in a manifold of directions, model innovations are mostly presumed to stem from an underlying IID distribution. For a cross section of 18 stock market indices, we notice that (threshold) (T)GARCH-implied model innovations are likely at odds with the commonly held IID assumption. Two complementary strategies are pursued to evaluate the conditional distributions of consecutive TGARCH innovations, a non-parametric approach and a class of standardized copula distributions. Modelling higher order dependence patterns is found to improve standard TGARCH-implied conditional value-at-risk and expected shortfall out-of-sample forecasts that rely on the notion of IID innovations. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--17}, 
number = {1}, 
volume = {17}
}
@article{10.1007/s10687-012-0164-z, 
year = {2013}, 
title = {{Second-order properties of risk concentrations without the condition of asymptotic smoothness}}, 
author = {Mao, Tiantian and Hu, Taizhong}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-012-0164-z}, 
abstract = {{For the purpose of risk management, the quantification of diversification benefits due to risk aggregation has received more attention in the recent literature. Consider a portfolio of n independent and identically distributed loss random variables with a common survival function F̄ possessing the property of second-order regular variation. Under the additional assumption that F̄ is asymptotically smooth, Degen et al. (Insur Math Econ 46:541-546, 2010) and Mao et al. (Insur Math Econ 51:449-456, 2012) derived second-order approximations of the risk concentrations based on the risk measures of Value-at-Risk and conditional tail expectation, respectively. In this paper, we remove the assumption of the asymptotic smoothness, and reestablish the second-order approximations of these two risk concentrations. © 2013 Springer Science+Business Media New York.}}, 
pages = {383--405}, 
number = {4}, 
volume = {16}
}
@article{10.1016/j.physa.2009.05.002, 
year = {2009}, 
title = {{On the closed form solutions for non-extensive Value at Risk}}, 
author = {Stavroyiannis, S. and Makris, I. and Nikolaidis, V.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2009.05.002}, 
abstract = {{We examine possible closed form solutions for the cumulative distribution function for systems where the probability density function can be adequately described by the generalized non-extensive statistics framework. Application to financial time series as a possible Value at Risk technique indicates reasonable agreement with the data under consideration, including all possible extremes and asymmetries of the returns. Numerical results to illustrate the efficiency of the method are presented. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {3536--3542}, 
number = {17}, 
volume = {388}
}
@article{10.1016/j.resourpol.2019.101497, 
year = {2019}, 
title = {{Modeling volatility of precious metals markets by using regime-switching GARCH models}}, 
author = {Naeem, Muhammad and Tiwari, Aviral Kumar and Mubashra, Sana and Shahbaz, Muhammad}, 
journal = {Resources Policy}, 
issn = {03014207}, 
doi = {10.1016/j.resourpol.2019.101497}, 
abstract = {{This paper aims to test the existence of regime changes by using MSGARCH models for modeling volatility of the four most famous precious metals, i.e. Gold, Silver, Palladium, and Platinum. We fitted around 72 MSGARCH models with different regimes (k=1,2,3) to the log-returns of each precious metal to test in-sample analysis of volatility. We compare 72 models for in-sample analysis by using the Akaike information criterion (AIC) and choose the best models for all precious metals’ series. Further, one-day ahead Value-at-Risk forecasting was conducted by the best MSGARCH. Our finding suggests the existence of regime changes in the GARCH process in most cases analyzed. We also find that regime-switching GARCH models outperform single–regime GARCH specifications when predicting the Value-at-Risk. The results indicate that using MSGARCH models may provide accurate Value-at-Risk predictions, and hence effective in portfolio optimization, pricing of derivatives and risk management etc. © 2019 Elsevier Ltd}}, 
pages = {101497}, 
number = {NA}, 
volume = {64}
}
@article{10.1016/j.ejor.2008.01.041, 
year = {2009}, 
title = {{Comments on "A mixed integer linear programming formulation of the optimal mean/Value-at-Risk portfolio problem"}}, 
author = {Lin, Chang-Chun}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2008.01.041}, 
abstract = {{Benati and Rizzi [S. Benati, R. Rizzi, A mixed integer linear programming formulation of the optimal mean/Value-at-Risk portfolio problem, European Journal of Operational Research 176 (2007) 423-434], in a recent proposal of two linear integer programming models for portfolio optimization using Value-at-Risk as the measure of risk, claimed that the two counterpart models are equivalent. This note shows that this claim is only partly true. The second model attempts to minimize the probability of the portfolio return falling below a certain threshold instead of minimizing the Value-at-Risk. However, the discontinuity of real-world probability values makes the second model impractical. An alternative model with Value-at-Risk as the objective is thus proposed. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {339--341}, 
number = {1}, 
volume = {194}
}
@article{10.1109/bife.2009.92, 
year = {2009}, 
title = {{A Markovian model for default risk in a network of sectors}}, 
author = {Ching, Wai-Ki and Leung, Ho-Yin and Jiang, Hao and Sun, Liang}, 
journal = {2009 International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2009.92}, 
abstract = {{In this paper, we study the problem of modeling the dependence of defaults in different sectors. We consider multiple default data sequences as a network and model them by using a Markov chain model. The new network model allows us to compute two important risk measures, namely, Value-at-Risk (VaR) and Expected Shortfall (ES). Numerical experiments are given to illustrate the practical implementation of the model. We also perform empirical studies of the model using real default data sequences and analyze the empirical behaviors of the risk measures arising from the model. © 2009 IEEE.}}, 
pages = {373--377}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/synasc.2013.77, 
year = {2013}, 
title = {{Managing risk behavior on an evolutionary market - A risk limits and value-at-risk measures approach}}, 
author = {Tirea, Monica and Negru, Viorel}, 
journal = {2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, 
issn = {NA}, 
doi = {10.1109/synasc.2013.77}, 
abstract = {{The goal of this paper is to develop a system able to coordinate a trader in optimizing a stock market portfolio in order to improve the profitability of a short or medium time period investment. The system is able to classify the risk and quantifies its effect on an investment based on sentiment analysis, certain characteristics, the traders confidence level, and by measuring the potential loss over a certain period of time. The system id also able to create certain types of portfolios based on different methods of computing the investment risk and on the associated level of confidence. We proposed a multi-agent system that uses sentiment analysis, volatility, Monte Carlo simulation, trust models and risk models/limits in order to choose the appropriate mix of investments in order to minimize the risk and maximize the gain on a stock portfolio taking in consideration also the traders possibilities to enter in an investment. A prototype was developed on which we validated our research. © 2013 IEEE.}}, 
pages = {543--550}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ijforecast.2019.05.014, 
year = {2020}, 
title = {{Forecast combinations for value at risk and expected shortfall}}, 
author = {Taylor, James W.}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2019.05.014}, 
abstract = {{Combining provides a pragmatic way of synthesising the information provided by individual forecasting methods. In the context of forecasting the mean, numerous studies have shown that combining often leads to improvements in accuracy. Despite the importance of the value at risk (VaR), though, few papers have considered quantile forecast combinations. One risk measure that is receiving an increasing amount of attention is the expected shortfall (ES), which is the expectation of the exceedances beyond the VaR. There have been no previous studies on combining ES predictions, presumably due to there being no suitable loss function for ES. However, it has been shown recently that a set of scoring functions exist for the joint estimation or backtesting of VaR and ES forecasts. We use such scoring functions to estimate combining weights for VaR and ES prediction. The results from five stock indices show that combining outperforms the individual methods for the 1\% and 5\% probability levels. © 2019 International Institute of Forecasters}}, 
pages = {428--441}, 
number = {2}, 
volume = {36}
}
@article{10.1057/s41274-017-0197-4, 
year = {2018}, 
title = {{Optimization of Value-at-Risk: Computational aspects of MIP formulations}}, 
author = {Pavlikov, Konstantin and Veremyev, Alexander and Pasiliao, Eduardo L.}, 
journal = {Journal of the Operational Research Society}, 
issn = {01605682}, 
doi = {10.1057/s41274-017-0197-4}, 
abstract = {{Optimization of Value-at-Risk is an important problem both from theoretical and practical standpoints. It can be represented through a class of chance-constrained optimization problems, which are generally hard to solve. Mixed integer problem formulations with big M constants is a standard way to approach such problems, where tightness of these constants is a crucial factor for good performance of a solver. This study aims to improve the tightness of existing big Ms by explicitly incorporating bounds on the optimal value of VaR into the problem formulation. Moreover, the lower bound is demonstrated to play an especially important role in obtaining tight big M constants, and a procedure to lift this bound is discussed. Finally, a "two-stage" solution approach is proposed, where the first stage solely deals with tightening the bounds, and the second stage employs improved bounds to redefine big Ms and solves the problem to optimality. Numerical experiments suggest that proposed solution methods can decrease solution time by up to 75\% compared to the most recent benchmark, which may allow to handle larger problem instances while using the same hardware. © 2017 Operational Research Society.}}, 
pages = {1--15}, 
number = {5}, 
volume = {69}
}
@article{10.1016/j.insmatheco.2014.04.006, 
year = {2014}, 
title = {{Optimal reinsurance with regulatory initial capital and default risk}}, 
author = {Cai, Jun and Lemieux, Christiane and Liu, Fangda}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.04.006}, 
abstract = {{In a reinsurance contract, a reinsurer promises to pay the part of the loss faced by an insurer in exchange for receiving a reinsurance premium from the insurer. However, the reinsurer may fail to pay the promised amount when the promised amount exceeds the reinsurer's solvency. As a seller of a reinsurance contract, the initial capital or reserve of a reinsurer should meet some regulatory requirements. We assume that the initial capital or reserve of a reinsurer is regulated by the value-at-risk (VaR) of its promised indemnity. When the promised indemnity exceeds the total of the reinsurer's initial capital and the reinsurance premium, the reinsurer may fail to pay the promised amount or default may occur. In the presence of the regulatory initial capital and the counterparty default risk, we investigate optimal reinsurance designs from an insurer's point of view and derive optimal reinsurance strategies that maximize the expected utility of an insurer's terminal wealth or minimize the VaR of an insurer's total retained risk. It turns out that optimal reinsurance strategies in the presence of the regulatory initial capital and the counterparty default risk are different both from optimal reinsurance strategies in the absence of the counterparty default risk and from optimal reinsurance strategies in the presence of the counterparty default risk but without the regulatory initial capital. © 2014.}}, 
pages = {13--24}, 
number = {1}, 
volume = {57}
}
@article{10.3390/risks8010016, 
year = {2020}, 
title = {{Assessing asset-liability risk with neural networks}}, 
author = {Cheridito, Patrick and Ery, John and Wüthrich, Mario V.}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks8010016}, 
eprint = {2105.12432}, 
abstract = {{We introduce a neural network approach for assessing the risk of a portfolio of assets and liabilities over a given time period. This requires a conditional valuation of the portfolio given the state of the world at a later time, a problem that is particularly challenging if the portfolio contains structured products or complex insurance contracts which do not admit closed form valuation formulas. We illustrate the method on different examples from banking and insurance. We focus on value-at-risk and expected shortfall, but the approach also works for other risk measures. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {16}, 
number = {1}, 
volume = {8}
}
@article{10.1109/icfte.2010.5499428, 
year = {2010}, 
title = {{Application of EGARCH-GED model in VaR measurement}}, 
author = {YU, Tianjun and WANG, Yang}, 
journal = {2010 International Conference on Financial Theory and Engineering}, 
issn = {NA}, 
doi = {10.1109/icfte.2010.5499428}, 
abstract = {{The GARCH model is used in simulating the volatility and VaR of the financial assets. The paper established an EGARCH-GED model to calculate the time varying VaR. Compared the VaR of the EGARCH-GED model and the GARCH model under the normal distribution and T distribution respectively, The paper checked the anticipated VaR in the previous step by employing failure rate test and back-testing. The result shows that GED distribution is fitted with the fat tail feature of the financial assets. Under different confident levels, the VaR predicated by EGARCH-GED is more accurate and has more low level risk to be overestimated or underestimated. © 2010 IEEE.}}, 
pages = {32--36}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2006.10.005, 
year = {2007}, 
title = {{Value at risk and the cross-section of hedge fund returns}}, 
author = {Bali, Turan G. and Gokcan, Suleyman and Liang, Bing}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2006.10.005}, 
abstract = {{Using two large hedge fund databases, this paper empirically tests the presence and significance of a cross-sectional relation between hedge fund returns and value at risk (VaR). The univariate and bivariate portfolio-level analyses as well as the fund-level regression results indicate a significantly positive relation between VaR and the cross-section of expected returns on live funds. During the period of January 1995 to December 2003, the live funds with high VaR outperform those with low VaR by an annual return difference of 9\%. This risk-return tradeoff holds even after controlling for age, size, and liquidity factors. Furthermore, the risk profile of defunct funds is found to be different from that of live funds. The relation between downside risk and expected return is found to be negative for defunct funds because taking high risk by these funds can wipe out fund capital, and hence they become defunct. Meanwhile, voluntary closure makes some well performed funds with large assets and low risk fall into the defunct category. Hence, the risk-return relation for defunct funds is more complicated than what implies by survival. We demonstrate how to distinguish live funds from defunct funds on an ex ante basis. A trading rule based on buying the expected to live funds and selling the expected to disappear funds provides an annual profit of 8-10\% depending on the investment horizons. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {1135--1166}, 
number = {4}, 
volume = {31}
}
@article{10.1007/s10687-006-0027-6, 
year = {2006}, 
title = {{Extreme VaR scenarios in higher dimensions}}, 
author = {Embrechts, Paul and Höing, Andrea}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-006-0027-6}, 
abstract = {{For a sequence of random variables X1, ..., Xn, the dependence scenario yielding the worst possible Value-at-Risk at a given level α for X1+...+ Xn is known for n=2. In this paper we investigate this problem for higher dimensions. We provide a geometric interpretation highlighting the dependence structures which imply the worst possible scenario. For a portfolio (X1,..., Xn) with given uniform marginals, we give an analytical solution sustaining the main result of Rüschendorf (Adv. Appl. Probab. 14(3):623-632, 1982). In general, our approach allows for numerical computations. © 2007 Springer Science+Business Media, LLC.}}, 
pages = {177--192}, 
number = {3-4}, 
volume = {9}
}
@article{10.1142/s0218539305001872, 
year = {2005}, 
title = {{Reliability design and RVaR}}, 
author = {TAPIERO, CHARLES S}, 
journal = {International Journal of Reliability, Quality and Safety Engineering}, 
issn = {02185393}, 
doi = {10.1142/s0218539305001872}, 
abstract = {{This paper provides an alternative valuation approach to reliability design based a concept of Value at Risk (VaR) used in finance to measure risk exposure. An accounting of the direct and indirect costs combined with the costs needed to meet contingent claims in case of default is used to redefine the traditional reliability design problem, resulting is a nonlinear optimization problem which we can solve by the usual methods. Applications are used to demonstrate the approach used. © World Scientific Publishing Company.}}, 
pages = {347--353}, 
number = {4}, 
volume = {12}
}
@article{10.21314/jrmv.2018.182, 
year = {2018}, 
title = {{A central limit theorem formulation for empirical bootstrap value-at-risk}}, 
author = {Mitic, Peter and Bloxham, Nicholas}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2018.182}, 
abstract = {{In this paper, the importance of the empirical bootstrap (EB) in assessing minimal operational risk capital is discussed, and an alternative way of estimating minimal operational risk capital using a central limit theorem (CLT) formulation is presented. The results compare favorably with risk capital obtained by fitting appropriate distributions to the same data. The CLT formulation is significant in validation because it provides an alternative approach to the calculation that is independent of both the empirical severity distribution and any dependent fitted distribution. © 2018 Infopro Digital Risk (IP) Limited.}}, 
pages = {49--83}, 
number = {1}, 
volume = {12}
}
@article{10.1111/j.1467-6419.2009.00590.x, 
year = {2009}, 
title = {{The ten commandments for managing value at risk under the basel II accord}}, 
author = {Jiménez‐Martín, Juan‐Ángel and McAleer, Michael and Pérez‐Amaral, Teodosio}, 
journal = {Journal of Economic Surveys}, 
issn = {09500804}, 
doi = {10.1111/j.1467-6419.2009.00590.x}, 
abstract = {{Under the Basel II Accord, banks and other authorized deposit-taking institutions are required to communicate their daily market risk estimates to the relevant national monetary authority at the beginning of each trading day, using one of a variety of value-at-risk (VaR) models to measure risk. The purpose of this paper is to provide a simple explanation and a set of prescriptions for managing VaR under the Basel II Accord. The commandments deal with understanding the Basel II colours, understanding the risk model before choosing, varying the choice of risk model, avoiding the green zone and being willing to violate, incurring large violations, stopping before the red zone, avoiding frequent violations, avoiding the estimation of large portfolios, aggregating portfolios into a single index and interpreting commandments sensibly as guidelines. © 2009 Blackwell Publishing Ltd.}}, 
pages = {850--855}, 
number = {5}, 
volume = {23}
}
@article{10.1080/01605682.2019.1654416, 
year = {2020}, 
title = {{Optimal package pricing in healthcare services}}, 
author = {Tanwar, Tushar and Kumar, U. Dinesh and Mustafee, Navonil}, 
journal = {Journal of the Operational Research Society}, 
issn = {01605682}, 
doi = {10.1080/01605682.2019.1654416}, 
abstract = {{Fixed pricing for healthcare services is emerging as an attractive business model for private healthcare service providers. Under fixed pricing (or flat rate) contract, the patient is charged a fixed price for the healthcare services irrespective of the actual cost incurred by the hospital. Such contracts increase the risks for the healthcare service provider, thus making pricing decision crucial. In this paper, we study uncertainty and analyse the flat rate pricing contract for a profit maximising hospital to find the optimal price of treatment and examined value-at-risk (VaR) associated with such contracts for a risk minimising hospital. Bounds on price were derived to support healthcare providers with price negotiations. We extended the basic models by adding constraints to obtain risk-adjusted optimal price. We proved analytically that the optimal price lies between profit maximisation value and risk minimisation value of price, which we refer to as the efficient pricing interval. Our models and insights provide practical support to private healthcare service providers for optimal pricing and keep them informed about their risk position. © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--13}, 
number = {11}, 
volume = {71}
}
@article{10.1016/j.insmatheco.2014.11.004, 
year = {2015}, 
title = {{Reducing model risk via positive and negative dependence assumptions}}, 
author = {Bignozzi, Valeria and Puccetti, Giovanni and Rüschendorf, Ludger}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.11.004}, 
abstract = {{We give analytical bounds on the Value-at-Risk and on convex risk measures for a portfolio of random variables with fixed marginal distributions under an additional positive dependence structure. We show that assuming positive dependence information in our model leads to reduced dependence uncertainty spreads compared to the case where only marginals information is known. In more detail, we show that in our model the assumption of a positive dependence structure improves the best-possible lower estimate of a risk measure, while leaving unchanged its worst-possible upper risk bounds. In a similar way, we derive for convex risk measures that the assumption of a negative dependence structure leads to improved upper bounds for the risk while it does not help to increase the lower risk bounds in an essential way. As a result we find that additional assumptions on the dependence structure may result in essentially improved risk bounds. © 2014 Elsevier B.V.}}, 
pages = {17--26}, 
number = {NA}, 
volume = {61}
}
@article{10.1016/j.procs.2017.05.156, 
year = {2017}, 
title = {{Utilizing Intel Advanced Vector Extensions for Monte Carlo Simulation based Value at Risk Computation}}, 
author = {Liyanage, D.N.S.S. and Fernando, G.V.M.P.A. and Arachchi, D.D.M.M. and Karunathilaka, R.D.D.T. and Perera, A.S.}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2017.05.156}, 
abstract = {{Value at Risk (VaR) is a statistical method of predicting market risk associated with financial portfolios. There are numerous statistical models which forecast VaR and out of those, Monte Carlo Simulation is a commonly used technique with a high accuracy though it is computationally intensive. Calculating VaR in real time is becoming a need of short term traders in current day markets and adapting Monte Carlo method of VaR computation for real time calculation poses a challenge due to the computational complexity involved with the simulation step of the Monte Carlo Simulation. The simulation process has an independent set of tasks. Hence a performance bottleneck occurs during the sequential execution of these independent tasks. By parallelizing these tasks, the time taken to calculate the VaR for a portfolio can be reduced significantly. In order to address this issue, we looked at utilizing the Advanced Vector Extensions (AVX) technology to parallelize the simulation process. We compared the performance of the AVX based solution against the sequential approach as well as against a multithreaded solution and a GPU based solution. The results showed that the AVX approach outperformed the GPU approach for up to an iteration count of 200000. Since such a number of iterations is generally not required to gain a sufficiently accurate VaR measure, it makes sense both computationally and economically to utilize AVX for Monte Carlo method of VaR computation. © 2017 The Authors. Published by Elsevier B.V.}}, 
pages = {626--634}, 
number = {NA}, 
volume = {108}
}
@article{10.1142/s2010139218400074, 
year = {2018}, 
title = {{Financial Companies' Failures: Early Warning Information from Systematic and Systemic Risk Measures}}, 
author = {Cipollini, Fabrizio and Giannozzi, Alessandro and Menchetti, Fiammetta and Roggi, Oliviero}, 
journal = {Quarterly Journal of Finance}, 
issn = {20101392}, 
doi = {10.1142/s2010139218400074}, 
abstract = {{Following the 2007-2008 financial crisis, advanced risk measures were proposed with the specific aim of quantifying systemic risk, since the existing systematic (market) risk measures seemed inadequate to signal the collapse of an entire financial system. The paper aims at comparing the systemic risk measures and the earlier market risk measures regarding their predictive ability toward the failure of financial companies. Focusing on the 2007-2008 period and considering 28 large US financial companies (among which nine defaulted in the period), four systematic and four systemic risk measures are used to rank the companies according to their risk and to estimate their relationship with the company's failure through a survival Cox model. We found that the two groups of risk measures achieve similar scores in the ranking exercise, and that both show a significant effect on the time-to-default of the financial institutions. This last result appears even stronger when the Cox model uses, as covariates, the risk measures evaluated one, three and six months before. Considering this last case, the most predictive risk measures about the default risk of financial institutions were the Expected Shortfall, the Value-at-Risk, the CoVaR and the SES. We contribute to the literature in two ways. We provide a way to compare risk measures based on their predictive ability toward a situation, the company's failure, which is the most catastrophic event for a company. The survival model approach allows to map each risk measure in terms of probability of default over a given time horizon. We note, finally, that although focused on the Great Recession in US, the analysis can be applied to different periods and countries. © 2018 World Scientific Publishing Company.}}, 
pages = {1840007}, 
number = {4}, 
volume = {8}
}
@article{10.1155/2020/9763065, 
year = {2020}, 
title = {{Modeling and Risk Analysis Using Parametric Distributions with an Application in Equity-Linked Securities}}, 
author = {Choi, Sun-Yong and Yoon, Ji-Hun}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2020/9763065}, 
abstract = {{In this study, we model the returns of a stock index using various parametric distribution models. There are four indices used in this study: HSCEI, KOSPI 200, S\&P 500, and EURO STOXX 50. We applied 12 distributions to the data of these stock indices - Cauchy, Laplace, normal, Student's t, skew normal, skew Cauchy, skew Laplace, skew Student's t, hyperbolic, normal inverse Gaussian, variance gamma, and general hyperbolic - for the parametric distribution model. In order to choose the best-fit distribution for describing the stock index, we used the information criteria, goodness-of-fit test, and graphical tail test for each stock index. We estimated the value-at-risk (VaR), one of the most popular management concepts in the area of risk management, for the return of stock indices. Furthermore, we applied the parametric distributions to the risk analysis of equity-linked securities (ELS) as they are a very popular financial product on the Korean financial market. Relevant risk measures, such as VaR and conditional tail expectation, are calculated using various distributions. For calculating the risk measures, we used Monte Carlo simulations under the best-fit distribution. According to the empirical results, investing in ELS is more risky than investing in securities, and the risk measure of the ELS heavily depends on the type of security. © 2020 Sun-Yong Choi and Ji-Hun Yoon.}}, 
pages = {1--20}, 
number = {NA}, 
volume = {2020}
}
@article{10.5506/aphyspolb.43.2001, 
year = {2012}, 
title = {{On the existence of jumps in financial time series}}, 
author = {Kostrzewski, M}, 
journal = {Acta Physica Polonica B}, 
issn = {05874254}, 
doi = {10.5506/aphyspolb.43.2001}, 
abstract = {{In this research two methods of detecting jumps are presented. One is based on the nonparametric approach, whereas the other-on the JD(M)J model. Bayesian inference is applied to detect jumps in the JD(M)J model. Intraday and daily rates of return are under consideration. The empirical results imply the existence of jumps. The information on existing jumps is exploited in a forecasting experiment focused on Value at Risk predictions.}}, 
pages = {2001}, 
number = {10}, 
volume = {43}
}
@article{10.21314/jor.2020.445, 
year = {2020}, 
title = {{Bias-corrected estimators for the vasicek model: An application in risk measure estimation}}, 
author = {Guo, Zi-Yi}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2020.445}, 
abstract = {{We evaluate the usefulness of bias-correction methods in enhancing the Vasicek model for market risk and counterparty risk management practices. The naive bias-corrected estimator, the Tang and Chen bias-corrected estimator and the Bao et al bias-corrected estimator are selected to be compared against the benchmark least squares (LS) estimator. Our Monte Carlo experiment shows that the bias-corrected estimators substantially reduce the small sample bias of the LS estimator for the Vasicek model and project much more accurate value-at-risk and potential future exposure estimations. Even if the sample length is as long as 30 years, the improvements are still significant, especially for the cases where the mean-reversion parameter is close to zero. The applications to real data further demonstrate that the small sample bias of the LS estimator cannot be ignored and one should consider bias-corrected estimators for the Vasicek model. © 2021 Infopro Digital Risk (IP) Limited.}}, 
pages = {71--104}, 
number = {2}, 
volume = {23}
}
@article{10.1016/s0378-4266(00)00159-x, 
year = {2001}, 
title = {{Returns synchronization and daily correlation dynamics between international stock markets}}, 
author = {Martens, Martin and Poon, Ser-Huang}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(00)00159-x}, 
abstract = {{The use of close-to-close returns underestimates returns correlation because international stock markets have different trading hours. With the availability of 16:00 (London time) stock market series, we find dynamics of daily correlation and covariance, estimated using two non-synchroneity adjustment procedures, to be substantially different from their synchronous counterparts. Conditional correlation may have different signs depending on the model and data type used. Other findings include volatility spillover from the US to the UK (and France), and a reverse spillover which is not documented before. Also, unlike previous findings, we found the increase in daily correlation is prominent only under extremely adverse conditions when a large negative return has been registered. © 2001 Elsevier Science B.V.}}, 
pages = {1805--1827}, 
number = {10}, 
volume = {25}
}
@article{10.1109/cso.2014.158, 
year = {2014}, 
title = {{VaR for loan portfolio in uncertain environment}}, 
author = {Ning, Yufu. and Wang, Xiao. and Pan, Dongjing.}, 
journal = {2014 Seventh International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2014.158}, 
abstract = {{As a risk measure method, VaR (value at risk) has been applied widely in many domains. This paper researches the VaR measure way in uncertain environment, and applies it in loan portfolio. When all the return rates are the special uncertain variables, we can solve the crisp equivalents of VaR for loan portfolio. When return rates are generic uncertain variables, uncertain simulation is designed to calculate the VaR. Finally, numerical examples are given to illustrate the feasibility and effectiveness of the proposed method. © 2014 IEEE.}}, 
pages = {294--297}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jor.2015.302, 
year = {2015}, 
title = {{Extreme value theory, asset ranking and threshold choice: A practical note on VaR estimation}}, 
author = {Auer, Benjamin R}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.302}, 
abstract = {{We analyze asset rankings derived from state-of-the-art peak-over-threshold (POT) approaches to estimate value-at-risk (VaR). Supported by a variety of robustness checks, we gain three important insights for portfolio managers investing in equity and commodity markets. First, even though POT methods are known to yield more precise VaR estimates than classic techniques based on the normal distribution assumption or historical simulation, all techniques yield almost identical rankings. Second, even though the choice of threshold crucially influences VaR estimates, it does not significantly change asset rankings. These two results are most pronounced when the portfolio manager’s focus is on identifying the best or worst assets in terms of VaR. Third, unconditional and conditional POT approaches differ considerably in the rankings they generate. Thus, neglecting the non-independent-and-identically-distributed property of returns can lead to distinctly different decisions in a risk-based asset selection process. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {27--44}, 
number = {1}, 
volume = {18}
}
@article{10.1080/00036846.2013.854299, 
year = {2014}, 
title = {{Do futures prices exhibit maturity effect? A nonparametric revisit}}, 
author = {Liu, Wei-han}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2013.854299}, 
abstract = {{The maturity effect (ME) of futures prices postulated by Samuelson (1965) is re-examined using three nonparametric tests. The consistent entropy asymmetry test by Racine and Maasoumi (2007) indicates that variance is an appropriate risk or uncertainty measure for ME, and value-at-risk and expected shortfall are also adopted. The Kolmogorov-Smirnov dominance test and Wilcoxon rank sum and signed rank test are employed to rank the estimates of the three risk measures under a moving-window framework. The testing outcomes are contingent on futures type, testing method and risk measures. The testing outcomes show mild support for ME. © 2013 Taylor \& Francis.}}, 
pages = {813--825}, 
number = {8}, 
volume = {46}
}
@article{10.1142/s0219024918500103, 
year = {2018}, 
title = {{A liquidation risk adjustment for value at risk and expected shortfall}}, 
author = {WAGALATH, LAKSHITHE and ZUBELLI, JORGE P}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024918500103}, 
abstract = {{This paper proposes an intuitive and flexible framework to quantify liquidation risk for financial institutions. We develop a model where the "fundamental" dynamics of assets is modified by price impacts from fund liquidations. We characterize mathematically the liquidation schedule of financial institutions and study in detail the fire sales resulting endogenously from margin constraints when a financial institution trades through an exchange. Our study enables to obtain tractable formulas for the value at risk and expected shortfall of a financial institution in the presence of fund liquidation. In particular, we find an additive decomposition for liquidation-adjusted risk measures. We show that such a measure can be expressed as a "fundamental" risk measure plus a liquidation risk adjustment that is proportional to the size of fund positions as a fraction of asset market depths. Our results can be used by risk managers in financial institutions to tackle liquidity events arising from fund liquidations better and adjust their portfolio allocations to liquidation risk more accurately. © 2018 World Scientific Publishing Company.}}, 
pages = {1850010}, 
number = {3}, 
volume = {21}
}
@article{10.3390/e22121425, 
year = {2020}, 
title = {{Portfolio tail risk: A multivariate extreme value theory approach}}, 
author = {Božović, Miloš}, 
journal = {Entropy}, 
issn = {10994300}, 
doi = {10.3390/e22121425}, 
pmid = {33348820}, 
abstract = {{This paper develops a method for assessing portfolio tail risk based on extreme value theory. The technique applies separate estimations of univariate series and allows for closed-form expressions for Value at Risk and Expected Shortfall. Its forecasting ability is tested on a portfolio of U.S. stocks. The in-sample goodness-of-fit tests indicate that the proposed approach is better suited for portfolio risk modeling under extreme market movements than comparable multivariate parametric methods. Backtesting across multiple quantiles demonstrates that the model cannot be rejected at any reasonable level of significance, even when periods of stress are included. Numerical simulations corroborate the empirical results. © 2020 by the author. Licensee MDPI, Basel, Switzerland.}}, 
pages = {1425}, 
number = {12}, 
volume = {22}
}
@article{10.1007/s10436-007-0081-3, 
year = {2008}, 
title = {{Optimal portfolio allocation under the probabilistic VaR constraint and incentives for financial innovation}}, 
author = {Daníelsson, Jón and Jorgensen, Bjørn N. and Vries, Casper G. de and Yang, Xiaoguang}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-007-0081-3}, 
abstract = {{We characterize the investor's optimal portfolio allocation subject to a budget constraint and a probabilistic VaR constraint in complete markets environments with a finite number of states. The set of feasible portfolios might no longer be connected or convex, while the number of local optima increases exponentially with the number of states, implying computational complexity. The optimal constrained portfolio allocation may therefore not be monotonic in the state-price density. We propose a type of financial innovation, which splits states of nature, that is shown to weakly enhance welfare, restore monotonicity of the optimal portfolio allocation in the state-price density, and reduce computational complexity. © 2007 Springer-Verlag.}}, 
pages = {345--367}, 
number = {3}, 
volume = {4}
}
@article{10.1007/978-3-642-30864-2_27, 
year = {2012}, 
title = {{A news-based approach for computing historical value-at-risk}}, 
author = {Hogenboom, Frederik and Winter, Michael de and Frasincar, Flavius and Hogenboom, Alexander}, 
journal = {Advances in Intelligent Systems and Computing}, 
issn = {21945357}, 
doi = {10.1007/978-3-642-30864-2\_27}, 
abstract = {{Within the field of finance, Value-at-Risk (VaR) is a widely adopted tool to assess portfolio risk. When calculating VaR based on historical stock return data, the data could be sensitive to outliers caused by seldom occurring news events in the sampled period. Using a data set of news events, of which the irregular events are identified using a Poisson distribution, we research whether the VaR accuracy can be improved by considering news events as additional input in the calculation. Our experiments show that when a rare event occurs, removing the event-generated noise from the stock prices for a small, optimized time window can improve VaR predictions. © 2012 Springer-Verlag.}}, 
pages = {283--292}, 
number = {NA}, 
volume = {171 AISC}
}
@article{10.1007/978-981-13-3441-2_12, 
year = {2019}, 
title = {{Improved conditional value-at-risk (CVaR) based method for diversified bond portfolio optimization}}, 
author = {Rifin, Nor Idayu Mat and Othman, Nuru’l-‘Izzah and Ambia, Shahirulliza Shamsul and Ismail, Rashidah}, 
journal = {Communications in Computer and Information Science}, 
issn = {18650929}, 
doi = {10.1007/978-981-13-3441-2\_12}, 
abstract = {{In this study, an improved CVaR-based Portfolio Optimization Method is presented. The method was used to test the performance of a diversified bond portfolio in providing low expected loss and optimal CVaR. A hypothetical diversified bond portfolio, which is a combination of Islamic bond or Sukuk and conventional bond, was constructed using bonds issued by four banking institutions. The performance of the improved method is determined by comparing the generated returns of the method against the existing CVaR-based Portfolio Optimization Method. The simulation of the optimization process of both methods was carried out by using the Geometric Brownian Motion-based Monte Carlo Simulation method. The results of the improved CVaR portfolio optimization method show that by restricting the upper and lower bounds with certain floor and ceiling bond weights using volatility weighting schemes, the expected loss can be reduced and an optimal CVaR can be achieved. Thus, this study shows that the improved CVaR-based Portfolio Optimization Method is able to provide a better optimization of a diversified bond portfolio in terms of reducing the expected loss, and hence maximizes the returns. © Springer Nature Singapore Pte Ltd. 2019.}}, 
pages = {149--160}, 
number = {NA}, 
volume = {937}
}
@article{10.21314/jrmv.2018.185, 
year = {2018}, 
title = {{The validation of filtered historical value-at-risk models}}, 
author = {Gurrola-Perez, Pedro}, 
journal = {Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2018.185}, 
abstract = {{Recent value-at-risk (VaR) models based on historical simulation often incorporate approaches where the volatility of the historical sample is rescaled or filtered to better reflect current market conditions. These filtered historical simulation (FHS)VaR models are nowwidely used in the industry and, as is usually the case withVaRmodels, they are validated through backtesting. However, while backtesting is a natural way of testing a percentile forecast, it is not specifically designed to capture other features of the model, such as its efficiency in adapting to new volatility conditions. In this paper, we discuss the limitations of backtesting as a tool to assess the performance of FHS models and, using a Monte Carlo simulation framework, we examine whether incorporating information about the size of the breaches (through the use of score functions, for example) can improve the efficiency of these tests. The results show that, even when incorporating the size of theVaR violations, tests based solely on the breaches generally fail as a tool to discriminate between different calibrations of the decay factor; they also tend to be biased. Among the alternative tests considered, the asymmetric piecewise linear score performs best overall, followed by the dynamicquantile test.We conclude by considering some empirical examples. © 2018 Infopro Digital Risk (IP) Limited.}}, 
number = {1}, 
volume = {12}
}
@article{10.1016/j.irfa.2009.12.001, 
year = {2010}, 
title = {{Systematic risk and time scales: New evidence from an application of wavelet approach to the emerging Gulf stock markets}}, 
author = {Masih, Mansur and Alzahrani, Mohammed and Al-Titi, Omar}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2009.12.001}, 
abstract = {{The paper is the first attempt to estimate systematic risk 'beta' at different time scales in the context of the emerging Gulf Cooperation Council (GCC) equity markets by applying a relatively new approach in finance known as wavelet analysis. Our results indicate that on average beta coefficients in all GCC countries show a multiscale tendency. This is consistent with our theoretical expectation that stock market investors have different time horizons due to different trading strategies and that is also reflective of the characteristics of the GCC markets in particular in that they are less developed, less liquid, involve more transaction costs, highly dependent on individual investors, and prone to infrequent trading. Further, we analyze the impact of different time scales on Value at Risk (VaR) and find that VaR measured at different time scales suggests that risk tends to be concentrated more at the higher frequencies (lower time scales) of the data. The results are plausible and intuitive and have strong policy implications. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {10--18}, 
number = {1}, 
volume = {19}
}
@article{10.1016/j.iref.2010.11.013, 
year = {2011}, 
title = {{Modeling the fat tails in Asian stock markets}}, 
author = {Kittiakarasakun, Jullavut and Tse, Yiuman}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2010.11.013}, 
abstract = {{We test whether stock returns in the Asian markets are characterized by infinite variance or just large variance, which has an important implication for the applicability of many financial models in Asian market data. Employing the extreme value framework, we find that the Asian index return distributions are fat-tailed but have finite variance. However, the tails of the distributions behave similarly to those in the U.S. and the MSCI World index returns, suggesting that any financial model or risk management tool that incorporates the second moment would work equally well for the Asian market data as it does for developed market data. We apply the Value-at-Risk method using Asian and U.S. data and find no significant difference in performance. © 2010 Elsevier Inc.}}, 
pages = {430--440}, 
number = {3}, 
volume = {20}
}
@article{10.1016/j.cor.2010.09.011, 
year = {2011}, 
title = {{Selection of a dynamic supply portfolio in make-to-order environment withrisks}}, 
author = {Sawik, Tadeusz}, 
journal = {Computers \& Operations Research}, 
issn = {03050548}, 
doi = {10.1016/j.cor.2010.09.011}, 
abstract = {{The problem of a multi-period supplier selection and order allocation in make-to-order environment in the presence of supply chain disruption and delay risks is considered. Given a set of customer orders for finished products, the decision maker needs to decide from which supplier and when to purchase product-specific parts required for each customer order to meet customer requested due date at a low cost and to mitigate the impact of supply chain risks. The selection of suppliers and the allocation of orders over time is based on price and quality of purchased parts and reliability of supplies. For selection of dynamic supply portfolio a mixed integer programming approach is proposed to incorporate risk that uses conditional value-at-risk via scenario analysis. In the scenario analysis, the low-probability and high-impact supply disruptions are combined with the high probability and low impact supply delays. The proposed approach is capable of optimizing the dynamic supply portfolio by calculating value-at-risk of cost per part and minimizing expected worst-case cost per part simultaneously. Numerical examples are presented and some computational results are reported. © 2010 Elsevier Ltd. All rights reserved.}}, 
pages = {782--796}, 
number = {4}, 
volume = {38}
}
@article{10.1016/j.jeconom.2005.06.026, 
year = {2006}, 
title = {{Analysis of high dimensional multivariate stochastic volatility models}}, 
author = {Chib, Siddhartha and Nardari, Federico and Shephard, Neil}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2005.06.026}, 
abstract = {{This paper is concerned with the Bayesian estimation and comparison of flexible, high dimensional multivariate time series models with time varying correlations. The model proposed and considered here combines features of the classical factor model with that of the heavy tailed univariate stochastic volatility model. A unified analysis of the model, and its special cases, is developed that encompasses estimation, filtering and model choice. The centerpieces of the estimation algorithm (which relies on MCMC methods) are: (1) a reduced blocking scheme for sampling the free elements of the loading matrix and the factors and (2) a special method for sampling the parameters of the univariate SV process. The resulting algorithm is scalable in terms of series and factors and simulation-efficient. Methods for estimating the log-likelihood function and the filtered values of the time-varying volatilities and correlations are also provided. The performance and effectiveness of the inferential methods are extensively tested using simulated data where models up to 50 dimensions and 688 parameters are fit and studied. The performance of our model, in relation to various multivariate GARCH models, is also evaluated using a real data set of weekly returns on a set of 10 international stock indices. We consider the performance along two dimensions: the ability to correctly estimate the conditional covariance matrix of future returns and the unconditional and conditional coverage of the 5\% and 1\% value-at-risk (VaR) measures of four pre-defined portfolios. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {341--371}, 
number = {2}, 
volume = {134}
}
@article{10.1016/j.insmatheco.2020.02.007, 
year = {2020}, 
title = {{Risk analysis with categorical explanatory variables}}, 
author = {Kang, Seul Ki and Peng, Liang and Xiao, Hongmin}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.02.007}, 
abstract = {{To better forecast the Value-at-Risk of the aggregate insurance losses, Heras et al. (2018) propose a two-step inference of using logistic regression and quantile regression without providing detailed model assumptions, deriving the related asymptotic properties, and quantifying the inference uncertainty. This paper argues that the application of quantile regression at the second step is not necessary when explanatory variables are categorical. After describing the explicit model assumptions, we propose another two-step inference of using logistic regression and the sample quantile. Also, we provide an efficient empirical likelihood method to quantify the uncertainty. A simulation study confirms the good finite sample performance of the proposed method. © 2020 Elsevier B.V.}}, 
pages = {238--243}, 
number = {NA}, 
volume = {91}
}
@article{10.1080/14697688.2018.1459806, 
year = {2018}, 
title = {{How does the choice of Value-at-Risk estimator influence asset allocation decisions?}}, 
author = {Scheller, Felix and Auer, Benjamin R.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2018.1459806}, 
abstract = {{Considering the growing need for managing financial risk, Value-at-Risk (VaR) prediction and portfolio optimisation with a focus on VaR have taken up an important role in banking and finance. Motivated by recent results showing that the choice of VaR estimator does not crucially influence decision-making in certain practical applications (e.g. in investment rankings), this study analyses the important question of how asset allocation decisions are affected when alternative VaR estimation methodologies are used. Focusing on the most popular, successful and conceptually different conditional VaR estimation techniques (i.e. historical simulation, peak over threshold method and quantile regression) and the flexible portfolio model of Campbell et al. [J. Banking Finance. 2001, 25(9), 1789–1804], we show in an empirical example and in a simulation study that these methods tend to deliver similar asset weights. In other words, optimal portfolio allocations appear to be not very sensitive to the choice of VaR estimator. This finding, which is robust in a variety of distributional environments and pre-whitening settings, supports the notion that, depending on the specific application, simple standard methods (i.e. historical simulation) used by many commercial banks do not necessarily have to be replaced by more complex approaches (based on, e.g. extreme value theory). © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--18}, 
number = {12}, 
volume = {18}
}
@article{10.1016/j.orl.2008.04.006, 
year = {2008}, 
title = {{Polymatroids and mean-risk minimization in discrete optimization}}, 
author = {Atamtürk, Alper and Narayanan, Vishnu}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2008.04.006}, 
abstract = {{We study discrete optimization problems with a submodular mean-risk minimization objective. For 0-1 problems a linear characterization of the convex lower envelope is given. For mixed 0-1 problems we derive an exponential class of conic quadratic valid inequalities. We report computational experiments on risk-averse capital budgeting problems with uncertain returns. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {618--622}, 
number = {5}, 
volume = {36}
}
@article{10.1016/j.jspi.2012.01.008, 
year = {2012}, 
title = {{Large-sample confidence intervals for risk measures of location-scale families}}, 
author = {Bae, Taehan and Iscoe, Ian}, 
journal = {Journal of Statistical Planning and Inference}, 
issn = {03783758}, 
doi = {10.1016/j.jspi.2012.01.008}, 
abstract = {{For a loss distribution belonging to a location-scale family, F μ,σ, the risk measures, Value-at-Risk and Expected Shortfall are linear functions of the parameters: μ+τσ where τ is the corresponding risk measure of the mean-zero and unit-variance member of the family. For each risk measure, we consider a natural estimator by replacing the unknown parameters μ and σ by the sample mean and (bias corrected) sample standard deviation, respectively. The large-sample parametric confidence intervals for the risk measures are derived, relying on the asymptotic joint distribution of the sample mean and sample standard deviation. Simulation studies with the Normal, Laplace and Gumbel families illustrate that the derived asymptotic confidence intervals for Value-at-Risk and Expected Shortfall outperform those of Bahadur (1966) and Brazauskas et al. (2008), respectively. The method can also be effectively applied to Log-location-scale families whose supports are positive reals; an illustrative example is given in the area of financial credit risk. © 2012 Elsevier B.V.}}, 
pages = {2032--2046}, 
number = {7}, 
volume = {142}
}
@article{10.1093/jjfinec/nbv015, 
year = {2016}, 
title = {{The geometric-VaR backtesting method}}, 
author = {Pelletier, Denis and Wei, Wei}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbv015}, 
abstract = {{This article develops a new test to evaluate value-at-risk (VaR) forecasts. VaR is a standard risk measure widely utilized by financial institutions and regulators, yet estimating VaR is a challenging problem, and popular VaR forecast relies on unrealistic assumptions. Hence, assessing the performance of VaR is of great importance. We propose the geometric-VaR test which utilizes the duration between the violations of VaR as well as the value of VaR. We conduct a Monte Carlo study based on desk-level data and we find that our test has high power against various alternatives. © The Author, 2015. Published by Oxford University Press. All rights reserved.}}, 
pages = {725--745}, 
number = {4}, 
volume = {14}
}
@article{10.1080/03610926.2017.1343844, 
year = {2018}, 
title = {{Some new results on aggregate claim amounts from two heterogeneous Marshall–Olkin extended exponential portfolios}}, 
author = {Barmalzan, Ghobad and Najafabadi, Amir T Payandeh and Balakrishnan, Narayanaswamy}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2017.1343844}, 
abstract = {{In this work, we discuss some stochastic comparisons of two aggregate claim amounts. Applications of our results to the value-at-risk and tail-value-at-risk are also mentioned. It is also shown that the aggregate claim amounts of risks exhibiting a weak form of dependence known as positive cumulative dependence (negatively associated) is larger (smaller) in convex order than the corresponding aggregate claim amounts under the theoretical independence assumption. The obtained results show that the correlations between individual risks increase stop-loss premiums corresponding to aggregate claim amounts. The results established here complete and extend the results of Barmalzan, Payandeh Najafabadi, and Balakrishnan (2017, Theory of Probability and its Applications, to appear). © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {2779--2794}, 
number = {11}, 
volume = {47}
}
@article{10.1002/9781118267080.ch17, 
year = {2011}, 
title = {{Risk Management: Techniques in Search of a Strategy}}, 
author = {Rizzi, Joe and Fraser, John and Simkins, Betty J.}, 
issn = {NA}, 
doi = {10.1002/9781118267080.ch17}, 
abstract = {{[No abstract available]}}, 
pages = {303--320}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/cccm.2009.5267956, 
year = {2009}, 
title = {{Mathematics and applications of risk management in E-commerce}}, 
author = {Gang, Chen}, 
journal = {2009 ISECS International Colloquium on Computing, Communication, Control, and Management}, 
issn = {NA}, 
doi = {10.1109/cccm.2009.5267956}, 
abstract = {{Risk management mathematics and applications in E-commerce have four periods: plan, value risk, manage risk, analysis and improve. The plan period is the preparative time. Value at Risk is the maximum loss not exceeded with a given probability defined as the confidence level, over a given period of time. Manage risk control the risk in order to reduce the loss and the cost. Authentication and analysis of risk case would help to improve the risk manage. Vale risk management would help get CIO make risk management investment more efficiency. ©2009 IEEE.}}, 
pages = {160--163}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.amc.2009.08.005, 
year = {2009}, 
title = {{Value-at-Risk via mixture distributions reconsidered}}, 
author = {Haas, Markus}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2009.08.005}, 
abstract = {{Value-at-Risk (VaR) has evolved as one of the most prominent measures of downside risk in financial markets. Zhang and Cheng [M.-H. Zhang, Q.-S. Cheng, An Approach to VaR for capital markets with Gaussian mixture, Applied Mathematics and Computation 168 (2005) 1079-1085] proposed an approach to VaR for daily returns based on Gaussian mixtures, which have become rather popular in empirical economics and finance since the seminal paper of Hamilton [J.D. Hamilton, A new approach to the economic analysis of nonstationary time series and the business cycle, Econometrica 57 (2) (1989) 357-384]. However, they do not conduct tests to assess the accuracy of the mixture-implied VaR measures. Recently, Guidolin and Timmermann [M. Guidolin, A. Timmermann, Term structure of risk under alternative econometric specifications, Journal of Econometrics, 131 (2006) 285-308] showed that Markov mixture models do well in measuring VaR at a monthly frequency, but the results may not hold for daily returns due to their more pronounced non-Gaussian features. This paper provides an extensive application of various Markov mixture models to VaR for daily returns of major European stock markets, including out-of-sample backtesting. To accommodate the properties of daily returns, we consider both Gaussian and Student's t mixtures, and we compare the performance of both uni- and multivariate models under different parameter updating schemes. We find that a univariate mixture of two Student's t distributions performs best overall. However, by the example of the recent turmoil in financial markets, we also highlight a weak point of the approach. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {2103--2119}, 
number = {6}, 
volume = {215}
}
@article{10.3917/redp.235.0763, 
year = {2013}, 
title = {{Risk management soft information and bankers' incentives}}, 
author = {Godbillon-Camus, Brigitte and Godlewski, Christophe J}, 
journal = {Revue d'économie politique}, 
issn = {03732630}, 
doi = {10.3917/redp.235.0763}, 
abstract = {{We investigate the influence of soft information on integrated risk management in banks. Risk management, capital budgeting and capital structure policies are performed by bank management using information on loan quality provided by a loan officer. The information can be a combination of hard and soft information, the latter being more precise but not verifiable. We set-up a principal-agent framework with moral hazard with hidden information to perform our theoretical analysis. The results show the existence of an incentive of the loan officer to manipulate the soft information, which can be prevented provided an incentive compatible compensation contract. We then show that access to more accurate soft information allows bank management to economize on capital allocation for Value at Risk coverage and to expand capital budgeting for loans origination.}}, 
pages = {763}, 
number = {5}, 
volume = {123}
}
@article{10.1016/j.insmatheco.2018.09.004, 
year = {2018}, 
title = {{Extreme quantile estimation for β-mixing time series and applications}}, 
author = {Chavez-Demoulin, Valérie and Guillou, Armelle}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2018.09.004}, 
abstract = {{In this paper, we discuss the application of extreme value theory in the context of stationary β-mixing sequences that belong to the Fréchet domain of attraction. In particular, we propose a methodology to construct bias-corrected tail estimators. Our approach is based on the combination of two estimators for the extreme value index to cancel the bias. The resulting estimator is used to estimate an extreme quantile. In a simulation study, we outline the performance of our proposals that we compare to alternative estimators recently introduced in the literature. Also, we compute the asymptotic variance in specific examples when possible. Our methodology is applied to two datasets on finance and environment. © 2018 Elsevier B.V.}}, 
pages = {59--74}, 
number = {NA}, 
volume = {83}
}
@article{10.1061/41127(382)438, 
year = {2010}, 
title = {{A multi-attribute stochastic programming approach for supply chain planning with var}}, 
author = {Lu, Huapu and Yu, Xinxin and Zhao, Xiaoqiang and Li, Yue and Cheng, Nan}, 
journal = {ICCTP 2010}, 
issn = {NA}, 
doi = {10.1061/41127(382)438}, 
abstract = {{This study deals with the problem of supply chain planning with risk management. A new multi-attribute stochastic programming model is proposed. Demand, supply, transportation, shortage, and capacity expansion costs are considered as uncertain parameters in the supply chain investment decision. To develop a robust model, an additional attribute function is added to the traditional supply chain design problem. This multi-attribute model includes: (i) the minimization of the sum of current investment costs and the expected future processing, transportation, shortage, and capacity expansion costs, and (ii) the minimization of the VaR (Value-at-Risk) of the total future cost risk. Finally, a numerical example is given to illustrate the ability of the proposed model to supply chain planning. © 2010 ASCE.}}, 
pages = {4060--4067}, 
number = {NA}, 
volume = {382}
}
@article{10.1239/jap/1118777180, 
year = {2005}, 
title = {{Multiperiod conditional distribution functions for conditionally normal garch(1, 1) models}}, 
author = {Brummelhuis, Raymond and Guégan, Dominique}, 
journal = {Journal of Applied Probability}, 
issn = {00219002}, 
doi = {10.1239/jap/1118777180}, 
abstract = {{We study the asymptotic tail behavior of the conditional probability distributions of r t+k and r t+1 + ··· + r t+k when (r t) t∈ℕ is a GARCH(1, 1) process. As an application, we examine the relation between the extreme lower quantiles of these random variables. © Applied Probability Trust 2005.}}, 
pages = {426--445}, 
number = {2}, 
volume = {42}
}
@article{10.1287/mnsc.2018.3157, 
year = {2020}, 
title = {{Option-implied intrahorizon value at risk}}, 
author = {Leippold, Markus and Vasiljević, Nikola}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.2018.3157}, 
abstract = {{In this paper, we theoretically and empirically study the intrahorizon value at risk (iVaR) in a general jump-diffusion setting. We propose a new class of models of asset returns, the displaced mixed exponential model, which can arbitrarily closely approximate finite and infinite activity Lévy processes. We then derive analytical results for the iVaR and disentangle, in a theoretically consistent way, the jump and diffusion contributions to the intrahorizon risk. We estimate historical and option-implied value at risk and iVaR for several popular jump models using the Standard \& Poor’s (S\&P) 100 Index and American options. Empirically disentangling the contribution of the jumps from the contribution of the diffusion, we conclude that jumps account for about 90\% of the iVaR on average. Our back-testing results indicate that the option-implied estimates are much more responsive to market changes than their historical counterparts, which perform poorly. © 2019 INFORMS}}, 
pages = {397--414}, 
number = {1}, 
volume = {66}
}
@article{10.1108/ijqrm-07-2020-0238, 
year = {2021}, 
title = {{Worst Expected Best method for assessment of probabilistic network expected value at risk: application in supply chain risk management}}, 
author = {Qazi, Abroon and Simsekler, Mecit Can Emre}, 
journal = {International Journal of Quality \& Reliability Management}, 
issn = {0265671X}, 
doi = {10.1108/ijqrm-07-2020-0238}, 
abstract = {{Purpose: The purpose of this paper is to develop and operationalize a process for prioritizing supply chain risks that is capable of capturing the value at risk (VaR), the maximum loss expected at a given confidence level for a specified timeframe associated with risks within a network setting. Design/methodology/approach: The proposed “Worst Expected Best” method is theoretically grounded in the framework of Bayesian Belief Networks (BBNs), which is considered an effective technique for modeling interdependency across uncertain variables. An algorithm is developed to operationalize the proposed method, which is demonstrated using a simulation model. Findings: Point estimate-based methods used for aggregating the network expected loss for a given supply chain risk network are unable to project the realistic risk exposure associated with a supply chain. The proposed method helps in establishing the expected network-wide loss for a given confidence level. The vulnerability and resilience-based risk prioritization schemes for the model considered in this paper have a very weak correlation. Originality/value: This paper introduces a new “Worst Expected Best” method to the literature on supply chain risk management that helps in assessing the probabilistic network expected VaR for a given supply chain risk network. Further, new risk metrics are proposed to prioritize risks relative to a specific VaR that reflects the decision-maker's risk appetite. © 2021, Emerald Publishing Limited.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.irfa.2004.02.003, 
year = {2004}, 
title = {{Managing extreme risks in tranquil and volatile markets using conditional extreme value theory}}, 
author = {Byström, Hans N.E.}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2004.02.003}, 
abstract = {{Financial risk management typically deals with low-probability events in the tails of asset price distributions. To capture the behavior of these tails, one should therefore rely on models that explicitly focus on the tails. Extreme value theory (EVT)-based models do exactly that, and in this paper, we apply both unconditional and conditional EVT models to the management of extreme market risks in stock markets. We find conditional EVT models to give particularly accurate Value-at-Risk (VaR) measures, and a comparison with traditional (Generalized ARCH (GARCH)) approaches to calculate VaR demonstrates EVT as being the superior approach both for standard and more extreme VaR quantiles. © 2004 Elsevier Inc. All rights reserved.}}, 
pages = {133--152}, 
number = {2}, 
volume = {13}
}
@article{10.1016/j.enpol.2016.04.027, 
year = {2016}, 
title = {{Evaluating investments in renewable energy under policy risks}}, 
author = {Gatzert, Nadine and Vogl, Nikolai}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2016.04.027}, 
abstract = {{The considerable amount of required infrastructure and renewable energy investments expected in the forthcoming years also implies an increasingly relevant contribution of private and institutional investors. In this context, especially regulatory and policy risks have been shown to play a major role for investors when evaluating investments in renewable energy and should thus also be taken into account in risk assessment and when deriving risk-return profiles. In this paper, we provide a stochastic model framework to quantify policy risks associated with renewable energy investments (e.g. a retrospective reduction of a feed-in tariff), thereby also taking into account energy price risk, resource risk, and inflation risk. The model is illustrated by means of simulations and scenario analyses, and it makes use of expert estimates and fuzzy set theory for quantifying policy risks. Our numerical results for a portfolio of onshore wind farms in Germany and France show that policy risk can strongly impact risk-return profiles, and that cross-country diversification effects can considerably decrease the overall risk for investors. © 2016 Elsevier Ltd}}, 
pages = {238--252}, 
number = {NA}, 
volume = {95}
}
@article{10.1109/wsc.2010.5678970, 
year = {2010}, 
title = {{Confidence intervals for quantiles and value-at-risk when applying importance sampling}}, 
author = {Chu, Fang and Nakayama, Marvin K.}, 
journal = {Proceedings of the 2010 Winter Simulation Conference}, 
issn = {08917736}, 
doi = {10.1109/wsc.2010.5678970}, 
abstract = {{We develop methods to construct asymptotically valid confidence intervals for quantiles and value-at-risk when applying importance sampling (IS). We first employ IS to estimate the cumulative distribution function (CDF), which we then invert to obtain a point estimate of the quantile. To construct confidence intervals, we show that the IS quantile estimator satisfies a Bahadur-Ghosh representation, which implies a central limit theorem (CLT) for the quantile estimator and can be used to obtain consistent estimators of the variance constant in the CLT. ©2010 IEEE.}}, 
pages = {2751--2761}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.eneco.2011.01.007, 
year = {2011}, 
title = {{Value-at-risk estimation of crude oil price using MCA based transient risk modeling approach}}, 
author = {He, Kaijian and Lai, Kin Keung and Yen, Jerome}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2011.01.007}, 
abstract = {{With the increasing level of volatility in the crude oil market, the transient data feature becomes more prevalent in the market and is no longer ignorable during the risk measurement process. Since there are multiple representations for these transient data features using a set of bases available, the sparsity measure based Morphological Component Analysis (MCA) model is proposed in this paper to find the optimal combinations of representations to model these transient data features. Therefore, this paper proposes a MCA based hybrid methodology for analyzing and forecasting the risk evolution in the crude oil market. The underlying transient data components with distinct behaviors are extracted and analyzed using MCA model. The proposed algorithm incorporates these transient data features to adjust for conservative risk estimates from traditional approach based on normal market condition during its risk measurement process. The reliability and stability of Value at Risk (VaR) estimated improve as a result of finer modeling procedure in the multi frequency and time domain while maintaining competent accuracy level, as supported by empirical studies in the representative West Taxes Intermediate (WTI) and Brent crude oil market. © 2011 Elsevier B.V.}}, 
pages = {903--911}, 
number = {5}, 
volume = {33}
}
@article{10.1002/for.842, 
year = {2003}, 
title = {{Market risk management of banks: Implications from the accuracy of value-at-risk forecasts}}, 
author = {Wong, Michael Chak Sham and Cheng, Wai Yan and Wong, Clement Yuk Pang}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.842}, 
abstract = {{This paper adopts the backtesting criteria of the Basle Committee to compare the performance of a number of simple Value-at-Risk (VAR) models. These criteria provide a new standard on forecasting accuracy. Currently central banks in major money centres, under the auspices of the Basle Committee of the Bank of International settlement, adopt the VaR system to evaluate the market risk of their supervised banks. Banks are required to report VaRs to bank regulators with their internal models. These models must comply with Basle's backtesting criteria. If a bank fails the VaR backtesting, higher capital requirements will be imposed. VaR is a function of volatility forecasts. Past studies mostly conclude that ARCH and GARCH models provide better volatility forecasts. However, this paper finds that ARCH- and GARCH-based VaR models consistently fail to meet Basle's backtesting criteria. These findings suggest that the use of ARCH- and GARCH-based models to forecast their VARs is not a reliable way to manage a bank's market risk. Copyright © 2003 John Wiley \& Sons, Ltd.}}, 
pages = {23--33}, 
number = {1}, 
volume = {22}
}
@article{10.1016/j.amc.2021.126129, 
year = {2021}, 
title = {{Managing the risk based on entropic value-at-risk under a normal-Rayleigh distribution}}, 
author = {Ahmed, Dilan and Soleymani, Fazlollah and Ullah, Malik Zaka and Hasan, Hataw}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2021.126129}, 
abstract = {{Market observations basically reveal that the data do not follow a normal distribution and fat tails occur. On the other hand, the common measures of risk, like, value-at-risk (VaR) and conditional value-at-risk (CVaR) may not yield in reliable values in managing the risk of a portfolio under some conditions. To overcome these shortcomings, two ideas are furnished in this work. First, a mixture distribution is constructed based on the normal and Rayleigh distributions to provide fatter tails and to be more consistent on market data. And second, the entropic VaR (EVaR) is used to give reliable values for risk management. Finally, several simulation workouts on different stocks from real data are presented and compared to uphold the discussions of this work. © 2021 Elsevier Inc.}}, 
pages = {126129}, 
number = {NA}, 
volume = {402}
}
@article{10.2143/ast.42.2.2182805, 
year = {2012}, 
title = {{On the calculation of the solvency capital requirement based on nested simulations}}, 
author = {Bauer, Daniel and Reuss, Andreas and Singer, Daniela}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.42.2.2182805}, 
abstract = {{Within the European Union, risk-based funding requirements for insurance companies are currently being revised as part of the Solvency II project. However, many life insurers struggle with the implementation, which to a large extent appears to be due to a lack of know-how regarding both, stochastic modeling and effi cient techniques for the numerical implementation. The current paper addresses these problems by providing a mathematical framework for the derivation of the required risk capital and by reviewing different alternatives for the numerical implementation based on nested simulations. In particular, we seek to provide guidance for practitioners by illustrating and comparing the different techniques based on numerical experiments. © 2012 by Astin Bulletin. All rights reserved.}}, 
pages = {453--499}, 
number = {2}, 
volume = {42}
}
@article{10.1007/s10436-010-0166-2, 
year = {2011}, 
title = {{Value at risk and efficiency under dependence and heavy-tailedness: Models with common shocks}}, 
author = {Ibragimov, Rustam and Walden, Johan}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-010-0166-2}, 
abstract = {{This paper presents an analysis of diversification and portfolio value at risk for heavy-tailed dependent risks in models with multiple common shocks. We show that, in the framework of value at risk comparisons, diversification is optimal for moderately heavy-tailed dependent risks with common shocks and finite first moments, provided that the model is balanced, i. e., that all the risks are available for portfolio formation. However, diversification is inferior in balanced extremely heavy-tailed risk models with common factors. Finally, in several unbalanced dependent models, diversification is optimal, even though there is extreme heavy-tailedness in common shocks or in idiosyncratic parts of the risks. Analogues of the obtained results further hold for efficiency comparisons of linear estimators in random effects models with dependent and heavy-tailed observations. © 2010 Springer-Verlag.}}, 
pages = {285--318}, 
number = {3}, 
volume = {7}
}
@article{10.1016/j.jbankfin.2012.04.004, 
year = {2012}, 
title = {{Pitfalls in backtesting Historical Simulation VaR models}}, 
author = {Escanciano, Juan Carlos and Pei, Pei}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.04.004}, 
abstract = {{Historical Simulation (HS) and its variant, the Filtered Historical Simulation (FHS), are the most popular Value-at-Risk forecast methods at commercial banks. These forecast methods are traditionally evaluated by means of the unconditional backtest. This paper formally shows that the unconditional backtest is always inconsistent for backtesting HS and FHS models, with a power function that can be even smaller than the nominal level in large samples. Our findings have fundamental implications in the determination of market risk capital requirements, and also explain Monte Carlo and empirical findings in previous studies. We also propose a data-driven weighted backtest with good power properties to evaluate HS and FHS forecasts. A Monte Carlo study and an empirical application with three US stocks confirm our theoretical findings. The empirical application shows that multiplication factors computed under the current regulatory framework are downward biased, as they inherit the inconsistency of the unconditional backtest. © 2012 Elsevier B.V.}}, 
pages = {2233--2244}, 
number = {8}, 
volume = {36}
}
@article{10.3390/econometrics5020026, 
year = {2017}, 
title = {{The realized hierarchical archimedean copula in risk modelling}}, 
author = {Okhrin, Ostap and Tetereva, Anastasija}, 
journal = {Econometrics}, 
issn = {22251146}, 
doi = {10.3390/econometrics5020026}, 
abstract = {{This paper introduces the concept of the realized hierarchical Archimedean copula (rHAC). The proposed approach inherits the ability of the copula to capture the dependencies among financial time series, and combines it with additional information contained in high-frequency data. The considered model does not suffer from the curse of dimensionality, and is able to accurately predict high-dimensional distributions. This flexibility is obtained by using a hierarchical structure in the copula. The time variability of the model is provided by daily forecasts of the realized correlation matrix, which is used to estimate the structure and the parameters of the rHAC. Extensive simulation studies show the validity of the estimator based on this realized correlation matrix, and its performance, in comparison to the benchmark models. The application of the estimator to one-day-ahead Value at Risk (VaR) prediction using high-frequency data exhibits good forecasting properties for a multivariate portfolio. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {26}, 
number = {2}, 
volume = {5}
}
@article{10.1016/j.scitotenv.2015.03.099, 
year = {2016}, 
title = {{Modelling climate change impacts on tourism demand: A comparative study from Sardinia (Italy) and Cap Bon (Tunisia)}}, 
author = {Köberl, Judith and Prettenthaler, Franz and Bird, David Neil}, 
journal = {Science of The Total Environment}, 
issn = {00489697}, 
doi = {10.1016/j.scitotenv.2015.03.099}, 
pmid = {25891683}, 
abstract = {{Tourism represents an important source of income and employment in many Mediterranean regions, including the island of Sardinia (Italy) and the Cap Bon peninsula (Tunisia). Climate change may however impact tourism in both regions, for example, by altering the regions' climatic suitability for common tourism types or affecting water availability. This paper assesses the potential impacts of climate change on tourism in the case study regions of Sardinia and Cap Bon. Direct impacts are studied in a quantitative way by applying a range of climate scenario data on the empirically estimated relationship between climatic conditions and tourism demand, using two different approaches. Results indicate a potential for climate-induced tourism revenue gains especially in the shoulder seasons during spring and autumn, but also a threat of climate-induced revenue losses in the summer months due to increased heat stress. Annual direct net impacts are nevertheless suggested to be (slightly) positive in both case study regions. Significant climate-induced reductions in total available water may however somewhat counteract the positive direct impacts of climate change by putting additional water costs on the tourism industry. © 2015 Elsevier B.V.}}, 
pages = {1039--1053}, 
number = {NA}, 
volume = {543}
}
@article{10.1117/12.820489, 
year = {2009}, 
title = {{Risk analysis of laser elements for complex characterization of damages by space radiation}}, 
author = {Brodyn, Mikhailo and Bezrodnyi, Vladimir and Negriyko, Anatoliy and Yatsenko, Vitaliy}, 
journal = {Damage to VUV, EUV, and X-Ray Optics II}, 
issn = {0277786X}, 
doi = {10.1117/12.820489}, 
abstract = {{This report concentrates on dynamic probabilistic risk analysis of optical elements for complex characterization of damages using physical model of solid state lasers and predictable level of ionizing radiation and space weather. The following main subjects will be covered by our report: (a) a solid-state laser model; (b) mathematical models for dynamic probabilistic risk assessment; and (c) software for modeling and prediction of ionizing radiation. A probabilistic risk assessment method for solid-state lasers is presented with consideration of some deterministic and stochastic factors. Probabilistic risk assessment is a comprehensive, structured, and logical analysis method aimed at identifying and assessing risks in solid-state lasers for the purpose of cost-effectively improving their safety and performance. This method is based on the Conditional Value-at-Risk measure (CVaR) and the expected loss exceeding Value-at-Risk (VaR). We propose a new dynamical-information approach for radiation damage risk assessment of laser elements by cosmic radiation. Our approach includes the following steps: (a)laser modeling, modeling of ionizing radiation influences on laser elements, (b) probabilistic risk assessment methods, and (c) risk minimization. For computer simulation of damage processes at microscopic and macroscopic levels the following methods are used: (a) statistical; (b) dynamical; (c) optimization; (d) acceleration modeling, and (e) mathematical modeling of laser functioning. Mathematical models of space ionizing radiation influence on laser elements were developed for risk assessment in laser safety analysis. This is a so-called 'black box' or 'input-output' model, which seeks only to reproduce the behaviour of the system's output in response to changes in its inputs. The model inputs are radiation influences on laser systems and output parameters are dynamical characteristics of the solid laser. © 2009 SPIE.}}, 
pages = {73610W--73610W-9}, 
number = {NA}, 
volume = {7361}
}
@article{10.1063/1.5041543, 
year = {2018}, 
title = {{Portfolio risk measurement based on value at risk (VaR)}}, 
author = {Amin, Farah Azaliney Mohd and Yahya, Siti Fatimah and Ibrahim, Siti Ainazatul Shazlin and Kamari, Mohammad Shafiq Mohammad}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.5041543}, 
abstract = {{Generally, the risk level of an investment is directly correlated with the returns to be earned by investors in the future. In current situation, it is difficult for investors, shareholders and financial managers to determine the total loss of their asset portfolio because standard deviation is insufficient to describe the actual total loss. Therefore, this research discuss on the concept of Value at Risk (VaR) which has been proven as one of the risk measurement instrument that is effective in describing the total loss of investment in exact currency that will be bear by the investors. VaR is defined as the maximum potential loss in a value of a portfolio over a defined period for a given confidence interval in normal market condition. The three approaches which are Delta Normal, Historical Simulation and Monte Carlo Simulation are used to calculate VaR for a hypothetical portfolio of a stock. The study finally shows that Monte Carlo Simulation approach is the most applicable and flexible in measuring Value at Risk as compared to the other two methods. © 2018 Author(s).}}, 
pages = {020012}, 
number = {NA}, 
volume = {1974}
}
@article{10.1080/14697688.2018.1540880, 
year = {2019}, 
title = {{Bayesian realized-GARCH models for financial tail risk forecasting incorporating the two-sided Weibull distribution}}, 
author = {Wang, Chao and Chen, Qian and Gerlach, Richard}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2018.1540880}, 
abstract = {{The realized-GARCH framework is extended to incorporate the two-sided Weibull distribution, for the purpose of volatility and tail risk forecasting in a financial time series. Further, the realized range, as a competitor for realized variance or daily returns, is employed as the realized measure in the realized-GARCH framework. Sub-sampling and scaling methods are applied to both the realized range and realized variance, to help deal with inherent micro-structure noise and inefficiency. A Bayesian Markov Chain Monte Carlo (MCMC) method is adapted and employed for estimation and forecasting, while various MCMC efficiency and convergence measures are employed to assess the validity of the method. In addition, the properties of the MCMC estimator are assessed and compared with maximum likelihood, via a simulation study. Compared to a range of well-known parametric GARCH and realized-GARCH models, tail risk forecasting results across seven market indices, as well as two individual assets, clearly favour the proposed realized-GARCH model incorporating the two-sided Weibull distribution; especially those employing the sub-sampled realized variance and sub-sampled realized range. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--26}, 
number = {6}, 
volume = {19}
}
@article{10.1016/j.csda.2005.11.002, 
year = {2006}, 
title = {{An analysis of the flexibility of Asymmetric Power GARCH models}}, 
author = {Ané, Thierry}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2005.11.002}, 
abstract = {{The Asymmetric Power GARCH (APGARCH) model allows a wider class of power transformations than simply taking the absolute value or squaring the data as in classical heteroscedastic models. A dynamic estimation is used to compare the three GARCH families and examine their forecasting performances in a value-at-risk setting. The results suggest that the optimal power transformation obtained with the APGARCH model is virtually never statistically different from 1 or 2. Moreover, although some indices switch between these two values over the time, the measures of accuracy and efficiency used to assess the performance of VaR forecasts indicate that the additional flexibility brought by the APGARCH model provides little, if any, improvements for risk management. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {1293--1311}, 
number = {2}, 
volume = {51}
}
@article{10.1016/j.ejor.2012.08.023, 
year = {2014}, 
title = {{Mean-variance approximations to expected utility}}, 
author = {Markowitz, Harry}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2012.08.023}, 
abstract = {{It is often asserted that the application of mean-variance analysis assumes normal (Gaussian) return distributions or quadratic utility functions. This common mistake confuses sufficient versus necessary conditions for the applicability of modern portfolio theory. If one believes (as does the author) that choice should be guided by the expected utility maxim, then the necessary and sufficient condition for the practical use of mean-variance analysis is that a careful choice from a mean-variance efficient frontier will approximately maximize expected utility for a wide variety of concave (risk-averse) utility functions. This paper reviews a half-century of research on mean-variance approximations to expected utility. The many studies in this field have been generally supportive of mean-variance analysis, subject to certain (initially unanticipated) caveats. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {346--355}, 
number = {2}, 
volume = {234}
}
@article{10.1016/j.jmva.2009.07.004, 
year = {2010}, 
title = {{Bounds for the sum of dependent risks having overlapping marginals}}, 
author = {Embrechts, Paul and Puccetti, Giovanni}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2009.07.004}, 
abstract = {{We describe several analytical and numerical procedures to obtain bounds on the distribution function of a sum of n dependent risks having fixed overlapping marginals. As an application, we produce bounds on quantile-based risk measures for portfolios of financial and actuarial interest. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {177--190}, 
number = {1}, 
volume = {101}
}
@article{10.1016/j.jbankfin.2004.08.008, 
year = {2005}, 
title = {{Functional gradient descent for financial time series with an application to the measurement of market risk}}, 
author = {Audrino, Francesco and Barone-Adesi, Giovanni}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2004.08.008}, 
abstract = {{The estimation and forecast of the volatility matrix are two of the main tasks of financial econometrics since they are essential ingredients in many practical applications. Unfortunately the use of classical multivariate methods in large dimensions is difficult because of the curse of dimensionality. We present a general semiparametric technique, based on functional gradient descent (FGD) and able to overcome most problems associated with a multivariate GARCH-type estimation. By testing the accuracy of the volatility estimates for the measurement of market risk on real data we provide empirical evidence of the strong predictive potential of the FGD approach, also in comparison to other standard methods. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {959--977}, 
number = {4}, 
volume = {29}
}
@article{10.1109/bcd.2019.8884865, 
year = {2019}, 
title = {{Using Artificial Neural Network to Predict Value-at-Risk of S\&P 500 Market Index with External Information}}, 
author = {Wang, Yanning and Jiang, Yue and He, Guoqian}, 
journal = {2019 IEEE International Conference on Big Data, Cloud Computing, Data Science \& Engineering (BCD)}, 
issn = {NA}, 
doi = {10.1109/bcd.2019.8884865}, 
abstract = {{Previous studies on forecasting Value-at-Risk mostly focus on using the information directly obtained from the market itself. In this paper, we propose a new method for predicting Value-at-Risk by incorporating both internal and external information using artificial neural network. We show that artificial neural network can provide us a flexible framework for including external information, and these additional factors can help to improve the accuracy of Value-at-Risk prediction. © 2019 IEEE.}}, 
pages = {90--93}, 
number = {NA}, 
volume = {NA}
}
@article{10.1515/demo-2016-0021, 
year = {2016}, 
title = {{VaR bounds for joint portfolios with dependence constraints}}, 
author = {Puccetti, Giovanni and Rüschendorf, Ludger and Manko, Dennis}, 
journal = {Dependence Modeling}, 
issn = {23002298}, 
doi = {10.1515/demo-2016-0021}, 
abstract = {{Based on a novel extension of classical Hoeffding-Fréchet bounds, we provide an upper VaR bound for joint risk portfolios with fixed marginal distributions and positive dependence information. The positive dependence information can be assumed to hold in the tails, in some central part, or on a general subset of the domain of the distribution function of a risk portfolio. The newly provided VaR bound can be interpreted as a comonotonic VaR computed at a distorted confidence level and its quality is illustrated in a series of examples of practical interest. © 2017 Piotr Jaworski, published by De Gruyter Open. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License.}}, 
number = {1}, 
volume = {4}
}
@article{10.1016/j.intfin.2014.11.003, 
year = {2015}, 
title = {{Co-movement between sharia stocks and sukuk in the GCC markets: A time-frequency analysis}}, 
author = {Aloui, Chaker and Hammoudeh, Shawkat and Hamida, Hela Ben}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2014.11.003}, 
abstract = {{We assess the co-movement between the sharia-compliant stocks and sukuk in the Gulf Cooperation Council (GCC) countries. The wavelet squared coherency approach is applied to daily data covering GCC global, corporate and financial services sukuk indexes as well as GCC sharia stocks. The empirical evidence indicates a strong dependence between these sharia stock and sukuk indexes. The degree of co-movement power is varying over time and frequency and the long-run is dominant. To highlight the importance of the wavelet analysis, we perform the value-at-risk (VaR) for a GCC multi-country portfolio. The finding provides strong evidence that the benefits of portfolio diversification vary across frequencies and time. Our results provide several practical implications for Islamic funds when selecting sharia-compliant assets and designing their optimal weights. © 2014 Elsevier B.V.}}, 
pages = {69--79}, 
number = {NA}, 
volume = {34}
}
@article{10.1515/demo-2016-0020, 
year = {2016}, 
title = {{Robustness regions for measures of risk aggregation}}, 
author = {Pesenti, Silvana M. and Millossovich, Pietro and Tsanakas, Andreas}, 
journal = {Dependence Modeling}, 
issn = {23002298}, 
doi = {10.1515/demo-2016-0020}, 
abstract = {{One of risk measures' key purposes is to consistently rank and distinguish between different risk profiles. From a practical perspective, a risk measure should also be robust, that is, insensitive to small perturbations in input assumptions. It is known in the literature [14, 39], that strong assumptions on the risk measure's ability to distinguish between risks may lead to a lack of robustness. We address the trade-off between robustness and consistent risk ranking by specifying the regions in the space of distribution functions, where law-invariant convex risk measures are indeed robust. Examples include the set of random variables with bounded second moment and those that are less volatile (in convex order) than random variables in a given uniformly integrable set. Typically, a risk measure is evaluated on the output of an aggregation function defined on a set of random input vectors. Extending the definition of robustness to this setting, we find that law-invariant convex risk measures are robust for any aggregation function that satisfies a linear growth condition in the tail, provided that the set of possible marginals is uniformly integrable. Thus, we obtain that all law-invariant convex risk measures possess the aggregation-robustness property introduced by [26] and further studied by [40]. This is in contrast to the widely-used, non-convex, risk measure Value-at-Risk, whose robustness in a risk aggregation context requires restricting the possible dependence structures of the input vectors. © 2017 Piotr Jaworski, published by De Gruyter Open. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License.}}, 
number = {1}, 
volume = {4}
}
@article{10.1002/for.1241, 
year = {2012}, 
title = {{Improving hull and white's method of estimating portfolio value-at-risk}}, 
author = {Changchien, Chang‐Cheng and Lin, Chu‐Hsiung and Yang, Hsien‐Chueh Peter}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1241}, 
abstract = {{We propose a method approach. We use six international stock price indices and three hypothetical portfolios formed by these indices. The sample was observed daily from 1 January 1996 to 31 December 2006. Confirmed by the failure rates and backtesting developed by Kupiec (Technique for verifying the accuracy of risk measurement models. Journal of Derivatives 1995; 3: 73-84) and Christoffersen (Evaluating interval forecasts. International Economic Review 1998; 39: 841-862), the empirical results show that our method can considerably improve the estimation accuracy of value-at-risk. Thus the study establishes an effective alternative model for risk prediction and hence also provides a reliable tool for the management of portfolios. Copyright © 2011 John Wiley \& Sons, Ltd. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {706--720}, 
number = {8}, 
volume = {31}
}
@article{10.1007/s11424-017-5147-3, 
year = {2017}, 
title = {{Copula-based risk management models for multivariable RMB exchange rate in the process of RMB internationalization}}, 
author = {Du, Jiangze and Lai, Kin Keung}, 
journal = {Journal of Systems Science and Complexity}, 
issn = {10096124}, 
doi = {10.1007/s11424-017-5147-3}, 
abstract = {{This paper investigates the dependence of the exchange rate of onshore Renminbi (RMB) and offshore RMB against US dollar (i.e., CNY and CNH) based on copula models. Eleven different copulas were selected to construct multivariate distribution and estimate the value-at-risk for RMB exchange rate. Empirical results show that time-invariant Student-t copula is the best model to fit the sample data. The positive upper and lower dependence indicates that CNY and CNH series tend to move in the same direction. Moreover, the dependence between the two exchange rates is asymmetric, which means that traditional models, such as Pearson’s correlation, are inappropriate to measure the correlations between these markets. The best fitted model is chosen to estimate the financial risk, which can help business practitioners and policymakers track risk evolution and make good decisions. © 2017, Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg.}}, 
pages = {660--679}, 
number = {3}, 
volume = {30}
}
@article{10.3390/risks8040123, 
year = {2020}, 
title = {{Modeling multivariate financial series and computing risk measures via gram–charlier-like expansions}}, 
author = {Zoia, Maria Grazia and Vacca, Gianmarco and Barbieri, Laura}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks8040123}, 
abstract = {{This paper develops an approach based on Gram–Charlier-like expansions for modeling financial series to take in due account features such as leptokurtosis. A Gram–Charlier-like expansion adjusts the moments of interest of a given distribution via its own orthogonal polynomials. This approach, formerly adopted for univariate series, is here extended to a multivariate context by means of spherical densities. Previous works proposed the Gram–Charlier of the multivariate Gaussian, obtained by using Hermite polynomials. This work shows how polynomial expansions of an entire class of spherical laws can be worked out with the aim of obtaining a wide set of leptokurtic multivariate distributions. A Gram–Charlier-like expansion is a distribution characterized by an additional parameter with respect to the parent spherical law. This parameter, which measures the increase in kurtosis due to the polynomial expansion, can be estimated so as to make the resulting distribution capable of describing the empirical kurtosis found in the data. An application of the Gram–Charlier-like expansions to a set of financial assets proves their effectiveness in modeling multivariate financial series and assessing risk measures, such as the value at risk and the expected shortfall. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {123}, 
number = {4}, 
volume = {8}
}
@article{10.22495/rgcv3i1c1art2, 
year = {2013}, 
title = {{Current exposure method for CCP’s under Basel III}}, 
author = {Kotzé, Antonie and Preez, Paul du}, 
journal = {Risk Governance and Control: Financial Markets \& Institutions}, 
issn = {2077429X}, 
doi = {10.22495/rgcv3i1c1art2}, 
abstract = {{Exposure-at-default is one of the most interesting and most difficult parameters to estimate in counterparty credit risk. Basel I offered only the non-internal Current Exposure Method for estimating this quantity whilst Basel II further introduced the Standardized Method and an Internal Model Method. Under new Basel III rules a central counterparty is defined as being a financial institution. New principles set out by the Basel Committee on Banking Supervision forces Central Counterparties in using the Current Exposure Method when estimating the credit exposures to Clearing Member banks notwithstanding its shortcomings. The Current Exposure Method relies on the Value-at-Risk methodology and its characteristics are discussed in this note. We will particularly investigate exposures to SAFCOM, the South African clearing house and point to a mathematical discrepancy on how netting is effected through the Basel accord. © 2013, Virtus Interpress. All rights reserved.}}, 
pages = {82--92}, 
number = {1CONTINUED1}, 
volume = {3}
}
@article{10.1016/j.jimonfin.2005.05.004, 
year = {2005}, 
title = {{An evaluation framework for alternative VaR-models}}, 
author = {Bams, Dennis and Lehnert, Thorsten and Wolff, Christian C.P.}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2005.05.004}, 
abstract = {{In this paper we investigate the ability of different models to produce useful VaR-estimates for exchange rate positions. Our analysis shows that it is important to take into account parameter uncertainty, since this leads to uncertainty in the predicted VaR. We make this uncertainty in the VaR explicit by means of simulation. Our empirical results suggest that more sophisticated tail-modeling approaches come at the cost of more uncertainty about the VaR-estimate itself. We show how to adjust VaR calculations in order to take the parameter uncertainty into account. This is accomplished through a data-driven method to deliver not just a point estimate of the VaR, but a region. © 2005 Elsevier Ltd. All rights reserved.}}, 
pages = {944--958}, 
number = {6}, 
volume = {24}
}
@article{10.1002/wics.1234, 
year = {2012}, 
title = {{Higher-order asymptotics in finance}}, 
author = {Chan, N. H. and Yam, S. C. P.}, 
journal = {Wiley Interdisciplinary Reviews: Computational Statistics}, 
issn = {19395108}, 
doi = {10.1002/wics.1234}, 
abstract = {{A primary motivation of higher-order asymptotic statistical analysis is to improve the first-order limiting result in accordance with the celebrated Central Limit Theorem in the sense that a better approximation with higher order accuracy can be attained. In this article, several important tools in asymptotic analysis for obtaining higher-order approximations, including Edgeworth expansions, saddle-point approximations and Laplace integral method, will be revisited together with an introduction of some of their applications in finance. A new result on bounds for the difference between American and European calls on small dividend paying stock is also provided. © 2012 Wiley Periodicals, Inc.}}, 
pages = {571--587}, 
number = {6}, 
volume = {4}
}
@article{10.1007/978-981-15-3250-4_163, 
year = {2020}, 
title = {{Risk Measurement of Supply Chain Finance Based on the VaR Model}}, 
author = {Lin, Xun-Liang and Li, Hong and Ruan, Chuan-Yang}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-981-15-3250-4\_163}, 
abstract = {{This paper combines the practical activities of the supply chain with existing theories to explore new ways to measure financial risks in the supply chain. We establish a supply chain financial risk measurement index system based on the VaR model, and use the Monte Carlo simulation method to conduct empirical analysis. The results show that banks can use the VaR model to investigate the business status of the enterprise according to the financial data such as profit rate and return on assets in the actual operation of these enterprises. At the same time, the β value is introduced on the basis of the traditional VaR model. The use of the VaR model and the β value can help the bank to quantitatively screen the financing object according to its own risk preference. When the β value of the enterprise with financing demand is greater than the set value, then the bank will without lending, the β value also helps banks scientifically allocate financing quotas and effectively control risks. © 2020, Springer Nature Singapore Pte Ltd.}}, 
pages = {1267--1275}, 
number = {NA}, 
volume = {551 LNEE}
}
@article{10.21003/ea.v158-19, 
year = {2016}, 
title = {{Estimation of confidence level for Value-at-Risk: Statistical analysis}}, 
author = {Banking, Lviv Institute of the University of and Zabolotskyy, Taras}, 
journal = {Economic Annals-ХХI}, 
issn = {17286220}, 
doi = {10.21003/ea.v158-19}, 
abstract = {{The paper investigates the problem of estimation of the confidence level for Value-at-Risk to get the minimum VaR portfolio with a predefined level of expected return. The equation which describes the relation between the confidence level and the rate of the expected return depends on the unknown parameters of distribution of asset returns which should be estimated. The classical sample estimators for unknown parameters are used. The author has examined the properties of the estimator for the confidence level in considerable detail. Under the assumption that the asset returns are multivariate, we find the asymptotic distribution of the estimator for the confidence level. Moreover, we extend this result to the case of elliptically contoured distributed asset returns. Based on the distributional properties, the confidence interval for the confidence level for VaR is constructed and the test procedure whether the resulting portfolio is statistically different from the global minimum variance portfolio is provided. Using a simulation study, we demonstrate that our results give a good approximation even in the case of moderate sample sizes n=250, n=500 not only in the case of normally distributed asset returns, but also when asset returns follow the elliptically countered distribution. We have concluded that investors can use the results of the paper with regard to all sectors of the economy. We used monthly asset returns of five stocks included into Dow Jones Index, namely: McDonald's, Johnson\&Johnson, Procter\&Gamble, AT\&T, and Verizon Communications from 01 October 2010 to 01 September 2015 to give numerical illustration of our fundamental results. © Institute of Society Transformation, 2016.}}, 
pages = {83--87}, 
number = {3-4}, 
volume = {158}
}
@article{10.1016/j.mulfin.2020.100666, 
year = {2021}, 
title = {{Connectedness between cryptocurrencies and foreign exchange markets: Implication for risk management}}, 
author = {Chemkha, Rahma and BenSaïda, Ahmed and Ghorbel, Ahmed}, 
journal = {Journal of Multinational Financial Management}, 
issn = {1042444X}, 
doi = {10.1016/j.mulfin.2020.100666}, 
abstract = {{This paper examines the connectedness between cryptocurrencies and major fiat currencies in a multivariate framework using vine copulas. One of the advantages of this method is the flexibility in the choice of distributions used to model complex dependencies. The results show that the dependence, measured conditionally or unconditionally, is positive and higher for the pairs of the same market than those across markets. Moreover, a low significant dependency is found between cryptocurrencies and the main conventional currencies. Based on the Value-at-Risk (VaR) and expected shortfall (ES) analyses, vine copulas produce accurate risk measures by adding cryptocurrencies to a portfolio of fiat currencies. © 2020 Elsevier B.V.}}, 
pages = {100666}, 
number = {NA}, 
volume = {59}
}
@article{10.3934/jimo.2008.4.81, 
year = {2008}, 
title = {{Optimal portfolios under a value-at-risk constraint with applications to inventory control in supply chains}}, 
author = {Yiu, K F Cedric and Wang, S Y and Mak, K L}, 
journal = {Journal of Industrial \& Management Optimization}, 
issn = {15475816}, 
doi = {10.3934/jimo.2008.4.81}, 
abstract = {{The optimal portfolio problem under a VaR (value at risk) constraint is reviewed. Two different formulations, namely with and without consumption, are illustrated. This problem can be formulated as a constrained stochastic optimal control problem. The optimality conditions can be derived using the dynamic programming technique and the method of Lagrange multiplier can be applied to handle the VaR constraint. The method is extended for inventory management. Different from traditional inventory models of minimizing overall cost, the cashflow dynamic of a manufacturer is derived by considering a portfolio of inventory of raw materials together with income and consumption. The VaR of the portfolio of assets is derived and imposed as a constraint. Furthermore, shortage cost and holding cost can also be formulated as probabilistic constraints. Under this formulation, we find that holdings in high risk inventory are optimally reduced by the imposed value-at-risk constraint.}}, 
pages = {81--94}, 
number = {1}, 
volume = {4}
}
@article{10.1080/00207543.2011.564666, 
year = {2011}, 
title = {{Value-at-risk and data envelopment analysis: Comments on Wu and Olson (2010)}}, 
author = {Wei, Guiwu and Wang, Jiamin}, 
journal = {International Journal of Production Research}, 
issn = {00207543}, 
doi = {10.1080/00207543.2011.564666}, 
abstract = {{In this note, we identify three major errors contained in Wu and Olson [Wu, D.D. and Olson, D., 2010. Enterprise risk management: a DEA VaR approach in vendor selection. International Journal of Production Research, 48 (16), 4919-4932]. It is shown that the 'DEA VaR' model named by the authors is not truly a Value-at-risk (VaR) minimisation problem. It is also pointed out that the authors confused two concepts of stochastic efficiency. Finally, it is revealed that the linearisation technique proposed by the authors is questionable under some conditions and a correction is suggested. © 2011 Taylor \& Francis.}}, 
pages = {7189--7193}, 
number = {23}, 
volume = {49}
}
@article{10.1016/j.jbankfin.2012.01.013, 
year = {2012}, 
title = {{Portfolio credit-risk optimization}}, 
author = {Iscoe, Ian and Kreinin, Alexander and Mausser, Helmut and Romanko, Oleksandr}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2012.01.013}, 
abstract = {{This paper evaluates several alternative formulations for minimizing the credit risk of a portfolio of financial contracts with different counterparties. Credit risk optimization is challenging because the portfolio loss distribution is typically unavailable in closed form. This makes it difficult to accurately compute Value-at-Risk (VaR) and expected shortfall (ES) at the extreme quantiles that are of practical interest to financial institutions. Our formulations all exploit the conditional independence of counterparties under a structural credit risk model. We consider various approximations to the conditional portfolio loss distribution and formulate VaR and ES minimization problems for each case. We use two realistic credit portfolios to assess the in- and out-of-sample performance for the resulting VaR- and ES-optimized portfolios, as well as for those which we obtain by minimizing the variance or the second moment of the portfolio losses. We find that a Normal approximation to the conditional loss distribution performs best from a practical standpoint. © 2012 Elsevier B.V.}}, 
pages = {1604--1615}, 
number = {6}, 
volume = {36}
}
@article{10.1016/j.najef.2018.03.010, 
year = {2018}, 
title = {{Mutual excitation between OECD stock and oil markets: A conditional intensity extreme value approach}}, 
author = {Herrera, Rodrigo and González, Sergio and Clements, Adam}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2018.03.010}, 
abstract = {{We analyze the degree of mutual excitation that exists between extreme events across the stock markets of OECD member nations and the Brent and WTI crude oil markets. For this analysis, marked point process models are proposed which are able to capture the dynamics of the intensity of occurrence and comovement during periods of crisis. The results show a significant, negative interdependence between most OECD markets, especially those of the USA, Japan and France. These major oil importing countries display links between equity market losses and positive returns in both oil markets. However, positive interdependence is not observed between any of the OECD countries except for South Korea. The great advantage of this methodology is that, apart from using the size distribution of extreme events, it also uses the occurrence times of extreme events as a source of information. With this information, these models are better able to capture the stylized facts of extreme events in financial markets such as clustering behavior and cross-excitation. © 2018 Elsevier Inc.}}, 
pages = {70--88}, 
number = {NA}, 
volume = {46}
}
@article{10.1007/s10690-012-9160-1, 
year = {2013}, 
title = {{Forecasting Intraday Volatility and Value-at-Risk with High-Frequency Data}}, 
author = {So, Mike K. P. and Xu, Rui}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1007/s10690-012-9160-1}, 
abstract = {{In this paper, we develop modeling tools to forecast Value-at-Risk and volatility with investment horizons of less than one day. We quantify the market risk based on the study at a 30-min time horizon using modified GARCH models. The evaluation of intraday market risk can be useful to market participants (day traders and market makers) involved in frequent trading. As expected, the volatility features a significant intraday seasonality, which motivates us to include the intraday seasonal indexes in the GARCH models. We also incorporate realized variance (RV) and time-varying degrees of freedom in the GARCH models to capture more intraday information on the volatile market. The intrinsic tail risk index is introduced to assist with understanding the inherent risk level in each trading time interval. The proposed models are evaluated based on their forecasting performance of one-period-ahead volatility and Intraday Value-at-Risk (IVaR) with application to the 30 constituent stocks. We find that models with seasonal indexes generally outperform those without; RV can improve the out-of-sample forecasts of IVaR; student GARCH models with time-varying degrees of freedom perform best at 0.5 and 1 \% IVaR, while normal GARCH models excel for 2. 5 and 5 \% IVaR. The results show that RV and seasonal indexes are useful to forecasting intraday volatility and Intraday VaR. © 2012 Springer Japan.}}, 
pages = {83--111}, 
number = {1}, 
volume = {20}
}
@article{10.1080/03610918.2019.1622721, 
year = {2021}, 
title = {{Generalized quasi maximum likelihood estimation for generalized autoregressive score models: simulations and real applications}}, 
author = {Gammoudi, Imed and Nani, Asma and Ghourabi, Mohamed El}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2019.1622721}, 
abstract = {{In this article, we develop a two-step method for conditional Value at Risk (VaR) estimation in the context of the Generalized Autoregressive Score (GAS) models. The first step consists of estimating the volatility parameter by the generalized Quasi Maximum Likelihood Estimator (gQMLE) and in the second step we estimate the theoretical quantile of the innovations by the empirical quantile of the residuals. When the instrumental density q of the gQMLE is not the Gaussian density used in the standard QMLE, or is not the true distribution of the innovations, both the estimations of the volatility and of the quantile are asymptotically biased. In spite of that the two errors counterbalance each other, and we finally get a consistent estimator of the conditional VaR. We establish the asymptotic properties of the gQMLE for GAS models as well as the conditional VaR two-step estimator. Moreover, we discuss how to apply the gQMLE by giving examples of densities distribution and we get worthwhile results. © 2019 Taylor \& Francis Group, LLC.}}, 
pages = {1--26}, 
number = {11}, 
volume = {50}
}
@article{10.1590/0104-530x3228-16, 
year = {2018}, 
title = {{Value-at-Risk (VaR) Brazilian Real and currencies of emerging and developing markets}}, 
author = {Ogawa, Marina Andreotti and Costa, Naijela Janaina da and Moralles, Herick Fernando}, 
journal = {Gestão \&amp; Produção}, 
issn = {0104530X}, 
doi = {10.1590/0104-530x3228-16}, 
abstract = {{Globalization has been responsible for increasing exposure to risks related to currency factors for companies and countries. The literature has addressed the issue of currency volatility in emerging and developed countries without a consensus on such a dynamic and there are few studies that estimate and place the risk of the Brazilian real. Thus, this study aimed to measure the risk of the Brazilian exchange rate, as well as test the hypothesis that there is a significant difference in volatility between currencies of emerging and developing countries. Through the parametric VaR set to extreme values distributions, it was observed that the Brazilian real presented at greater risk of exchange rate. The results also showed that the distributional assumptions do not seem to have any specific pattern when faced emerging and developed countries and also emerging countries showed less volatility, reflecting the preponderance of factors external to the foreign exchange market in determining the risk of some currencies. © 2018 Brazilian Institute for Information in Science and Technology. All rights reserved.}}, 
pages = {485--499}, 
number = {3}, 
volume = {25}
}
@article{10.1016/j.jfs.2015.10.006, 
year = {2016}, 
title = {{Systemic risk spillovers in the European banking and sovereign network}}, 
author = {Betz, Frank and Hautsch, Nikolaus and Peltonen, Tuomas A. and Schienle, Melanie}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2015.10.006}, 
abstract = {{We propose a framework for estimating time-varying systemic risk contributions that is applicable to a high-dimensional and interconnected financial system. Tail risk dependencies and systemic risk contributions are estimated using a penalized two-stage fixed-effects quantile approach, which explicitly links time-varying interconnectedness to systemic risk contributions. For the purposes of surveillance and regulation of financial systems, network dependencies in extreme risks are more relevant than simple (mean) correlations. Thus, the framework provides a tool for supervisors, reflecting the market's view of tail dependences and systemic risk contributions. The model is applied to a system of 51 large European banks and 17 sovereigns during the period from 2006 through 2013, utilizing both equity and CDS prices. We provide new evidence on how banking sector fragmentation and sovereign-bank linkages evolved over the European sovereign debt crisis, and how they are reflected in estimated network statistics and systemic risk measures. Finally, our evidence provides an indication that the fragmentation of the European financial system has peaked. © 2015 Elsevier B.V.}}, 
pages = {206--224}, 
number = {NA}, 
volume = {25}
}
@article{10.1080/1351847x.2013.802249, 
year = {2015}, 
title = {{Value-at-risk capital requirement regulation, risk taking and asset allocation: a mean–variance analysis}}, 
author = {Kaplanski, Guy and Levy, Haim}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2013.802249}, 
abstract = {{In this study, the mean–variance framework is employed to analyze the impact of the Basel value-at-risk (VaR) market risk regulation on the institution's optimal investment policy, the stockholders’ welfare, as well as the tendency of the institution to change the risk profile of the held portfolio. It is shown that with the VaR regulation, the institution faces a new regulated capital market line, which induces resource allocation distortion in the economy. Surprisingly, only when a riskless asset is available does VaR regulation induce the institution to reduce risk. Otherwise, the regulation may induce higher risk, accompanied by asset allocation distortion. On the positive side, the regulation implies an upper bound on the risk the institution takes and it never induces the firm to select an inefficient portfolio. Moreover, when the riskless asset is available, tightening the regulation always increases the amount of maintained eligible capital and decreases risk. © 2013 Taylor \& Francis.}}, 
pages = {215--241}, 
number = {3}, 
volume = {21}
}
@article{10.1002/ijfe.1780, 
year = {2021}, 
title = {{Does volume really matter? A risk management perspective using cross-country evidence}}, 
author = {Patra, Saswat and Bhattacharyya, Malay}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.1780}, 
abstract = {{This paper examines the impact of volume on conditional volatility and value at risk (VaR) in the context of mixture of distribution hypothesis (MDH). We test whether the support for or against the hypothesis is unconditional and holds true universally irrespective of the time period under study, the stock market under study, and the distributional assumptions so made on the residuals of the returns. We find that the persistence in volatility shows negligible reduction in all the indices across subperiods, thus refuting the claims of the MDH: that volume can explain the heteroscedasticity of returns. However, we do find that volume can act as a proxy for information post the sub-prime financial crisis, and it does impact VaR as the estimates improve significantly for some of these indices, which exhibit a strong correlation between volume and volatility. © 2019 John Wiley \& Sons, Ltd.}}, 
pages = {118--135}, 
number = {1}, 
volume = {26}
}
@article{10.1109/fuzzy.2009.5277422, 
year = {2009}, 
title = {{Value-at-Risk-based fuzzy stochastic optimization problems}}, 
author = {Wang, Shuming and Watada, Junzo}, 
journal = {2009 IEEE International Conference on Fuzzy Systems}, 
issn = {10987584}, 
doi = {10.1109/fuzzy.2009.5277422}, 
abstract = {{A new class of fuzzy stochastic optimization models - two-stage fuzzy stochastic programming with Value-at-Risk (VaR) criteria is established in this paper. An approximation algorithm is proposed to compute the VaR by combining discretization method of fuzzy variable, random simulation technique and bisection method. The convergence theorem of the approximation algorithm is also proved. To solve the twostage fuzzy stochastic programming problems with VaR criteria, we integrate the approximation algorithm, neural network (NN) and particle swarm optimization (PSO) algorithm, and hence produce a hybrid PSO algorithm to search for the optimal solution. A numerical example is provided to illustrate the designed hybrid PSO algorithm. ©2009 IEEE.}}, 
pages = {1402--1407}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.intfin.2004.05.002, 
year = {2005}, 
title = {{Estimation of Value-at-Risk by extreme value and conventional methods: A comparative evaluation of their predictive performance}}, 
author = {Bekiros, Stelios D. and Georgoutsos, Dimitris A.}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2004.05.002}, 
abstract = {{This paper conducts a comparative evaluation of the predictive performance of various Value-at- Risk (VaR) models. Special emphasis is paid on two methodologies related to the Extreme Value Theory (EVT): the Peaks over Threshold (POT) and the Blocks Maxima (BM). Both estimation techniques are based on limit results for the excess distribution over high thresholds and block maxima, respectively. They are applied on, USD-denominated, daily returns of the Dow Jones Industrial Average (DJIA) and the Cyprus Stock Exchange (CSE) indices with the intension to compare the performance of the various estimation techniques on markets with different capitalization and trading practices. The sample extends over the period November 21, 1997 to April 19, 2002 while the sub-period April 12, 2001 to April 19, 2002 has been reserved for backtesting purposes. The results we report reinforce previous ones according to which at very high confidence levels the EVT-based methodology produces the most accurate forecasts of extreme losses. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {209--228}, 
number = {3}, 
volume = {15}
}
@article{10.1016/j.eswa.2021.115724, 
year = {2021}, 
title = {{An empirical analysis of the cardinality constrained expectile-based VaR portfolio optimization problem}}, 
author = {Avci, Mualla Gonca and Avci, Mustafa}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2021.115724}, 
abstract = {{Expectiles are asymmetric generalizations of mean that are extensively employed by statisticians in regression analysis. In the last decade, the coherence and elicitability characteristics of expectiles have attracted attention of the researchers in risk management field. Recently, expectile has been recommended as an alternative risk measure to value-at-risk (VaR) and conditional value-at-risk (CVaR). As an analogy to VaR and CVaR, expectile is defined as a risk measure called expectile-based value-at-risk (EVaR). In this study, EVaR optimization model is extended with a set of practical constraints such as no short-selling, target return, proportional bounds, and portfolio cardinality constraints. The ex-ante and ex-post risk-adjusted return performances of the proposed model are compared with those of CVaR model by using historical data of the stocks listed in the BIST 100 and the S\&P 100 indices. Furthermore, we perform an extensive numerical investigation to reveal the impact of important parameters on the performances of the models. The obtained results show the potential benefits of using EVaR model in practical investment decisions. © 2021 Elsevier Ltd}}, 
pages = {115724}, 
number = {NA}, 
volume = {186}
}
@article{10.1109/hicss.2007.603, 
year = {2007}, 
title = {{Value-at-risk in IT services contracts}}, 
author = {Kauffman, Robert J. and Sougstad, Ryan}, 
journal = {2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)}, 
issn = {15301605}, 
doi = {10.1109/hicss.2007.603}, 
abstract = {{AS information systems (IS) and technology solutions become increasingly service-driven, managers are faced with the task of choosing parameters such as servicelevels, pricing, and contract duration. Information technology (IT) services vendors manage portfolios of contracts in which parameters, decided at inception, are often subject to future risks. The contract profit maximization decision may adversely affect the risk position of the firm's portfolio of services contracts. We propose a model to inform vendors on setting optimal parameters for IS contracts subject to acceptable levels of risk. The analytic model presented draws from IS economics research and the principles of value-atrisk (VaR) from financial economics. We provide examples which illustrate the trade-offs of profit maximizing contractual decisions to portfolio profit-at-risk (PaR). The contribution of this research is the application of VaR analysis to IS contractual decisions and the conceptualization of an economic model of IS service contracts which embeds value-atrisk constraints. © 2007 IEEE.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/tee.21834, 
year = {2013}, 
title = {{Multiobjective particle swarm optimization for a novel fuzzy portfolio selection problem}}, 
author = {Wang, Bo and Watada, Junzo}, 
journal = {IEEJ Transactions on Electrical and Electronic Engineering}, 
issn = {19314973}, 
doi = {10.1002/tee.21834}, 
abstract = {{On the basis of the portfolio selection theory, this paper proposes a novel fuzzy multiobjective model that can evaluate investment risk properly and increase the probability of obtaining an expected return. In building this model, fuzzy value-at-risk (VaR) is used to evaluate the exact future risk in terms of loss. The VaR can directly reflect the greatest loss of a selection case under a given confidence level. Conversely, variance, the measure of the spread of a distribution around its expected value, is utilized to make the selection more stable. This model can provide investors with more significant information for decision making. To solve this model, an improved Pareto-optimal-set-based multiobjective particle swarm optimization (IMOPSO) algorithm is designed to obtain better solutions in the Pareto front. The proposed model and algorithm are exemplified by specific numerical examples. Furthermore, comparisons are made between IMOPSO and other existing approaches. Experiments show that the model and algorithm are effective in solving the multiobjective portfolio selection problem. © 2012 Institute of Electrical Engineers of Japan.}}, 
pages = {146--154}, 
number = {2}, 
volume = {8}
}
@article{10.21314/jrmv.2015.132, 
year = {2015}, 
title = {{The role of the loss function in value-at-risk comparisons}}, 
author = {Abad, Pilar and Muela, Sonia Benito and Martín, Carmen López}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2015.132}, 
abstract = {{This paper examines whether the comparison of value-at-risk (VaR) models depends on the loss function used for such a purpose.We showa detailed comparison for several VaR models for two groups of loss functions: designed for regulators/supervisors and for risk managers/firms. Additionally, we propose a firm's loss function (FLF) that exactly measures the opportunity capital cost of the firm when the losses are covered. We find that theVaR model that minimizes the total losses is robust to the regulator's loss function (RLF) but differs from FLF, although not in an arbitrary way. The results indicate that we must distinguish between two subgroups of FLFs: functions that proxy the opportunity cost of the firm when losses are covered, and functions that penalize in the same way when losses are covered or noncovered. The best VaR model is robust to these subgroups. In any case, what is clear is that the best VaR model depends on the family of functions used: RLFs and/or FLFs, as these families provide different results. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--19}, 
number = {1}, 
volume = {9}
}
@article{10.1016/j.eswa.2012.10.038, 
year = {2013}, 
title = {{Heavy-tailed mixture GARCH volatility modeling and Value-at-Risk estimation}}, 
author = {Nikolaev, Nikolay Y. and Boshnakov, Georgi N. and Zimmer, Robert}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2012.10.038}, 
abstract = {{This paper presents a heavy-tailed mixture model for describing time-varying conditional distributions in time series of returns on prices. Student-t component distributions are taken to capture the heavy tails typically encountered in such financial data. We design a mixture MT(m)-GARCH(p, q) volatility model for returns, and develop an EM algorithm for maximum likelihood estimation of its parameters. This includes formulation of proper temporal derivatives for the volatility parameters. The experiments with a low order MT(2)-GARCH(1, 1) show that it yields results with improved statistical characteristics and economic performance compared to linear and nonlinear heavy-tail GARCH, as well as normal mixture GARCH. We demonstrate that our model leads to reliable Value-at-Risk performance in short and long trading positions across different confidence levels. © 2012 Elsevier Ltd. All rights reserved.}}, 
pages = {2233--2243}, 
number = {6}, 
volume = {40}
}
@article{10.1109/bife.2010.87, 
year = {2010}, 
title = {{Constructing risk measurement models by quantile regression method}}, 
author = {Ou, Shide and Yi, Danhui}, 
journal = {2010 Third International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2010.87}, 
abstract = {{In order to use the states of price trends or historical volatility to interpret value-at-risk without distributional assumptions, quantile regression method is used to solve the problem. We present the risk measurement model using five lag returns as explanatory variables. To describe the relationship between risk and status we introduce the explanatory variables of price trend states into the model. To research the relationship between risk and volatility, we introduce the explanatory variable of historical volatility into quantile regression model. The results estimated by both the model and IGARCH model are compared. We find out that the states of price trends interpret effectively the relationship between value-at-risk and states. By using the historical volatility of 30 days as explanatory variable, the risk measurement model is more effective than IGARCH model. © 2010 IEEE.}}, 
pages = {346--350}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/03461230510009844, 
year = {2005}, 
title = {{Standard approaches to asset \& liability risk}}, 
author = {Djehiche, Boualem and Hörfelt, Per}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461230510009844}, 
abstract = {{We compare two different models for assets and liabilities for an insurance company that can be considered in the standard approach to solvency assessment and in particular, in determining the required target capital. The first model is suggested by a joint working party by members in CEA, Comité Européen des Assurances, and is based on the duration concept and the second one is an application of ideas by Samuelson and Vasicek. © 2005 Taylor \& Francis Group, LLC.}}, 
pages = {377--400}, 
number = {5}, 
volume = {2005}
}
@article{10.11113/mjfas.v16n1.1356, 
year = {2020}, 
title = {{The distribution of extreme share return in different Malaysian economic circumstances}}, 
author = {Marsani, Muhammad Fadhil and Shabri, Ani}, 
journal = {Malaysian Journal of Fundamental and Applied Sciences}, 
issn = {2289599X}, 
doi = {10.11113/mjfas.v16n1.1356}, 
abstract = {{This paper presents a study on the performance of probability distribution in various financial periods by investigating the effect of economic cycle on extreme stock return activity. Malaysian stock price KLCI data from 1994-2008 were split into three economy periods corresponding to the growth, financial crisis, and recovery. Four prevalent distributions, specifically generalized lambda distribution (GLD), generalized extreme value (GEV), generalized logistic (GLO), and generalized pareto (GPA) had been employed to model weekly and monthly maximum and minimum share returns of Kuala Lumpur Composite Index (KLCI). L-moment approach had been used to estimate the parameter, while k-sample Anderson darling (k-ad) test had been applied to measure the goodness of fit estimation. In conclusion, GLD is the most appropriate distribution to represent weekly maximum and minimum returns for overall three economic scenarios in Malaysia. © 2021 Malaysian Journal of Fundamental and Applied Sciences. All Rights Reserved.}}, 
pages = {75--80}, 
number = {1}, 
volume = {16}
}
@article{10.1109/cso.2009.417, 
year = {2009}, 
title = {{Coherent risk management and its portfolio optimization model}}, 
author = {He, Linjie and Ma, Chaoqun}, 
journal = {2009 International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2009.417}, 
abstract = {{In market risk measurement field, the return-loss distribution exist the severe phenomenon of excess kurtosis and heavy tail. In the meanwhile, method of Value at Risk itself cannot correspond with coherent risk management system, because VaR can not obey with its sub-additivity rule, which make local optimal be the whole optimal when selecting the optimal portfolio. In order to resolve these problems, proceeding from the theory for coherent risk measurement, we put forward a new technique of risk measure-Coherent Value at Risk(CVaR)-to measure market risk of portfolio, on which we build portfolio optimization model of CVaR and select the optimal portfolio with linear programming. In our empirical research, we use random data of Shanghai Security market from Jan. 1 st, 2007 to May 30th, 2008. We conclude that the result based on optimal model of CVaR is better than that of on optimal model of Value at Risk. © 2009 IEEE.}}, 
pages = {536--539}, 
number = {NA}, 
volume = {2}
}
@article{10.21314/jor.2019.420, 
year = {2019}, 
title = {{Static and dynamic risk capital allocations with the euler rule}}, 
author = {Boonen, Tim J}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2019.420}, 
abstract = {{Risk capital allocations are of central importance in performance measurement. A popular solution concept in the academic literature is the Euler rule. This paper studies the volatility of the Euler rule for capital allocation in static and dynamic empirical applications with a simulated history. The Euler rule is not continuous with respect to small changes in the underlying risk capital allocation problem. We show that, when combined with value-at-risk, the Euler rule is very sensitive to empirical measurement error. The use of a known distribution with estimated parameters helps to reduce this error. The Euler rule with an expected shortfall risk measure is less volatile, but it is still more volatile than the proportional rule. © 2019 Infopro Digital Risk (IP) Limited.}}, 
number = {1}, 
volume = {22}
}
@article{10.1080/14697688.2012.738934, 
year = {2014}, 
title = {{How to mitigate the impact of inappropriate distributional settings when the parametric value-at-risk approach is used}}, 
author = {Su, Jung-Bin}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2012.738934}, 
abstract = {{This study utilizes four types of model, which are composed of two volatility specifications (generalized autoregressive conditional heteroskedasticity--GARCH--and autoregressive jump intensity--ARJI) and two value-at-risk (VaR) approaches (parametric and semi-parametric) with three return distribution settings (normal, GED and SGED), to explore, under the same return distribution settings and GARCH-based models as the benchmark, which variance specification and which VaR approach can enhance the VaR performance of GARCH-based models through assessments of accuracy, and further investigate approaches to mitigating the impact of inappropriate distributional settings that may cause misestimation of the parametric approach to VaR via a range of statistics. Empirical results show that, with regard to a long position, especially for the filter historical simulation (FHS) approach, both the variance specification of the ARJI approach and the FHS VaR approach can not only improve the VaR forecasting performance of GARCH-based models but also significantly reduce the influence of inappropriate return distribution settings on the VaR estimate of GARCH-based models. Moreover, FHS-ARJI-based models seem to be superior to FHS-GARCH-based models, but less significantly so than for the other groups competing above. As to short positions, both ARJI-based models and GARCH-based models seem to have similar VaR performance whereas FHS-GARCH-based models are superior to both GARCH-based models and ARJI-based models. In addition, the other three types of model (ARJI, FHS-GARCH and FHS-ARJI) can reduce more effectively the impact of incorrect return distribution specifications of GARCH-based models as compared with the case of a long position. Finally, within the same type of model, the SGED has just about the best performance among these three distribution specifications for a long position, whereas these phenomena seem to be less significant for a short position. Additionally, the VaR forecasting performance of a short position is less significantly affected by return distribution specifications, volatility specifications or VaR approaches as compared with that of a long position. © 2014 Copyright Taylor and Francis Group, LLC.}}, 
pages = {305--325}, 
number = {2}, 
volume = {14}
}
@article{10.1016/s0378-4266(02)00269-8, 
year = {2002}, 
title = {{Incentives for effective risk management}}, 
author = {Danı́elsson, Jón and Jorgensen, Bjørn N. and Vries, Casper G. de}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00269-8}, 
abstract = {{Under the new Capital Accord, banks choose between two different types of risk management systems, the standard or the internal rating based approach. The paper considers how a bank's preference for a risk management system is affected by the presence of supervision by bank regulators. The model uses a principal-agent setting between a bank's owner and its risk management. The main conclusion is that previously unregulated institutions can be expected to switch to the lower quality standard approach subsequent to becoming regulated, i.e., the presence of regulation may induce a bank to decrease the quality of its risk management system. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {1407--1425}, 
number = {7}, 
volume = {26}
}
@article{10.1007/s10479-008-0486-z, 
year = {2012}, 
title = {{Multi-resource allocation in stochastic project scheduling}}, 
author = {Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Berç}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-008-0486-z}, 
abstract = {{We propose a resource allocation model for project scheduling. Our model accommodates multiple resources and decision-dependent activity durations inspired by microeconomic theory. First, we elaborate a deterministic problem formulation. In a second stage, we enhance this model to account for uncertain problem parameters. Assuming that the first and second moments of these parameters are known, the stochastic model minimises an approximation of the value-at-risk of the project makespan. As a salient feature, our approach employs a scenario-free formulation which is based on normal approximations of the activity path durations. We extend our model to situations in which the moments of the random parameters are ambiguous and describe an iterative solution procedure. Extensive numerical results are provided. © 2008 Springer Science+Business Media, LLC.}}, 
pages = {193--220}, 
number = {1}, 
volume = {193}
}
@article{10.1007/s10640-019-00385-0, 
year = {2019}, 
title = {{Can Variations in Temperature Explain the Systemic Risk of European Firms?}}, 
author = {Tzouvanas, Panagiotis and Kizys, Renatas and Chatziantoniou, Ioannis and Sagitova, Roza}, 
journal = {Environmental and Resource Economics}, 
issn = {09246460}, 
doi = {10.1007/s10640-019-00385-0}, 
abstract = {{We employ a ΔCoVaR model in order to measure the potential impact of temperature fluctuations on systemic risk, considering all companies from the STOXX Europe 600 Index, which covers a wide range of industries for the period from 1/1/1990 to 29/12/2017. Furthermore, in this study, we decompose temperature into 3 factors; namely (1) trend, (2) seasonality and (3) anomaly. Findings suggest that, temperature has indeed a significant impact on systemic risk. In fact, we provide significant evidence of either positive or nonlinear temperature effects on financial markets, while the nonlinear relationship between temperature and systemic risk follows an inverted U-shaped curve. In addition, hot temperature shocks strongly increase systemic risk, while we do witness the opposite for cold shocks. Additional analysis shows that deviations of temperature by 1∘C can increase the daily Value at Risk by up to 0.24 basis points. Overall, higher temperatures are highly detrimental for the financial system. Results remain robust under the different proxies that were employed to capture systemic risk or temperature. © 2019, The Author(s).}}, 
pages = {1723--1759}, 
number = {4}, 
volume = {74}
}
@article{10.1016/j.insmatheco.2017.03.002, 
year = {2017}, 
title = {{Characterization of acceptance sets for co-monotone risk measures}}, 
author = {Rieger, Marc Oliver}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2017.03.002}, 
abstract = {{We present a geometric characterization of acceptance sets for monotone, co-monotone and convex risk measures on finite state spaces. Geometrically, such acceptance sets can be represented by convex polygons with edges only on certain hyperplanes. We also provide some lower dimensional examples, and study acceptance sets for value at risk and expected shortfall. © 2017 Elsevier B.V.}}, 
pages = {147--152}, 
number = {NA}, 
volume = {74}
}
@article{10.1016/j.eneco.2008.01.011, 
year = {2008}, 
title = {{Comparison of historically simulated VaR: Evidence from oil prices}}, 
author = {Costello, Alexandra and Asem, Ebenezer and Gardner, Eldon}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2008.01.011}, 
abstract = {{Cabedo and Moya [Cabedo, J.D., Moya, I., 2003. Estimating oil price 'Value at Risk' using the historical simulation approach. Energy Economics 25, 239-253] find that ARMA with historical simulation delivers VaR forecasts that are superior to those from GARCH. We compare the ARMA with historical simulation to the semi-parametric GARCH model proposed by Barone-Adesi et al. [Barone-Adesi, G., Giannopoulos, K., Vosper, L., 1999. VaR without correlations for portfolios of derivative securities. Journal of Futures Markets 19 (5), 583-602]. The results suggest that the semi-parametric GARCH model generates VaR forecasts that are superior to the VaR forecasts from the ARMA with historical simulation. This is due to the fact that GARCH captures volatility clustering. Our findings suggest that Cabedo and Moya's conclusion is mainly driven by the normal distributional assumption imposed on the future risk structure in the GARCH model. Crown Copyright © 2008.}}, 
pages = {2154--2166}, 
number = {5}, 
volume = {30}
}
@article{10.1080/13504851.2015.1039696, 
year = {2015}, 
title = {{Forecasting intraday volatility and VaR using multiplicative component GARCH model}}, 
author = {Diao, Xundi and Tong, Bin}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2015.1039696}, 
abstract = {{We use the multiplicative component GARCH model (mcsGARCH) to decompose the volatility of high-frequency returns of CSI 300 index into three components, namely the daily, the diurnal and the stochastic intraday volatilities. As expected, the diurnal volatility features an important intraday seasonality. Surprisingly due to the unique ‘T + 1 trading rule’ in Chinese stock market, the diurnal volatility of the 5-minute returns of CSI 300 index does not show a U-shaped pattern as in European and American stock markets. Moreover, we investigate the out-of-sample performance of the mcsGARCH model in forecasting the intraday volatility of the CSI 300 index. The results show that the mcsGARCH model performs well in Chinese stock market. © 2015 Taylor \& Francis.}}, 
pages = {1457--1464}, 
number = {18}, 
volume = {22}
}
@article{10.1016/j.jeconbus.2012.09.003, 
year = {2013}, 
title = {{Higher order moment risk in efficient futures portfolios}}, 
author = {You, Leyuan and Nguyen, Duong}, 
journal = {Journal of Economics and Business}, 
issn = {01486195}, 
doi = {10.1016/j.jeconbus.2012.09.003}, 
abstract = {{This study examined whether mean-variance (M-V) framework helps to efficiently diversify away tail risk due to skewness, kurtosis, and other higher moments. We found that M-V efficient portfolios do not have significant higher moment risk, because the risk measured by the two-moment value-at-risk (VaR) was not significantly different from risk measured by the higher moment VaRs. This result was not caused by the large number of assets included in the portfolio. With only nine assets in the M-V portfolio, about 85\% of the diversifiable loss measured by higher moment VaR was diversified away. Furthermore, with less than nine assets in M-V efficient portfolios, the M-V technique diversified away the higher moment risk even more efficiently than did volatility. © 2012 Elsevier Inc.}}, 
pages = {33--54}, 
number = {NA}, 
volume = {65}
}
@article{10.1080/00949651003752320, 
year = {2011}, 
title = {{Value-at-risk forecasting based on gaussian mixture ARMA-GARCH model}}, 
author = {Lee, Sangyeol and Lee, Taewook}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949651003752320}, 
abstract = {{In this paper, we develop a new forecasting algorithm for value-at-risk (VaR) based on ARMA-GARCH (autoregressive moving average-generalized autoregressive conditional heteroskedastic) models whose innovations follow a Gaussian mixture distribution. For the parameter estimation, we employ the conditional least squares and quasi-maximum-likelihood estimator (QMLE) for ARMA and GARCH parameters, respectively. In particular, Gaussian mixture parameters are estimated based on the residuals obtained from the QMLE of GARCH parameters. Our algorithm provides a handy methodology, spending much less time in calculation than the existing resampling and bias-correction method developed in Hartz et al. [Accurate value-at-risk forecasting based on the normal-GARCH model, Comput. Stat. Data Anal. 50 (2006), pp. 3032-3052]. Through a simulation study and a real-data analysis, it is shown that our method provides an accurate VaR prediction.}}, 
pages = {1131--1144}, 
number = {9}, 
volume = {81}
}
@article{10.1016/j.insmatheco.2013.05.005, 
year = {2013}, 
title = {{Optimal risk transfer under quantile-based risk measurers}}, 
author = {Asimit, Alexandru V. and Badescu, Alexandru M. and Verdonck, Tim}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.05.005}, 
abstract = {{The classical problem of identifying the optimal risk transfer from one insurance company to multiple reinsurance companies is examined under some quantile-based risk measure criteria. We develop a new methodology via a two-stage optimisation procedure which not only allows us to recover some existing results in the literature, but also makes possible the analysis of high-dimensional problems in which the insurance company diversifies its risk with multiple reinsurance counter-parties, where the insurer risk position and the premium charged by the reinsurers are functions of the underlying risk quantile. Closed-form solutions are elaborated for some particular settings, although numerical methods for the second part of our procedure represent viable alternatives for the ease of implementing it in more complex scenarios. Furthermore, we discuss some approaches to obtain more robust results. © 2013 Elsevier B.V.}}, 
pages = {252--265}, 
number = {1}, 
volume = {53}
}
@article{10.1007/978-3-319-74454-4_9, 
year = {2018}, 
title = {{Risk assessment of emission abatement technologies for clean shipping}}, 
author = {Atari, Sina and Prause, Gunnar}, 
journal = {Lecture Notes in Networks and Systems}, 
issn = {23673370}, 
doi = {10.1007/978-3-319-74454-4\_9}, 
abstract = {{The purpose of this study is recognizing and assessing the existing risks of SECA related investments of ship owners and the consideration of their risk attributes. Complying with the SECA regulations, maritime stakeholders have to choose among different abatement strategies, which are generally linked to high and risky investments. The paper focusses on the evaluation of scrubber technologies and their relationship to other abatement techniques. Literature review reveals shortcomings in investment risk evaluation among the ship owners operating in emission control areas (ECA). The research fills this gap by presenting a comprehensive compilation of identified risks attributes in an analytical framework together with a risk assessment in the context of HFO and MGO fuel and scrubber related performance indicators comprising CAPEX and OPEX. The results in a classification framework categorize the investment risks and different elements of value at risk (VaR) as well as historical and parametric evaluation of risks. Besides that, this study contributes to new knowledge in the disciplines of green transport and shipping. For future research, the identified risk and investment must be tested in a real business case study and in different scenarios to measure and analyze its performance and efficiency. The results of the paper are based on empiric activities, which were realized during 2017 in the frame of the EU project “EnviSuM”. The empiric measures comprise primary and secondary data analysis, focus group meetings and expert interviews with specialists from shipping sector in BSR. © Springer International Publishing AG 2018.}}, 
pages = {93--101}, 
number = {NA}, 
volume = {36}
}
@article{10.1080/14697688.2017.1330551, 
year = {2018}, 
title = {{A Bayesian encompassing test using combined value-at-risk estimates}}, 
author = {Tsiotas, Georgios}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2017.1330551}, 
abstract = {{The Value at Risk (VaR) is a risk measure that is widely used by financial institutions in allocating risk. VaR forecast estimation involves the conditional evaluation of quantiles based on the currently available information. Recent advances in VaR evaluation incorporate conditional variance into the quantile estimation, yielding the Conditional Autoregressive VaR (CAViaR) models. However, the large number of alternative CAViaR models raises the issue of identifying the optimal quantile predictor. To resolve this uncertainty, we propose a Bayesian encompassing test that evaluates various CAViaR models predictions against a combined CAViaR model based on the encompassing principle. This test provides a basis for forecasting combined conditional VaR estimates when there are evidences against the encompassing principle. We illustrate this test using simulated and financial daily return data series. The results demonstrate that there are evidences for using combined conditional VaR estimates when forecasting quantile risk. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--23}, 
number = {3}, 
volume = {18}
}
@article{10.1093/imaman/dpu015, 
year = {2016}, 
title = {{Measuring the risk of a non-linear portfolio with fat-tailed risk factors through a probability conserving transformation}}, 
author = {Date, Paresh and Bustreo, Roberto}, 
journal = {IMA Journal of Management Mathematics}, 
issn = {1471678X}, 
doi = {10.1093/imaman/dpu015}, 
abstract = {{This paper presents a new heuristic for fast approximation of VaR (Value-at-Risk) and CVaR (conditional Value-at-Risk) for financial portfolios, where the net worth of a portfolio is a non-linear function of possibly non-Gaussian risk factors. The proposed method is based on mapping non-normal marginal distributions into normal distributions via a probability conserving transformation and then using a quadratic, i.e. Delta-Gamma, approximation for the portfolio value. The method is very general and can deal with a wide range of marginal distributions of risk factors, including non-parametric distributions. Its computational load is comparable with the Delta-Gamma-Normal method based on Fourier inversion. However, unlike the Delta-Gamma-Normal method, the proposed heuristic preserves the tail behaviour of the individual risk factors, which may be seen as a significant advantage. We demonstrate the utility of the new method with comprehensive numerical experiments on simulated as well as real financial data. © 2014 The authors.}}, 
pages = {157--180}, 
number = {2}, 
volume = {27}
}
@article{10.1016/j.jeconbus.2013.04.005, 
year = {2013}, 
title = {{Risk contagion in the north-western and southern European stock markets}}, 
author = {Araújo, André da Silva de and Garcia, Maria Teresa Medeiros}, 
journal = {Journal of Economics and Business}, 
issn = {01486195}, 
doi = {10.1016/j.jeconbus.2013.04.005}, 
abstract = {{The paper examines risk spillover among major European, American and Japanese stock exchanges using daily stock prices from 1998 to 2011 period. More specifically, we focus more on risk spillover among major north-western stock markets (i.e. France, Germany, and United Kingdom) and southern European stock markets (Greece, Italy, Portugal, and Spain). The main motivation of the study is to use the idea of rapidly increasing interconnectedness of major stock exchanges around the World to detect the direction and the time lag of risk spillover among major stock markets. We find that the direction of statistically significant spillover is from DAX and FTSE100 to CAC40, from S\&P500 to major north-western European stock markets, and from Europe to Japan (i.e. NIKKEI225). Finally, there is also a strong risk spillover effect between southern European stock markets as well as from S\&P500 to southern European stock market indices. © 2013 Elsevier Inc..}}, 
pages = {1--34}, 
number = {NA}, 
volume = {69}
}
@article{10.1142/9789814619998_0008, 
year = {2014}, 
title = {{Fuzzy Laplace distribution with VaR applied in investment portfolio}}, 
author = {Moraes, Ronei Marcos de and Kerre, Etienne E and Machado, Liliane dos Santos and Lu, Jie and ROCHA, M P C and COSTA, L M and BEDREGAL, B R C}, 
journal = {Decision Making and Soft Computing}, 
issn = {NA}, 
doi = {10.1142/9789814619998\_0008}, 
abstract = {{We present a new possibilistic mean-variance model using the Fuzzy Laplace distribution (PMVFL). We generated a sequence of results and concluded that results showed an expected behavior of model of possibilistic mean-variance. When we increase the VaR (Value at Risk), in other words, when we consider further loss of market value, we mean that the risk rate will be higher, i.e., larger return rate, higher will be risk rate, this fact has been demonstrated in model. © 2014 by World Scientific Publishing Co. Pte. Ltd. All rights reserved.}}, 
pages = {30--35}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.qref.2019.10.007, 
year = {2020}, 
title = {{Are NPL-backed securities an investment opportunity?}}, 
author = {Bolognesi, Enrica and Stucchi, Patrizia and Miani, Stefano}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2019.10.007}, 
abstract = {{This paper focuses on the risk-return profile of the asset-backed securities deriving from the securitization of non-performing loans (NPLs). We test several hypotheses concerning the portfolio sell price, tranche note size and use of a public guarantee supporting the senior notes. We observe the return distribution of junior notes as a function of the portfolio recovery rate, assuming a lognormal distribution. Moreover, we focus on the probability of achieving selected target returns using a value-at-risk approach. Our results provide evidence that securitization is the most valuable deleveraging strategy from the perspective of both investors and banks. © 2019 Board of Trustees of the University of Illinois}}, 
pages = {327--339}, 
number = {NA}, 
volume = {77}
}
@article{10.1016/j.eneco.2017.01.012, 
year = {2017}, 
title = {{Modeling and forecasting extreme commodity prices: A Markov-Switching based extreme value model}}, 
author = {Herrera, Rodrigo and Rodriguez, Alejandro and Pino, Gabriel}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2017.01.012}, 
abstract = {{We propose a Markov-Switching Multifractal Peaks-Over-Threshold (MSM-POT) model to capture the dynamic behavior of the random occurrences of extreme events exceeding a high threshold in time series of returns. This approach allows introducing changes of regimes in the conditional mean function of the inter-exceedance times (i.e., the time between two consecutive extreme events) in order to admit the presence of short- and long-term memory patterns. Further, through its multifractal structure, the MSM-POT approach is able to capture the typical stylized facts of extreme events observed in financial time series, such as temporal clustering of the size of exceedances and temporal behavior of tail thickness. We compare the performance of the MSM-POT model with competing self-exciting models and a GARCH-EVT approach in an in- and out-of-sample VaR forecasting exercise based on the extreme returns of six daily commodity futures prices (i.e. Brent and WTI crude oil, cocoa, cotton, copper, and gold). Empirical results suggest that the VaR estimates generated by the MSM-POT model for the returns analyzed produce the most accurate forecasts. © 2017 Elsevier B.V.}}, 
pages = {129--143}, 
number = {NA}, 
volume = {63}
}
@article{10.1002/for.2249, 
year = {2013}, 
title = {{The role of high-frequency intra-daily data, daily range and implied volatility in multi-period value-at-risk forecasting}}, 
author = {Louzis, Dimitrios P. and Xanthopoulos‐Sisinis, Spyros and Refenes, Apostolos P.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2249}, 
abstract = {{This paper assesses the informational content of alternative realized volatility estimators, daily range and implied volatility in multi-period out-of-sample Value-at-Risk (VaR) predictions. We use the recently proposed Realized GARCH model combined with the skewed Student's t distribution for the innovations process and a Monte Carlo simulation approach in order to produce the multi-period VaR estimates. Our empirical findings, based on the S\&P 500 stock index, indicate that almost all realized and implied volatility measures can produce statistically and regulatory precise VaR forecasts across forecasting horizons, with the implied volatility being especially accurate in monthly VaR forecasts. The daily range produces inferior forecasting results in terms of regulatory accuracy and Basel II compliance. However, robust realized volatility measures, which are immune against microstructure noise bias or price jumps, generate superior VaR estimates in terms of capital efficiency, as they minimize the opportunity cost of capital and the Basel II regulatory capital. Copyright © 2013 John Wiley \& Sons, Ltd. Copyright © 2013 John Wiley \& Sons, Ltd.}}, 
pages = {561--576}, 
number = {6}, 
volume = {32}
}
@article{10.4067/s0718-52862016000100002, 
year = {2016}, 
title = {{The new hybrid value at risk approach based on the extreme value theory [El nuevo enfoque híbrido de value at risk basado en la teoría de valores extremos]}}, 
author = {Radivojevic, Nikola and Cvjetkovic, Milena and Stepanov, Sasa}, 
journal = {Estudios de economía}, 
issn = {03042758}, 
doi = {10.4067/s0718-52862016000100002}, 
abstract = {{In this paper the authors introduce a new hybrid approach based on the Extreme Value Theory (EVT) to joint estimation of Value at Risk (VaR) and Expected Shortfall (ES) for high quantiles of return distributions. The approach is suitable for measuring market risk in the emerging markets. It is designed to capture the empirical features of returns wiThemerging markets, such as leptokurtosis, asymmetry, autocorrelation and heteroscedasticity. © 2016, Universidad de Chile. All rights reserved.}}, 
pages = {29--52}, 
number = {1}, 
volume = {43}
}
@article{10.1016/j.jeconom.2008.09.028, 
year = {2008}, 
title = {{Dynamic quantile models}}, 
author = {Gourieroux, C. and Jasiak, J.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2008.09.028}, 
abstract = {{This paper introduces the Dynamic Additive Quantile (DAQ) model that ensures the monotonicity of conditional quantile estimates. The DAQ model is easily estimable and can be used for computation and updating of the Value-at-Risk. An asymptotically efficient estimator of the DAQ is obtained by maximizing an objective function based on the inverse KLIC measure. An alternative estimator proposed in the paper is the Method of L-Moments estimator (MLM). The MLM estimator is consistent, but generally not fully efficient. Goodness-of-fit tests and diagnostic tools for the assessment of the model are also provided. For illustration, the DAQ model is estimated from a series of returns on the Toronto Stock Exchange (TSX) market index. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {198--205}, 
number = {1}, 
volume = {147}
}
@article{10.1016/j.egypro.2018.10.037, 
year = {2018}, 
title = {{Influences and uncertainty of battery-swapping electric scooters on energy system in Taiwan}}, 
author = {Hsieh, Pei-Ying and Yu, Tai-Yi and Wu, Kuang-Chong and Chang, Len-Fu W.}, 
journal = {Energy Procedia}, 
issn = {18766102}, 
doi = {10.1016/j.egypro.2018.10.037}, 
abstract = {{This paper proposes a new formula to estimate the electricity demand from battery-swapping stations (BSSs) at peak hours, combining parameters of the number of battery-swapping electric scooters (NBSES) and the number of scooters served per BSS. It also presents a novel decision-support analysis for assessing future impact on energy system with an increasing NBSES in Taiwan. The VaR (Value at Risk) values and Monte Carlo method are combined to assess key variables of NBSES and potential benefits. This study finds that the probability for the percentage of operating reserve (OR), R, beyond 6.0 percent is only 86.3\% in the past four years. When NBSES reaches 1.28 million, the probability for R beyond 6.0 percent is down to 69.0\% and R is 2.9\% (95\%CI) without considering the storage ability of BSSs. However, R could be higher than 6.0\% (95\%CI) if considering the storage ability of BSSs. © 2018 The Authors. Published by Elsevier Ltd.}}, 
pages = {95--100}, 
number = {NA}, 
volume = {153}
}
@article{10.1061/41139(387)295, 
year = {2010}, 
title = {{Loan-to-value ratio setting of inventory impawn financing based on VaR method}}, 
author = {Hu, Min and Hu, Qifan}, 
journal = {ICLEM 2010}, 
issn = {NA}, 
doi = {10.1061/41139(387)295}, 
abstract = {{This paper study price risk control problems of inventory impawn financing in copper, for example. The value-at-risk (VaR) is the basis of setting reasonable loan-to-value ratio. Choose VaR method to calculate the VaR of Yangtze River 1\# copper in different conditions when the loan period is three months, and then inspect the result by back testing, as a basis to determine loan-to-value ratio. Use two indicators which are the risk ratio and the efficiency loss ratio to measure the effectiveness of loan-to-value ratio. Inventory financing means for those enterprises who do not satisfy the requirements for the conventional mortgages, they hand over their own products to a logistic firm and sign off relative contract, therefore the financial institute would provide short term loan to them, it's a new type of value adding service the logistic firms provided to their customers. Besides its efficiency in helping corporate finance, the Inventory financing also improves the benefits for logistic businesses. The development of Inventory financing needs to be based on its efficiency in control risk; among these risks, the price risk is caused by the price drop of collateral material, and there are possible loan losses from lack of collateral value. The method of control is often set up reasonable loan-to-value ratio with the precondition being precisely predict the value of quality material at risk (VaR). © 2010 ASCE.}}, 
pages = {2118--2123}, 
number = {NA}, 
volume = {387}
}
@article{10.1080/14697681003785967, 
year = {2012}, 
title = {{Estimation of multiple period expected shortfall and median shortfall for risk management}}, 
author = {So, Mike K. P. and Wong, Chi-Ming}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697681003785967}, 
abstract = {{With the regulatory requirements for risk management, Value at Risk (VaR) has become an essential tool in determining capital reserves to protect the risk induced by adverse market movements. The fact that VaR is not coherent has motivated the industry to explore alternative risk measures such as expected shortfall. The first objective of this paper is to propose statistical methods for estimating multiple-period expected shortfall under GARCH models. In addition to the expected shortfall, we investigate a new tool called median shortfall to measure risk. The second objective of this paper is to develop backtesting methods for assessing the performance of expected shortfall and median shortfall estimators from statistical and financial perspectives. By applying our expected shortfall estimators and other existing approaches to seven international markets, we demonstrate the superiority of our methods with respect to statistical and practical evaluations. Our expected shortfall estimators likely provide an unbiased reference for setting the minimum capital required for safeguarding against expected loss. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {739--754}, 
number = {5}, 
volume = {12}
}
@article{10.1016/j.jbankfin.2006.01.008, 
year = {2006}, 
title = {{Extreme spectral risk measures: An application to futures clearinghouse margin requirements}}, 
author = {Cotter, John and Dowd, Kevin}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2006.01.008}, 
abstract = {{This paper applies the extreme-value (EV) generalised pareto distribution to the extreme tails of the return distributions for the S\&P500, FT100, DAX, Hang Seng, and Nikkei225 futures contracts. It then uses tail estimators from these contracts to estimate spectral risk measures, which are coherent risk measures that reflect a user's risk-aversion function. It compares these to VaR and expected shortfall (ES) risk measures, and compares the precision of their estimators. It also discusses the usefulness of these risk measures in the context of clearinghouses setting initial margin requirements, and compares these to the SPAN measures typically used. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {3469--3485}, 
number = {12}, 
volume = {30}
}
@article{10.2143/ast.31.1.996, 
year = {2001}, 
title = {{Analytical Evaluation of Economic Risk Capital for Portfolios of Gamma Risks}}, 
author = {Hürlimann, Werner}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.31.1.996}, 
abstract = {{Based on the notions of value-at-risk and expected shortfall, we consider two functionals, abbreviated VaR and RaC, which represent the economic risk capital of a risky business over some time period required to cover losses with a high probability. These functionals are consistent with the risk preferences of profit-seeking (and risk averse) decision makers and preserve the stochastic dominance order (and the stop-loss order). Quantitatively, RaC is equal to VaR plus an additional stop-loss dependent term, which takes into account the average amount at loss. Furthermore, RaC is additive for comonotonic risks, which is an important extremal situation encountered in the modeling of dependencies in multivariate risk portfolios. Numerical illustrations for portfolios of gamma distributed risks follow. As a result of independent interest, new analytical expressions for the exact probability density of sums of independent gamma random variables are included, which are similar but different to previous expressions by Provost (1989) and Sim (1992). © 2001, International Actuarial Association. All rights reserved.}}, 
pages = {107--122}, 
number = {1}, 
volume = {31}
}
@article{10.1016/j.eneco.2008.04.002, 
year = {2008}, 
title = {{Estimating 'Value at Risk' of crude oil price and its spillover effect using the GED-GARCH approach}}, 
author = {Fan, Ying and Zhang, Yue-Jun and Tsai, Hsien-Tang and Wei, Yi-Ming}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2008.04.002}, 
abstract = {{Estimation has been carried out using GARCH-type models, based on the Generalized Error Distribution (GED), for both the extreme downside and upside Value-at-Risks (VaR) of returns in the WTI and Brent crude oil spot markets. Furthermore, according to a new concept of Granger causality in risk, a kernel-based test is proposed to detect extreme risk spillover effect between the two oil markets. Results of an empirical study indicate that the GED-GARCH-based VaR approach appears more effective than the well-recognized HSAF (i.e. historical simulation with ARMA forecasts). Moreover, this approach is also more realistic and comprehensive than the standard normal distribution-based VaR model that is commonly used. Results reveal that there is significant two-way risk spillover effect between WTI and Brent markets. Supplementary study indicates that at the 99\% confidence level, when negative market news arises that brings about a slump in oil price return, historical information on risk in the WTI market helps to forecast the Brent market. Conversely, it is not the case when positive news occurs and returns rise. Historical information on risk in the two markets can facilitate forecasts of future extreme market risks for each other. These results are valuable for anyone who needs evaluation and forecasts of the risk situation in international crude oil markets. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {3156--3171}, 
number = {6}, 
volume = {30}
}
@article{10.3390/math8071181, 
year = {2020}, 
title = {{On partial stochastic comparisons based on tail values at risk}}, 
author = {Bello, Alfonso J. and Mulero, Julio and Sordo, Miguel A. and Suárez-Llorens, Alfonso}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math8071181}, 
abstract = {{The tail value at risk at level p, with p ∈ (0, 1), is a risk measure that captures the tail risk of losses and asset return distributions beyond the p quantile. Given two distributions, it can be used to decide which is riskier. When the tail values at risk of both distributions agree, whenever the probability level p ∈ (0, 1), about which of them is riskier, then the distributions are ordered in terms of the increasing convex order. The price to pay for such a unanimous agreement is that it is possible that two distributions cannot be compared despite our intuition that one is less risky than the other. In this paper, we introduce a family of stochastic orders, indexed by confidence levels p0 ∈ (0, 1), that require agreement of tail values at risk only for levels p \&gt; p0. We study its main properties and compare it with other families of stochastic orders that have been proposed in the literature to compare tail risks. We illustrate the results with a real data example. © 2020 by the authors.}}, 
pages = {1181}, 
number = {7}, 
volume = {8}
}
@article{10.1080/00036846.2019.1644442, 
year = {2020}, 
title = {{The impact of liquidity on portfolio value-at-risk forecasts}}, 
author = {Hung, Jui-Cheng and Su, Jung-Bin and Chang, Matthew C. and Wang, Yi-Hsien}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2019.1644442}, 
abstract = {{Historical crisis events have highlighted the insufficiency of Value-at-Risk (VaR) as a measure of market risk because such metric does not take liquidity into account. Unlike previous studies analyzing with only a single asset, we examine the impact of liquidity on computing VaR forecasts from a portfolio level. To this end, we use multivariate GARCH-t and GJR-GARCH-t models, as compared with univariate models, to seize the liquidity property embedded in individual stock returns and evaluate their accuracy and efficiency in computing VaR forecasts for portfolios with different liquidity levels. The empirical results indicate that computing portfolio VaR forecasts with multivariate models outperform the univariate models for full and subsample periods in terms of accuracy and efficiency evaluations, in particular for less-liquid portfolios. These results suggest the importance of liquidity in computing portfolio VaR forecasts. Ignorance of the impact of liquidity in computing portfolio VaR forecasts might result in inadequate coverage and insufficient market risk capital requirements. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--18}, 
number = {3}, 
volume = {52}
}
@article{10.1007/978-3-319-18029-8_23, 
year = {2015}, 
title = {{A mean-of-order- p class of value-at-risk estimators}}, 
author = {Gomes, M. Ivette and Brilhante, M. Fátima and Pestana, Dinis}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-18029-8\_23}, 
abstract = {{The main objective of statistics of univariate extremes lies in the estimation of quantities related to extreme events. In many areas of application, like finance, insurance and statistical quality control, a typical requirement is to estimate a high quantile, i.e. the Value at Risk at a level q(VaRq), high enough, so that the chance of exceedance of that value is equal to q, with q small. In this paper we deal with the semi-parametric estimation of VaRq, for heavy tails, introducing a new class of VaR-estimators based on a class of mean-of-order-p (MOP) extreme value index (EVI)-estimators, recently introduced in the literature. Interestingly, the MOP EVI-estimators can have a mean square error smaller than that of the classical EVI-estimators, even for small values of k. They are thus a nice basis to build alternative VaR-estimators not only around optimal levels, but for other levels too.The new VaR-estimators are compared with the classical ones, not only asymptotically, but also for finite samples, through Monte-Carlo techniques. © Springer International Publishing Switzerland 2015.}}, 
pages = {305--320}, 
number = {NA}, 
volume = {136}
}
@article{10.1016/j.physa.2014.01.024, 
year = {2014}, 
title = {{Multifractality and value-at-risk forecasting of exchange rates}}, 
author = {Batten, Jonathan A. and Kinateder, Harald and Wagner, Niklas}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2014.01.024}, 
abstract = {{This paper addresses market risk prediction for high frequency foreign exchange rates under nonlinear risk scaling behaviour. We use a modified version of the multifractal model of asset returns (MMAR) where trading time is represented by the series of volume ticks. Our dataset consists of 138,418 5-min round-the-clock observations of EUR/USD spot quotes and trading ticks during the period January 5, 2006 to December 31, 2007. Considering fat-tails, long-range dependence as well as scale inconsistency with the MMAR, we derive out-of-sample value-at-risk (VaR) forecasts and compare our approach to historical simulation as well as a benchmark GARCH(1,1) location-scale VaR model. Our findings underline that the multifractal properties in EUR/USD returns in fact have notable risk management implications. The MMAR approach is a parsimonious model which produces admissible VaR forecasts at the 12-h forecast horizon. For the daily horizon, the MMAR outperforms both alternatives based on conditional as well as unconditional coverage statistics. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {71--81}, 
number = {NA}, 
volume = {401}
}
@article{10.5220/0007378101020110, 
year = {2019}, 
title = {{Modeling the esscher premium principle for a system of elliptically distributed risks}}, 
author = {Shushi, Tomer}, 
journal = {Proceedings of the 8th International Conference on Operations Research and Enterprise Systems}, 
issn = {NA}, 
doi = {10.5220/0007378101020110}, 
abstract = {{The Esscher premium principle provides an important framework for allocating a certain loaded premium for some claim (risk) in order to manage the risks of insurance companies. In this paper, we show how to model the celebrated Esscher premium principle for a system of elliptically distributed dependent risks, where each risk is greater or equal than its value-at-risk. Furthermore, we present calculations of the proposed multivariate risk measure, investigate its properties and formulas, and show how special elliptical models can be implemented in the theory. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.}}, 
pages = {102--110}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/isit.2011.6033932, 
year = {2011}, 
title = {{An information-theoretic approach to constructing coherent risk measures}}, 
author = {Ahmadi-Javid, A.}, 
journal = {2011 IEEE International Symposium on Information Theory Proceedings}, 
issn = {21578104}, 
doi = {10.1109/isit.2011.6033932}, 
abstract = {{In the past decade, the new concept of coherent risk measure has found many applications in finance, insurance and operations research. In this paper, we introduce a new class of coherent risk measures constructed by using information-type pseudo-distances that generalize the Kullback-Leibler divergence, also known as the relative entropy. We first analyze the primal and dual representations of this class. We then study entropic value-at-risk (EVaR) which is the member of this class associated with relative entropy. We also show that conditional value-at-risk (CVaR), which is the most popular coherent risk measure, belongs to this class and is a lower bound for EVaR. © 2011 IEEE.}}, 
pages = {2125--2127}, 
number = {NA}, 
volume = {NA}
}
@article{10.1287/opre.2018.1743, 
year = {2018}, 
title = {{Surplus-invariant, law-invariant, and conic acceptance sets must be the sets induced by value at risk}}, 
author = {He, Xue Dong and Peng, Xianhua}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.2018.1743}, 
abstract = {{The regulator is interested in proposing a capital adequacy test by specifying an acceptance set for firms’ capital positions at the end of a given period. This set needs to be surplus invariant; i.e., not to depend on the surplus of firms’ shareholders, because the test means to protect firms’ liability holders. We prove that any surplus-invariant, law-invariant, and conic acceptance set must be the set of capital positions whose value at risk at a given level is less than zero. The result still holds if we replace conicity with numéraire invariance, a property stipulating that whether a firm passes the test should not depend on the currency used to denominate its assets. © 2018 INFORMS.}}, 
pages = {1268--1275}, 
number = {5}, 
volume = {66}
}
@article{10.1007/3-540-26993-2_39, 
year = {2005}, 
title = {{Does risk management make financial markets riskier?}}, 
author = {Harper, Ian R. and Keller, Joachim G. and Pfeil, Christian M.}, 
issn = {NA}, 
doi = {10.1007/3-540-26993-2\_39}, 
abstract = {{Value-at-risk figures are calculated on the basis of historical market volatility and capital requirements are determined on the basis of these calculations. A rise in historical market volatility leads to an increase of the regulatory capital requirement. If market participants engage in forced selling to decrease risk exposure to meet imposed capital requirements, volatility may be amplified. Risk management on the individual firm level may thus actually lead to an increase of market volatility in the economy as a whole and the regulatory aim to limit the chances of systemic effects is undermined. We present an informal exposition of this argument as well as supporting empirical and anecdotal evidence. © 2005 Springer Berlin · Heidelberg.}}, 
pages = {765--783}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ecosta.2017.08.001, 
year = {2018}, 
title = {{Combining Value-at-Risk forecasts using penalized quantile regressions}}, 
author = {Bayer, Sebastian}, 
journal = {Econometrics and Statistics}, 
issn = {24523062}, 
doi = {10.1016/j.ecosta.2017.08.001}, 
abstract = {{Penalized quantile regressions are proposed for the combination of Value-at-Risk forecasts. The primary reason for regularization of the quantile regression estimator with the elastic net, lasso and ridge penalties is multicollinearity among the standalone forecasts, which results in poor forecast performance of the non-regularized estimator due to unstable combination weights. This new approach is applied to combining the Value-at-Risk forecasts of a wide range of frequently used risk models for stocks comprising the Dow Jones Industrial Average Index. Within a thorough comparison analysis, the penalized quantile regressions perform better in terms of backtesting and tick losses than the standalone models and several competing forecast combination approaches. This is particularly evident during the global financial crisis of 2007–2008. © 2017 EcoSta Econometrics and Statistics}}, 
pages = {56--77}, 
number = {NA}, 
volume = {8}
}
@article{10.21314/jop.2018.214, 
year = {2018}, 
title = {{A review of the state of the art in quantifying operational risk}}, 
author = {Benito, Sonia and Lopez-Martin, Carmen}, 
journal = {Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2018.214}, 
abstract = {{In this paper, we provide a comprehensive review of the different approaches developed to model operational risk, specifically focusing on the actuarial approach. We highlight their relative strengths and weaknesses. In the case of the actuarial approach – that most commonly used by financial institutions – we review the challenges faced (scarcity of data, truncated data, and modeling dependence in both frequency and severity) and offer proposals to overcome them. Our paper’s objective is to provide financial risk researchers with all of the models and proposed developments for operational risk estimation. © 2018 Infopro Digital Risk (IP) Limited.}}, 
pages = {89--129}, 
number = {4}, 
volume = {13}
}
@article{10.1109/tpwrs.2011.2172005, 
year = {2012}, 
title = {{Exploitation of electric-drive vehicles in electricity markets}}, 
author = {Pantos, M.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2011.2172005}, 
abstract = {{The paper presents the optimization algorithm which may eventually be used by electric energy suppliers to coordinate charging and discharging of electric-drive vehicles (EDVs) exploited in electricity markets. The research is focused on a day-ahead market and a provision of system regulation in an ancillary-service market. The proposed optimization minimizes the charging costs that can be partly compensated with profits obtained from participation in the energy markets. Due to the stochastic nature of transportation patterns, the Monte Carlo simulation is applied to model uncertainties presented by numerous scenarios. To reduce the problem complexity, the simulated driving patterns are not individually considered in the optimization but clustered into fleets using the GAMS/SCENRED tool. Uncertainties of energy requirements in the market and energy prices are presented by statistical central moments that are further considered in Hong's \$2 - \{\textbackslashrm point\} +1\$ estimation method in order to define points considered in the optimization. Finally, each energy supplier has to offer competitive energy prices to EDV users for transportation. Due to uncertainties, the final prices cannot be deterministically calculated; thus, the paper proposes the risk-based approach applying value at risk. Case studies illustrate the application of the proposed optimization in achieving competitive prices for EDV users. © 2012 IEEE.}}, 
pages = {682--694}, 
number = {2}, 
volume = {27}
}
@article{10.37920/sasj.2020.54.1.5, 
year = {2020}, 
title = {{The recovery theorem with application to risk management}}, 
author = {Maré, Eben and Appel, Vaughan Van}, 
journal = {South African Statistical Journal}, 
issn = {0038271X}, 
doi = {10.37920/sasj.2020.54.1.5}, 
abstract = {{The forward-looking nature of option prices provides an appealing way to extract risk measures. In this paper, we extract forecast densities from option prices that can be used in forecasting risk measures. More specifically, we extract a real-world return density forecast, implied from option prices, using the recovery theorem. In addition, we backtest and compare the predictive power of this real-world return density forecast with a risk-neutral return density forecast, implied from option prices, and a simple historical simulation approach. In an empirical study, using the South African FTSE/JSE Top 40 index, we found that the extracted real-world density forecasts, using the recovery theorem, yield satisfying forecasts of risk measures. © 2020 South African Statistical Association}}, 
pages = {65--91}, 
number = {1}, 
volume = {54}
}
@article{10.26350/000518-000009, 
year = {2018}, 
title = {{Systemic risk determinants in the European banking industry during financial crises, 2006-2012}}, 
author = {}, 
issn = {0035676X}, 
doi = {10.26350/000518-000009}, 
abstract = {{The recent financial turmoil has stimulated a rich debate in banking and financial literature on the identification of systemic risk determinants and devices to forecast and prevent crises. This paper explores the contribution of corporate variables to systemic risk using the CoVaR approach (Adrian and Brunnermeier, 2016). Using balanced panel data on 141 European banks from 24 countries, which were listed from 2006Q1 to 2012Q4, we investigated the impact of corporate variables during the three regimes that characterised the European banking sector-the subprime crisis (2007Q3-2008Q3), the European Great Financial Depression (2008Q4-2010Q2), and the sovereign debt crisis (2010Q3-2012Q4). Our results show that size did not play a significant role in spreading systemic risk, while maturity mismatch did. However, the nature and intensity of these two determinants varied across the three regimes. © 2018 Vita e Pensiero / Pubblicazioni dell'Università Cattolica del Sacro Cuore.}}, 
number = {2}, 
volume = {NA}
}
@article{10.1080/03461238.2010.540154, 
year = {2013}, 
title = {{Raising and allocation capital principles as optimal managerial contracts}}, 
author = {Mierzejewski, Fernando}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2010.540154}, 
abstract = {{A unified framework is presented to characterise the capital structure of firms that face borrowing restrictions - which extends the classic theory of capital by incorporating elements from actuarial and agency theory. It is demonstrated that the bankruptcy and agency costs afforded by these firms can be expressed in terms of the actuarial prices of the underlying exposures. Then the optimal surplus is determined in order to maximise value - which is equivalent to minimise the cost of bankruptcy plus the opportunity cost of capital. The capital principle thus obtained explicitly depends on risk and expectations, and can be applied to allocate reserves both in financial and insurance companies. An optimal decentralised mechanism is also defined that stimulates the exchange of information inside multidivisional corporations. © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {24--48}, 
number = {1}, 
volume = {NA}
}
@article{10.1016/s0927-5398(00)00016-5, 
year = {2000}, 
title = {{Portfolio selection with limited downside risk}}, 
author = {Jansen, Dennis W. and Koedijk, Kees G. and Vries, Casper G. de}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(00)00016-5}, 
abstract = {{A safety-first investor maximizes expected return subject to a downside risk constraint. Arzac and Bawa [Arzac, E.R., Bawa, V.S., 1977. Portfolio choice and equilibrium in capital markets with safety-first investors. Journal of Financial Economics 4, 277-288.] use the Value at Risk as the downside risk measure. The paper by Gourieroux, Laurent and Scaillet estimates the optimal safety-first portfolio by a kernel-based method, we exploit the fact that returns are fat-tailed, and propose a semi-parametric method for modeling tail events. We also analyze a portfolio containing the two stocks used by Gourieroux et al. and discuss the merits of the safety-first approach. © 2000 Elsevier Science B.V.}}, 
pages = {247--269}, 
number = {3-4}, 
volume = {7}
}
@article{10.1016/j.asoc.2021.107785, 
year = {2021}, 
title = {{Applying fuzzy scenarios for the measurement of operational risk}}, 
author = {Bonet, Isis and Peña, Alejandro and Lochmuller, Christian and Patiño, Héctor Alejandro and Chiclana, Francisco and Góngora, Mario}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2021.107785}, 
abstract = {{Operational risk measurement assesses the probability to suffer financial losses in an organisation. The assessment of this risk is based primarily on the organisation's internal data. However, other factors, such as external data and scenarios are also key elements in the assessment process. Scenarios enrich the data of operational risk events by simulating situations that still have not occurred and therefore are not part of the internal databases of an organisation but which might occur in the future or have already happened to other companies. Internal data scenarios often represent extreme risk events that increase the operational Value at Risk (OpVaR) and also the average loss. In general, OpVaR and the loss distribution are an important part of risk measurement and management. In this paper, a fuzzy method is proposed to add risk scenarios as a valuable data source to the data for operational risk measurement. We compare adding fuzzy scenarios with the possibility of adding non fuzzy or crisp scenarios. The results show that by adding fuzzy scenarios the tail of the aggregated loss distribution increases but that the effect on the expected average loss and on the OpVaR is lesser in its extent. © 2021 Elsevier B.V.}}, 
pages = {107785}, 
number = {NA}, 
volume = {112}
}
@article{10.1108/ijrdm-09-2016-0152, 
year = {2017}, 
title = {{No more blaming the weather: a retailer’s approach to measuring and managing weather variability}}, 
author = {Bertrand, Jean-Louis and Parnaudeau, Miia}, 
journal = {International Journal of Retail \& Distribution Management}, 
issn = {09590552}, 
doi = {10.1108/ijrdm-09-2016-0152}, 
abstract = {{Purpose: Retailers have long been aware that weather affects the sales of a myriad of products, but until now, most were not in a position to manage the risks weather presents. Rising weather variability combined with advances in weather-index financial instruments have prompted new interest in investigating the relationship between sales and weather. The purpose of this paper is to explore the impact of changes in weather on UK retail sales, to estimate the contribution of weather to sales, and evaluate the maximum potential loss caused by adverse weather, for each season and retail sector. Design/methodology/approach: The authors present a methodology to identify and quantify the extent to which a company is exposed to weather risks, in order to incorporate them into its risk management policy and take actions to mitigate these risks. For each season and each retail category, the authors provide a measure of the impact of weather on sales that can be used as a benchmark to analyse sales performance. Findings: The authors propose a new risk assessment indicator to evaluate the potential losses caused by adverse weather (WeatherRisk). The authors show that intra-annual changes in weather significantly affect retail sales. The exposure of retail categories to weather are not the same depending on the season, and the response of individual retail categories to the same change in weather varies considerably. Although temperature is a predominant explanatory variable, the authors show that weather-sensitivity analysis should include precipitation, humidity rate and wind. Research limitations/implications: One limitation of this study is that the authors individually compute WeatherRisk for each significant weather variable. Further research could explore new approaches to evaluate Total WeatherRisk, which take into account potential multicollinearity issues between weather variables. Practical implications: The methodology allows retailers to measure the effects of weather on sales performance, evaluate the risks at stake, and protect sales and margins from weather risks, with newly available index-based financial instruments. Managers may now actively use weather as a differential advantage, and at the same time focus their efforts on improving resiliency to increasing climate variability. Originality/value: In this paper, the authors produce a detailed analysis of the exposure of each retail sectors to unseasonal weather. This is the first time all retail sectors are analysed and ranked per season at a national level. The authors provide managers with actionable information to improve their understanding of how weather impact sales over each season, and to allow them to structure weather-index-based instruments with financial partners. © 2017, © Emerald Publishing Limited.}}, 
pages = {730--761}, 
number = {7-8}, 
volume = {45}
}
@article{10.1007/s00291-010-0225-0, 
year = {2012}, 
title = {{Value-at-Risk optimization using the difference of convex algorithm}}, 
author = {Wozabal, David}, 
journal = {OR Spectrum}, 
issn = {01716468}, 
doi = {10.1007/s00291-010-0225-0}, 
abstract = {{Value-at-Risk (VaR) is an integral part of contemporary financial regulations. Therefore, the measurement of VaR and the design of VaR optimal portfolios are highly relevant problems for financial institutions. This paper treats a VaR constrained Markowitz style portfolio selection problem when the distribution of returns of the considered assets are given in the form of finitely many scenarios. The problem is a non-convex stochastic optimization problem and can be reformulated as a difference of convex (D. C.) program. We apply the difference of convex algorithm (DCA) to solve the problem. Numerical results comparing the solutions found by the DCA to the respective global optima for relatively small problems as well as numerical studies for large real-life problems are discussed. © 2010 Springer-Verlag.}}, 
pages = {861--883}, 
number = {4}, 
volume = {34}
}
@article{10.1142/s0219091506000720, 
year = {2006}, 
title = {{Incorporating the time-varying tail-fatness into the historical simulation method for portfolio value-at-risk}}, 
author = {Lin, Chu-Hsiung and Chien, Chang-Cheng Chang and Chen, Sunwu Winfred}, 
journal = {Review of Pacific Basin Financial Markets and Policies}, 
issn = {02190915}, 
doi = {10.1142/s0219091506000720}, 
abstract = {{This study extends the method of Guermat and Harris (2002), the Power EWMA (exponentially weighted moving average) method in conjunction with historical simulation to estimating portfolio Value-at-Risk (VaR). Using historical daily return data of three hypothetical portfolios formed by international stock indices, we test the performance of this modified approach to see if it can improve the precise forecasting capability of historical simulation. We explicitly highlight the extended Power EWMA owns privileged flexibilities to capture time-varying tail-fatness and volatilities of financial returns, and therefore may promote the quality of extreme risk management. Our empirical results, derived from the Kupiec (1995) tests and failure ratios, show that our proposed method indeed offers substantial improvements on capturing dynamic returns distributions, and can significantly enhance the estimation accuracy of portfolio VaR. © World Scientific Publishing Co. and Center for Pacific Basin Business, Economics and Finance Research.}}, 
pages = {257--274}, 
number = {2}, 
volume = {9}
}
@article{10.3934/jimo.2016046, 
year = {2017}, 
title = {{Auction and contracting mechanisms for channel coordination with consideration of participants' risk attitudes}}, 
author = {Ma, Cheng and Lee, Y. C. E. and Chan, Chi Kin and Wei, Yan}, 
journal = {Journal of Industrial \& Management Optimization}, 
issn = {15475816}, 
doi = {10.3934/jimo.2016046}, 
abstract = {{This paper considers a two-supplier one-retailer coordinated supply chain system with auction and contracting mechanism incorporating participants' risk attitudes. The risk attitude is quantified using the value-at-risk (VaR) measure and the retailer faces a stochastic linear price-dependent demand function. In the supply chain, the suppliers (providing identical products) compete with each other in order to win the ordering contract of the retailer. Several auction and contracting mechanisms are developed and compared. It can be analytically shown that the retail price of the risk-averse system is higher than that of the risk-neutral system, but the order quantity is lower than that of the risk-neutral system.}}, 
pages = {775--801}, 
number = {2}, 
volume = {13}
}
@article{10.1016/j.najef.2020.101175, 
year = {2020}, 
title = {{Risk dependence and cointegration between pharmaceutical stock markets: The case of China and the USA}}, 
author = {Zhou, Xinmiao and Qian, Huanhuan and Pérez-Rodríguez, Jorge. V. and López-Valcárcel, Beatriz González}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2020.101175}, 
abstract = {{This paper analyses risk-integration and the degree of dependence between the Values-at-Risk (VaRs) estimates for the two major pharmaceutical stock markets in the world: USA and China. To do this, we study the dependence and fractional cointegration properties among risks. Using daily returns for an eleven-year period, we estimated the VaRs obtained for pharmaceutical market portfolios in China (Shanghai) and the USA (NYSE) using the market model and considering both long and short trading positions. We conclude that the Shanghai pharmaceutical market is riskier than NYSE, although is predictable and losses in both markets exhibit tail dependence between VaR estimates. Particularly, there is lower tail VaR dependence for long position and upper tail dependence for short positions, both being small and fairly constant. On the other hand, we have not found fractional cointegration between risks, suggesting that China's pharmaceutical sector is not integrated into the global pharmaceutical market. © 2020 Elsevier Inc.}}, 
pages = {101175}, 
number = {NA}, 
volume = {52}
}
@article{10.1023/a:1014303013248, 
year = {2001}, 
title = {{A Value at Risk Approach to Background Risk}}, 
author = {Luciano, Elisa and Kast, Robert}, 
journal = {The Geneva Papers on Risk and Insurance Theory}, 
issn = {09264957}, 
doi = {10.1023/a:1014303013248}, 
abstract = {{This paper studies the effects of an uninsurable background risk (BR) on the demand for insurance (proportional and with deductible). We study both the case of BR uncorrelated with the insurable one and the perfectly correlated one, in a Gaussian world. In order to perform our study, we exploit the new risk measure known as Value at Risk (VaR) and consider insurance contracts which are Mean-VaR efficient. We obtain results which depend on the parameters (moments) of both risks and on the magnitude of loadings charged by the insurance company, instead of depending on the risk attitudes of the insured, such as risk aversion and prudence. We demonstrate that, if loadings are not too high, the demand for insurance increases with positively correlated BR; it decreases with BR negatively correlated if the latter is less risky than the insurable one (in this case it can even go to zero, if loadings are too high); it goes to zero with BR which is negatively correlated and more risky than the insurable one.}}, 
pages = {91--115}, 
number = {2}, 
volume = {26}
}
@article{10.1177/0972652718777124, 
year = {2018}, 
title = {{Testing Conditional Asset Pricing in Pakistan: The Role of Value-at-risk and Illiquidity Factors}}, 
author = {Azher, Sara and Iqbal, Javed}, 
journal = {Journal of Emerging Market Finance}, 
issn = {09726527}, 
doi = {10.1177/0972652718777124}, 
abstract = {{This article investigates performance of conditional and unconditional Capital Asset Pricing Model and Fama–French model augmented with a downside risk, that is, the value-at-risk (VaR) factor and an illiquidity factor as additional risk factors using the discount factor methodology of Cochrane (1996). Using monthly portfolio data as test assets from the Pakistani stock market from January 1993 to January 2013 we provide empirical evidence on the efficacy of the VaR and illiquidity factors in asset pricing. We find that these factors improve the efficacy of the Fama–French model and including these factors reduces the explanatory power of co-kurtosis factor. © 2018, 2018 Institute of Financial Management and Research.}}, 
pages = {S259--S281}, 
number = {2\_suppl}, 
volume = {17}
}
@article{10.1016/j.eswa.2008.01.086, 
year = {2009}, 
title = {{Portfolio value-at-risk forecasting with GA-based extreme value theory}}, 
author = {Lin, Ping-Chen and Ko, Po-Chang}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2008.01.086}, 
abstract = {{Value-at-risk (VaR) has become a popular risk measure since it was adopted by the Bank for International Settlements and US regulatory agencies in 1988. The VaR concept has also been further extended to the portfolio value-at-risk (PVaR) measure used for managing risks and returns under a multiple-asset portfolio. Precise prediction of PVaR provides better evaluation criteria in areas such as investment decision-making and risk management. The two issues concerned with portfolio risk are efficient set selection and volatility forecasting. Most of the statistical portfolio selection models are based on linear functions under specific assumptions. Due to the fat-tailed distribution in most real financial time-series data, extreme value theory (EVT) is powerful in determining the VaR of a portfolio by concentrating on estimating the shape of the fat-tailed probability distribution. However, using EVT to evaluate the portfolio's volatility is very difficult, because each asset within the portfolio has its own distinct peak threshold value. This study introduces an evolutionary portfolio volatility forecasting model to optimize portfolios under their maximum expected returns subject to a risk constraint. We use a genetic algorithm (GA) to extract the best portfolio set and most suitable peak threshold in order to estimate the portfolio's VaR by means of EVT. © 2008.}}, 
pages = {2503--2512}, 
number = {2 PART 1}, 
volume = {36}
}
@article{10.1061/(asce)co.1943-7862.0001397, 
year = {2017}, 
title = {{Sharing the Big Risk: Assessment Framework for Revenue Risk Sharing Mechanisms in Transportation Public-Private Partnerships}}, 
author = {Liu, Ting and Bennon, Michael and Garvin, Michael J. and Wang, Shouqing}, 
journal = {Journal of Construction Engineering and Management}, 
issn = {07339364}, 
doi = {10.1061/(asce)co.1943-7862.0001397}, 
abstract = {{The allocation and management of revenue risk is a critical issue in the development of public-private partnership (PPP) concessions for new roadways. In order to attract financing, governments often provide fiscal support ranging from availability payments (APs) to minimum revenue guarantees (MRGs) to flexible-term contracts when demand and thus the financial viability of a project are uncertain. However, a government's inability to evaluate these alternatives can lead to either surplus fiscal support ex ante or unexpected liabilities ex post. The authors propose a quantitative methodology to guide governments' decisions when choosing among the fiscal support mechanisms. Based on a comprehensive literature review, we establish a two-dimensional framework to evaluate fiscal support alternatives by comparing their effects on the financing costs of a project and the risk retained by a procuring government. We use a stochastic revenue projection model to quantify the revenue risk and the framework's measurable indicators. We apply our framework using a hypothetical case study. The case results demonstrate that flexible-term contracts do little to increase project leverage, MRGs are most applicable to projects with significant revenue volatility, and APs are more appropriate in projects with lower revenue volatility. The framework allows governments to make more informed decisions about revenue risk sharing mechanisms to improve public budgeting and deficit control. © 2017 American Society of Civil Engineers.}}, 
pages = {04017086}, 
number = {12}, 
volume = {143}
}
@article{10.1109/ijcnn48605.2020.9207399, 
year = {2020}, 
title = {{Prediction with Expert Advice for Value at Risk}}, 
author = {Dzhamtyrova, Raisa and Kalnishkan, Yuri}, 
journal = {2020 International Joint Conference on Neural Networks (IJCNN)}, 
issn = {NA}, 
doi = {10.1109/ijcnn48605.2020.9207399}, 
abstract = {{We propose to apply the method of online prediction with expert advice for estimation of Value at Risk. We show that in some cases the combination of different methods can produce better results compared to a single model. Our approach is based on Weak Aggregating Algorithm (WAA), which is similar to the Bayesian method, where the prediction is the average over all models based on the likelihood of the available data. WAA provides a theoretical guarantee that the prediction strategy is asymptotically as good as the best expert. We propose two ways of combining predictions of different experts. The first approach combines predictions of normal distribution experts, whereas the second method combines predictions of conventional models that are used to estimate Value at Risk. The experimental results on three stocks show that WAA performs close to or better than the best expert model. In addition, backtesting with Kupiec unconditional coverage test and Christoffersen conditional coverage test shows that WAA is the only method that fails to reject the null hypothesis for all test cases. © 2020 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/9781118650318.ch12, 
year = {2016}, 
title = {{Estimation Methods for Value at Risk}}, 
author = {Nadarajah, Saralees and Chan, Stephen}, 
issn = {NA}, 
doi = {10.1002/9781118650318.ch12}, 
abstract = {{Value at risk is the most popular measure of financial risk. It was introduced in the 1980s. Much theory have been developed since then. The developments have been most intensive in recent years. However, we are not aware of any comprehensive review of known estimation methods for value at risk. We feel it is timely that such a review is written. This paper attempts that task with emphasis on recent developments. We review: general properties of value at risk including ordering properties, upper comonotonicity, multivariate extensions and inequalities; parametric estimation methods for value at risk based on well-known univariate distributions, time series models, approximations, copulas, principal components, quantile regression, Bayesian methods and Brownian motion; nonparametric estimation methods for value at risk based on historical methods, bootstrapping, importance sampling and kernel density estimation; semiparametric estimation methods for value at risk based on the extreme value theory method, the generalized Pareto distribution and M-estimation methods; computer software for value at risk based on the R platform and other platforms. We expect that this review could serve as a source of reference and encourage further research with respect to measures of financial risk. © 2017 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {283--356}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2019.12.007, 
year = {2020}, 
title = {{Partially censored posterior for robust and efficient risk evaluation}}, 
author = {Borowska, Agnieszka and Hoogerheide, Lennart and Koopman, Siem Jan and Dijk, Herman K. van}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2019.12.007}, 
abstract = {{A novel approach to inference for a specific region of the predictive distribution is introduced. An important domain of application is accurate prediction of financial risk measures, where the area of interest is the left tail of the predictive density of logreturns. Our proposed approach originates from the Bayesian approach to parameter estimation and time series forecasting, however it is robust in the sense that it provides a more accurate estimation of the predictive density in the region of interest in case of misspecification. The first main contribution of the paper is the novel concept of the Partially Censored Posterior (PCP), where the set of model parameters is partitioned into two subsets: for the first subset of parameters we consider the standard marginal posterior, for the second subset of parameters (that are particularly related to the region of interest) we consider the conditional censored posterior. The censoring means that observations outside the region of interest are censored: for those observations only the probability of being outside the region of interest matters. This quasi-Bayesian approach yields more precise parameter estimation than a fully censored posterior for all parameters, and has more focus on the region of interest than a standard Bayesian approach. The second main contribution is that we introduce two novel methods for computationally efficient simulation: Conditional MitISEM, a Markov chain Monte Carlo method to simulate model parameters from the Partially Censored Posterior, and PCP-QERMit, an Importance Sampling method that is introduced to further decrease the numerical standard errors of the Value-at-Risk and Expected Shortfall estimators. The third main contribution is that we consider the effect of using a time-varying boundary of the region of interest. Extensive simulation and empirical studies show the ability of the introduced method to outperform standard approaches. © 2019 Elsevier B.V.}}, 
pages = {335--355}, 
number = {2}, 
volume = {217}
}
@article{10.1007/s10614-017-9661-0, 
year = {2018}, 
title = {{Time Series Simulation with Randomized Quasi-Monte Carlo Methods: An Application to Value at Risk and Expected Shortfall}}, 
author = {Tzeng, Yu-Ying and Beaumont, Paul M. and Ökten, Giray}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-017-9661-0}, 
abstract = {{Quasi-Monte Carlo methods are designed to produce efficient estimates of simulated values but the error statistics of these estimates are difficult to compute. Randomized quasi-Monte Carlo methods have been developed to address this shortcoming. In this paper we compare quasi-Monte Carlo and randomized quasi-Monte Carlo techniques for simulating time series. We use randomized quasi-Monte Carlo to compute value-at-risk and expected shortfall measures for a stock portfolio whose returns follow a highly nonlinear Markov switching stochastic volatility model which does not admit analytical solutions for the returns distribution. Quasi-Monte Carlo methods are more accurate but do not allow the computation of reliable confidence intervals about risk measures. We find that randomized quasi-Monte Carlo methods maintain many of the advantages of quasi-Monte Carlo while also providing the ability to produce reliable confidence intervals of the simulated risk measures. However, the advantages in speed of convergence of randomized quasi-Monte Carlo diminish as the forecast horizon increases. © 2017, Springer Science+Business Media New York.}}, 
pages = {55--77}, 
number = {1}, 
volume = {52}
}
@article{10.1016/j.eneco.2010.11.002, 
year = {2011}, 
title = {{Value-at-risk estimation with the optimal dynamic biofuel portfolio}}, 
author = {Chang, Ting-Huan and Su, Hsin-Mei and Chiu, Chien-Liang}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2010.11.002}, 
abstract = {{In the past, petroleum companies only paid attention to hedging the variation in the crude oil price and volatility. However, they have now expanded their analysis to encompass renewable sources, such as corn and soybeans, under the current low-carbon biofuel obligations. This paper employs GARCH(1,1) and ARJI models to estimate the one-day-ahead Value-at-Risk (VaR) of the optimal dynamic biofuel portfolio, which consists of crude oil, corn and soybeans. The optimal blended standard is subject to the dual limitations of minimum production costs and the lowest biofuel using requirements. Our empirical findings confirm that the ARJI model is more suitable than the GARCH (1,1) model and further captures the discontinuous jump behavior from the in-the-sample data. The results of out-of-sample forecasts also are represented that our models play important roles in VaR estimation and risk management for biofuel portfolio. We therefore suggest that the petroleum companies should simultaneously pay attention to jump risk in hedging material costs in the prices of energy-related crops. © 2010 Elsevier B.V.}}, 
pages = {264--272}, 
number = {2}, 
volume = {33}
}
@article{10.1016/j.jeconom.2010.03.041, 
year = {2010}, 
title = {{Bayesian non-parametric signal extraction for Gaussian time series}}, 
author = {Macaro, Christian}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2010.03.041}, 
abstract = {{We consider the problem of unobserved components in time series from a Bayesian non-parametric perspective. The identification conditions are treated as unknown and analyzed in a probabilistic framework. In particular, informative prior distributions force the spectral decomposition to be in an identifiable region. Then, the likelihood function adapts the prior decompositions to the data. A full Bayesian analysis of unobserved components will be presented for financial high frequency data. Particularly, a three component model (long-term, intra-daily and short-term) will be analyzed to emphasize the importance and the potential of this work when dealing with the Value-at-Risk analysis. A second astronomical application will show how to deal with multiple periodicities. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {381--395}, 
number = {2}, 
volume = {157}
}
@article{10.1111/j.1477-9552.2005.00002.x, 
year = {2005}, 
title = {{Measuring price risk on UK arable farms}}, 
author = {White, Ben and Dawson, P. J.}, 
journal = {Journal of Agricultural Economics}, 
issn = {0021857X}, 
doi = {10.1111/j.1477-9552.2005.00002.x}, 
abstract = {{Price risk is estimated for a representative UK arable farm using value-at-risk (VaR). To determine the distribution of commodity returns, two multivariate generalised autoregressive conditional heteroscedasticity (GARCH) models, with t-distributed and normally distributed errors, and a RiskMetrics™ model are estimated. Returns show excess kurtosis and that the GARCH model with t-distributed errors fits best. Estimates of VaR differ between models: both GARCH models perform well but the RiskMetrics™ model underestimates expected losses. UK arable farms face substantial price risk. © Blackwell Publishing Ltd. 2005.}}, 
pages = {239--252}, 
number = {2}, 
volume = {56}
}
@article{10.1016/j.eneco.2011.03.005, 
year = {2011}, 
title = {{Oil and stock market volatility: A multivariate stochastic volatility perspective}}, 
author = {Vo, Minh}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2011.03.005}, 
abstract = {{This paper models the volatility of stock and oil futures markets using the multivariate stochastic volatility structure in an attempt to extract information intertwined in both markets for risk prediction. It offers four major findings. First, the stock and oil futures prices are inter-related. Their correlation follows a time-varying dynamic process and tends to increase when the markets are more volatile. Second, conditioned on the past information, the volatility in each market is very persistent, i.e., it varies in a predictable manner. Third, there is inter-market dependence in volatility. Innovations that hit either market can affect the volatility in the other market. In other words, conditioned on the persistence and the past volatility in their respective markets, the past volatility of the stock (oil futures) market also has predictive power over the future volatility of the oil futures (stock) market. Finally, the model produces more accurate Value-at-Risk estimates than other benchmarks commonly used in the financial industry. © 2011 Elsevier B.V.}}, 
pages = {956--965}, 
number = {5}, 
volume = {33}
}
@article{10.1016/j.amc.2017.04.034, 
year = {2017}, 
title = {{Optimal investment–consumption strategy with liability and regime switching model under Value-at-Risk constraint}}, 
author = {Hu, Fengxia and Wang, Rongming}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2017.04.034}, 
abstract = {{This paper considers the optimal investment–consumption problem with liability subject to a maximum Value-at-Risk (denoted by MVaR) constraint. The model also contains regime-switching market modes, whose states are interpreted as the states of the economy. In each state of economy state, we constrain a VaR value for the portfolio in a short time duration, and MVaR is defined as the maximum value of the VaRs in all economy states. We suppose that both the price dynamics of the risky asset and the liability value process are governed by a Markov-modulated geometric Brownian motion. With the objective of maximizing the discounted utility of consumption, we obtain a system of HJB equations corresponding to the economy states by using the dynamic programming principle. Then, by adopting the techniques of Chen et al. (2010), we get explicit expressions of value functions. Moreover, with the help of Lagrange multiplier method, we derive the optimal investment and the optimal consumption. Finally, a numerical example is investigated, and the effects of many parameters on the optimal investment, on the optimal consumption and on VaR value are studied. Furthermore, we also explore how VaR value affects the optimal investment and the optimal consumption. © 2017 Elsevier Inc.}}, 
pages = {103--118}, 
number = {NA}, 
volume = {313}
}
@article{10.1007/s11766-006-0001-8, 
year = {2006}, 
title = {{Study on the interrelation of efficient portfolios and their frontier under t distribution and various risk measures}}, 
author = {Wang, Yi and Chen, Zhiping and Zhang, Kecun}, 
journal = {Applied Mathematics-A Journal of Chinese Universities}, 
issn = {10051031}, 
doi = {10.1007/s11766-006-0001-8}, 
abstract = {{In order to study the effect of different risk measures on the efficient portfolios (frontier) while properly describing the characteristic of return distributions in the stock market, it is assumed in this paper that the joint return distribution of risky assets obeys the multivariate t-distribution. Under the mean-risk analysis framework, the interrelationship of efficient portfolios (frontier) based on risk measures such as variance, value at risk (VaR), and expected shortfall (ES) is analyzed and compared. It is proved that, when there is no riskless asset in the market, the efficient frontier under VaR or ES is a subset of the mean-variance (MV) efficient frontier, and the efficient portfolios under VaR or ES are also MV efficient; when there exists a riskless asset in the market, a portfolio is MV efficient if and only if it is a VaR or ES efficient portfolio. The obtained results generalize relevant conclusions about investment theory, and can better guide investors to make their investment decision. © 2006, Springer Verlag. All rights reserved.}}, 
pages = {369--382}, 
number = {4}, 
volume = {21}
}
@article{10.1016/j.ijforecast.2012.09.001, 
year = {2013}, 
title = {{On downside risk predictability through liquidity and trading activity: A dynamic quantile approach}}, 
author = {Rubia, Antonio and Sanchis-Marco, Lidia}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2012.09.001}, 
abstract = {{Most downside risk models implicitly assume that returns are a sufficient statistic with which to forecast the daily conditional distribution of a portfolio. In this paper, we analyze whether the variables that proxy for market-wide liquidity and trading conditions convey valid information for forecasting the quantiles of the conditional distribution of several representative market portfolios, including volume- and value-weighted market portfolios, and several Book-to-Market- and Size-sorted portfolios. Using dynamic quantile regression techniques, we report evidence of conditional tail predictability in terms of these variables. A comprehensive backtesting analysis shows that this link can be exploited in dynamic quantile modelling, in order to considerably improve the performances of day-ahead Value at Risk forecasts. © 2012 International Institute of Forecasters.}}, 
pages = {202--219}, 
number = {1}, 
volume = {29}
}
@article{10.1002/cpe.1790, 
year = {2012}, 
title = {{Accelerating Value-at-Risk estimation on highly parallel architectures}}, 
author = {Dixon, M. F. and Chong, J. and Keutzer, K.}, 
journal = {Concurrency and Computation: Practice and Experience}, 
issn = {15320626}, 
doi = {10.1002/cpe.1790}, 
abstract = {{Values of portfolios in modern financial markets may change precipitously with changing market conditions. The utility of financial risk management tools is dependent on whether they can estimate Value-at-Risk (VaR) of portfolios on-demand when key decisions need to be made. However, VaR estimation of portfolios uses the Monte Carlo method, which is a computationally intensive method often run as an overnight batch job. With the proliferation of highly parallel computing platforms such as multicore CPUs and manycore graphics processing units (GPUs), teraFLOPS of computation capability is now available on a desktop computer, enabling the VaR of large portfolios with thousands of risk factors to be computed within only a fraction of a second. Achieving such performance in practice requires the assimilation of expertise in the following three areas: (i) application domain; (ii) statistical analytics; and (iii) parallel computing. This paper demonstrates that these areas of expertise inform optimization perspectives that, when combined, lead to 127×speedup on our CPU-based implementation and 538×speedup on our GPU-based implementation. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {895--907}, 
number = {8}, 
volume = {24}
}
@article{10.1007/s10614-021-10164-z, 
year = {2021}, 
title = {{Estimation of Expected Shortfall Using Quantile Regression: A Comparison Study}}, 
author = {Christou, Eliana and Grabchak, Michael}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-021-10164-z}, 
abstract = {{Expected Shortfall (ES) is one of the most heavily used measures of financial risk. It is defined as a scaled integral of the quantile of the profit-and-loss distribution up to a certainly confidence level. As such, quantile regression (QR) and the closely related expectile regression (ER) methods are natural techniques for estimating ES. In this paper, we survey QR and ER based estimators of ES and introduce several novel variants. We compare the performance of these methods through simulation and through a data analysis based on four major US market indices: the S\&P 500 Index, the Russell 2000 Index, the Dow Jones Industrial Average, and the NASDAQ Composite Index. Our results suggest that QR and ER methods often work better than other, more standard, approaches. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1--29}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.csda.2014.07.011, 
year = {2016}, 
title = {{Managing risk with a realized copula parameter}}, 
author = {Fengler, Matthias R. and Okhrin, Ostap}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2014.07.011}, 
abstract = {{A dynamic copula model is introduced, in which the copula structure is inferred from the realized covariance matrix estimated from within-day high-frequency data. The estimation is carried out in a method-of-moments fashion using Hoeffding's lemma. Applying this procedure day by day gives rise to a time series of daily copula parameters which can be approximated by an autoregressive time series model. This allows one to capture time-varying dependence. In an application to portfolio risk-management, it is found that this time-varying realized copula model exhibits very good forecasting properties for the one-day ahead value at risk. © 2014 Elsevier B.V.}}, 
pages = {131--152}, 
number = {NA}, 
volume = {100}
}
@article{10.1016/j.jfs.2011.11.003, 
year = {2012}, 
title = {{Value-at-Risk models and Basel capital charges. Evidence from Emerging and Frontier stock markets}}, 
author = {Rossignolo, Adrian F. and Fethi, Meryem Duygun and Shaban, Mohamed}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2011.11.003}, 
abstract = {{In the wake of the subprime crisis of 2007 which uncovered shortfalls in capital levels of most financial institutions, the Basel Committee planned to strengthen current regulations contained in Basel II. While maintaining the Internal Model Approach based on Value-at-Risk, a stressed VaR calculated over highly strung periods is to be added to present directives to constitute Minimum Capital Requirements. Consequently, the adoption of the appropriate VaR specification remains a subject of paramount importance as it determines the financial condition of the firm. In this article I explore the performance of several models to compute MCR in the context of Emerging and Frontier stock markets within the present and proposed capital structures. Considering the evidence gathered, two major contributions arise: (a) heavy-tailed distributions - particularly Extreme Value (EV) ones-, reveal as the most accurate technique to model market risks, hence preventing huge capital deficits under current measures; (b) the application of such methods could allow slight modifications to present mandate and simultaneously avoid sVaR or at least reduce its scope, thus mitigating the impact regarding the enhancement of capital base. Therefore, I suggest that the inclusion of EV in planned supervisory accords should reduce development costs and foster healthier financial structures. © 2011 Elsevier B.V.}}, 
pages = {303--319}, 
number = {4}, 
volume = {8}
}
@article{10.1016/j.enpol.2009.07.064, 
year = {2009}, 
title = {{Complementarity of hydro and wind power: Improving the risk profile of energy inflows}}, 
author = {Denault, Michel and Dupuis, Debbie and Couture-Cardinal, Sébastien}, 
journal = {Energy Policy}, 
issn = {03014215}, 
doi = {10.1016/j.enpol.2009.07.064}, 
abstract = {{The complementarity of two renewable energy sources, namely hydro and wind, is investigated. We consider the diversification effect of wind power to reduce the risk of water inflow shortages, an important energy security concern for hydropower-based economic zones (e.g. Québec and Norway). Our risk measure is based on the probability of a production deficit, in a manner akin to the value-at-risk, simulation analysis of financial portfolios. We examine whether the risk level of a mixed hydro-and-wind portfolio of generating assets improves on the risk of an all-hydro portfolio, by relaxing the dependence on water inflows and attenuating the impact of droughts. Copulas are used to model the dependence between the two sources of energy. The data considered, over the period 1958-2003, are for the province of Québec, which possesses large hydro and wind resources. Our results indicate that for all scenarios considered, any proportion of wind up to 30\% improves the production deficit risk profile of an all-hydro system. We can also estimate the value, in TW h, of any additional one percent of wind in the portfolio. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {5376--5384}, 
number = {12}, 
volume = {37}
}
@article{10.1016/j.physa.2004.06.031, 
year = {2004}, 
title = {{Value-at-risk and Tsallis statistics: Risk analysis of the aerospace sector}}, 
author = {Mattedi, Adriana P. and Ramos, Fernando M. and Rosa, Reinaldo R. and Mantegna, Rosario N.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2004.06.031}, 
abstract = {{In this study, we analyze the aerospace stocks prices in order to characterize the sector behavior. The data analyzed cover the period from January 1987 to April 1999. We present a new index for the aerospace sector and investigate the statistical characteristics of this index. Our results show that this index is well described by Tsallis distribution. We explore this result and modify the standard value-at-risk (VaR) financial risk assessment methodology in order to reflect an asset which obeys Tsallis non-extensive statistics. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {554--561}, 
number = {3-4 SPEC. ISS.}, 
volume = {344}
}
@article{10.1205/fbp06002, 
year = {2007}, 
title = {{Capacity investment planning for multiple vaccines under uncertainty: 2: Financial risk analysis}}, 
author = {Tsang, K.H. and Samsatli, N.J. and Shah, N.}, 
journal = {Food and Bioproducts Processing}, 
issn = {09603085}, 
doi = {10.1205/fbp06002}, 
abstract = {{Capacity investment planning is a major decision for a vaccine company. Traditionally, due to the inherent flexibility used in almost all vaccine processes and risk-averse decisions, companies always started with limited capacities, thereby reducing the initial capital investment. However, in order to fulfil fast-growing vaccine demands, good and balanced financial risk management for capacity expansion is required to satisfy future demand without over committing capital. To complement the use of financial risk management, known probabilistic definitions of some classical risk measures such as expected downside risk (EDR), opportunity value (OV), value-at-risk (VaR) and conditional value-at-risk (CVaR) are adapted to be used in a scenario-based model for capacity investment planning for manufacture of multiple vaccines. Using these definitions, new models that manage financial risks and aid decisions are developed. Computational results and decision-making analysis methods are also presented and discussed. Numerical results show that this approach enables one to consider and manage the financial risk associated with the different designdjjf\textbackslash options, resulting in a set of solutions that can be used for decision-making. © 2007 Institution of Chemical Engineers.}}, 
pages = {129--140}, 
number = {2 C}, 
volume = {85}
}
@article{10.1002/9781118650318.ch15, 
year = {2016}, 
title = {{Extreme Value Theory and Risk Management in Electricity Markets}}, 
author = {Chan, Kam Fong and Gray, Philip}, 
issn = {NA}, 
doi = {10.1002/9781118650318.ch15}, 
abstract = {{This chapter explores the relative merits of a number of alternate approaches to estimating Value at Risk (VaR) for electricity markets. The distinctive features of electricity markets present non-trivial challenges for the trading and hedging activities of market participants. Compared to traditional approaches to forecasting VaR, the empirical findings provide strong support for the use of Extreme Value Theory (EVT). However, more sophisticated conditional EVT approaches do not necessarily outperform vanilla EVT approaches. Furthermore, the left tail of the return distribution proves particularly challenging to model. Given the idiosyncrasies of each electricity market, it is unlikely that a single approach is optimal across the board. © 2017 by John Wiley \& Sons, Inc. All rights reserved.}}, 
pages = {405--425}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/fuzzy.2011.6007314, 
year = {2011}, 
title = {{Building a fuzzy multi-objective portfolio selection model with distinct risk measurements}}, 
author = {Li, You and Wang, Bo and Watada, Junzo}, 
journal = {2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)}, 
issn = {10987584}, 
doi = {10.1109/fuzzy.2011.6007314}, 
abstract = {{Based on portfolio selection theory, this study proposes an improved fuzzy multi-objective model that can evaluate the invest risk exactly and increase the probability of obtaining the expected return. In building the model, fuzzy Value-at-Risk (VaR) is used to evaluate the exact future risk, in term of loss. The VaR can directly reflect the greatest loss of a selection case under a given confidence level. On the other hand, variance is utilized to make the selection more stable. This model can provide investors with more significant information in decision-making. To better solve this model, an improved particle swarm optimization algorithm is designed to mitigate the conventional local convergence problem. Finally, the proposed model and algorithm are exemplified by some numerical examples. Experiment results show that the model and algorithm are effective in solving the multi-objective portfolio selection problem. © 2011 IEEE.}}, 
pages = {1096--1102}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10687-008-0071-5, 
year = {2009}, 
title = {{Multivariate extremes and the aggregation of dependent risks: Examples and counter-examples}}, 
author = {Embrechts, Paul and Lambrigger, Dominik D. and Wüthrich, Mario V.}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-008-0071-5}, 
abstract = {{Properties of risk measures for extreme risks have become an important topic of research. In the present paper we discuss sub- and superadditivity of quantile based risk measures and show how multivariate extreme value theory yields the ideal modeling environment. Numerous examples and counter-examples highlight the applicability of the main results obtained. © 2008 Springer Science+Business Media, LLC.}}, 
pages = {107--127}, 
number = {2}, 
volume = {12}
}
@article{10.3390/en13051179, 
year = {2020}, 
title = {{Can one reinforce investments in renewable energy stock indices with the ESG index?}}, 
author = {Liu, Guizhou and Hamori, Shigeyuki}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13051179}, 
abstract = {{Studies on the environmental, social, and governance (ESG) index have become increasingly important since the ESG index offers attractive characteristics, such as environmental friendliness. Scholars and institutional investors are evaluating if investment in the ESG index can positively change current portfolios. It is crucial that institutional investors seek related assets to diversify their investments when such investors create funds in the renewable energy sector, which is highly related to environmental issues. The ESG index has proven to be a good investment choice, but we are not aware of its performance when combined with renewable energy securities. To uncover this nature, we investigate the dependence structure of the ESG index and four renewable energy indices with constant and time‐varying copula models and evaluate the potential performance of using different ratios of the ESG index in the portfolio. Criteria such as risk‐adjusted return, standard deviation, and conditional value‐at‐risk (CVaR) show that the ESG index can provide satisfactory results in lowering the potential CVaR and maintaining a high return. A goodness‐of‐fit test is then used to ensure the results obtained from the copula models. © 2020 by the authors.}}, 
pages = {1179}, 
number = {5}, 
volume = {13}
}
@article{10.1198/073500105000000018, 
year = {2005}, 
title = {{Evaluation and combination of conditional quantile forecasts}}, 
author = {Giacomini, Raffaella and Komunjer, Ivana}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/073500105000000018}, 
abstract = {{We propose an encompassing test for comparing conditional quantile forecasts in an out-of-sample frame-work. Our test provides a basis for forecast combination when encompassing is rejected. Its central features are (1) use of the "tick" loss function, (2) a conditional approach to out-of-sample evaluation, and (3) derivation in an environment with asymptotically nonvanishing estimation uncertainty. Our approach is valid under general conditions; the forecasts can be based on nested or nonnested models and can be obtained by general estimation procedures. We illustrate the test properties in a Monte Carlo experiment and apply it to evaluate and compare four popular value-at-risk models. © 2005 American Statistical Association.}}, 
pages = {416--431}, 
number = {4}, 
volume = {23}
}
@article{10.1081/sac-120023877, 
year = {2003}, 
title = {{Nonparametric Estimation for Risk in Value-at-Risk Estimator}}, 
author = {Chang, Yi-Ping and Hung, Ming-Chin and Wu, Yi-Fang}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1081/sac-120023877}, 
abstract = {{Value-at-Risk (VaR) has become the standard tool used by many financial institutions to measure market risk. However, the performance of a VaR estimator may be affected by sample variation or estimation risk caused from heavy-tailed distributions. After surveying several existing procedures proposed by Jorin (Jorion, P. (1996). Risk2 - Measuring the risk in value at risk. Financial Analysis Journal 52:47-56), Huschens (Huschens, S. (1997). Confidence intervals for the value-at-risk. In: Bol. G., Nakhaeizadeh, G., Vollmer, K. H., eds. Risk Measurement, Econometrics and Neural Networks. Heidelberg: Physica-Verlag, pp. 233-244), and Ridder (Ridder, T. (1997). Basics of statistical VaR-estimation. In: Bol, G., Nakhaeizadeh, G., Vollmer, K. H., eds. Risk Measurement, Econometrics and Neural Networks. Heidelberg: Physica-Verlag, pp. 161-187) etc., this article strives to propose several new estimators in measuring the risk involved in VaR estimation. We compare the performance of these VaR models through Monte Carlo simulation studies. We find that the newly proposed methods provide better accuracy and robustness in the estimation of the risk in VaR estimator.}}, 
pages = {1041--1064}, 
number = {4}, 
volume = {32}
}
@article{10.1109/icassp39728.2021.9413935, 
year = {2021}, 
title = {{Bayesian estimation of a tail-index with marginalized threshold}}, 
author = {Johnston, Douglas E. and Djurić, Petar M.}, 
journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
issn = {15206149}, 
doi = {10.1109/icassp39728.2021.9413935}, 
abstract = {{In this paper, we develop a new method for estimating the tail-index found in extreme value statistics. Using a fixed quantile, model-selection approach, we derive the posterior distribution of the tail-index marginalizing out the unknown threshold and nuisance parameters. Our marginalized threshold method relies on a spliced likelihood density for the bulk and extreme tail of the underlying distribution where the switch-point is specified as a fixed quantile. We derive a closed form expression for the posterior of the tail-index and illustrate its application to quantile, or value-at-risk, estimation. Our simulation results show that the marginalized threshold outperforms the maximum likelihood method, or the Hill estimate, for both tail-index and quantile estimation. We also illustrate our method using returns for the S\&P 500 stock market index from 1928 - 2020. © 2021 IEEE}}, 
pages = {5569--5573}, 
number = {NA}, 
volume = {2021-June}
}
@article{10.1109/iccsmt51754.2020.00068, 
year = {2020}, 
title = {{A Semi-Parametric Method to Estimate VaR with EVT and ES}}, 
author = {xiangxian, Zhang}, 
journal = {2020 International Conference on Computer Science and Management Technology (ICCSMT)}, 
issn = {NA}, 
doi = {10.1109/iccsmt51754.2020.00068}, 
abstract = {{Value-at-Risk (VaR) is probably the most widely used risk measure in financial institutions, which is a quantile of the loss distribution in nature. In this article, a semi-parametric method which is based on the Extreme Value Theory(EVT) and the Historical Simulation(HS) is proposed to estimate VaR. We use EVT to estimate the tails and HS to simulate the interior. An empirical study using the daily returns of SP 500 index is also presented. It's shown that that our approach is an adequate risk measure. © 2020 IEEE.}}, 
pages = {301--304}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.econmod.2012.08.033, 
year = {2013}, 
title = {{Dependence of defaults and recoveries in structural credit risk models}}, 
author = {Schäfer, Rudi and Koivusalo, Alexander F.R.}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2012.08.033}, 
eprint = {1102.3150}, 
abstract = {{The current research on credit risk is primarily focused on modelling default probabilities. Recovery rates are often treated as an afterthought; they are modelled independently, in many cases they are even assumed to be constant. This despite their pronounced effect on the tail of the loss distribution. Here, we take a step back, historically, and start again from the Merton model, where defaults and recoveries are both determined by an underlying process. Hence, they are intrinsically connected. For the diffusion process, we can derive the functional relation between expected recovery rate and default probability. This relation depends on a single parameter only. In Monte Carlo simulations we find that the same functional dependence also holds for jump-diffusion and GARCH processes. We discuss how to incorporate this structural recovery rate into reduced-form models, in order to restore essential structural information which is usually neglected in the reduced-form approach. © 2012 Elsevier B.V.}}, 
pages = {1--9}, 
number = {1}, 
volume = {30}
}
@article{10.1109/mlsd.2019.8911097, 
year = {2019}, 
title = {{Risk management in hierarchical systems}}, 
author = {Gorelov, M.A.}, 
journal = {2019 Twelfth International Conference "Management of large-scale system development" (MLSD)}, 
issn = {155166}, 
doi = {10.1109/mlsd.2019.8911097}, 
abstract = {{Two hierarchical games with random factors are studied, in which the top-level player, besides choosing his control, also chooses an acceptable degree of risk. The definition of the maximum guaranteed result under the assumption of the benevolence of the lower-level player is given and analyzed. Three substantive interpretations of the studied models are proposed. © 2019 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/ical.2010.5585369, 
year = {2010}, 
title = {{The risk measures based on GARCH model in tanker shipping market}}, 
author = {Ma, Jinlin}, 
journal = {2010 IEEE International Conference on Automation and Logistics}, 
issn = {NA}, 
doi = {10.1109/ical.2010.5585369}, 
abstract = {{The purpose of this paper is to investigate the risk measures based on general autoregress conditional heterostedasticity (GARCH) model in tanker shipping industry, to choose Baltic Dirty Tanker Index as study object. This paper applies Value-at-Risk model, widespread used in financial field, for measuring risks in shipping market. The parameters of the model are estimated by statistical software package Eviews. According to analyze the economic insignificant of parameters in the model, and get the conclusions that the freight return possesses persistence, and the VaR model is valid on 99\% confidence level. © 2010 IEEE.}}, 
pages = {685--689}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jmva.2005.04.001, 
year = {2006}, 
title = {{Bounds for functions of multivariate risks}}, 
author = {Embrechts, Paul and Puccetti, Giovanni}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2005.04.001}, 
abstract = {{Li et al. [Distributions with Fixed Marginals and Related Topics, vol. 28, Institute of Mathematics and Statistics, Hayward, CA, 1996, pp. 198-212] provide bounds on the distribution and on the tail for functions of dependent random vectors having fixed multivariate marginals. In this paper, we correct a result stated in the above article and we give improved bounds in the case of the sum of identically distributed random vectors. Moreover, we provide the dependence structures meeting the bounds when the fixed marginals are uniformly distributed on the k-dimensional hypercube. Finally, a definition of a multivariate risk measure is given along with actuarial/ financial applications. © 2005 Elsevier Inc. All rights reserved.}}, 
pages = {526--547}, 
number = {2}, 
volume = {97}
}
@article{10.1007/978-3-642-22589-5_6, 
year = {2011}, 
title = {{A dynamic value-at-risk portfolio model}}, 
author = {Yoshida, Yuji}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-22589-5\_6}, 
abstract = {{A mathematical dynamic portfolio allocation model with uncertainty is discussed. Introducing a value-at-risk under a condition, this paper formulates value-at-risks in a dynamic stochastic environment. By dynamic programming approach, an optimality condition of the optimal portfolio for dynamic value-at-risks is derived. It is shown that the optimal time-average value-at-risk is a solution of the optimality equation under a reasonable assumption, and an optimal trading strategy is obtained from the equation. A numerical example is given to illustrate our idea. © 2011 Springer-Verlag Berlin Heidelberg.}}, 
pages = {43--54}, 
number = {NA}, 
volume = {6820 LNAI}
}
@article{10.1002/ijfe.2009, 
year = {2021}, 
title = {{Backtesting expected shortfall for world stock index ETFs with extreme value theory and Gram–Charlier mixtures}}, 
author = {Molina‐Muñoz, Enrique and Mora‐Valencia, Andrés and Perote, Javier}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.2009}, 
abstract = {{This paper analyses risk quantification for three main stock market index exchange-traded funds in world financial markets. We compare the relative performance of a set of parametric and semi-nonparametric models in terms of both value-at-risk and expected shortfall backtesting techniques. To this end, we explore the result of the jointly elicitability of these two risk measures. We provide a new mixture of Gram–Charlier distributions that have been used in this framework for the first time and derive a close formula for directly computing expected shortfall. This model is compared to the Gaussian (benchmark model), Student's t, generalized Pareto (a case of the extreme value theory) and mixtures of Gram–Charlier distributions. The results show that peaks-over-threshold (extreme value theory) and flexible Gram–Charlier approximations are suitable to quantify market risk and mitigate concerns about possible financial instabilities generated by misuse of exchange-traded funds trading. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {4163--4189}, 
number = {3}, 
volume = {26}
}
@article{10.1016/j.ejor.2018.10.003, 
year = {2019}, 
title = {{Reverse sensitivity testing: What does it take to break the model?}}, 
author = {Pesenti, Silvana M. and Millossovich, Pietro and Tsanakas, Andreas}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2018.10.003}, 
abstract = {{Sensitivity analysis is an important component of model building, interpretation and validation. A model comprises a vector of random input factors, an aggregation function mapping input factors to a random output, and a (baseline) probability measure. A risk measure, such as Value-at-Risk and Expected Shortfall, maps the distribution of the output to the real line. As is common in risk management, the value of the risk measure applied to the output is a decision variable. Therefore, it is of interest to associate a critical increase in the risk measure to specific input factors. We propose a global and model-independent framework, termed ‘reverse sensitivity testing’, comprising three steps: (a) an output stress is specified, corresponding to an increase in the risk measure(s); (b) a (stressed) probability measure is derived, minimising the Kullback–Leibler divergence with respect to the baseline probability, under constraints generated by the output stress; (c) changes in the distributions of input factors are evaluated. We argue that a substantial change in the distribution of an input factor corresponds to high sensitivity to that input and introduce a novel sensitivity measure to formalise this insight. Implementation of reverse sensitivity testing in a Monte Carlo setting can be performed on a single set of input/output scenarios, simulated under the baseline model. Thus the approach circumvents the need for additional computationally expensive evaluations of the aggregation function. We illustrate the proposed approach through numerical examples with a simple insurance portfolio and a model of a London Insurance Market portfolio used in industry. © 2018 Elsevier B.V.}}, 
pages = {654--670}, 
number = {2}, 
volume = {274}
}
@article{10.1080/13657305.2017.1262475, 
year = {2017}, 
title = {{A study on price volatility in the aquaculture market using value-at-Risk (VaR)}}, 
author = {Dahl, Roy Endré}, 
journal = {Aquaculture Economics \& Management}, 
issn = {13657305}, 
doi = {10.1080/13657305.2017.1262475}, 
abstract = {{Aquaculture production is increasingly important in seafood production and is currently the fastest growing food production technology worldwide. To continue the industry growth, price risk management is an essential part in creating successful projects and sustainable business operations. This research uses trade data on fish and invertebrate prices, and evaluates the relative price uncertainty from farmed and captured products. Moreover, we consider a set of species groups (fish, invertebrates), species’ families (e.g., Salmonidae, white fish and crustaceans), and individual species (e.g., salmon, seabream and prawns), and evaluate their price volatility over time using Value-at-Risk (VaR). The results indicate that the introduction of aquaculture production to a species’ total supply will reduce price volatility. Consequently, incentives and regulations should support research and development of new aquaculture species. © 2017 Taylor \& Francis.}}, 
pages = {1--19}, 
number = {1}, 
volume = {21}
}
@article{10.1007/s12197-021-09558-4, 
year = {2021}, 
title = {{Fuelling fire sales? Prudential regulation and crises: evidence from the Italian market}}, 
author = {Leardi, Alessandro}, 
journal = {Journal of Economics and Finance}, 
issn = {10550925}, 
doi = {10.1007/s12197-021-09558-4}, 
abstract = {{The ECB announcement to reduce capital requirements for market risk to smooth pro-cyclicality, published in April 2020, is a good starting point to discuss the impact of regulation on individual banks on the stability of the whole banking and financial system. A large number of theoretical articles and a few empirical papers back the existence of an amplification effect on market volatility caused by the use of risk management measures (e.g. value-at-risk, VaR, for market risk) for regulatory purposes. However, to the best of my knowledge, no paper has empirically investigated the direct relation between the level of tightness of VaR risk limits and market volatility. In this article, I show that market volatility is positively related to past values of the measure of the tightness of the market risk limit, with an overshooting process of adjustment toward equilibrium. The analysis is limited to Italy. The empirical results, based on a unique dataset of VaR values and on other publicly available market data, are in line with the theoretical findings and are novel empirical evidence. They open the way to additional research on how to manage the channels of transmission of the amplification and overshooting effects from the risk management measure to systemic variables, to avoid unintended consequences of the application of individual supervision measures on the whole system. © 2021, Academy of Economics and Finance.}}, 
pages = {1--24}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2021.08.010, 
year = {2021}, 
title = {{Enhancing an insurer's expected value by reinsurance and external financing}}, 
author = {Chi, Yichun and Liu, Fangda}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2021.08.010}, 
abstract = {{In this paper, we analyze a decision-making problem for an insurer with limited liability, who is subject to a solvency constraint and wants to maximize the expected value through reinsurance purchase and external financing. We impose mild conditions on the reinsurance premium principle, which are the axioms of law invariance, risk loading and preserving the convex order. These three axioms are satisfied by all the widely used premium principles, except the Esscher principle, listed in Young (2004). If value at risk (VaR) or conditional value at risk (CVaR) is adopted to calculate the insurer's regulatory capital, we show that the optimal reinsurance can be in the form of two layers. The optimal reinsurance form is reduced to one layer once the premium principle further satisfies a weak condition. In particular, under Wang's, expected value, variance and Dutch premium principles, we derive the insurer's optimal strategies of reinsurance and financing, which are obtained explicitly under the VaR risk measure but have to be solved numerically for the CVaR risk measure. Results indicate that the insurer has three types of optimal strategies: the reinsurance only strategy, external financing only strategy, and mixed strategy. Moreover, the insurer's attitude toward ceding the risk through reinsurance and increasing capital through external financing is often greatly affected by the regulatory regime, the financing cost and the reinsurance price. © 2021 Elsevier B.V.}}, 
pages = {466--484}, 
number = {NA}, 
volume = {101}
}
@article{10.1016/j.econmod.2017.02.014, 
year = {2017}, 
title = {{Calculating Value-at-Risk for high-dimensional time series using a nonlinear random mapping model}}, 
author = {Zhang, Heng-Guo and Su, Chi-Wei and Song, Yan and Qiu, Shuqi and Xiao, Ran and Su, Fei}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2017.02.014}, 
abstract = {{In this study, we propose a non-linear random mapping model called GELM. The proposed model is based on a combination of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model and the Extreme Learning Machine (ELM), and can be used to calculate Value-at-Risk (VaR). Alternatively, the GELM model is a non-parametric GARCH-type model. Compared with conventional models, such as the GARCH models, ELM, and Support Vector Machine (SVM), the computational results confirm that the GELM model performs better in volatility forecasting and VaR calculation in terms of efficiency and accuracy. Thus, the GELM model can be an essential tool for risk management and stress testing. © 2017 Elsevier Ltd}}, 
pages = {355--367}, 
number = {NA}, 
volume = {67}
}
@article{10.1007/978-3-319-25826-3_18, 
year = {2015}, 
title = {{Dependence uncertainty for aggregate risk: Examples and simple bounds}}, 
author = {Embrechts, Paul and Jakobsons, Edgars}, 
issn = {NA}, 
doi = {10.1007/978-3-319-25826-3\_18}, 
abstract = {{Over the recent years, numerous results have been derived in order to assess the properties of regulatory risk measures (in particular VaR and ES) under dependence uncertainty. In this paper we complement this mainly methodological research by providing several numerical examples for both homogeneous as well as inhomogeneous portfolios. In particular, we investigate under which circumstances the so-called worst-case VaR can be well approximated by the worst-case (i.e. comonotonic) ES. We also study best-case values and simple lower bounds. © Springer International Publishing Switzerland 2016.}}, 
pages = {395--417}, 
number = {NA}, 
volume = {NA}
}
@article{10.1017/s174849952000010x, 
year = {2020}, 
title = {{Asymmetry in mortality volatility and its implications on index-based longevity hedging}}, 
author = {Zhou, Kenneth Q. and Li, Johnny Siu-Hang}, 
journal = {Annals of Actuarial Science}, 
issn = {17484995}, 
doi = {10.1017/s174849952000010x}, 
abstract = {{Mortality volatility is crucially important to many aspects of index-based longevity hedging, including instrument pricing, hedge calibration and hedge performance evaluation. This paper sets out to develop a deeper understanding of mortality volatility and its implications on index-based longevity hedging. First, we study the potential asymmetry in mortality volatility by considering a wide range of generalised autoregressive conditional heteroskedasticity (GARCH)-type models that permit the volatility of mortality improvement to respond differently to positive and negative mortality shocks. We then investigate how the asymmetry of mortality volatility may impact index-based longevity hedging solutions by developing an extended longevity Greeks framework, which encompasses longevity Greeks for a wider range of GARCH-type models, an improved version of longevity vega, and a new longevity Greek known as dynamic Delta. Our theoretical work is complemented by two real-data illustrations, the results of which suggest that the effectiveness of an index-based longevity hedge could be significantly impaired if the asymmetry in mortality volatility is not taken into account when the hedge is calibrated. ©}}, 
pages = {278--301}, 
number = {2}, 
volume = {14}
}
@article{10.1080/00036846.2020.1808182, 
year = {2021}, 
title = {{Unconditional density vs conditional density functions in estimating value-at-risk}}, 
author = {Chiu, Yen-Chen and Chuang, I-Yuan}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2020.1808182}, 
abstract = {{This research compares the performance of 10 Value-at-Risk (VaR) models, including pure density models (student’s t, mixture of normal, double gamma, kernel, and GPD) and conditional density models (GARCH-student’s t, GARCH-mixture of normal, GARCH-double gamma, GARCH-kernel, and GARCH-GPD). We employ these models and test their performance in Asian markets, including over the 2008 financial crisis period. Findings show that the density functions used in conjunction with the GARCH models are capable of capturing the high peak, fat-tails, and volatility clustering of returns and can improve the accuracy of VaR forecasts. Furthermore, the proposed GARCH-double gamma provides the best fit for all tested Asian markets, both developed and emerging markets, and for foreign exchange rates markets. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--13}, 
number = {4}, 
volume = {53}
}
@article{10.1002/qre.1238, 
year = {2011}, 
title = {{Estimating value at risk and conditional value at risk for count variables}}, 
author = {Göb, Rainer}, 
journal = {Quality and Reliability Engineering International}, 
issn = {07488017}, 
doi = {10.1002/qre.1238}, 
abstract = {{Risk management and risk measures like value at risk and conditional value at risk originated in the financial and insurance industries. In recent years, the interest in risk management and risk measurement has spread over all industrial sectors. Finance and insurance applications focused on continuous data like financial return, profit or loss. In other contexts, e.g. in operational risk management, discrete count data are of like importance. The present study analyses theoretical and empirical aspects of the risk measures value at risk, tail conditional expectation, conditional value at risk under the binomial, negative binomial, and Poisson distribution. Particular emphasis is on interval estimation of the risk measures. © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {659--672}, 
number = {5}, 
volume = {27}
}
@article{10.1108/03074351311323428, 
year = {2013}, 
title = {{VaR and time-varying volatility: a comparative study of three international portfolios}}, 
author = {Obi, Pat and Sil, Shomir}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/03074351311323428}, 
abstract = {{Purpose – This study aims to evaluate the market risk exposure of three international equity portfolios using value-at-risk (VaR). This risk metric calculates the worst case loss for a business in the course of its daily transactions. To ensure that the calculated VaR reflects emerging risk characteristics, this paper introduces an approach that incorporates time-varying volatility. Design/methodology/approach – This study uses the GARCH technique to calculate the volatility metric with which VaR estimates are obtained. The out-of-sample performance of the VaRs is then assessed by comparing them to the actual market risk losses in that period. Findings – Empirical results show that regardless of market conditions, the VaR calculated with this (GARCH) approach is more robust and more reliable than the traditional methods. Pursuant to the banking regulation on market risk capital stipulated by the Basel Committee on Banking Supervision, the out-of-sample VaRs are at least equal to actual daily market risk losses at the 99 percent confidence level. Practical implications – The key goal of banking regulation is to ensure that financial firms have sufficient capital for the types of risks they take. Determining the right amount of capital requires these firms to first estimate their worst case loss, which is the value-at-risk. The approach to the calculation of VaR introduced in this paper enhances the accuracy in the measurement of market risk capital for financial institutions. Originality/value – This paper recognizes that for VaR to fully account for market risk losses, the risk metric must be correctly measured. The unparalleled approach in this paper of incorporating time-varying volatility in VaR calculations offers banking institutions a more reliable means of determining their capital adequacy. © 2013, Emerald Group Publishing Limited.}}, 
pages = {625--640}, 
number = {7}, 
volume = {39}
}
@article{10.1108/00215040380001145, 
year = {2003}, 
title = {{Could the government manage its exposure to crop reinsurance risk?}}, 
author = {Hayes, Dermot J. and Lence, Sergio H. and Mason, Chuck}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00215040380001145}, 
abstract = {{This study estimates the probability density function of the government's net income from reinsuring crop insurance for corn, wheat, and soybeans. Based on 1997 data, it is estimated there is a 5\% probability that the government will need to reimburse at least \$1 billion to insurance companies, and that the fair value of the government's reinsurance services to insurance firms equals \$78.7 million. In addition, various hedging strategies are examined for their potential to reduce the government's reinsurance risk. The risk reduction achievable by hedging is appreciable, but use of derivative contracts alone is clearly no panacea. © 2003, MCB UP Limited}}, 
pages = {127--142}, 
number = {2}, 
volume = {63}
}
@article{10.1108/03074351311323446, 
year = {2013}, 
title = {{Extreme loss risk in financial turbulence – evidence from the global financial crisis}}, 
author = {Uppal, Jamshed Y. and Mangla, Inayat Ullah}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/03074351311323446}, 
abstract = {{Purpose – This study aims to examine the stock returns distributions in ten countries in the periods before and after the global financial crisis (GFC) to evaluate how well the empirical distributions conformed to the extreme value theory (EVT) which underlies a family of risk management models. Design/methodology/approach – The authors’ sample consists of the G5 countries and the five leading emerging economies. Parameters of the General Pareto Distribution (GPD) for each country are estimated for the pre- and the crisis period. The authors follow a two-step procedure: a GARCH(1,1) model is fitted to the historical return data by pseudo maximum likelihood method; Hill's GPD tail estimation procedure is employed on the residuals from the first step. Goodness-of-fit is evaluated for the empirical distributions. Findings – The authors find that the EVT explains the observed distributions well in both the pre-GFC and the GFC periods, with the important exceptions of the US and the UK markets in the crisis period. Moreover, the estimated distribution parameters are quite different for the two periods. The results underscore the inadequacy of the quantitative risk models in times of financial turbulence, and the need for prudential exercise of judgment in risk management. Originality/value – The global financial crisis (GFC) provides a unique and historical experiment to evaluate the models of tail distributions. Although the EVT provides a sound basis for modeling extreme risks, the study highlights the fundamental problem of dealing with uncertainty. © 2013, Emerald Group Publishing Limited.}}, 
pages = {653--666}, 
number = {7}, 
volume = {39}
}
@article{10.1007/s10479-018-2792-4, 
year = {2019}, 
title = {{Computation of the corrected Cornish–Fisher expansion using the response surface methodology: application to VaR and CVaR}}, 
author = {Amédée-Manesme, Charles-Olivier and Barthélémy, Fabrice and Maillard, Didier}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-018-2792-4}, 
abstract = {{The Cornish–Fisher expansion is a simple way to determine quantiles of non-normal distributions. It is frequently used by practitioners and by academics in risk management, portfolio allocation, and asset liability management. It allows us to consider non-normality and, thus, moments higher than the second moment, using a formula in which terms in higher-order moments appear explicitly. This paper has two primary objectives. First, we resolve the classic confusion between the skewness and kurtosis coefficients of the formula and the actual skewness and kurtosis of the distribution when using the Cornish–Fisher expansion. Second, we use the response surface approach to estimate a function for these two values. This helps to overcome the difficulties associated with using the Cornish–Fisher expansion correctly to compute value at risk. In particular, it allows a direct computation of the quantiles. Our methodology has many practical applications in risk management and asset allocation. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {423--453}, 
number = {1-2}, 
volume = {281}
}
@article{10.1504/ijgei.2011.045027, 
year = {2011}, 
title = {{Estimating the 'value at risk' of EUA futures prices based on the extreme value theory}}, 
author = {Mi, Zhi Fu and Zhang, Yue Jun}, 
journal = {International Journal of Global Energy Issues}, 
issn = {09547118}, 
doi = {10.1504/ijgei.2011.045027}, 
abstract = {{This paper employs the Extreme Value Theory (EVT) to measure the 'Value at Risk' (VaR) of EUA futures prices. The results show that during the sample period: first, the EVT approach can be used to reliably measure the extreme risk of carbon futures markets of the European Union Emissions Trading Scheme, both for Phase I and Phase II. Second, the downside extreme risk of carbon futures market outweighs the upside risk, with evident asymmetric features. Moreover, the average VaR of carbon futures contract DEC10 proves much less than that of contract DEC07 during the sample period. © 2011 Inderscience Enterprises Ltd.}}, 
pages = {145}, 
number = {2-4}, 
volume = {35}
}
@article{10.1080/1351847x.2015.1104370, 
year = {2017}, 
title = {{How robust is the value-at-risk of credit risk portfolios?}}, 
author = {Bernard, Carole and Rüschendorf, Ludger and Vanduffel, Steven and Yao, Jing}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2015.1104370}, 
abstract = {{In this paper, we assess the magnitude of model uncertainty of credit risk portfolio models, that is, what is the maximum and minimum value-at-risk (VaR) of a portfolio of risky loans that can be justified given a certain amount of available information. Puccetti and Rüschendorf [2012a. “Computation of Sharp Bounds on the Distribution of a Function of Dependent Risks”. Journal of Computational and Applied Maths 236, 1833–1840] and Embrechts, Puccetti, and Rüschendorf [2013. “Model Uncertainty and VaR Aggregation”. Journal of Banking and Finance 37, 2750–2764] propose the rearrangement algorithm (RA) as a general method to approximate VaR bounds when the loss distributions of the different loans are known but not their interdependence (unconstrained bounds). Their numerical results show that the gap between worst-case and best-case VaR is typically very high, a feature that can only be explained by lack of using dependence information. We propose a modification of the RA that makes it possible to approximate sharp VaR bounds when besides the marginal distributions also higher order moments of the aggregate portfolio such as variance and skewness are available as sources of dependence information. A numerical study shows that the use of moment information makes it possible to significantly improve the (unconstrained) VaR bounds. However, VaR assessments of credit portfolios that are performed at high confidence levels (as it is the case in Solvency II and Basel III) remain subject to significant model uncertainty and are not robust. © 2015 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--28}, 
number = {6}, 
volume = {23}
}
@article{10.1016/s0378-4266(99)00119-3, 
year = {2000}, 
title = {{A synthetic factor approach to the estimation of value-at-risk of a portfolio of interest rate swaps}}, 
author = {Niffikeer, Cindy I and Hewins, Robin D and Flavell, Richard B}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(99)00119-3}, 
abstract = {{In this paper we decompose the interest rate swap yield curves of 10 major currencies into their common factors and find that the first two factors, interpreted as parallel shift and rotation, explain between 97.1\% and 98.6\% of the variation in the interest rate swap rates across all 10 currencies. The main contribution of the paper however is that we then model these two factors as simplified synthetic factors so that they may be used to develop an innovative approach to the computation of Value-at-Risk (VaR) for a portfolio of interest rate swaps. © 2000 Elsevier Science B.V.}}, 
pages = {1903--1932}, 
number = {12}, 
volume = {24}
}
@article{10.1080/14697688.2018.1453166, 
year = {2018}, 
title = {{Forecasting market risk using ultra-high-frequency data and scaling laws}}, 
author = {Qi, Jun and Yi, Lan and Chen, Yiyun}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2018.1453166}, 
abstract = {{This paper develops a new multiple time scale-based empirical framework for market risk estimates and forecasts. Ultra-high frequency data are used in the empirical analysis to estimate the parameters of empirical scaling laws which gives a better understanding of the dynamic nature of the market. A comparison of the new approach with the popular Value-at-Risk and expected tail loss measures with respect to their risk forecasts during the crisis period in 2008 is presented. The empirical results show the outperformance of the new scaling law method which turns out to be more accurate and flexible due to the scale invariance. The scaling law method promotes the use of massive real data in developing risk measurement and forecasting models. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--15}, 
number = {12}, 
volume = {18}
}
@article{10.2298/yjor110308013j, 
year = {2012}, 
title = {{Quantile estimation for the generalized pareto distribution with application to finance}}, 
author = {Jocković, Jelena}, 
journal = {Yugoslav Journal of Operations Research}, 
issn = {03540243}, 
doi = {10.2298/yjor110308013j}, 
abstract = {{Generalized Pareto distributions (GPD) are widely used for modeling excesses over high thresholds (within the framework of the POT-approach to modeling extremes). The aim of the paper is to give the review of the classical techniques for estimating GPD quantiles, and to apply these methods in finance - to estimate the Valueat- Risk (VaR) parameter, and discuss certain difficulties related to this subject.}}, 
pages = {297--311}, 
number = {2}, 
volume = {22}
}
@article{10.1016/j.jbankfin.2009.02.003, 
year = {2009}, 
title = {{Are interest rate options important for the assessment of interest rate risk?}}, 
author = {Almeida, Caio and Vicente, José}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2009.02.003}, 
abstract = {{Fixed income options contain substantial information on the price of interest rate volatility risk. In this paper, we ask if those options will also provide information related to other moments of the objective distribution of interest rates. Based on dynamic term structure models within the class of affine models, we find that interest rate options are useful for the identification of interest rate quantiles. Two three-factor models are adopted and their adequacy to estimate Value at Risk of zero-coupon bonds is tested. We find significant difference on the quantitative assessment of risk when options are (or not) included in the estimation process of each of these dynamic models. Statistical backtests indicate that bond estimated risk is clearly more adequate when options are adopted, although not yet completely satisfactory. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {1376--1387}, 
number = {8}, 
volume = {33}
}
@article{10.1016/j.jempfin.2003.07.001, 
year = {2005}, 
title = {{The econometrics of efficient portfolios}}, 
author = {Gourieroux, C. and Monfort, A.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2003.07.001}, 
abstract = {{It is well known that the standard mean variance approach can be inappropriate when return distributions feature skewness, fat tails or multimodes. This is typically the situation for portfolios including derivatives. In this case, it can be necessary to come back to the basic expected utility approach. In this paper, an efficient portfolio maximizes the expected utility of future wealth. This paper presents an analysis of the efficiency frontier, formed by a set of efficient portfolios corresponding to a parameterized class of utility functions. First, we discuss the estimation of an efficient portfolio and introduce several tests of the efficiency hypothesis, depending on what is known about the utility function and the budget level. Next we analyse the shape of the frontier and develop a procedure for testing the separability of the efficiency frontier into K independent funds. The inference is semi-nonparametric because the return distribution is left unspecified. We illustrate our approach by an application to portfolios including derivatives. © 2005 Published by Elsevier B.V.}}, 
pages = {1--41}, 
number = {1}, 
volume = {12}
}
@article{10.1016/j.eneco.2017.06.015, 
year = {2017}, 
title = {{Forecasting the VaR of crude oil market: Do alternative distributions help?}}, 
author = {Lyu, Yongjian and Wang, Peng and Wei, Yu and Ke, Rui}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2017.06.015}, 
abstract = {{Accurate modeling of the empirical distribution of crude oil market returns is extremely important in estimating risk measures. In addition to several commonly used distributions, alternative distributions are explored in this study, some of which account for the asymmetry and heavy tails simultaneously found in distributions, and contain more tail parameters to separately depict the right and left tails when forecasting the Value-at-Risk (VaR) of crude oil markets during highly volatile periods. Seven backtests are also conducted to compare the VaR forecasting accuracy among different distributions. The empirical results indicate that a highly volatile environment challenges the commonly used distributions, and the four risk models based on commonly used distributions are rejected about 27\% to 38\% of the time. The alternative distributions, i.e., skewed general error distributions (SGED), generalized hyperbolic skewed Student-t distributions (GHST), and generalized asymmetric Student-t (GAST) distributions, generally produce more accurate VaR measurement, and GAST gives the best measurement accuracy. The empirical results imply that risk managers or policymakers should further consider more flexible distributions, such as SGED, GHST, or GAST in particular, when quantifying or managing the risk in turbulent market times. © 2017 Elsevier B.V.}}, 
pages = {523--534}, 
number = {NA}, 
volume = {66}
}
@article{10.1007/978-3-030-52348-0_7, 
year = {2020}, 
title = {{Application of hill estimator to assess extreme risks in the metals market}}, 
author = {Krężołek, Dominik}, 
journal = {Studies in Classification, Data Analysis, and Knowledge Organization}, 
issn = {14318814}, 
doi = {10.1007/978-3-030-52348-0\_7}, 
abstract = {{Rare phenomena are extremely important for risk assessment. The main reason is that their occurrence usually results in significant consequences. There are many methods for identifying such events. Some of these include extreme statistics, which are part of the extreme value theory. High-order quantiles allow to determine the level of risk for which the probability of a risky event occurring is negligible. In this paper, the issue of estimation of tail index of probability distribution using Hill estimator and its modification is presented. The results of comparative analysis for selected nonparametric and parametric models were compared. Empirical analysis was carried out on the example of assets from the base metals market: aluminium and copper. © Springer Nature Switzerland AG 2020.}}, 
pages = {103--113}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.amc.2020.125869, 
year = {2021}, 
title = {{Analysis of an aggregate loss model in a Markov renewal regime}}, 
author = {Ramírez-Cobo, Pepa and Carrizosa, Emilio and Lillo, Rosa E.}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2020.125869}, 
abstract = {{In this article we consider an aggregate loss model with dependent losses. The loss occurrence process is governed by a two-state Markovian arrival process (MAP2), a Markov renewal process that allows for (1) correlated inter-loss times, (2) non-exponentially distributed inter-loss times and, (3) overdisperse loss counts. Some quantities of interest to measure persistence in the loss occurrence process are obtained. Given a real OpRisk database, the aggregate loss model is estimated by fitting separately the inter-loss times and severities. The MAP2 is estimated via direct maximization of the likelihood function, and severities are modeled by the heavy-tailed, double-Pareto Lognormal distribution. In comparison with the fit provided by the Poisson process, the results point out that taking into account the dependence and overdispersion in the inter-loss times distribution leads to higher capital charges. © 2020}}, 
pages = {125869}, 
number = {NA}, 
volume = {396}
}
@article{10.1109/eem.2012.6254791, 
year = {2012}, 
title = {{Risk aversion approach for energy trading based on multistage stochastic programming}}, 
author = {Arfux, G. A. B. and Teive, R. C. G.}, 
journal = {2012 9th International Conference on the European Energy Market}, 
issn = {NA}, 
doi = {10.1109/eem.2012.6254791}, 
abstract = {{This paper presents a methodology to define the trading policy of an energy generation agent. The proposed approach uses stochastic programming to represent the uncertainty related to the short-term price fluctuation. The aim of the model (based on possible prices) is to identify the composition of an optimum portfolio in accordance with the decision-maker risk perception. The most important contribution of this paper is the fact that the risk criteria are considered in the decision-making model. This is done through the use of tools such as Value at Risk (VaR) and the Conditional Value at Risk (CVaR). Despite the fact that the calculation of CVaR requires previous knowledge of VaR, in the proposed model both risk measures are simultaneously determined. © 2012 IEEE.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ejor.2007.12.028, 
year = {2009}, 
title = {{Importance sampling for integrated market and credit portfolio models}}, 
author = {Grundke, Peter}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2007.12.028}, 
abstract = {{A sophisticated approach for computing the total economic capital needed for various stochastically dependent risk types is the bottom-up approach. In this approach, usually, market and credit risks of financial instruments are modeled simultaneously. As integrating market risk factors into standard credit portfolio models increases the computational burden of calculating risk measures, it is analyzed to which extent importance sampling techniques previously developed either for pure market portfolio models or for pure credit portfolio models can be successfully applied to integrated market and credit portfolio models. Specific problems which arise in this context are discussed. The effectiveness of these techniques is tested by numerical experiments for linear and non-linear portfolios. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {206--226}, 
number = {1}, 
volume = {194}
}
@article{10.5220/0007905706600665, 
year = {2019}, 
title = {{Real time financial risk monitoring as a data-intensive application}}, 
author = {Ristau, Petra and Krain, Lukas}, 
journal = {Proceedings of the 9th International Conference on Cloud Computing and Services Science}, 
issn = {NA}, 
doi = {10.5220/0007905706600665}, 
abstract = {{This paper will examine the possibility of real-time risk calculations within the financial services industry. Due to regulatory standards, this paper will focus mainly on the calculations of value-at-risk (VaR) and expected shortfall (ES). Their computation currently requires simplified theory in order to be done within real-time. This demonstrates a real-world disadvantage to investment professionals since they need to comply with regulatory requirements when doing real-time decisions without knowing the accurate risk numbers at any one time. Within the CloudDBAppliance project, we designed an architecture that shall make real-time risk monitoring possible using cloud computing and a fast analytical processing platform. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved}}, 
pages = {660--665}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.irfa.2021.101674, 
year = {2021}, 
title = {{Is Bitcoin a better portfolio diversifier than gold? A copula and sectoral analysis for China}}, 
author = {Pho, Kim Hung and Ly, Sel and Lu, Richard and Hoang, Thi Hong Van and Wong, Wing-Keung}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2021.101674}, 
abstract = {{This paper aims to compare Bitcoin with gold in the diversification of Chinese portfolios using daily data over the 2010–2020 period. We propose a new development of copula-based joint distribution function of returns to simulate the Value-at-Risk and expected shortfall of portfolios including Bitcoin (or gold) and those without it. The stochastic dominance method is also used to compare the return distributions of the three types of portfolios. Empirical results show that gold is a better portfolio diversifier than Bitcoin as it helps better reduce the risk of portfolios. On the other hand, Bitcoin better increases the return but also increases the risk. The stochastic dominance results further show that portfolios diversified by gold dominate those diversified by Bitcoin. Based on these findings, we conclude that in China, gold is a better portfolio diversifier than Bitcoin for risk-averse investors. However, for risk-seeking investors, Bitcoin can be a better choice. This result is found to be robust to the time, frequency and currency effects. © 2021 Elsevier Inc.}}, 
pages = {101674}, 
number = {NA}, 
volume = {74}
}
@article{10.1007/978-94-007-4839-2_2, 
year = {2012}, 
title = {{The value of information in a risk management approach to climate change}}, 
author = {Kousky, Carolyn and Cooke, Roger M.}, 
issn = {NA}, 
doi = {10.1007/978-94-007-4839-2\_2}, 
abstract = {{The standard economic approach to analyzing the climate change problem has been to search for efficient abatement policies. The massive uncertainties and the possibility for cataclysmic climate damages, however, suggest that a risk management approach is more appropriate. This shifts the policy question to how much risk of catastrophe society is willing to accept. Intuitively, this change in focus may shift our information needs, and the needs should be assessed through a value-of-information analysis. Such calculations should allow for examination of how improved information alters the probability of exceeding a given policy target, incorporate rigorous expert judgment for determining beliefs and quantifying uncertainties, and highlight what scientific information is most valuable for a policymaker attempting to keep the probability of catastrophic climate impacts below a set threshold. We discuss how Bayesian belief nets can be useful tools for this type of analysis. © Springer Science+Business Media Dordrecht 2012.}}, 
pages = {19--43}, 
number = {NA}, 
volume = {NA}
}
@article{10.4337/ejeep.2019.0040, 
year = {2020}, 
title = {{What do the value-at-risk measure and the respective legislative framework really offer to financial stability? Critical views and pro-cyclicality}}, 
author = {Vasileiou, Evangelos and Pantos, Themistoclis}, 
journal = {European Journal of Economics and Economic Policies}, 
issn = {20527764}, 
doi = {10.4337/ejeep.2019.0040}, 
abstract = {{In this paper, we examine how value at risk (VaR) contributes to the financial market’s stability. We apply the Guidelines on Risk Measurement and the Calculation of Global Exposure and Counterparty Risk for UCITS of the Committee of European Securities Regulators (CESR 2010) to the main indices of the 12 stock markets of the countries that have used the euro as their official currency since its initial circulation. We show that gaps in the legislative framework give incentives to investment funds to adopt conventional models for the VaR estimation in order to avoid the increased costs that the advanced models involve. For this reason, we apply the commonly used historical simulation VaR (HVaR) model, which is: (i) taught at most finance classes; (ii) widely applied in the financial industry; and (iii) accepted by CESR (2010). The empirical evidence shows the HVaR does not really contribute to financial stability, and the legislative framework does not offer the appropriate guidance. The HVaR model is not representative of the real financial risk, and does not give any signal for trends in the near future. The HVaR is absolutely backward-looking and this increases the stock market’s overreaction. The fact that the suggested confidence level in CESR (2010) is set at 99 percent leads to hidden pro-cyclicality. Scholars and researchers should focus on issues such as the abovementioned, otherwise the VaR estimations will become, sooner or later, just a formality, and such conventional statistical measures rarely contribute to financial stability. © 2020 The Author.}}, 
pages = {39--60}, 
number = {1}, 
volume = {17}
}
@article{10.1155/2016/9191360, 
year = {2016}, 
title = {{Research on a Risk Assessment Method considering Risk Association}}, 
author = {Zhang, Zhan and Li, Kai and Zhang, Lei}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2016/9191360}, 
abstract = {{Regarding risk assessment problems with multiple associated risks, a risk assessment method (RAM) is proposed in this paper. According to the risk-associated assessment information offered by expert panel, a comprehensive associated matrix is constructed to identify the influence relationship among risks so as to determine the hierarchical structure of risks. Then, based on the determined divided or undivided risk hierarchical structure as well as the possibility and loss of risks provided by expert panel, each value at risk (VAR) is calculated through knowledge related to probability theory. Finally, the feasibility and efficiency of the proposed method are demonstrated through a calculating case. © 2016 Zhan Zhang et al.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {2016}
}
@article{10.1016/j.jfs.2008.09.003, 
year = {2008}, 
title = {{Blame the models}}, 
author = {Daníelsson, Jón}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2008.09.003}, 
abstract = {{The quality of statistical risk models is much lower than often assumed. Such models are useful for measuring the risk of frequent small events, such as in internal risk management, but not for systemically important events. Unfortunately, it is common to see unrealistic demands placed on risk models. Having a number representing risk seems to be more important than having a number which is correct. Here, it is demonstrated that even in what may be the easiest and most reliable modeling exercise, value-at-risk forecasts from the most commonly used risk models provide very inconsistent results. © 2008.}}, 
pages = {321--328}, 
number = {4}, 
volume = {4}
}
@article{10.1007/s10687-014-0202-0, 
year = {2014}, 
title = {{Bounds on total economic capital: the DNB case study}}, 
author = {Aas, Kjersti and Puccetti, Giovanni}, 
journal = {Extremes}, 
issn = {13861999}, 
doi = {10.1007/s10687-014-0202-0}, 
abstract = {{Most banks use the top-down approach to aggregate their risk types when computing total economic capital. Following this approach, marginal distributions for each risk type are first independently estimated and then merged into a joint model using a copula function. Due to lack of reliable data, banks tend to manually select the copula as well as its parameters. In this paper we assess the model risk related to the choice of a specific copula function. The aim is to compute upper and lower bounds on the total economic capital for the aggregate loss distribution of DNB, the largest Norwegian bank, and the key tool for computing these bounds is the Rearrangement Algorithm introduced in Embrechts et al. (J. Bank. Financ. 37(8):2750–2764 2013). The application of this algorithm to a real situation poses a series of numerical challenges and raises a number of warnings which we illustrate and discuss. © 2014, Springer Science+Business Media New York.}}, 
pages = {693--715}, 
number = {4}, 
volume = {17}
}
@article{10.1111/j.1467-9965.2011.00482.x, 
year = {2012}, 
title = {{The expected shortfall of quadratic portfolios with heavy-tailed risk factors}}, 
author = {Broda, Simon A.}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/j.1467-9965.2011.00482.x}, 
abstract = {{Computable expressions are derived for the Expected Shortfall of portfolios whose value is a quadratic function of a number of risk factors, as arise from a Delta-Gamma-Theta approximation. The risk factors are assumed to follow an elliptical multivariatetdistribution, reflecting the heavy-tailed nature of asset returns. Both an exact expression and a uniform asymptotic expansion are presented. The former involves only a single rapidly convergent integral. The latter is essentially explicit, and numerical experiments suggest that its error is negligible compared to that incurred by the Delta-Gamma-Theta approximation. © 2011 Wiley Periodicals, Inc.}}, 
pages = {710--728}, 
number = {4}, 
volume = {22}
}
@article{10.1016/j.fss.2009.02.007, 
year = {2009}, 
title = {{An estimation model of value-at-risk portfolio under uncertainty}}, 
author = {Yoshida, Yuji}, 
journal = {Fuzzy Sets and Systems}, 
issn = {01650114}, 
doi = {10.1016/j.fss.2009.02.007}, 
abstract = {{A value-at-risk portfolio model under uncertainty is discussed. In the proposed model, randomness and fuzziness are evaluated, respectively, by the probabilistic expectation and the mean values with evaluation weights and λ-mean functions. The means, the variances and the measurements of imprecision for fuzzy numbers/fuzzy random variables are evaluated in the possibility case and the necessity case, and the rate of return in portfolio is estimated regarding the both random factors and imprecise factors. By analytical approach, we derive a solution of the value-at-risk portfolio problem. A numerical example is given to illustrate our idea. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {3250--3262}, 
number = {22}, 
volume = {160}
}
@article{10.1023/b:rast.0000028191.05396.0f, 
year = {2004}, 
title = {{Discussion of "how banks' value-at-risk disclosures predict their total and priced risk: Effects of bank technical sophistication and learning over time"}}, 
author = {Ke, Bin}, 
journal = {Review of Accounting Studies}, 
issn = {13806653}, 
doi = {10.1023/b:rast.0000028191.05396.0f}, 
abstract = {{Liu et al. (2004, this issue) show that technical sophistication and learning over time help improve the ability of bank trading portfolios' value-at-risk (VaR) disclosures to predict future trading income risk, and that trading VaRs predict bank-wide total risk and systematic risk. While the results suggest that VaRs are a reliable measure of risk for the sample firms, the study's incremental contribution is limited because of the nature of the sample firms and problems in variable measurement.}}, 
pages = {295--299}, 
number = {2-3}, 
volume = {9}
}
@article{10.5755/j01.ee.28.2.14225, 
year = {2017}, 
title = {{The possibilities of application of the parametric and nonparametric VaR daily returns estimation – Regional perspective}}, 
author = {Djakovic, Vladimir Dj. and Andjelic, Goran B.}, 
journal = {Engineering Economics}, 
issn = {13922785}, 
doi = {10.5755/j01.ee.28.2.14225}, 
abstract = {{The subject of this research is to test the performances of the parametric and nonparametric VaR models in the markets of the countries of the Southeast European region. The research objective is to provide concrete results regarding the possibilities of application of aforementioned VaR models in the observed markets. The research hypothesis is that the application of both parametric and nonparametric VaR models can provide optimal results regarding investment optimization. The methodology used in this research includes the application of MANOVA analysis, discriminant analysis, and Roy's test in the case of selected regional countries. The research results indicate the significance of the analysed VaR models application in the analysed markets and expand the potential for further research in the subject field. The results obtained in the research (rolling windows 100 and 300 days) implicate that statistically significant differences exists in the application of both parametric and nonparametric VaR models. Also, these results have significant international importance having in mind that there are very few studies in this area with the focus on the markets of the Southeast European region, especially with so wide and systemic approach as in this research. Having this fact in mind, the results obtained in this research significantly expand both academic and practical knowledge about possibilities and limitations of different Value at Risk models in everyday business practice. © 2017, Kauno Technologijos Universitetas. All rights reserved.}}, 
pages = {127 -- 134}, 
number = {2}, 
volume = {28}
}
@article{10.1080/02664763.2019.1579305, 
year = {2019}, 
title = {{How might sovereign bond yields in Asia Pacific react to US monetary normalisation under turbulent market conditions?*}}, 
author = {Fong, Tom and Hui, Ceara and Wong, Alfred Y.-T.}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664763.2019.1579305}, 
abstract = {{This paper examines the potential impact of US monetary normalisation on sovereign bond yields in Asia Pacific. We apply the quantile vector autoregressive model with principal component analysis to the assessment of tail risk of sovereign debt, which may not be detectable using traditional OLS-based analysis. Our empirical evidence suggests that US Treasury bond yields can have a significant impact on sovereign bond yields in the region, an important channel through which monetary normalisation by the Fed can affect Asia-Pacific economies. Increases in sovereign bond yields will not only compromise the ability of the sovereigns in the region to service their debt but also translate into higher costs of borrowing for the rest of economy. The results show how much the outsized impact could potentially be if US monetary normalisation somehow turns out to be much more disorderly than expected. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--26}, 
number = {11}, 
volume = {46}
}
@article{10.1080/09603107.2012.671922, 
year = {2012}, 
title = {{A time dynamic pair copula construction: With financial applications}}, 
author = {Vesper, Andrew}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2012.671922}, 
abstract = {{A recent technology in the statistics and econometrics literature is the Pair-Copula Construction (PCC), an extremely flexible modelling technique for capturing complex, but static, multivariate dependency. There are several available tools for time-varying bivariate copulas, but none for time-varying multivariate copulas in more than two dimensions. We use a Bayesian framework to extend the PCC to account for time dynamic dependence structures, introducing time dynamics to the multivariate copula through its PCC decomposition. In particular, we model the time series of a transformation of select parameters of the PCC as a first order autoregressive model (AR(1)) and conduct inference using a Markov Chain Monte Carlo (MCMC) algorithm. The Bayesian approach proves to be a powerful tool for estimating parameters, despite some additional computational effort. We use financial data to illustrate empirical evidence for the existence of time dynamic dependence structures, to show improved out-of-sample forecasts for our time dynamic PCC relative to the current time static PCC models, and to compare the relative performance of dynamic and static PCC models for Value at Risk (VaR) measures. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1697--1711}, 
number = {20}, 
volume = {22}
}
@article{10.1007/978-3-319-89824-7_56, 
year = {2018}, 
title = {{A copula-based quantile model}}, 
author = {Luca, Giovanni De and Rivieccio, Giorgia and Corsaro, Stefania}, 
issn = {NA}, 
doi = {10.1007/978-3-319-89824-7\_56}, 
abstract = {{A copula-based quantile model is built. The estimates are compared to the estimates obtained using the multivariate CAViaR model, which extends the univariate version of the model. The comparison is firstly made in terms of Kupiec and Christoffersen test. Moreover, a further comparison is made using two loss functions that evaluate the distances between the losses and the VaR measures in presence of a violation. The results show that the copula approach is highly competitive providing, in particular, estimated quantiles which generally imply a lower value for the two loss functions. © Springer International Publishing AG.}}, 
pages = {311--315}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jfs.2015.01.003, 
year = {2015}, 
title = {{Calculating systemic risk capital: A factor model approach}}, 
author = {Avramidis, Panagiotis and Pasiouras, Fotios}, 
journal = {Journal of Financial Stability}, 
issn = {15723089}, 
doi = {10.1016/j.jfs.2015.01.003}, 
abstract = {{We treat the banking system as a traded credit portfolio and calculate systemic risk capital as the amount of capital that insures the portfolio's value against unexpected losses. Using data from the largest global financial institutions, we find evidence of extreme event dependence between banks during the recent financial crisis. Subsequently, we extend the existing Gaussian approach by proposing a model that accounts for the extreme event dependence, and we quantify the level of capital shortfall when this characteristic is ignored. Furthermore, the mark to market valuation approach incorporates the economic loss of credit downgrades into the estimates. © 2015 Elsevier B.V.}}, 
pages = {138--150}, 
number = {NA}, 
volume = {16}
}
@article{10.1016/j.orl.2010.02.007, 
year = {2010}, 
title = {{Asymptotic representations for importance-sampling estimators of value-at-risk and conditional value-at-risk}}, 
author = {Sun, Lihua and Hong, L. Jeff}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2010.02.007}, 
abstract = {{Value-at-risk (VaR) and conditional value-at-risk (CVaR) are important risk measures. They are often estimated by using importance-sampling (IS) techniques. In this paper, we derive the asymptotic representations for IS estimators of VaR and CVaR. Based on these representations, we are able to prove the consistency and asymptotic normality of the estimators and to provide simple conditions under which the IS estimators have smaller asymptotic variances than the ordinary Monte Carlo estimators. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {246--251}, 
number = {4}, 
volume = {38}
}
@article{10.1016/j.stamet.2004.08.004, 
year = {2004}, 
title = {{The use of GARCH models in VaR estimation}}, 
author = {Angelidis, Timotheos and Benos, Alexandros and Degiannakis, Stavros}, 
journal = {Statistical Methodology}, 
issn = {15723127}, 
doi = {10.1016/j.stamet.2004.08.004}, 
abstract = {{We evaluate the performance of an extensive family of ARCH models in modeling the daily Value-at-Risk (VaR) of perfectly diversified portfolios in five stock indices, using a number of distributional assumptions and sample sizes. We find, first, that leptokurtic distributions are able to produce better one-step-ahead VaR forecasts; second, the choice of sample size is important for the accuracy of the forecast, whereas the specification of the conditional mean is indifferent. Finally, the ARCH structure producing the most accurate forecasts is different for every portfolio and specific to each equity index. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {105--128}, 
number = {1-2}, 
volume = {1}
}
@article{10.1007/s11009-013-9389-9, 
year = {2015}, 
title = {{Bayesian Estimation of a Skew-Student-t Stochastic Volatility Model}}, 
author = {Abanto-Valle, C. A. and Lachos, V. H. and Dey, Dipak K.}, 
journal = {Methodology and Computing in Applied Probability}, 
issn = {13875841}, 
doi = {10.1007/s11009-013-9389-9}, 
abstract = {{In this paper we present a stochastic volatility (SV) model assuming that the return shock has a skew-Student-t distribution. This allows a parsimonious, flexible treatment of skewness and heavy tails in the conditional distribution of returns. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed and used for parameter estimation and forecasting. The MCMC method exploits a skew-normal mixture representation of the error distribution with a gamma distribution as the mixing distribution. The developed methodology is applied to the NASDAQ daily index returns. Bayesian model selection criteria as well as out-of-sample forecasting in a value-at-risk (VaR) study reveal that the SV model based on skew-Student-t distribution provides significant improvement in model fit as well as prediction to the NASDAQ index data over the usual normal model. © 2013, Springer Science+Business Media New York.}}, 
pages = {721--738}, 
number = {3}, 
volume = {17}
}
@article{10.1109/wsc.2007.4419690, 
year = {2007}, 
title = {{Kernel estimation for quantile sensitivities}}, 
author = {Liu, Guangwu and Hong, L. Jeff}, 
journal = {2007 Winter Simulation Conference}, 
issn = {08917736}, 
doi = {10.1109/wsc.2007.4419690}, 
abstract = {{Quantiles, also known as value-at-risk in financial applications, are important measures of random performance. Quantile sensitivities provide information on how changes in the input parameters affect the output quantiles. In this paper, we study the estimation of quantile sensitivities using simulation. We propose a new estimator by employing kernel method and show its consistency and asymptotic normality for i.i.d. data. Numerical results show that our estimator works well for the test problems. © 2007 IEEE.}}, 
pages = {941--948}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/14697688.2014.971857, 
year = {2015}, 
title = {{A smooth non-parametric estimation framework for safety-first portfolio optimization}}, 
author = {Yao, Haixiang and Li, Yong and Benson, Karen}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2014.971857}, 
abstract = {{In this paper, we adopt a smooth non-parametric estimation to explore the safety-first portfolio optimization problem. We obtain a non-parametric estimation calculation formula for loss (truncated) probability using the kernel estimator of the portfolio returns’ cumulative distribution function, and embed it into two types of safety-first portfolio selection models. We numerically and empirically test our non-parametric method to demonstrate its accuracy and efficiency. Cross-validation results show that our non-parametric kernel estimation method outperforms the empirical distribution method. As an empirical application, we simulate optimal portfolios and display return-risk characteristics using China National Social Security Fund strategic stocks and Shanghai Stock Exchange 50 Index components. © 2014 Taylor \& Francis.}}, 
pages = {1865--1884}, 
number = {11}, 
volume = {15}
}
@article{10.1109/tpwrs.2003.810685, 
year = {2003}, 
title = {{Risk assessment in energy trading}}, 
author = {Dahlgren, R. and Liu, Chen-Ching and Lawarree, J.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2003.810685}, 
abstract = {{This paper provides a state-of-the-art summary of risk assessment in energy trading. Techniques from financial engineering are needed by electric energy companies to manage price risk. These tools are needed by suppliers, distributors, and traders in a competitive electric power marketplace. Tools that have been adapted to the specific environment of the electric power system include portfolio analysis and hedging instruments. This paper provides a comprehensive critical literature survey of what has been applied to date in the power markets and which areas continue to need additional research. One example market scenario is used throughout the paper to demonstrate the usefulness of the risk assessment methods.}}, 
pages = {503--511}, 
number = {2}, 
volume = {18}
}
@article{10.1080/09603107.2011.605752, 
year = {2012}, 
title = {{Extreme risk measures for REITs: A comparison among alternative methods}}, 
author = {Zhou, Jian}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2011.605752}, 
abstract = {{Real Estate Investment Trusts (REITs), traditionally known as an asset of low volatility, have been undergoing a period of unprecedentedly high volatility due to the current financial crisis. This has increased the need to search for appropriate methods to cope with extreme risks. This study aims to meet this need by comparing the performances of several commonly used methods in predicting the conditional Value at Risk (VaR) and Expected Shortfall (ES) for REITs. Our competing methods cover all three broad categories (i.e. nonparametric, parametric and semiparametric) classified by Manganelli and Engle (2004) and display a varying degree of complexity. Overall, our results show that the trio of EGARCH skewed t (EGARCH, Exponential Generalized Autoregressive Conditional Heteroscedacity), GARCH t, and GARCH EVT (EVT, Extreme Value Theory) provide the most reliable forecasts among all methods considered. Their good performance, with only a few exceptions, holds up for a variety of quantiles and is robust to the size of the moving window used to make the forecasts. We also find that GARCH normal and RiskMetrics of J.P. Morgan are the worst performers. Filtered Historical Simulation (FHS) models fall somewhere in between. © 2012 Taylor \& Francis.}}, 
pages = {113--126}, 
number = {2}, 
volume = {22}
}
@article{10.1017/asb.2020.20, 
year = {2020}, 
title = {{Risk-based capital for variable annuity under stochastic interest rate}}, 
author = {Wang, JinDong and Xu, Wei}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2020.20}, 
abstract = {{Interest rate is one of the main risks for the liability of the variable annuity (VA) due to its long maturity. However, most existing studies on the risk measures of the VA assume a constant interest rate. In this paper, we propose an efficient two-dimensional willow tree method to compute the liability distribution of the VA with the joint dynamics of the mutual fund and interest rate. The risk measures can then be computed by the backward induction on the tree structure. We also analyze the sensitivity and impact on the risk measures with regard to the market model parameters, contract attributes, and monetary policy changes. It illustrates that the liability of the VA is determined by the long-term interest rate whose increment leads to a decrease in the liability. The positive correlation between the interest rate and mutual fund generates a fat-tailed liability distribution. Moreover, the monetary policy change has a bigger impact on the long-term VAs than the short-term contracts. © 2020 by Astin Bulletin. All rights reserved.}}, 
pages = {959--999}, 
number = {3}, 
volume = {50}
}
@article{10.1080/00036846.2015.1100257, 
year = {2016}, 
title = {{A new test procedure for the choice of dependence structure in risk measurement: application to the US and UK stock market indices}}, 
author = {Shim, Jeungbo and Lee, Eun-Joo and Lee, Seung-Hwan}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2015.1100257}, 
abstract = {{The choice of an appropriate dependence structure in modelling multivariate risks is an important issue because different tail structure embedded in copula leads to a different capital requirement for the institution. We present how to select a well-specified dependence structure to given application data. Using a simple simulation technique, we develop a statistical test to assess the adequacy of a specific dependence structure. We examine the sensitivity of risk estimates to the choice of copulas using the S\&P 500 and FTSE 100 stock indices. © 2015 Taylor \& Francis.}}, 
pages = {1382--1389}, 
number = {15}, 
volume = {48}
}
@article{10.1016/j.eneco.2009.02.005, 
year = {2009}, 
title = {{Extreme Value Theory and Value at Risk: Application to oil market}}, 
author = {Marimoutou, Velayoudoum and Raggad, Bechir and Trabelsi, Abdelwahed}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2009.02.005}, 
abstract = {{Recent increases in energy prices, especially oil prices, have become a principal concern for consumers, corporations, and governments. Most analysts believe that oil price fluctuations have considerable consequences on economic activity. Oil markets have become relatively free, resulting in a high degree of oil-price volatility and generating radical changes to world energy and oil industries. Consequently, oil markets are naturally vulnerable to significant high price shifts. An example of such a case is the oil embargo crisis of 1973. In this newly created climate, protection against market risk has become a necessity. Value at Risk (VaR) measures risk exposure at a given probability level and is very important for risk management. Appealing aspects of Extreme Value Theory (EVT) have made convincing arguments for its use in managing energy price risks. In this paper, we model VaR for long and short trading positions in oil market by applying both unconditional and conditional EVT models to forecast Value at Risk. These models are compared to the performances of other well-known modelling techniques, such as GARCH, Historical Simulation and Filtered Historical Simulation. Both conditional EVT and Filtered Historical Simulation procedures offer a major improvement over the conventional methods. Furthermore, GARCH(1, 1)-t model may provide equally good results which are comparable to two combined procedures. Finally, our results confirm the importance of filtering process for the success of standard approaches. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {519--530}, 
number = {4}, 
volume = {31}
}
@article{10.1109/pes.2007.385903, 
year = {2007}, 
title = {{Evaluation of new generation entry in a deregulated electricity market}}, 
author = {Wong, K.O. and Saha, T.K. and Dong, Z.Y.}, 
journal = {2007 IEEE Power Engineering Society General Meeting}, 
issn = {NA}, 
doi = {10.1109/pes.2007.385903}, 
abstract = {{This paper proposed a new evaluation method to investigate new generation entry in deregulated markets. This method can evaluate the expected net revenue of the new generator when the unit is committed into a trading region in the Australia electricity market. The cumulated annual price and load duration curve are used, together with a randomly distributed bid price profile to determine the loading priority of the candidate unit. The approximated bids stacking priority of the generator enables the computation of the revenue for the new unit. Furthermore, stochastic generator conditions such as outages are simulated with Monte Carlo simulation. Value at Risk and Conditional Value at Risk are two of the risk assessment tools used to evaluate the risks associated with the commitment of the candidate units. The simulated and historical price data for the trading region of New South Wales (NSW) in the National Electricity Market (NEM) of Australia are used in this study. © 2007 IEEE.}}, 
pages = {1--7}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.physa.2005.12.034, 
year = {2006}, 
title = {{Clearing margin system in the futures markets-Applying the value-at-risk model to Taiwanese data}}, 
author = {Chiu, Chien-Liang and Chiang, Shu-Mei and Hung, Jui-Cheng and Chen, Yu-Lung}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2005.12.034}, 
abstract = {{This article sets out to investigate if the TAIFEX has adequate clearing margin adjustment system via unconditional coverage, conditional coverage test and mean relative scaled bias to assess the performance of three value-at-risk (VaR) models (i.e., the TAIFEX, RiskMetrics and GARCH-t). For the same model, original and absolute returns are compared to explore which can accurately capture the true risk. For the same return, daily and tiered adjustment methods are examined to evaluate which corresponds to risk best. The results indicate that the clearing margin adjustment of the TAIFEX cannot reflect true risks. The adjustment rules, including the use of absolute return and tiered adjustment of the clearing margin, have distorted VaR-based margin requirements. Besides, the results suggest that the TAIFEX should use original return to compute VaR and daily adjustment system to set clearing margin. This approach would improve the funds operation efficiency and the liquidity of the futures markets. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {353--374}, 
number = {NA}, 
volume = {367}
}
@article{10.1016/j.jfds.2016.12.001, 
year = {2016}, 
title = {{Daily value-at-risk modeling and forecast evaluation: The realized volatility approach}}, 
author = {Wong, Zhen Yao and Chin, Wen Cheong and Tan, Siow Hooi}, 
journal = {The Journal of Finance and Data Science}, 
issn = {24059188}, 
doi = {10.1016/j.jfds.2016.12.001}, 
abstract = {{One of the main applications of conditional volatility modeling and forecasting of financial assets is the value-at-risk (VaR) estimation that is used by financial institutions for reporting the daily capital in risk. It remains a question on whether realized volatility (RV) models that incorporate the use of intraday data produce better VaR forecasts compared to methodologies that are based solely on daily returns. This study provides extensive comparison of out-of-sample volatility and VaR forecast performance on three equity market indices: S\&P500, FTSE100, and DAX30 using 13 risk models that consist of 5 GARCH specifications, 4 ARFIMAX specifications and 4 HARX specifications. The out-of-sample volatility forecasts are evaluated by various loss functions and simple scoring procedures in order to identity the model that produces the overall best volatility forecasts. For VaR forecasts, the models are evaluated using a two-stage backtesting procedure where the models undergo unconditional and conditional coverage tests to eliminate underperforming models and the qualified models are then evaluated using the quadratic probability score (QPS) function that is computed based on various VaR loss functions. The results showed that RV models outperform GARCH models for volatility forecasts, but a simple EGARCH model outperforms the rest models for most of the VaR forecasts. The results also indicated that capturing the asymmetric behavior of volatility dynamics is essential for accurate volatility and VaR forecasts. The findings of this study provide useful information for market risk regulation, financial risk management and further investigations such as extension to derivative markets and options pricing. © 2016 China Science Publishing \& Media Ltd.}}, 
pages = {171--187}, 
number = {3}, 
volume = {2}
}
@article{10.1287/mnsc.2015.2228, 
year = {2016}, 
title = {{Robust growth-optimal portfolios}}, 
author = {Rujeerapaiboon, Napat and Kuhn, Daniel and Wiesemann, Wolfram}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.2015.2228}, 
abstract = {{The growth-optimal portfolio is designed to have maximum expected log return over the next rebalancing period. Thus, it can be computed with relative ease by solving a static optimization problem. The growthoptimal portfolio has sparked fascination among finance professionals and researchers because it can be shown to outperform any other portfolio with probability 1 in the long run. In the short run, however, it is notoriously volatile. Moreover, its computation requires precise knowledge of the asset return distribution, which is not directly observable but must be inferred from sparse data. By using methods from distributionally robust optimization, we design fixed-mix strategies that offer similar performance guarantees as the growth-optimal portfolio but for a finite investment horizon and for a whole family of distributions that share the same first- and second-order moments. We demonstrate that the resulting robust growth-optimal portfolios can be computed efficiently by solving a tractable conic program whose size is independent of the length of the investment horizon. Simulated and empirical backtests show that the robust growth-optimal portfolios are competitive with the classical growth-optimal portfolio across most realistic investment horizons and for an overwhelming majority of contaminated return distributions. © 2016 INFORMS.}}, 
pages = {2090--2109}, 
number = {7}, 
volume = {62}
}
@article{10.1007/s00186-018-0637-1, 
year = {2018}, 
title = {{Risk management with multiple VaR constraints}}, 
author = {Chen, An and Nguyen, Thai and Stadje, Mitja}, 
journal = {Mathematical Methods of Operations Research}, 
issn = {14322994}, 
doi = {10.1007/s00186-018-0637-1}, 
abstract = {{We study a utility maximization problem under multiple Value-at-Risk (VaR)-type constraints. The optimization framework is particularly important for financial institutions which have to follow short-time VaR-type regulations under some realistic regulatory frameworks like Solvency II, but need to serve long-term liabilities. Deriving closed-form solutions, we show that risk management using multiple VaR constraints is more useful for loss prevention at intertemporal time instances compared with the well-known result of the one-VaR problem in Basak and Shapiro (Rev Financ Stud 14:371–405, 2001), confirming the numerical analysis of Shi and Werker (J Bank Finance 36(12):3227–3238, 2012). In addition, the multiple-VaR solution at maturity on average dominates the one-VaR solution in a wide range of intermediate market scenarios, but performs worse in good and very bad market scenarios. The range of these very bad market scenarios is however rather limited. Finally, we show that it is preferable to reach a fixed terminal state through insured intertemporal states rather than through extreme up and down movements, showing that a multiple-VaR framework induces a preference for less volatility. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {297--337}, 
number = {2}, 
volume = {88}
}
@article{10.1016/j.csda.2007.05.007, 
year = {2008}, 
title = {{Modelling nonlinearities and heavy tails via threshold normal mixture GARCH models}}, 
author = {Giannikis, D. and Vrontos, I.D. and Dellaportas, P.}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2007.05.007}, 
abstract = {{A new class of flexible threshold normal mixture GARCH models is proposed for the analysis and modelling of the stylized facts appeared in many financial time series. A Bayesian stochastic method is developed and presented for the analysis of the proposed model allowing for automatic model determination and estimation of the thresholds and their unknown number. A computationally feasible algorithm that explores the posterior distribution of the threshold models is designed using Markov chain Monte Carlo stochastic search methods. A simulation study is conducted to assess the performance of the proposed method, and an empirical application of the proposed model is illustrated using real data. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {1549--1571}, 
number = {3}, 
volume = {52}
}
@article{10.1016/j.jbankfin.2013.02.027, 
year = {2013}, 
title = {{Systemic risk measurement: Multivariate GARCH estimation of CoVaR}}, 
author = {Girardi, Giulio and Ergün, A. Tolga}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.02.027}, 
abstract = {{We modify Adrian and Brunnermeier's (2011) CoVaR, the VaR of the financial system conditional on an institution being in financial distress. We change the definition of financial distress from an institution being exactly at its VaR to being at most at its VaR. This change allows us to consider more severe distress events, to backtest CoVaR, and to improve its consistency (monotonicity) with respect to the dependence parameter. We define the systemic risk contribution of an institution as the change from its CoVaR in its benchmark state (defined as a one-standard deviation event) to its CoVaR under financial distress. We estimate the systemic risk contributions of four financial industry groups consisting of a large number of institutions for the sample period June 2000 to February 2008 and the 12. months prior to the beginning of the crisis. We also investigate the link between institutions' contributions to systemic risk and their characteristics. © 2013.}}, 
pages = {3169--3180}, 
number = {8}, 
volume = {37}
}
@article{10.1109/upec.2013.6714966, 
year = {2013}, 
title = {{Risk assessment of power wheeling in extra high voltage transmission systems}}, 
author = {Fleckenstein, Marco and Rhein, Alexander and Neumann, Claus and Balzer, Gerd}, 
journal = {2013 48th International Universities' Power Engineering Conference (UPEC)}, 
issn = {NA}, 
doi = {10.1109/upec.2013.6714966}, 
abstract = {{Power wheeling can have an intensive impact on the reliability and availability of the transmission network that is utilized with it. The extended risk for the transmission system network should be considered by fixing the operation charge for the customer. With the method presented in this paper, the increased risk of disturbances, which are caused by asset outages in the 380 kV level, on feeds of the power plants and the supply of customers in the network area can be clearly identified with a self-developed method which uses a Value at Risk (VaR) analysis. For the presentation a transmission network model is used, which represents the main network of a German transmission system operator (TSO). The illustrated scenario for the application of the developed method consists of a power wheeling operation (PWO) with a transmission capacity of 1 GW, which is carried out from the northernmost to the southernmost coupling points of the transmission network. © 2013 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jrmv.2016.148, 
year = {2016}, 
title = {{Value-at-risk time scaling: A monte carlo approach}}, 
author = {Malataliana, Moepa and Rigotard, Michael}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2016.148}, 
abstract = {{This paper discusses a value-at-risk (VaR) time-scaling approach based on fitting a distribution function so as to apply a Monte Carlo simulation to determine long-term VaR. The paper uses composite normal-Pareto distribution to better capture tail risk. Due to the material model risk inherent in the long-term VaR calculation, kernel distribution is used as a benchmark distribution. © 2016 Incisive Risk Information (IP) Limited.}}, 
pages = {47--57}, 
number = {1}, 
volume = {10}
}
@article{10.1016/j.cam.2016.10.001, 
year = {2017}, 
title = {{Portfolio selection based on a benchmark process with dynamic value-at-risk constraints}}, 
author = {Zhang, Qingye and Gao, Yan}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2016.10.001}, 
abstract = {{Portfolio selection is an essential issue in finance. It studies how to allocate one's wealth in a basket of securities to maximize the return and minimize the risk. And dynamic portfolio selection based on a benchmark process is one of the most important types. Different from the existing literature, we impose a dynamic risk control on it. As a matter of fact, performing an optimal portfolio strategy in the light of a dynamic portfolio formulation does not eliminate the possibility of an investor going to bankruptcy or even more serious situations in a volatile financial market before the terminal time, so it is reasonable and necessary to impose a dynamic risk control on the instantaneous wealth throughout the investment horizon to ensure that the investment behavior can proceed and we intend to address this interesting issue in this paper. More specifically, we investigate the dynamic portfolio selection problem based on a benchmark process coupled with a dynamic value-at-risk constraint. By stochastic dynamic programming techniques, we derive the corresponding Hamilton–Jacobi–Bellman equation. Moreover, the optimal portfolio strategies are obtained by Lagrange multiplier method. To verify the model, two numerical examples are illustrated. The results show the difference of optimal portfolio strategies with and without the dynamic VaR constraint: the composition of the risky assets is constant but the investment proportion is reduced as the VaR constraint becomes binding. This research can provide a good decision-making reference for risk-averse investors. © 2016 Elsevier B.V.}}, 
pages = {440--447}, 
number = {NA}, 
volume = {313}
}
@article{10.1080/14697680802629384, 
year = {2009}, 
title = {{Portfolio diversification and value at risk under thick-tailednessy}}, 
author = {Ibragimov, Rustam}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680802629384}, 
abstract = {{This paper focuses on the study of portfolio diversification and value at risk analysis under heavy-tailedness. We use a notion of diversification based on majorization theory that will be explained in the text. The paper shows that the stylized fact that portfolio diversification is preferable is reversed for extremely heavy-tailed risks or returns. However, the stylized facts on diversification are robust to heavy-tailedness of risks or returns as long as their distributions are moderately heavy-tailed. Extensions of the results to the case of dependence, including convolutions of -symmetric distributions and models with common shocks are provided. © 2009 Taylor \& Francis.}}, 
pages = {565--580}, 
number = {5}, 
volume = {9}
}
@article{10.1007/s40565-019-0513-x, 
year = {2019}, 
title = {{Risk assessment of microgrid aggregators considering demand response and uncertain renewable energy sources}}, 
author = {GHOSE, Tirthadip and PANDEY, Harsh Wardhan and GADHAM, Kumar Raja}, 
journal = {Journal of Modern Power Systems and Clean Energy}, 
issn = {21965625}, 
doi = {10.1007/s40565-019-0513-x}, 
abstract = {{In power market environment, the growing importance of demand response (DR) and renewable energy source (RES) attracts more for-profit DR and RES aggregators to compete with each other to maximize their profit. Meanwhile, the intermittent natures of these alternative sources along with the competition add to the probable financial risk of the aggregators. The objective of the paper is to highlight this financial risk of aggregators in such uncertain environment while estimating DR magnitude and power generated by RES. This work develops DR modeling incorporating the effect of estimating power at different confidence levels and uncertain participation of customers. In this paper, two well-known risk assessment techniques, value at risk and conditional value at risk, are applied to predict the power from RES and DR programs at a particular level of risk in different scenarios generated by Monte Carlo method. To establish the linkage between financial risk taking ability of individuals, the aggregators are classified into risk neutral aggregator, risk averse aggregator and risk taking aggregator. The paper uses data from Indian Energy Exchange to produce realistic results and refers certain policies of Indian Energy Exchange to frame mathematical expressions for benefit function considering uncertainties for each type of three aggregators. Extensive results show the importance of assessing the risks involved with two unpredictable variables and possible impacts on technical and financial attributes of the microgrid energy market. © 2019, The Author(s).}}, 
pages = {1619--1631}, 
number = {6}, 
volume = {7}
}
@article{10.2298/pan1302231s, 
year = {2013}, 
title = {{Out of sample Value-at-Risk and backtesting with the standardized Pearson type-IV skewed distribution}}, 
author = {Stavroyiannis, Stavros and Zarangas, Leonidas}, 
journal = {Panoeconomicus}, 
issn = {1452595X}, 
doi = {10.2298/pan1302231s}, 
abstract = {{This paper studies the efficiency of an econometric model where the volatility is modeled by a GARCH (1,1) process, and the innovations follow a standardized form of the Pearson type-IV distribution. The performance of the model is examined by in sample and out of sample testing, and the accuracy is explored by a variety of Value-at-Risk methods, the success/failure ratio, the Kupiec-LR test, the independence and conditional coverage tests of Christoffersen, the expected shortfall measures, and the dynamic quantile test of Engle and Manganelli. Overall, the proposed model is a valid and accurate model performing better than the skewed Student-t distribution, providing the financial analyst with a good candidate as an alternative distributional scheme.}}, 
pages = {231--247}, 
number = {3 SPEC. ISSUE}, 
volume = {60}
}
@article{10.1109/csicc.2009.5349659, 
year = {2009}, 
title = {{A heuristic approach for value at risk based portfolio optimization}}, 
author = {Zeiaee, Mohammad and Jahed-Motlagh, Mohammad Reza}, 
journal = {2009 14th International CSI Computer Conference}, 
issn = {NA}, 
doi = {10.1109/csicc.2009.5349659}, 
abstract = {{Portfolio optimization under classic mean-variance framework of Markowitz must be revised as variance fails to be a good risk measure. This is especially true when the asset returns are not normal. In this paper, we utilize Value at Risk (VaR) as the risk measure and Historical Simulation (HS) is used to obtain an acceptable estimate of the VaR. Also, a well known multi-objective evolutionary approach is used to address the inherent bi-objective problem; In fact, NSGA-II is incorporated here. This method is tested on a set of past return data of 12 assets on Tehran Stock Exchange (TSE). A comparison of the obtained results, shows that the proposed method offers high quality solutions and a wide range of risk return trade-offs. ©2009 IEEE.}}, 
pages = {686--691}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jmse.2019.05.002, 
year = {2019}, 
title = {{Risk measures for variable annuities: A hermite series expansion approach}}, 
author = {Cui, Zhenyu and Kim, Jinhyoung and Lian, Guanghua and Liu, Yanchu}, 
journal = {Journal of Management Science and Engineering}, 
issn = {20962320}, 
doi = {10.1016/j.jmse.2019.05.002}, 
abstract = {{In this study, we propose an efficient approach to the calculation of risk measures for an insurer's liability from writing a variable annuity with guaranteed benefits. Our approach is based on a novel application of the Hermite series expansions on the transition density of a diffusion process to the insurance setting. We compare our method with existing methods in the literature, including the analytical method, spectral method and Green's function method, and illustrate its substantial advantages in calculating risk measures for variable annuities with different guarantee structures. The improved efficiency makes our method flexible to practical implementation in reporting risk measures on a daily basis. We also conduct a sensitivity analysis of the risk measures with respect to key parameters. © 2019 China Science Publishing \& Media Ltd.}}, 
pages = {119--141}, 
number = {2}, 
volume = {4}
}
@article{10.1016/j.econlet.2009.03.008, 
year = {2009}, 
title = {{Exact inference in diagnosing Value-at-Risk estimates - A Monte Carlo device}}, 
author = {Herwartz, Helmut}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2009.03.008}, 
abstract = {{A Monte Carlo approach is suggested for correctly sized backtesting of Value-at-Risk estimates by means of the dynamic quantile test and a Portmanteau statistic. The latter shows preferable power features but fails in case of unconditional VaR misspecification. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {160--162}, 
number = {3}, 
volume = {103}
}
@article{10.1111/j.1539-6924.2010.01549.x, 
year = {2011}, 
title = {{Failure probability under parameter uncertainty}}, 
author = {Gerrard, R. and Tsanakas, A.}, 
journal = {Risk Analysis}, 
issn = {02724332}, 
doi = {10.1111/j.1539-6924.2010.01549.x}, 
pmid = {21175720}, 
abstract = {{In many problems of risk analysis, failure is equivalent to the event of a random risk factor exceeding a given threshold. Failure probabilities can be controlled if a decisionmaker is able to set the threshold at an appropriate level. This abstract situation applies, for example, to environmental risks with infrastructure controls; to supply chain risks with inventory controls; and to insurance solvency risks with capital controls. However, uncertainty around the distribution of the risk factor implies that parameter error will be present and the measures taken to control failure probabilities may not be effective. We show that parameter uncertainty increases the probability (understood as expected frequency) of failures. For a large class of loss distributions, arising from increasing transformations of location-scale families (including the log-normal, Weibull, and Pareto distributions), the article shows that failure probabilities can be exactly calculated, as they are independent of the true (but unknown) parameters. Hence it is possible to obtain an explicit measure of the effect of parameter uncertainty on failure probability. Failure probability can be controlled in two different ways: (1) by reducing the nominal required failure probability, depending on the size of the available data set, and (2) by modifying of the distribution itself that is used to calculate the risk control. Approach (1) corresponds to a frequentist/regulatory view of probability, while approach (2) is consistent with a Bayesian/personalistic view. We furthermore show that the two approaches are consistent in achieving the required failure probability. Finally, we briefly discuss the effects of data pooling and its systemic risk implications. © 2010 Society for Risk Analysis.}}, 
pages = {727--744}, 
number = {5}, 
volume = {31}
}
@article{10.12011/setp2019-1522, 
year = {2021}, 
title = {{Optimal investment of DC pension plan under VaR constraint with minimum guarantee}}, 
author = {}, 
issn = {10006788}, 
doi = {10.12011/setp2019-1522}, 
abstract = {{This paper studies the optimal investment strategy of DC pension with VaR constraint and minimum guarantee. We assume that the terminal wealth is shared between the government and DC members. The manager aims to maximize the expected utility of the terminal payoff to DC members. We adopt a concavification technique and a Lagrange dual method to solve the problem and derive the representations of the optimal wealth process and trading strategies. We also carry out some numerical analysis to show the impacts of VaR constraint and the profit-sharing program on the optimal terminal wealth. Numerical results show that VaR constraint can provide an effective improvement in bad economic states. Furthermore, the profit-sharing program can increase the optimal terminal wealth in good economic states. © 2021, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.}}, 
number = {5}, 
volume = {41}
}
@article{10.1177/0972652712454516, 
year = {2012}, 
title = {{Risk Management in Trading and Investment Portfolios: An Optimisation Algorithm for Maximum Risk-budgeting Threshold}}, 
author = {Janabi, Mazin A.M. Al}, 
journal = {Journal of Emerging Market Finance}, 
issn = {09726527}, 
doi = {10.1177/0972652712454516}, 
abstract = {{The aim of this article is to bridge the gap in equity trading risk management literatures and particularly from the perspective of emerging and illiquid markets, such as in the context of the Gulf Cooperation Council (GCC) financial markets. In this article, we demonstrate a practical approach for the measurement and control of market risk exposure for financial trading portfolios that contain several illiquid equity securities during the unwinding (close-out) period. This approach is based on the renowned concept of Value-at-Risk (VaR) along with the development of an optimisation risk algorithm utilising matrix-algebra technique. Our thorough asset market risk modelling-algorithm can simultaneously handle VaR analysis under normal and severe market conditions, besides it takes into account the effects of illiquidity and short-sales of traded equity securities. In order to illustrate the proper use of VaR and stress-testing methods, real-world structured modelling techniques of trading risk management are presented for the GCC financial markets. To this end, comprehensive simulation case studies are accomplished with the objective of constructing a realistic framework for trading risk measurement and control in addition to the instigation of a risk optimisation algorithm-process for the computation of maximum authorised VaR risk-budgeting limits. © 2012 Institute for Financial Management and Research.}}, 
pages = {189--229}, 
number = {2}, 
volume = {11}
}
@article{10.1109/icit.2015.7125508, 
year = {2015}, 
title = {{Investment risk analysis for eolic power plants in the free contracting environment}}, 
author = {Neto, Daywes P. and Domingues, Elder G. and Calixto, Wesley P. and Alves, Aylton J. and Lima, Rodrigo A.}, 
journal = {2015 IEEE International Conference on Industrial Technology (ICIT)}, 
issn = {NA}, 
doi = {10.1109/icit.2015.7125508}, 
abstract = {{This research work presents a methodology for investment risk analysis for eolic power plants. A stochastic modelling was developed for the variables Wind Speed and Price Settlement of Differences (PSD), with the aim of analyzing cash flow during a power plant's lifespan. Simulated scenarios have been created with the Monte Carlo Method to analyze the random variables involved in the process. The investment analysis is based on the assessment of the probability distribution of the Net Present Value (NPV) and of the Modified Internal Rate of Return (MIRR) for the power plant's cash flow, as well as the Value at Risk (VaR) and the Conditional Value at Risk (CVaR). This research work presents a case study to verify the applicability of this methodology. © 2015 IEEE.}}, 
pages = {2783--2788}, 
number = {June}, 
volume = {2015-June}
}
@article{10.3390/en13010205, 
year = {2020}, 
title = {{Balancing generation from renewable energy sources: Profitability of an energy trader}}, 
author = {Kath, Christopher and Nitka, Weronika and Serafin, Tomasz and Weron, Tomasz and Zaleski, Przemysław and Weron, Rafał}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13010205}, 
abstract = {{Motivated by a practical problem faced by an energy trading company in Poland, we investigate the profitability of balancing intermittent generation from renewable energy sources (RES). We consider a company that buys electricity generated by a pool of wind farms and pays their owners the day-ahead system price minus a commission, then sells the actually generated volume in the day-ahead and balancing markets. We evaluate the profitability (measured by the Sharpe ratio) and market risk faced by the energy trader as a function of the commission charged and the adopted trading strategy. We show that publicly available, country-wide RES generation forecasts can be significantly improved using a relatively simple regression model and that trading on this information yields significantly higher profits for the company. Moreover, we address the issue of contract design as a key performance driver. We argue that by offering tolerance range contracts, which transfer some of the risk to wind farm owners, both parties can bilaterally agree on a suitable framework that meets individual risk appetite and profitability expectations. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).}}, 
pages = {205}, 
number = {1}, 
volume = {13}
}
@article{10.1109/tsg.2019.2927833, 
year = {2020}, 
title = {{A Risk-Averse Conic Model for Networked Microgrids Planning with Reconfiguration and Reorganizations}}, 
author = {Cao, Xiaoyu and Wang, Jianxue and Wang, Jianhui and Zeng, Bo}, 
journal = {IEEE Transactions on Smart Grid}, 
issn = {19493053}, 
doi = {10.1109/tsg.2019.2927833}, 
abstract = {{The advanced switching techniques enable the topology reconfiguration of microgrids (MGs) in active distribution network. In this paper, we enhance and generalize the traditional reconfiguration strategy resorting to the concept of 'dynamic MGs' (i.e., the reorganization of MGs boundaries), to achieve a higher operational feasibility against the emergency islandings. Also, a risk-averse two-stage mixed integer conic program model is presented to support the networked MGs planning with generalized reconfiguration decisions. The MGs capacity expansion and seasonal reconfiguration decisions are made in the first stage, and validated under stochastic islanding scenarios in the second stage, where the network operations are captured by a second-order conic program (SOCP). Furthermore, a conditional value-at-risk (CVaR) measure is involved to quantitatively control the islanding risks. By theoretically proving the strong duality of the SOCP subproblem, we develop and customize Benders decomposition method with the guaranteed finite convergence to the optimal value. Finally, numerical results on 33- and 56-bus networked MGs validate the effectiveness of proposed reconfiguration strategy as well as planning approach. Our method demonstrates a cost-saving up to 22.56\% when comparing to the traditional scheme with fixed MGs boundaries. © 2010-2012 IEEE.}}, 
pages = {696--709}, 
number = {1}, 
volume = {11}
}
@article{10.1198/073500104000000523, 
year = {2005}, 
title = {{A new class of multivariate skew densities, with application to generalized autoregressive conditional heteroscedasticity models}}, 
author = {Bauwens, Luc and Laurent, Sébastien}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/073500104000000523}, 
abstract = {{We propose a practical and flexible method to introduce skewness in multivariate symmetric distributions. Applying this procedure to the multivariate Student density leads to a "multivariate skew-Student" density in which each marginal has a specific asymmetry coefficient. Combined with a multivariate generalized autoregressive conditional heteroscedasticity model, this new family of distributions is found to be more useful than its symmetric counterpart for modeling stock returns and especially for forecasting the valueat-risk of portfolios. © 2005 American Statistical Association.}}, 
pages = {346--354}, 
number = {3}, 
volume = {23}
}
@article{10.1017/s0515036100014045, 
year = {2005}, 
title = {{Analysis of the Expected Shortfall of Aggregate Dependent Risks}}, 
author = {Alink, Stan and Löwe, Matthias and Wüthrich, Mario V.}, 
journal = {ASTIN Bulletin: The Journal of the IAA}, 
issn = {05150361}, 
doi = {10.1017/s0515036100014045}, 
abstract = {{We consider d identically and continuously distributed dependent risks X1,…, Xd. Our main result is a theorem on the asymptotic behaviour of expected shortfall for the aggregate risks: there is a constant cd such that for large u we have [formula omitted]. Moreover we study diversification effects in two dimensions, similar to our Value-at-Risk studies in [2]. © 2005, ASTIN Bulletin. All rights reserved.}}, 
pages = {25--43}, 
number = {1}, 
volume = {35}
}
@article{10.1007/s10700-012-9146-5, 
year = {2013}, 
title = {{Risk metrics of loss function for uncertain system}}, 
author = {Peng, Jin}, 
journal = {Fuzzy Optimization and Decision Making}, 
issn = {15684539}, 
doi = {10.1007/s10700-012-9146-5}, 
abstract = {{Real-life decisions are usually made in the state of uncertainty or risk. In this article we present two types of risk metrics of loss function for uncertain system. Firstly, the concept of value at risk (VaR) of loss function is introduced based on uncertainty theory and its fundamental properties are examined. Then the tail value at risk (TVaR) concept of loss function is evolved and some fundamental properties of the proposed TVaR are investigated. Both the VaR and TVaR are harmonious risk metrics. The suggested VaR or TVaR methodology can be widely used as tools of risk analysis in uncertain environments. © 2012 Springer Science+Business Media New York.}}, 
pages = {53--64}, 
number = {1}, 
volume = {12}
}
@article{10.1111/1475-3995.00380, 
year = {2002}, 
title = {{Dual stochastic dominance and quantile risk measures}}, 
author = {Ogryczak, Wlodzimierz and Ruszczyński, Andrzej}, 
journal = {International Transactions in Operational Research}, 
issn = {09696016}, 
doi = {10.1111/1475-3995.00380}, 
abstract = {{Following the seminal work by Markowitz, the portfolio selection problem is usually modeled as a bicriteria optimization problem where a reasonable trade–off between expected rate of return and risk is sought. In the classical Markowitz model, the risk is measured with variance. Several other risk measures have been later considered thus creating the entire family of mean–risk (Markowitz type) models. In this paper, we analyze mean–risk models using quantiles and tail characteristics of the distribution. Value at risk (VAR), defined as the maximum loss at a specified confidence level, is a widely used quantile risk measure. The corresponding second order quantile measure, called the worst conditional expectation or Tail VAR, represents the mean shortfall at a specified confidence level. It has more attractive theoretical properties and it leads to LP solvable portfolio optimization models in the case of discrete random variables, i.e., in the case of returns defined by their realizations under the specified scenarios. We show that the mean–risk models using the worst conditional expectation or some of its extensions are in harmony with the stochastic dominance order. For this purpose, we exploit duality relations of convex analysis to develop the quantile model of stochastic dominance for general distributions. International Federation of Operational Research Societies 2002.}}, 
pages = {661--680}, 
number = {5}, 
volume = {9}
}
@article{10.1007/978-3-319-00035-0_39, 
year = {2013}, 
title = {{Vulnerability of copula-VaR to misspecification of margins and dependence structure}}, 
author = {Kuziak, Katarzyna}, 
journal = {Studies in Classification, Data Analysis, and Knowledge Organization}, 
issn = {14318814}, 
doi = {10.1007/978-3-319-00035-0\_39}, 
abstract = {{Copula functions as tools for modelingmultivariate distributions are well known in theory of statistics and over the last decade have been gathering more and more popularity also in the field of finance. A Copula-based model of multivariate distribution includes both dependence structure and marginal distributions in such a way that the first may be analyzed separately from the later. Its main advantage is an elasticity allowing to merge margins of one type with a copula function of another one, or even bound margins of various types by a common copula into a single multivariate distribution. In this article copula functions are used to estimate Value at Risk (VaR). The goal is to investigate how misspecification of marginal distributions and dependence structure affects VaR. As dependence structure normal and student-t copula are considered. The analysis is based on simulation studies. © Springer International Publishing Switzerland 2013.}}, 
pages = {387--395}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-9892.2009.00644.x, 
year = {2010}, 
title = {{Empirical likelihood intervals for conditional value-at-risk in ARCH/GARCH models}}, 
author = {Gong, Yun and Li, Zhouping and Peng, Liang}, 
journal = {Journal of Time Series Analysis}, 
issn = {01439782}, 
doi = {10.1111/j.1467-9892.2009.00644.x}, 
abstract = {{Value-at-Risk (VaR) is a simple, but useful measure in risk management. When some volatility model is employed, conditional VaR is of importance. As autoregressive conditional heteroscedastic (ARCH) and generalized ARCH (GARCH) models are widely used in modelling volatilities, in this article, we propose empirical likelihood methods to obtain an interval estimation for the conditional VaR with the volatility model being an ARCH/GARCH model. Copyright © 2009 Blackwell Publishing Ltd.}}, 
pages = {65--75}, 
number = {2}, 
volume = {31}
}
@article{10.1142/s0219091518500108, 
year = {2018}, 
title = {{Volatility Forecast by Volatility Index and Its Use as a Risk Management Tool Under a Value-at-Risk Approach}}, 
author = {Siu, Yam Wing}, 
journal = {Review of Pacific Basin Financial Markets and Policies}, 
issn = {02190915}, 
doi = {10.1142/s0219091518500108}, 
abstract = {{This paper examines the predicting power of the volatility indexes of VIX and VHSI on the future volatilities (or called realized volatility, σ30,r) of their respective underlying indexes of S\&amp;P500 Index, SPX and Hang Seng Index, HSI. It is found that volatilities indexes of VIX and VHSI, on average, are numerically greater than the realized volatilities of SPX and HSI, respectively. Further analysis indicates that realized volatility, if used for pricing options, would, on some occasions, result in greatest losses of 2.21\% and 1.91\% of the spot price of SPX and HSI, respectively while the greatest profits are 2.56\% and 2.93\% of the spot price of SPX and HSI, respectively, making it not an ideal benchmark for validating volatility forecasting techniques in relation to option pricing. Hence, a new benchmark (fair volatility, σf) that considers the premium of option and the cost of dynamic hedging the position is proposed accordingly. It reveals that, on average, options priced by volatility indexes contain a risk premium demanded by the option sellers. However, the options could, on some occasions, result in greatest losses of 4.85\% and 3.60\% of the spot price of SPX and HSI, respectively while the greatest profits are 4.60\% and 5.49\% of the spot price of SPX and HSI, respectively. Nevertheless, it can still be a valuable tool for risk management. z-values of various significance levels for value-at-risk and conditional value-at-value have been statistically determined for US, Hong Kong, Australia, India, Japan and Korea markets. © 2018 World Scientific Publishing Co.}}, 
pages = {1850010}, 
number = {2}, 
volume = {21}
}
@article{10.1080/00949655.2014.913044, 
year = {2015}, 
title = {{A quasi-Bayesian model averaging approach for conditional quantile models}}, 
author = {Tsiotas, Georgios}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2014.913044}, 
abstract = {{The value at risk (VaR) is a risk measure that is widely used by financial institutions to allocate risk. VaR forecast estimation involves the evaluation of conditional quantiles based on the currently available information. Recent advances in VaR evaluation incorporate conditional variance into the quantile estimation, which yields the conditional autoregressive VaR (CAViaR) models. However, uncertainty with regard to model selection in CAViaR model estimators raises the issue of identifying the better quantile predictor via averaging. In this study, we propose a quasi-Bayesian model averaging method that generates combinations of conditional VaR estimators based on single CAViaR models. This approach provides us a basis for comparing single CAViaR models against averaged ones for their ability to forecast VaR. We illustrate this method using simulated and financial daily return data series. The results demonstrate significant findings with regard to the use of averaged conditional VaR estimates when forecasting quantile risk. © 2014,Taylor \& Francis.}}, 
pages = {1963--1986}, 
number = {10}, 
volume = {85}
}
@article{10.1108/03074350010766576, 
year = {2000}, 
title = {{Risk adjusted performance measurement and capital allocation for trading desks within banks}}, 
author = {Ploegmakers, Hein and Schweitzer, Mark and Rad, Alireza Tourani}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/03074350010766576}, 
abstract = {{In this paper we explore the possibilities of employing a Risk-Adjusted Performance Measure (RAPM) based on the Value-at-Risk to allocate capital within a bank. The concept is usually applied to measure and monitor the risk of a certain investment strategy and is as such defensive in nature. This paper puts forward the concept that an interface between risk management and performance measurement can be created that allows banks to use their risk management infrastructure in an offensive manner. Using a sample from a bank provided us with the opportunity to evaluate the broader usefulness of V A R within a performance measurement system. In the paper we show that such a system is able to increase transparency, to improve efficiency of capital allocation, and overall performance of the institution. © MCB UP Limited 2000.}}, 
pages = {39--50}, 
number = {3}, 
volume = {26}
}
@article{10.1016/j.jclepro.2019.118455, 
year = {2020}, 
title = {{Exploring the risk spillover effects among China's pilot carbon markets: A regular vine copula-CoES approach}}, 
author = {Zhu, Bangzhu and Zhou, Xinxing and Liu, Xianfeng and Wang, Haifang and He, Kaijian and Wang, Ping}, 
journal = {Journal of Cleaner Production}, 
issn = {09596526}, 
doi = {10.1016/j.jclepro.2019.118455}, 
abstract = {{In this study, value at risk and conditional value at risk are used to measure the risks of pilot carbon markets of Beijing, Shanghai, Guangdong, Tianjin, Hubei, Shenzhen and Chongqing in China. Regular vine copula-CoES is used to measure the risk spillover effects among carbon markets of Guangdong, Hubei and Shenzhen with high transactions. The empirical results show that compared with the traditional value at risk, conditional value at risk can better measure the risks of carbon markets. Carbon markets of Chongqing, Tianjin and Shenzhen have higher risks than those of Hubei and Guangdong. Risk spillover effects are found between carbon markets of Guangdong and Shenzhen, rather than between those of Hubei and Guangdong. © 2019}}, 
pages = {118455}, 
number = {NA}, 
volume = {242}
}
@article{10.1007/s40822-015-0036-3, 
year = {2016}, 
title = {{Is there a low-risk anomaly across countries?}}, 
author = {Zaremba, Adam}, 
journal = {Eurasian Economic Review}, 
issn = {1309422X}, 
doi = {10.1007/s40822-015-0036-3}, 
abstract = {{The aim of this paper is to examine the parallels between the country-level and the stock-level low-risk anomalies. The inter-market variation in returns do not follow the intra-market patterns. Country-level returns are positively related to standard deviation, value at risk, and idiosyncratic volatility, although the effect is largely explained by cross-national value, size and momentum effects. The risk-return relationship seems to be stronger in the cases idiosyncratic risk and is almost non-existent in the case of systematic risk (market beta). Furthermore, additional sorting on value at risk may markedly improve the performance of size and value strategies at the country level. The investigations are based on the cross-section analysis of 78 national stock markets for the period 1999–2014. © 2015, The Author(s).}}, 
pages = {45--65}, 
number = {1}, 
volume = {6}
}
@article{10.1016/j.jup.2017.12.001, 
year = {2018}, 
title = {{Measuring the risk-adjusted performance of CO2 emission markets: Evidence from SENDECO2}}, 
author = {Feria-Domínguez, José Manuel and Rodriguez-Carrillero, David and Guerra-Martinez, José Carlos}, 
journal = {Utilities Policy}, 
issn = {09571787}, 
doi = {10.1016/j.jup.2017.12.001}, 
abstract = {{This paper analyzes the historical risk-adjusted performance of CO2 emission allowances traded on SENDECO2 (the reference market for Southern Europe) by using the daily spot prices of the European Union Allowances (EUAs) and Certified Emission Reductions (CERs) from 2008 to 2012. We revisit the Sharpe-ratio, taking into account the modified version proposed by Ferruz and Sarto (1997), to propose a new performance indicator, the Sharpe-VaRFS, estimated by Monte Carlo simulation. Due to the existing imbalances between demand and supply for allowances, both the EUA and CER markets underperform when compared with financial stock markets, being unattractive to potential investors. © 2017 Elsevier Ltd}}, 
pages = {124--132}, 
number = {NA}, 
volume = {50}
}
@article{10.1016/j.nonrwa.2009.03.025, 
year = {2010}, 
title = {{Setting margin levels in futures markets: An extreme value method}}, 
author = {Kao, Tzu-chuan and Lin, Chu-hsiung}, 
journal = {Nonlinear Analysis: Real World Applications}, 
issn = {14681218}, 
doi = {10.1016/j.nonrwa.2009.03.025}, 
abstract = {{There are of course different types of margin requirements in futures clearinghouses, and this study focuses on setting initial and maintenance margin levels. This study provides an approach, the VaR-x method that incorporates a modification of the Hill estimator based on extreme value theory (EVT) into a Student-t distribution, for setting the unconditional and conditional margin levels (i.e. initial and maintenance margin levels). Empirical applications are based on daily data for three stock index futures returns: the FTSE100, Nasdaq100 and Nikkei225. The empirical results demonstrate that given lower probabilities of margin violation, the VaR-x approach to setting unconditional margin levels is more accurate than either the normal approach or the Hill non-parametric approach proposed by Cotter [J. Cotter, Margin exceedences for European Stock Index Futures using extreme value theory, Journal of Banking and Finance 25 (2001) 1475-1502]. Additionally, this study demonstrates that using the conditional VaR-x approach to setting margin levels can better capture extreme events, thus ensuring adequate prudence, something that is particularly crucial in periods of strong fluctuation. These empirical findings suggest that the proposed approach is very useful to setting the initial and maintenance margin levels. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {1704--1713}, 
number = {3}, 
volume = {11}
}
@article{10.1093/jjfinec/nbn012, 
year = {2008}, 
title = {{On variable selection for volatility forecasting: The role of focused selection criteria}}, 
author = {Brownlees, C T and Gallo, G M}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbn012}, 
abstract = {{This paper is concerned with the issues of modeling and projecting the dynamics of volatility when a group of potentially useful predetermined variables is available. We predict realized volatility and value at risk (VaR) with a nested set of multiplicative error models for realized volatility. We make use of recently proposed focused model selection/ combination strategies as well as the classic AIC/BIC. Focused strategies consist of choosing the model that minimizes the estimated MSE of a given function of the parameters of interest to the forecaster. Results show that VaR forecasts can significantly be improved upon using focused prediction strategies. © The Author 2008. Published by Oxford University Press. All rights reserved.}}, 
pages = {513--539}, 
number = {4}, 
volume = {6}
}
@article{10.1109/access.2020.3017813, 
year = {2020}, 
title = {{Some Approximate Results of Value-at-Risk for Dependent Compound Stochastic Sums of Heavy-Tailed Risks}}, 
author = {Lu, Zhaoyang}, 
journal = {IEEE Access}, 
issn = {21693536}, 
doi = {10.1109/access.2020.3017813}, 
abstract = {{According to in-depth research, a wide range of problems in applied science involve estimating the probability of compound stochastic sums of heavy-tailed risks over a large threshold. Many researchers have explored this issue from different aspects in recent times. There are two main difficulties here: one is how to deal with the heavy tail of risk, and another is how to handle the dependence of the aggregated processes. Aimed at these two main problems, we investigate the asymptotic properties of the tail of compound stochastic sums of heavy-tailed risks in a general dependence framework, and some approximate bounds and key characteristics related to value-at-risk are also derived. Several practical examples are given to demonstrate the effectiveness of the approximation results. Furthermore, the main results in this paper can be applied to studies of stochastic models in finance and econometrics and studies of dependent netput processes of the M/G/1 queuing systems, etc. © 2013 IEEE.}}, 
pages = {159307--159315}, 
number = {NA}, 
volume = {8}
}
@article{10.1016/s0167-6687(02)00209-3, 
year = {2003}, 
title = {{Risk capital allocation by coherent risk measures based on one-sided moments}}, 
author = {Fischer, T.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/s0167-6687(02)00209-3}, 
abstract = {{This paper proposes differentiability properties for positively homogeneous risk measures which ensure that the gradient can be applied for reasonable risk capital allocation on non-trivial portfolios. It is shown that these properties are fulfilled for a wide class of coherent risk measures based on the mean and the one-sided moments of a risky payoff. In contrast to quantile-based risk measures like Value-at-Risk (VaR), this class allows allocation in portfolios of very general distributions, e.g. discrete ones. Two examples show how risk capital given by the VaR can be allocated by adapting risk measures of this class to the VaR. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {135--146}, 
number = {1}, 
volume = {32}
}
@article{10.1108/sef-03-2016-0061, 
year = {2017}, 
title = {{Value-at-risk and expected shortfall using the unbiased extreme value volatility estimator}}, 
author = {Kumar, Dilip and Maheswaran, Srinivasan}, 
journal = {Studies in Economics and Finance}, 
issn = {10867376}, 
doi = {10.1108/sef-03-2016-0061}, 
abstract = {{Purpose: This paper aims to propose a framework based on the unbiased extreme value volatility estimator (namely, the AddRS estimator) to compute and predict the long position and the short position value-at-risk (VaR) and stressed expected shortfall (ES). The precise prediction of VaR and ES measures has important implications toward financial institutions, fund managers, portfolio managers, regulators and business practitioners. Design/methodology/approach: The proposed framework is based on the Giot and Laurent (2004) approach and incorporates characteristics like long memory, fat tails and skewness. The authors evaluate its VaR and ES forecasting performance using various backtesting approaches for both long and short positions on four global indices (S\&P 500, CAC 40, Indice BOVESPA [IBOVESPA] and S\&P CNX Nifty) and compare the results with that of various alternative models. Findings: The findings indicate that the proposed framework outperforms the alternative models in predicting the long and the short position VaR and stressed ES. The findings also indicate that the VaR forecasts based on the proposed framework provide the least total loss for various long and short position VaR, and this supports the superior properties of the proposed framework in forecasting VaR more accurately. Originality/value: The study contributes by providing a framework to predict more accurate VaR and stressed ES measures based on the unbiased extreme value volatility estimator. © 2017, © Emerald Publishing Limited.}}, 
pages = {506--526}, 
number = {4}, 
volume = {34}
}
@article{10.1016/j.jbankfin.2013.07.038, 
year = {2013}, 
title = {{Dynamic factor Value-at-Risk for large heteroskedastic portfolios}}, 
author = {Aramonte, Sirio and Rodriguez, Marius del Giudice and Wu, Jason}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.07.038}, 
abstract = {{We propose a methodology that can efficiently measure the Value-at-Risk (VaR) of large portfolios with time-varying volatility and correlations by bringing together the established historical simulation framework and recent contributions to the dynamic factor models literature. We find that the proposed methodology performs well relative to widely used VaR methodologies, and is a significant improvement from a computational point of view. © 2013 Elsevier B.V.}}, 
pages = {4299--4309}, 
number = {11}, 
volume = {37}
}
@article{10.1109/pes.2010.5589561, 
year = {2010}, 
title = {{Power portfolio optimization with traded contract products}}, 
author = {Sun, Yi and Wu, Felix F. and Zhou, Hui}, 
journal = {IEEE PES General Meeting}, 
issn = {NA}, 
doi = {10.1109/pes.2010.5589561}, 
abstract = {{Power sector restructuring has prompted the application of modern portfolio theory among market participants. Much research has been devoted to power portfolio optimization problems. However, the portfolio composition adopted in literature is rather hypothetical than realistic. From an engineering perspective, it is necessary to use real traded contract products to construct the portfolio. In this paper, clarification is made on commonly traded power contracts in the market, followed by a discussion of their pricing schemes. It is emphasized that actively traded electricity futures/forwards and options actually belong to commodity swaps and swaptions respectively. A power portfolio is then constructed for a generation company with these basic power contracts and the spot transaction as well. An optimization model is formulated to solve the asset allocation with Conditional Value at Risk (CVaR) as the risk measure. The viability of the model is tested through a numerical study. ©2010 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.21314/jrmv.2016.162, 
year = {2016}, 
title = {{Risk reduction in a time series momentum trading strategy}}, 
author = {Hong, KiHoon and Park, KiBong and Lee, Yong Woong}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2016.162}, 
abstract = {{In this paper, we investigate the four most commonly used risk measures - return volatility, beta, value-at-risk and stressed value-at-risk - of a time series momentum (TSM) trading strategy. We demonstrate that the TSM strategy results in reduced risk measures compared with the passive buy-and-hold strategy. We then validate the hypothesis with a bivariate risk model of AR(1) processes. The reduction in risk measures ranged from 24\% to 46\% under the given model of AR(1) processes. These findings should be relevant to portfolio managers, traders or risk managers who are interested in managing the financial risk of trading strategies based on TSM.}}, 
pages = {55--70}, 
number = {4}, 
volume = {10}
}
@article{10.1016/j.procs.2013.05.087, 
year = {2013}, 
title = {{Method of value-at-risk and empirical research for shanghai stock market}}, 
author = {Chen, Qi and Chen, Rongda}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2013.05.087}, 
abstract = {{Controlling financial risk is an important issue for financial institution. For the necessity of risk management, the first task is to measure risk. Value-at-risk (VaR) was developed by J.P. Morgan in 1996 and has been commonly used by practitioners to quantify risk. We will use equally weighted moving average approach, the exponential weighted moving average approach, Monte Carlo simulation and the history simulation approach to calculate VaR. The result shows that the financial risk is evaluated successfully by VaR. The higher of confidence level, the larger of VaR. If the confidence level is low, VaR is similar for different approaches. However, VaR is quite different for different approaches if the confidence level is high. © 2013 The Authors. Published by Elsevier B.V.}}, 
pages = {671--677}, 
number = {NA}, 
volume = {17}
}
@article{10.1007/s10479-018-2982-0, 
year = {2019}, 
title = {{Measuring the risk of European carbon market: an empirical mode decomposition-based value at risk approach}}, 
author = {Zhu, Bangzhu and Ye, Shunxin and He, Kaijian and Chevallier, Julien and Xie, Rui}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-018-2982-0}, 
abstract = {{Unlike common financial markets, the European carbon market is a typically heterogeneous market, characterized by multiple timescales and affected by extreme events. The traditional Value-at-Risk (VaR) with single-timescale fails to deal with the multi-timescale characteristics and the effects of extreme events, which can result in the VaR overestimation for carbon market risk. To measure accurately the risk on the European carbon market, we propose an empirical mode decomposition (EMD) based multiscale VaR approach. Firstly, the EMD algorithm is utilized to decompose the carbon price return into several intrinsic mode functions (IMFs) with different timescales and a residue, which are modeled respectively using the ARMA–EGARCH model to obtain their conditional variances at different timescales. Furthermore, the Iterated Cumulative Sums of Squares algorithm is employed to determine the windows of an extreme event, so as to identify the IMFs influenced by an extreme event and conduct an exponentially weighted moving average on their conditional variations. Finally, the VaRs of various IMFs and the residue are estimated to reconstruct the overall VaR, the validity of which is verified later. Then, we illustrate the results by considering several European carbon futures contracts. Compared with the traditional VaR framework with single timescale, the proposed multiscale VaR–EMD model can effectively reduce the influences of the heterogeneous environments (such as the influences of extreme events), and obtain a more accurate overall risk measure on the European carbon market. By acquiring the distributions of carbon market risks at different timescales, the proposed multiscale VaR–EMD estimation is capable of understanding the fluctuation characteristics more comprehensively, which can provide new perspectives for exploring the evolution law of the risks on the European carbon market. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {373--395}, 
number = {1-2}, 
volume = {281}
}
@article{10.1108/mf-04-2020-0169, 
year = {2020}, 
title = {{VaR and market value of Fintech companies: an analysis and evidence from global data}}, 
author = {Najaf, Khakan and Schinckus, Christophe and Yoong, Liew Chee}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/mf-04-2020-0169}, 
abstract = {{Purpose: This study aims at determining the portfolio value at risk (VAR) and market value of Fintech firms and compare it with their counterparts. Design/methodology/approach: By using on a dataset from 46 countries between 2009 and 2018, the authors use five measures of VaR to investigate their empirical dynamics in relation with the market value of Fintech and non-Fintech companies. Findings: The empirical results indicate that Fintech firms' portfolios have a higher financial risk and a higher market value in comparison to non-fintech firms' portfolios. Furthermore, the authors also report that the Fintech firm portfolios experience more financial risk regardless of the holding period as long-term (one year) or short-term (quarter). Research limitations/implications: There are some limitations in this research. This research does not segregate Fintech firms into their different types of services, such as direct financial investment services, loan provision services, insurance services (InsurTech), etc. The authors only aggregate the Fintech firms by country and region. Future research may consider analysing Fintech firms by differentiating the kind of financial services they offer Practical implications: Given the importance of their market value, the results imply that Fintech companies might contribute significantly to financial fluctuations in case of large variations of the market. In terms of policy recommendation, this observation requires a particular attention from the regulatory bodies who need to find the best economic balance between promoting innovation/financial technology and regulating the Fintech companies. Originality/value: This paper is the first study clarifying the relation of financial risk and market value for the Fintech firms, using the large enough database to obtain significant results. This article implies that Fintech companies require a robust risk management framework © 2020, Emerald Publishing Limited.}}, 
pages = {915--936}, 
number = {7}, 
volume = {47}
}
@article{10.21314/jop.2019.232, 
year = {2019}, 
title = {{Estimation of value-at-risk for conduct risk losses using pseudo-marginal Markov chain Monte Carlo}}, 
author = {Mitic, Peter}, 
journal = {Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2019.232}, 
abstract = {{We propose a model for conduct risk losses, in which conduct risk losses are characterized by having a small number of extremely large losses (perhaps only one) with more numerous smaller losses. It is assumed that the largest loss is actually a provision from which payments to customers are made periodically as required. We use the pseudo-marginal (PM) Markov chain Monte Carlo method to decompose the largest loss into smaller partitions in order to estimate 99.9\% value-at-risk. The partitioning is done in a way that makes no assumption about the size of the partitions. The advantages and problems of using this method are discussed. The PM procedures were run on several representative data sets. The results indicate that, in cases where using approaches such as calculating a Monte Carlo-derived loss distribution yields a result that is not consistent with the risk profile expressed by the data, using the PM method yields results that have the required consistency. © 2019 Infopro Digital Risk (IP) Limited.}}, 
number = {4}, 
volume = {14}
}
@article{10.1016/j.joi.2010.10.002, 
year = {2011}, 
title = {{An approach for detecting, quantifying, and visualizing the evolution of a research field: A practical application to the Fuzzy Sets Theory field}}, 
author = {Cobo, M.J. and López-Herrera, A.G. and Herrera-Viedma, E. and Herrera, F.}, 
journal = {Journal of Informetrics}, 
issn = {1751-1577}, 
doi = {10.1016/j.joi.2010.10.002}, 
abstract = {{This paper presents an approach to analyze the thematic evolution of a given research field. This approach combines performance analysis and science mapping for detecting and visualizing conceptual subdomains (particular themes or general thematic areas). It allows us to quantify and visualize the thematic evolution of a given research field. To do this, co-word analysis is used in a longitudinal framework in order to detect the different themes treated by the research field across the given time period. The performance analysis uses different bibliometric measures, including the h-index, with the purpose of measuring the impact of both the detected themes and thematic areas. The presented approach includes a visualization method for showing the thematic evolution of the studied field.Then, as an example, the thematic evolution of the Fuzzy Sets Theory field is analyzed using the two most important journals in the topic: Fuzzy Sets and Systems and IEEE Transactions on Fuzzy Systems.}}, 
pages = {146--166}, 
number = {1}, 
volume = {5}
}
@article{10.1016/j.cie.2020.106986, 
year = {2021}, 
title = {{Quadratic convex reformulations for the portfolio selection problem with Value-at-Risk constraint}}, 
author = {Zheng, Xiaojin and Cui, Xueting}, 
journal = {Computers \& Industrial Engineering}, 
issn = {03608352}, 
doi = {10.1016/j.cie.2020.106986}, 
abstract = {{The paper investigates quadratic convex reformulations (QCR) for the portfolio selection problem with Value-at-Risk (VaR) constraint (PS-VaR). Problem (PS-VaR) is in fact equivalent to a chance constrained problem. With an assumption of discrete distribution, problem (PS-VaR) can be generally formulated as a standard mixed-integer problem (MIP). By means of copy constraints, we first introduce semicontinuous variables for problem (PS-VaR). By exploiting the special property of semicontinuous variables, we then propose a class of quadratic convex reformulations for problem (PS-VaR), which have continuous relaxation bounds at least as tight as that of the standard MIP reformulation. We further discuss how to find the QCR with the best continuous relaxation bound among this class via strong duality and lifting techniques. Computational experiments are then conducted to illustrate the effectiveness of the proposed approach in contrast to the standard MIP formulation. © 2020 Elsevier Ltd}}, 
pages = {106986}, 
number = {NA}, 
volume = {152}
}
@article{10.1057/s41283-018-00048-2, 
year = {2019}, 
title = {{Systemic risk in financial institutions of BRICS: measurement and identification of firm-specific determinants}}, 
author = {Zeb, Shumaila and Rashid, Abdul}, 
journal = {Risk Management}, 
issn = {14603799}, 
doi = {10.1057/s41283-018-00048-2}, 
abstract = {{The aim of this paper is twofold. First, it measures the systemic risk contribution of banks, financial services, and insurance firms of each of BRICS member country for the period 2000–2015. Second, it empirically examines how firm-specific factors determine systemic risk in financial institutions of BRICS countries. To carry out the empirical analysis, the unbalanced firm-level data are used. To gauge the systemic risk of banks, financial services, and insurance firms, the Delta Conditional Value-at-Risk (∆CoVaR) methodology is applied. The panel regression approach is used to examine how firm-specific variables determine the level of systemic risk in different financial institutions of BRICS countries. The empirical findings suggest that the size of institution, the tier 1 ratio, the liquidity ratio, the operating profit margin ratio, and the market-to-book value ratio statistically significantly determine systemic risk in BRICS countries. The results are significant in devising financial regulations to decrease the influence of systemic risk factors in the respective economies. © 2019, Springer Nature Limited.}}, 
pages = {243--264}, 
number = {4}, 
volume = {21}
}
@article{10.1016/j.insmatheco.2013.07.002, 
year = {2013}, 
title = {{Corrected phase-type approximations of heavy-tailed risk models using perturbation analysis}}, 
author = {Vatamidou, E. and Adan, I.J.B.F. and Vlasiou, M. and Zwart, B.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.07.002}, 
eprint = {1404.6411}, 
abstract = {{Numerical evaluation of performance measures in heavy-tailed risk models is an important and challenging problem. In this paper, we construct very accurate approximations of such performance measures that provide small absolute and relative errors. Motivated by statistical analysis, we assume that the claim sizes are a mixture of a phase-type and a heavy-tailed distribution and with the aid of perturbation analysis we derive a series expansion for the performance measure under consideration. Our proposed approximations consist of the first two terms of this series expansion, where the first term is a phase-type approximation of our measure. We refer to our approximations collectively as corrected phase-type approximations. We show that the corrected phase-type approximations exhibit a nice behavior both in finite and infinite time horizon, and we check their accuracy through numerical experiments. © 2013 Elsevier B.V.}}, 
pages = {366--378}, 
number = {2}, 
volume = {53}
}
@article{10.1093/imaman/dpu001, 
year = {2015}, 
title = {{Stochastic dynamical modelling of spot freight rates}}, 
author = {Benth, Fred Espen and Koekebakker, Steen and Taib, Che Mohd Imran Che}, 
journal = {IMA Journal of Management Mathematics}, 
issn = {1471678X}, 
doi = {10.1093/imaman/dpu001}, 
abstract = {{Based on empirical analysis of the Capesize and Panamax indices, we propose different continuous-time stochastic processes to model their dynamics. The models go beyond the standard geometric Brownian motion, and incorporate observed effects like heavy-tailed returns, stochastic volatility and memory. In particular, we suggest stochastic dynamics based on exponential Lévy processes with normal inverse Gaussian distributed logarithmic returns. The Barndorff-Nielsen and Shephard stochastic volatility model is shown to capture time-varying volatility in the data. Finally, continuous-time autoregressive processes provide a class of models sufficiently rich to incorporate short-term persistence of freight rates. Our proposed dynamical models are fitted to market data. Finally, a numerical Value-at-Risk exercise is provided that illustrate the significance of using more sophisticated models than geometric Brownian motion. © The authors 2014. Published by Oxford University Press on behalf of the Institute of Mathematics and its Applications.}}, 
pages = {273--297}, 
number = {3}, 
volume = {26}
}
@article{10.3390/risks8040114, 
year = {2020}, 
title = {{Good-deal bounds for option prices under value-at-risk and expected shortfall constraints}}, 
author = {Desmettre, Sascha and Laudagé, Christian and Sass, Jörn}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks8040114}, 
abstract = {{In this paper, we deal with the pricing of European options in an incomplete market. We use the common risk measures Value-at-Risk and Expected Shortfall to define good-deals on a financial market with log-normally distributed rate of returns. We show that the pricing bounds obtained from the Value-at-Risk admit a non-smooth behavior under parameter changes. Additionally, we find situations in which the seller’s bound for a call option is smaller than the buyer’s bound. We identify the missing convexity of the Value-at-Risk as main reason for this behavior. Due to the strong connection between good-deal bounds and the theory of risk measures, we further obtain new insights in the finiteness and the continuity of risk measures based on multiple eligible assets in our setting. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {114}, 
number = {4}, 
volume = {8}
}
@article{10.1016/s0140-9883(03)00052-5, 
year = {2003}, 
title = {{Market risk in commodity markets: A VaR approach}}, 
author = {Giot, Pierre and Laurent, Sébastien}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/s0140-9883(03)00052-5}, 
abstract = {{We put forward Value-at-Risk models relevant for commodity traders who have long and short trading positions in commodity markets. In a 5-year out-of-sample study on aluminium, copper, nickel, Brent crude oil and WTI crude oil daily cash prices and cocoa nearby futures contracts, we assess the performance of the RiskMetrics, skewed Student APARCH and skewed student ARCH models. While the skewed Student APARCH model performs best in all cases, the skewed Student ARCH model delivers good results and its estimation does not require non-linear optimization procedures. As such this new model could be relatively easily integrated in a spreadsheet-like environment and used by market practitioners. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {435--457}, 
number = {5}, 
volume = {25}
}
@article{10.1108/jrf-06-2019-0114, 
year = {2020}, 
title = {{Forecasting multivariate VaR and ES using MC-GARCH-Copula model}}, 
author = {Badaye, Hemant Kumar and Narsoo, Jason}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-06-2019-0114}, 
abstract = {{Purpose: This study aims to use a novel methodology to investigate the performance of several multivariate value at risk (VaR) and expected shortfall (ES) models implemented to assess the risk of an equally weighted portfolio consisting of high-frequency (1-min) observations for five foreign currencies, namely, EUR/USD, GBP/USD, EUR/JPY, USD/JPY and GBP/JPY. Design/methodology/approach: By applying the multiplicative component generalised autoregressive conditional heteroskedasticity (MC-GARCH) model on each return series and by modelling the dependence structure using copulas, the 95 per cent intraday portfolio VaR and ES are forecasted for an out-of-sample set using Monte Carlo simulation. Findings: In terms of VaR forecasting performance, the backtesting results indicated that four out of the five models implemented could not be rejected at 5 per cent level of significance. However, when the models were further evaluated for their ES forecasting power, only the Student’s t and Clayton models could not be rejected. The fact that some ES models were rejected at 5 per cent significance level highlights the importance of selecting an appropriate copula model for the dependence structure. Originality/value: To the best of the authors’ knowledge, this is the first study to use the MC-GARCH and copula models to forecast, for the next 1 min, the VaR and ES of an equally weighted portfolio of foreign currencies. It is also the first study to analyse the performance of the MC-GARCH model under seven distributional assumptions for the innovation term. © 2020, Emerald Publishing Limited.}}, 
pages = {493--516}, 
number = {5}, 
volume = {21}
}
@article{10.1016/j.jbankfin.2006.10.007, 
year = {2007}, 
title = {{Cyclicality in catastrophic and operational risk measurements}}, 
author = {Allen, Linda and Bali, Turan G.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2006.10.007}, 
abstract = {{Using equity returns for financial institutions we estimate both catastrophic and operational risk measures over the period 1973-2003. We find evidence of cyclical components in both the catastrophic and operational risk measures obtained from the generalized Pareto distribution and the skewed generalized error distribution. Our new, comprehensive approach to measuring operational risk shows that approximately 18\% of financial institutions' returns represent compensation for operational risk. However, depository institutions are exposed to operational risk levels that average 39\% of the overall equity risk premium. Moreover, operational risk events are more likely to be the cause of large unexpected catastrophic losses, although when they occur, the losses are smaller than those resulting from a combination of market risk, credit risk or other risk events. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {1191--1235}, 
number = {4}, 
volume = {31}
}
@article{10.1016/j.ins.2019.08.018, 
year = {2020}, 
title = {{The inverse 1-median location problem on uncertain tree networks with tail value at risk criterion}}, 
author = {Soltanpour, Akram and Baroughi, Fahimeh and Alizadeh, Behrooz}, 
journal = {Information Sciences}, 
issn = {00200255}, 
doi = {10.1016/j.ins.2019.08.018}, 
abstract = {{In an inverse 1-median location problem on a tree network, the goal is to modify the vertex weights of the underlying tree network at the minimum total cost such that a predetermined vertex becomes the 1-median. This paper investigates the case that the vertex weights and modification costs are considered as uncertain variables. We first obtain a necessary and sufficient condition for the α-1-median on uncertain trees. Based on this condition, we transform the problem into a linear programming model with deterministic constraints. Finally, we consider the proposed model with tail value at risk objective under the weighted l1 norm and present a solution algorithm for the problem with time complexity of O(nlog n). © 2019 Elsevier Inc.}}, 
pages = {383--394}, 
number = {NA}, 
volume = {506}
}
@article{10.1016/j.retrec.2012.05.009, 
year = {2013}, 
title = {{A simulation approach for estimating value at risk in transportation infrastructure investment decisions}}, 
author = {Mishra, Sabyasachee and Khasnabis, Snehamay and Dhingra, Sunder Lall}, 
journal = {Research in Transportation Economics}, 
issn = {07398859}, 
doi = {10.1016/j.retrec.2012.05.009}, 
abstract = {{Traditional economic analysis techniques used in the assessment of Public Private Partnership (PPP) projects are based upon the assumption that future cash flows are fully deterministic in nature and are not designed to account for risks involved in the assessment of future returns. In reality, many of these infrastructure projects are associated with significant risks stemming from the lack of knowledge about future cost and benefit streams. The fundamental premise of the PPP concept is to efficiently allocate risks between the public and the private partner. The return based on deterministic analysis may not depict a true picture of future economic outcomes of a PPP project for the multiple agencies involved. This deficiency underscores the importance of risk-based economic analysis for such projects. In this paper, the authors present the concept of Value-at-Risk (VaR) as a measure of effectiveness (MOE) to assess the risk share for the public and private entity in a PPP project. Bootstrap simulation is used to generate the risk profile savings in vehicle operating cost, and in travel time resulting from demand-responsive traffic. The VaR for Internal Rate of Return (IRR) is determined for public and private entity. The methodology is applied to a case study involving such a joint venture in India, the Mumbai Pune Expressway/National Highway 4 (MPEW/NH4), and fiscal implications from the perspective of the public and the private entities are examined. A comparison between deterministic and risk based economic analysis for MPEW/NH4 is presented. Risk analysis provides insightful results on the economic and financial implications from each participant's viewpoint. © 2012 Elsevier Ltd.}}, 
pages = {128--138}, 
number = {1}, 
volume = {38}
}
@article{10.1007/s00780-015-0273-z, 
year = {2015}, 
title = {{Aggregation-robustness and model uncertainty of regulatory risk measures}}, 
author = {Embrechts, Paul and Wang, Bin and Wang, Ruodu}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-015-0273-z}, 
abstract = {{Research related to aggregation, robustness and model uncertainty of regulatory risk measures, for instance, value-at-risk (VaR) and expected shortfall (ES), is of fundamental importance within quantitative risk management. In risk aggregation, marginal risks and their dependence structure are often modelled separately, leading to uncertainty arising at the level of a joint model. In this paper, we introduce a notion of qualitative robustness for risk measures, concerning the sensitivity of a risk measure to the uncertainty of dependence in risk aggregation. It turns out that coherent risk measures, such as ES, are more robust than VaR according to the new notion of robustness. We also give approximations and inequalities for aggregation and diversification of VaR under dependence uncertainty, and derive an asymptotic equivalence for worst-case VaR and ES under general conditions. We obtain that for a portfolio of a large number of risks, VaR generally has a larger uncertainty spread compared to ES. The results warn that unjustified diversification arguments for VaR used in risk management need to be taken with much care, and they potentially support the use of ES in risk aggregation. This in particular reflects on the discussions in the recent consultative documents by the Basel Committee on Banking Supervision. © 2015, Springer-Verlag Berlin Heidelberg.}}, 
pages = {763--790}, 
number = {4}, 
volume = {19}
}
@article{10.1109/icmss.2010.5576896, 
year = {2010}, 
title = {{The overseas investment VaR about mineral resources based on GARCH models}}, 
author = {Dong-bin, Hu and Zhan-ying, Zhang}, 
journal = {2010 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2010.5576896}, 
abstract = {{We studied the risks of financial market in the international minerals market .This article, we take London copper and the exchange rate of RMB/USD for example, using GARCH models for measuring value at risk (VaR). At last, we use the Kupiec test as a back testing of VaR model. The purpose of this study wants to provide reference for the overseas investment of China enterprises. © 2010 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/1099-131x(200007)19:4<313::aid-for776>3.0.co;2-e, 
year = {2000}, 
title = {{Conditional density and value-at-risk prediction of Asian currency exchange rates}}, 
author = {Mittnik, Stefan and Paolella, Marc S.}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/1099-131x(200007)19:4<313::aid-for776>3.0.co;2-e}, 
abstract = {{We first demonstrate the simultaneous need for both more general GARCH structures and non-normal innovation distributions for modelling the returns on certain return series such as the highly volatile exchange rates on East Asian currencies against the US dollar. This is accomplished not only via in-sample goodness-of-fit criteria, but also in terms of the precision of Value-at-Risk calculations made on out-of-sample density predictions. Second, a forecasting strategy using weighted maximum likelihood estimation is proposed. We show that it gives rise to considerably improved forecast performance over longer horizons. Copyright © 2000 John Wiley \& Sons, Ltd.}}, 
pages = {313--333}, 
number = {4}, 
volume = {19}
}
@article{10.1016/j.eneco.2014.06.018, 
year = {2014}, 
title = {{Risk factors and value at risk in publicly traded companies of the nonrenewable energy sector}}, 
author = {Bianconi, Marcelo and Yoshino, Joe A.}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2014.06.018}, 
abstract = {{We analyze a sample of 64 oil and gas companies of the nonrenewable energy sector from 24 countries using daily observations on return on stock from July 15, 2003 to August 14, 2012.We show that specific and common risk factors are priced. Specific risk factors including company size and leverage are important in explaining returns of energy companies and those companies became more exposed to credit concerns after the financial crisis of 2008. Common risk factors including the U.S. Dow Jones market excess return, the VIX, the WTI price of crude oil, and the FX of the Euro, Chinese yuan, Brazilian real, Japanese yen and British pound vis-à-vis the U.S. dollar are also important in explaining energy company returns. The foreign exchange effect accounts for the fact that many companies in the sector receive revenues denominated in domestic currency while their costs are in foreign currency. © 2014 Elsevier B.V.}}, 
pages = {19--32}, 
number = {NA}, 
volume = {45}
}
@article{10.1007/s13198-011-0062-9, 
year = {2012}, 
title = {{Enhanced well-being analysis and value-at-risk (VaR) dependent reserve determination in deregulated power systems}}, 
author = {Venu, V. Vijay and Verma, A. K.}, 
journal = {International Journal of System Assurance Engineering and Management}, 
issn = {09756809}, 
doi = {10.1007/s13198-011-0062-9}, 
abstract = {{Well-being analysis (WBA) encompasses deterministic and probabilistic measures to capture the comprehensive status of power system fitness with regards to ensuring adequacy. A key application is to be found in reserve allocation studies, both in the regulated and the liberalized scenarios. This paper presents a revised WBA approach applicable to the determination of reserve requirements at the Hierarchical Level I (HLI) (generation systems) of bilateral contracts market. Heuristic modification factors are proposed for a resourceful deployment of the traditional WBA. This is done with a view to overriding the odds of strict optimistic appraisals or lenient pessimistic appraisals, whose resultant implications in the planning and operation of power systems might yield insufficient contingency measures or unwarranted redundancy. Further, as in the risk management methods used in financial and insurance industries, the risk metric valueat-risk (VaR) is employed in this paper for deciding the beneficial reserve levels to be engaged by potential customers in bilateral contract markets. © 2012 The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.}}, 
pages = {24--32}, 
number = {1}, 
volume = {3}
}
@article{10.1088/1742-6596/953/1/012204, 
year = {2018}, 
title = {{Value at Risk on Composite Price Share Index Stock Data}}, 
author = {Oktaviarina, A}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/953/1/012204}, 
abstract = {{The financial servicest authority was declared Let's Save Campaign on n commemoration of the World Savings Day that falls on this day, October 31, 2016. The activity was greeted enthusiastically by Indonesia Stock Exchange by taking out the slogan Let's Save The Stocks. Stock is a form of investment that is expected to benefit in the future despite has risks. Value at Risk (VaR) is a method that can measure how much the risk of a financial investment. Composite Stock Price Indeks is the stock price index used by Indonesia Stock Exchange as stock volatility benchmarks in Indonesia. This study aimed to estimate Value at Risk (VaR) on closing price Composite Price Share Index Stock data on the period 20 September 2016 until 20 September 2017. Box-Pierce test results p value=0.9528 which is greater than a, that shows homoskedasticity. Value at Risk (VaR) with Variance Covariance Method is Rp.3.054.916,07 which means with 99\% confindence interval someone who invests Rp.100.000.000,00 will get Rp.3.054.916,07 as a maximum loss. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012204}, 
number = {1}, 
volume = {953}
}
@article{10.1016/j.irfa.2005.03.002, 
year = {2006}, 
title = {{Estimating the VaR of a portfolio subject to price limits and nonsynchronous trading}}, 
author = {Chou, Pin-Huang and Li, Wen-Shen and Lin, Jun-Biao and Wang, Jane-Sue}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2005.03.002}, 
abstract = {{Price limits and nonsynchronous trading are two main features in emerging markets. Price limits cause stock returns to be restricted within a prespecified range whereas infrequent trading induces spurious autocorrelation and biased estimate of the return variance. Both factors cause traditional measures of Value at Risk (VaR) to be biased. In this paper, we propose VaR measures based on a two-limit type Tobit model incorporating Scholes and Williams' [Scholes, M., \& Williams, J. (1977). Estimating betas from nonsynchronous data, Journal of Financial Economics 5, 309-328] estimator that adjusts for price limits and nonsynchronous trading. Based on the simulation design of Brown and Warner [Brown, S., \& Warner, J. (1985). Measuring security price performance, Journal of Financial Economics 8, 205-258], we compare the performance of our proposed methods with two traditional methods, one based on naive OLS estimates and the other based on historical simulation. Using daily data of all stocks listed on the Taiwan Stock Exchange and the OTC markets, the simulation results indicate that all methods perform reasonably well. The only exception is that the naive OLS yields a slightly higher failure rate when the portfolio under consideration is composed of only a few stocks. Thus, despite the potential problems induced by nonsynchronous trading and price limits, their practical impacts seem limited. © 2006 Elsevier Inc. All rights reserved.}}, 
pages = {363--376}, 
number = {4-5}, 
volume = {15}
}
@article{10.1016/j.econmod.2013.09.043, 
year = {2014}, 
title = {{Co-movements of GCC emerging stock markets: New evidence from wavelet coherence analysis}}, 
author = {Aloui, Chaker and Hkiri, Besma}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2013.09.043}, 
abstract = {{This paper examines the short term and long term dependencies between stock market returns for the Gulf Cooperation Council (GCC) Countries (Bahrain, Kuwait, Oman, Qatar, Saudi Arabia, and the United Arab Emirates) during the period 2005-2010. Our empirical investigation is based on the wavelet squared coherence which allows us to assess the co-movement in both time-frequency spaces. Our results reveal frequent changes in the pattern of the co-movements especially after 2007 for all the selected GCC markets at relatively higher frequencies. We further note an increasing strength of dependence among the GCC stock markets during the last financial crisis signifying enhanced portfolio benefits for investors in the short term relative to the long term. On the financial side, we uncover that the strength of co-movement between GCC markets may impact the multi-country portfolio's value at risk (VaR) levels. These findings provide potential implications for portfolio managers operating in the GCC region who are invited to consider co-movement through both frequencies and time when designing their portfolios. © 2013 Elsevier B.V.}}, 
pages = {421--431}, 
number = {NA}, 
volume = {36}
}
@article{10.1142/s0219024910005917, 
year = {2010}, 
title = {{A remark concerning value-at-risk}}, 
author = {NOVAK, S Y}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024910005917}, 
abstract = {{Over the past two decades Value-at-Risk (VaR) became arguably the most popular measure of financial risk. Major banks calculate VaR on daily basis in order to determine the amount of capital a bank needs to offset the market risk. Banks use calculation methods of their choice, and many estimations are based on the assumption that portfolio rates of return have normal distribution. The important question is whether the chosen method of VaR calculation is accurate. As the light-tail property of the normal distribution can cause significant underestimation of VaR, the Basel Committee suggested to calculate the amount of capital needed by multiplying the bank's internal estimate of VaR by the factor 3. It's also common to use the so-called "square root of time" rule when evaluating VaR over a longer time horizon. This article aims to refine Stahl's argument behind the "factor 3" rule and say a word of caution concerning the "square root of time" rule. © 2010 World Scientific Publishing Company.}}, 
pages = {507--515}, 
number = {4}, 
volume = {13}
}
@article{10.1109/ccdc.2013.6561075, 
year = {2013}, 
title = {{The solution of newsvendor problem based on value-at-risk}}, 
author = {Wen, Ping and Qin, Iingli}, 
journal = {2013 25th Chinese Control and Decision Conference (CCDC)}, 
issn = {NA}, 
doi = {10.1109/ccdc.2013.6561075}, 
abstract = {{This paper seeks to describe and explain managers' newsvendor decisions. These decisions may systematically deviate from profit maximization for several reasons. First, a decision maker may have preferences other than profit maximization. For example, a risk-averse decision maker will systematically order less than the profit maximizing order quantity. Second, a decision maker may apply a heuristic to choose an inventory degree. Under the three new decision making rules, this paper discusses the newsvendor problem and obtains the solution of newsvendor problem for loss averse newsvendor. Finally, comparative static effects of changes in the various price and cost are determined and related to the newsvendor's loss aversion. © 2013 IEEE.}}, 
pages = {1029--1033}, 
number = {NA}, 
volume = {NA}
}
@article{10.4310/sii.2014.v7.n4.a6, 
year = {2014}, 
title = {{Bayesian inference for stochastic volatility models using the generalized skew-t distribution with applications to the Shenzhen Stock Exchange returns}}, 
author = {Abanto-Valle, Carlos A and Wang, Caifeng and Wang, Xiaojing and Wang, Fei-Xing and Chen, Ming-Hui}, 
journal = {Statistics and Its Interface}, 
issn = {19387989}, 
doi = {10.4310/sii.2014.v7.n4.a6}, 
abstract = {{In this paper, we propose a new stochastic volatility model based on a generalized skew-Student-t distribution for stock returns. This new model allows a parsimonious and flexible treatment of the skewness and heavy tails in the conditional distribution of the returns. An efficient Markov chain Monte Carlo (MCMC) sampling algorithm is developed for computing the posterior estimates of the model parameters. Value-at-Risk (VaR) and Expected Shortfall (ES) forecasting via a computational Bayesian framework are considered. The MCMC-based method exploits a skewnormal mixture representation of the error distribution. The proposed methodology is applied to the Shenzhen Stock Exchange Component Index (SZSE-CI) daily returns. Bayesian model selection criteria reveal that there is a significant improvement in model fit to the SZSE-CI returns data by using the SV model based on a generalized skew-Student-t distribution over the usual normal and Student-t models. Empirical results show that the skewness can improve VaR and ES forecasting in comparison with the normal and Student-t models. We demonstrate that the generalized skew-Studentt tail behavior is important in modeling stock returns data.}}, 
pages = {487--502}, 
number = {4}, 
volume = {7}
}
@article{10.1017/asb.2015.21, 
year = {2015}, 
title = {{MODELING LONGEVITY RISK WITH GENERALIZED DYNAMIC FACTOR MODELS AND VINE-COPULAE}}, 
author = {Chuliá, Helena and Guillén, Montserrat and Uribe, Jorge M.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2015.21}, 
abstract = {{We present a methodology to forecast mortality rates and estimate longevity and mortality risks. The methodology uses generalized dynamic factor models fitted to the differences in the log-mortality rates. We compare their prediction performance with that of models previously described in the literature, including the traditional static factor model fitted to log-mortality rates. We also construct risk measures using vine-copula simulations, which take into account the dependence between the idiosyncratic components of the mortality rates. The methodology is applied to forecast mortality rates for a population portfolio for the UK and to estimate longevity and mortality risks. © 2015 by Astin Bulletin. All rights reserved.}}, 
pages = {165--190}, 
number = {1}, 
volume = {46}
}
@article{10.1587/transinf.e94.d.1378, 
year = {2011}, 
title = {{Re-scheduling of unit commitment based on customers' fuzzy requirements for power reliability}}, 
author = {WANG, Bo and LI, You and WATADA, Junzo}, 
journal = {IEICE Transactions on Information and Systems}, 
issn = {09168532}, 
doi = {10.1587/transinf.e94.d.1378}, 
abstract = {{The development of the electricity market enables us to provide electricity of varied quality and price in order to fulfill power consumers' needs. Such customers choices should influence the process of adjusting power generation and spinning reserve, and, as a result, change the structure of a unit commitment optimization problem (UCP). To build a unit commitment model that considers customer choices, we employ fuzzy variables in this study to better characterize customer requirements and forecasted future power loads. To measure system reliability and determine the schedule of real power generation and spinning reserve, fuzzy Value-at-Risk (VaR) is utilized in building the model, which evaluates the peak values of power demands under given confidence levels. Based on the information obtained using fuzzy VaR, we proposed a heuristic algorithm called local convergence-averse binary particle swarm optimization (LCA-PSO) to solve the UCP. The proposed model and algorithm are used to analyze several test systems. Comparisons between the proposed algorithm and the conventional approaches show that the LCA-PSO performs better in finding the optimal solutions. Copyright © 2011 The Institute of Electronics, Information and Communication Engineers.}}, 
pages = {1378--1385}, 
number = {7}, 
volume = {E94-D}
}
@article{10.1109/icsmc.2012.6377813, 
year = {2012}, 
title = {{Interactive multiobjective random fuzzy programming: Necessity-based value at risk model}}, 
author = {Katagiri, Hideki and Uno, Takeshi and Kato, Kosuke and Tsuda, Hiroshi and Tsubaki, Hiroe}, 
journal = {2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
issn = {1062922X}, 
doi = {10.1109/icsmc.2012.6377813}, 
abstract = {{This article considers multiobjective linear programming problems (MOLPP) where random fuzzy variables are contained in objective functions and constraints. The purpose of the proposed decision making model is to optimize values at risk under the constraints using a necessity measure. An interacitve algorithm is constructed in order to obtain a satisficing solution for the decision maker from among a set of Pareto optimal solutions. © 2012 IEEE.}}, 
pages = {727--732}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2010.01.011, 
year = {2010}, 
title = {{Risk concentration and diversification: Second-order properties}}, 
author = {Degen, Matthias and Lambrigger, Dominik D. and Segers, Johan}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2010.01.011}, 
abstract = {{The quantification of diversification benefits due to risk aggregation plays a prominent role in the (regulatory) capital management of large firms within the financial industry. However, the complexity of today's risk landscape makes a quantifiable reduction of risk concentration a challenging task. In the present paper we discuss some of the issues that may arise. The theory of second-order regular variation and second-order subexponentiality provides the ideal methodological framework to derive second-order approximations for the risk concentration and the diversification benefit. © 2010 Elsevier B.V.}}, 
pages = {541--546}, 
number = {3}, 
volume = {46}
}
@article{10.1109/tii.2009.2022542, 
year = {2009}, 
title = {{Value-at-risk-based two-stage fuzzy facility location problems}}, 
author = {Wang, Shuming and Watada, Junzo and Pedrycz, Witold}, 
journal = {IEEE Transactions on Industrial Informatics}, 
issn = {15513203}, 
doi = {10.1109/tii.2009.2022542}, 
abstract = {{Reducing risks in location decisions when coping with imprecise information is critical in supply chain management so as to increase competitiveness and profitability. In this paper, a two-stage fuzzy facility location problem with Value-at-Risk (VaR), called VaR-FFLP, is proposed, which results in a two-stage fuzzy zero-one integer programming problem. Some properties of the VaR-FFLP, including the value of perfect information (VPI), the value of fuzzy solution (VFS), and the bounds of the fuzzy solution, are discussed. Since the fuzzy parameters of the location problem are represented in the form of continuous fuzzy variables, the determination of VaR is inherently an infinite-dimensional optimization problem that cannot be solved analytically. Therefore, a method based on the discretization of the fuzzy variables is proposed to approximate the VaR. The Approximation Approach converts the original problem into a finite-dimensional optimization problem. A pertinent convergence theorem for the Approximation Approach is proved. Subsequently, by combining the Simplex Algorithm, the Approximation Approach, and a mechanism of genotype-phenotype- mutation-based binary particle swarm optimization (GPM-BPSO), a hybrid GPM-BPSO algorithm is being exploited to solve the VaR-FFLP. A numerical example illustrates the effectiveness of the hybrid GPM-BPSO algorithm and shows its enhanced performance in comparison with the results obtained by other approaches using genetic algorithm (GA), tabu search (TS), and Boolean BPSO (B-BPSO). © 2009 IEEE.}}, 
pages = {465--482}, 
number = {4}, 
volume = {5}
}
@article{10.1109/tem.2009.2033046, 
year = {2010}, 
title = {{Estimating the benefits and risks of implementing E-procurement}}, 
author = {Trkman, Peter and McCormack, Kevin}, 
journal = {IEEE Transactions on Engineering Management}, 
issn = {00189391}, 
doi = {10.1109/tem.2009.2033046}, 
abstract = {{In recent years, organizations have invested heavily in e-procurement technology solutions. However, an estimation of the value of the technology-enabled procurement process is often lacking. Our paper presents a rigorous methodological approach to the analysis of e-procurement benefits. Business process simulations are used to analyze the benefits of both technological and organizational changes related to e-procurement. The approach enables an estimation of both the average and variability of procurement costs and benefits, workload, and lead times. In addition, the approach enables optimization of a procurement strategy (e.g., approval levels). Finally, an innovative approach to estimation of value at risk is shown. © 2010 IEEE.}}, 
pages = {338--349}, 
number = {2}, 
volume = {57}
}
@article{10.1007/s10614-019-09901-2, 
year = {2020}, 
title = {{Measuring CoVaR: An Empirical Comparison}}, 
author = {Bianchi, Michele Leonardo and Sorrentino, Alberto Maria}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-019-09901-2}, 
abstract = {{Recent literature has proposed a market-based measure to assess the contribution of a single bank to the systemic risk, i.e. the delta conditional value-at-risk (Δ CoVaR). This measure could be useful to control the dynamics of systemic risk as perceived by the market. We estimate the Δ CoVaR of Italian and main European banks over the time span from January 2007 to December 2018 by considering three possible methodologies: (1) the quantile regression; (2) a closed form formula; (3) a non-parametric method. The estimates based on closed form formula do not differ substantially from those of the other two methodologies, moreover they provide more robust results. Furthermore, we compare the ranking derived by the Δ CoVaR with the global systemically important banks (GSIBs) buckets determining additional loss absorbency requirements. We show that there are differences in the ranking defined by the Δ CoVaR and the GSIBs bucket allocation provided by the Financial Stability Board even if the Δ CoVaR seems to be able to divide the good from the bad, from a systemic risk perspective. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {511--528}, 
number = {2}, 
volume = {55}
}
@article{10.1016/j.jmva.2018.04.004, 
year = {2018}, 
title = {{Cone distribution functions and quantiles for multivariate random variables}}, 
author = {Hamel, Andreas H. and Kostner, Daniel}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2018.04.004}, 
abstract = {{Set-valued quantiles for multivariate distributions with respect to a general convex cone are introduced which are based on a family of (univariate) distribution functions rather than on the joint distribution function. It is shown that these quantiles enjoy basically all the properties of univariate quantile functions. Relationships to families of univariate quantile functions and to depth functions are discussed. Finally, a corresponding Value-at-Risk for multivariate random variables as well as a stochastic (dominance) order based on quantiles are introduced via the set-valued approach. © 2018 Elsevier Inc.}}, 
pages = {97--113}, 
number = {NA}, 
volume = {167}
}
@article{10.1287/moor.1120.0560, 
year = {2013}, 
title = {{Risk preferences and their robust representation}}, 
author = {Drapeau, Samuel and Kupper, Michael}, 
journal = {Mathematics of Operations Research}, 
issn = {0364765X}, 
doi = {10.1287/moor.1120.0560}, 
abstract = {{To address the plurality of interpretations of the subjective notion of risk, we describe it by means of a risk order and concentrate on the context invariant features of diversification and monotonicity. Our main results are uniquely characterized robust representations of lower semicontinuous risk orders on vector spaces and convex sets. This representation covers most instruments related to risk and allows for a differentiated interpretation depending on the underlying context that is illustrated in different settings: for random variables, risk perception can be interpreted as model risk, and we compute among others the robust representation of the economic index of riskiness. For lotteries, risk perception can be viewed as distributional risk and we study the "value at risk." For consumption patterns, which excerpt an intertemporality dimension in risk perception, we provide an interpretation in terms of discounting risk and discuss some examples. ©2013 INFORMS.}}, 
pages = {28--62}, 
number = {1}, 
volume = {38}
}
@article{10.1111/mafi.12302, 
year = {2021}, 
title = {{Intra-Horizon expected shortfall and risk structure in models with jumps}}, 
author = {Farkas, Walter and Mathys, Ludovic and Vasiljević, Nikola}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12302}, 
abstract = {{The present article deals with intra-horizon risk in models with jumps. Our general understanding of intra-horizon risk is along the lines of the approach taken in Boudoukh et al. (2004); Rossello (2008); Bhattacharyya et al. (2009); Bakshi and Panayotov (2010); and Leippold and Vasiljević (2020). In particular, we believe that quantifying market risk by strictly relying on point-in-time measures cannot be deemed a satisfactory approach in general. Instead, we argue that complementing this approach by studying measures of risk that capture the magnitude of losses potentially incurred at any time of a trading horizon is necessary when dealing with (m)any financial position(s). To address this issue, we propose an intra-horizon analogue of the expected shortfall for general profit and loss processes and discuss its key properties. Our intra-horizon expected shortfall is well-defined for (m)any popular class(es) of Lévy processes encountered when modeling market dynamics and constitutes a coherent measure of risk, as introduced in Cheridito et al. (2004). On the computational side, we provide a simple method to derive the intra-horizon risk inherent to popular Lévy dynamics. Our general technique relies on results for maturity-randomized first-passage probabilities and allows for a derivation of diffusion and single jump risk contributions. These theoretical results are complemented with an empirical analysis, where popular Lévy dynamics are calibrated to the S\&P 500 index and Brent crude oil data, and an analysis of the resulting intra-horizon risk is presented. © 2021 Wiley Periodicals LLC}}, 
pages = {772--823}, 
number = {2}, 
volume = {31}
}
@article{10.1108/mf-05-2018-0218, 
year = {2018}, 
title = {{Assessing Sukuk defaults using value-at-risk techniques}}, 
author = {Alam, Nafis and Bhatti, Muhammad and Wong, James T.F.}, 
journal = {Managerial Finance}, 
issn = {03074358}, 
doi = {10.1108/mf-05-2018-0218}, 
abstract = {{Purpose: The purpose of this paper is to investigate the default characteristics of Sukuk issues by corporate firms in Malaysia using value-at-risk (VaR) techniques over a period of 16 years from 2000 to 2015 and across nine economic sectors. Design/methodology/approach: The paper employs non-parametric and Monte Carlo simulations to estimate Sukuk defaults. Findings: The authors analyses revealed that the VaR predictions were fairly consistent with the ratings provided by credit rating agencies, despite the limited tradability of Sukuk in the secondary market. The study was able to demonstrate that Sukuk is not riskier than conventional bonds in the Malaysian context. Research limitations/implications: The research findings suggested that VaR values will depend on the fundamental value of a firm based on the considerations of market, credit and operational risk. It does not rely on the type of debt instrument, whether a Sukuk or conventional bonds. Practical implications: The use of Sukuk along with conventional bonds as debt instruments creates opportunities for investors and bond issuers globally. Originality/value: Although Sukuk has generated much interest among financial market players, studies are lacking on how to predict Sukuk defaults and whether Sukuk has the same risk profile compared to conventional bonds. © 2018, Emerald Publishing Limited.}}, 
pages = {665--687}, 
number = {6}, 
volume = {44}
}
@article{10.1016/j.ejor.2021.03.012, 
year = {2021}, 
title = {{Risk sharing with multiple indemnity environments}}, 
author = {Asimit, Alexandru V. and Boonen, Tim J. and Chi, Yichun and Chong, Wing Fung}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2021.03.012}, 
abstract = {{Optimal risk sharing arrangements have been substantially studied in the literature, from the aspects of generalizing objective functions, incorporating more business constraints, and investigating different optimality criteria. This paper proposes an insurance model with multiple risk environments. We study the case where the two agents are endowed with the Value-at-Risk or the Tail Value-at-Risk, or when both agents are risk-neutral but have heterogeneous beliefs regarding the underlying probability distribution. We show that layer-type indemnities, within each risk environment, are Pareto optimal, which may be environment-specific. From Pareto optimality, we get that the premium can be chosen in a given interval, and we propose to allocate the gains from risk sharing equally between the buyer and seller. © 2021 The Authors}}, 
pages = {587--603}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2018.07.004, 
year = {2018}, 
title = {{Modeling maxima with autoregressive conditional Fréchet model}}, 
author = {Zhao, Zifeng and Zhang, Zhengjun and Chen, Rong}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2018.07.004}, 
abstract = {{This paper introduces a novel dynamic generalized extreme value (GEV) framework for modeling the time-varying behavior of maxima in financial time series. Specifically, an autoregressive conditional Fréchet (AcF) model is proposed in which the maxima are modeled by a Fréchet distribution with time-varying scale parameter (volatility) and shape parameter (tail index) conditioned on past information. The AcF provides a direct and accurate modeling of the time-varying behavior of maxima and furthermore offers a new angle to study the tail risk dynamics in financial markets. Probabilistic properties of AcF are studied, and a maximum likelihood estimator is used for model estimation, with its statistical properties investigated. Simulations show the flexibility of AcF and confirm the reliability of its estimators. Two real data examples on cross-sectional stock returns and high-frequency foreign exchange returns are used to demonstrate the AcF modeling approach, where significant improvement over the static GEV has been observed for market tail risk monitoring and conditional VaR estimation. Empirical result of AcF is consistent with the findings of the dynamic peak-over-threshold (POT) literature that the tail index of financial markets varies through time. © 2018 Elsevier B.V.}}, 
pages = {325--351}, 
number = {2}, 
volume = {207}
}
@article{10.1007/s11149-015-9281-3, 
year = {2015}, 
title = {{Efficiency impact of convergence bidding in the california electricity market}}, 
author = {Li, Ruoyang and Svoboda, Alva J. and Oren, Shmuel S.}, 
journal = {Journal of Regulatory Economics}, 
issn = {0922680X}, 
doi = {10.1007/s11149-015-9281-3}, 
abstract = {{The California Independent System Operator (CAISO) has implemented Convergence Bidding (CB) on February 1, 2011 under Federal Energy Regulatory Commission’s September 21, 2006 Market Redesign and Technology Upgrade Order. CB is a financial mechanism that allows market participants, including electricity suppliers, consumers and virtual traders, to arbitrage price differences between the day-ahead (DA) market and the real-time (RT) market without physically consuming or producing energy. In this paper, market efficiency is defined in terms of trading profitability, where a zero-profit competitive equilibrium implies market efficiency (Jensen in, J Financial Econ 6(2):95–101, 1978). We analyze market data in the CAISO electric power markets, and empirically test for market efficiency by assessing the performance of trading strategies from the perspective of virtual traders. By viewing DA–RT spreads as payoffs from a basket of correlated assets, we can formulate a chance constrained portfolio selection problem, where the chance constraint takes two different forms as a value-at-risk constraint and a conditional value-at-risk constraint, to find the optimal trading strategy. A hidden Markov model (HMM) is further proposed to capture the presence of the time-varying forward premium. This is meant to be a contribution to the modeling of regime shifts in the electricity forward premium with unobservable states. Our backtesting results cast doubt on the efficiency of the CAISO electric power markets, as the trading strategy generates consistent profits after the introduction of CB, even in the presence of transaction costs. Nevertheless, by comparing with the performance before the introduction of CB, we find that the profitability decreases significantly, which enables us to identify the efficiency gain brought about by CB. Convincing evidence for the improvement of market efficiency in the presence of CB is further provided by the test for the Bessembinder and Lemmon (J Finance 57(3):1347–1382, 2002) model. © 2015, Springer Science+Business Media New York.}}, 
pages = {245--284}, 
number = {3}, 
volume = {48}
}
@article{10.1007/978-3-319-01595-8_30, 
year = {2014}, 
title = {{Value-at-risk backtesting procedures based on loss functions: Simulation analysis of the power of tests}}, 
author = {Piontek, Krzysztof}, 
journal = {Studies in Classification, Data Analysis, and Knowledge Organization}, 
issn = {14318814}, 
doi = {10.1007/978-3-319-01595-8\_30}, 
abstract = {{The definition of Value at Risk is quite general. There are many approaches that may lead to various VaR values. Backtesting is a necessary statistical procedure to test VaR models and select the best one. There are a lot of techniques for validating VaR models. Usually risk managers are not concerned about their statistical power. The goal of this paper is to compare statistical power of specific backtest procedures but also to examine the problem of limited data sets (observed in practice). A loss function approach is usually used to rank correct VaR models, but it is also possible to evaluate VaR models by using that approach. This paper presents the idea of loss functions and compares the statistical power of backtests based on a various loss functionswith the Kupiec and Berkowitz approach. Simulated data representing asset returns are used here. This paper is a continuation of earlier pieces of research done by the author. © Springer International Publishing Switzerland 2014.}}, 
pages = {273--281}, 
number = {NA}, 
volume = {47}
}
@article{10.1080/1331677x.2010.11517402, 
year = {2010}, 
title = {{Optimisation of decay factor in time weighted (BRW) simulation: Implications for var performance in mediterranean countries}}, 
author = {Žiković, Saša and Prohaska, Zdenko}, 
journal = {Economic Research-Ekonomska Istraživanja}, 
issn = {1331677X}, 
doi = {10.1080/1331677x.2010.11517402}, 
abstract = {{In this paper we propose an optimisation approach to determining the optimal decay factor in time weighted (BRW) simulation. Testing of BRW simulation with different decay factors and competing VaR models is performed on a sample of nine Mediterranean countries, over a four year period that includes the ongoing financial crisis. After optimisation the BRW simulation is among the best performing tested VaR models, second only to EVT approaches. Optimising the decay factor in regards to Lopez function results in decay factor estimates that are higher than usually employed 0.97 and 0.99. The optimal decay factors are stable over time and provide significantly better backtesting results than the standard assumptions.}}, 
pages = {73--85}, 
number = {1}, 
volume = {23}
}
@article{10.1016/j.jbankfin.2007.09.024, 
year = {2008}, 
title = {{An econometric analysis of emission allowance prices}}, 
author = {Paolella, Marc S. and Taschini, Luca}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2007.09.024}, 
abstract = {{Knowledge of the statistical distribution of the prices of emission allowances, and their forecastability, are crucial in constructing, among other things, purchasing and risk management strategies in the emissions-constrained markets. This paper analyzes the two emission permits markets, CO2 in Europe, and SO2 in the US, and investigates a model for dealing with the unique stylized facts of this type of data. Its effectiveness in terms of model fit and out-of-sample value-at-risk-forecasting, as compared to models commonly used in risk-forecasting contexts, is demonstrated. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {2022--2032}, 
number = {10}, 
volume = {32}
}
@article{10.1007/978-3-030-79606-8_13, 
year = {2022}, 
title = {{Predicting and Assessing Road Accidents Using Autoregressive Model and Value at Risk Approach}}, 
author = {Roslan, Teh Raihana Nazirah and Ch’ng, Chee Keong and Misiran, Masnita and Phewchean, Nattakorn}, 
issn = {21984182}, 
doi = {10.1007/978-3-030-79606-8\_13}, 
abstract = {{Road accidents have claimed many lives with approximately 1.35 million deaths worldwide and deemed as a critical issue in most countries. Understanding the common factors that contribute to road accidents is not enough, and it is essential to assess the risk involved to prepare for precautionary actions. In this study, we utilize the autoregressive model and value at risk approach in predicting Malaysian road accidents occurrences and assessing the respective risk. We construct a current risk analysis theoretical framework pertain to the vehicle’s condition, forecast and investigate the relationship between the involved variables, and obtain the value at risk for road accidents. From our findings, the road accidents will increase 1.12\% higher than the year 2019, along with a 2.77\% increase in the number of transportations in 2020. In addition, there is a 95\% confidence that in year 2020, the number of road accidents will reduce not more than 3.81\%. Coherent from the analysis, there is a potential to adopt these approaches more extensively for this issue as the quantitative analysis are feasible. In addition, a strong positive relationship is found between the likelihood of road accidents and the number of transportations. Thus, prediction of road accidents and identification of its value at risk on yearly basis are beneficial to project the best course of action to deal with road accidents occurrences in the country. © 2022, Institute of Technology PETRONAS Sdn Bhd.}}, 
pages = {163--175}, 
number = {NA}, 
volume = {383}
}
@article{10.1080/00949655.2015.1123262, 
year = {2016}, 
title = {{On double hysteretic heteroskedastic model}}, 
author = {Chen, Cathy W.S. and Truong, Buu-Chau}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2015.1123262}, 
abstract = {{This paper proposes a hysteretic autoregressive model with GARCH specification and a skew Student's t-error distribution for financial time series. With an integrated hysteresis zone, this model allows both the conditional mean and conditional volatility switching in a regime to be delayed when the hysteresis variable lies in a hysteresis zone. We perform Bayesian estimation via an adaptive Markov Chain Monte Carlo sampling scheme. The proposed Bayesian method allows simultaneous inferences for all unknown parameters, including threshold values and a delay parameter. To implement model selection, we propose a numerical approximation of the marginal likelihoods to posterior odds. The proposed methodology is illustrated using simulation studies and two major Asia stock basis series. We conduct a model comparison for variant hysteresis and threshold GARCH models based on the posterior odds ratios, finding strong evidence of the hysteretic effect and some asymmetric heavy-tailness. Versus multi-regime threshold GARCH models, this new collection of models is more suitable to describe real data sets. Finally, we employ Bayesian forecasting methods in a Value-at-Risk study of the return series. © 2015 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {2684--2705}, 
number = {13}, 
volume = {86}
}
@article{10.1080/14697688.2011.642810, 
year = {2015}, 
title = {{A model of returns for the post-credit-crunch reality: hybrid Brownian motion with price feedback}}, 
author = {Shaw, William T. and Schofield, Marcus}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2011.642810}, 
abstract = {{Recent market events have reinvigorated the search for realistic return models that capture greater likelihoods of extreme movements. In this paper we model the medium-term log-return dynamics in a market with both fundamental and technical traders. This is based on a trade arrival model with variable size orders and a general arrival-time distribution. With simplifications we are led in the jump-free case to a local volatility model defined by a hybrid SDE mixing both arithmetic and geometric or CIR Brownian motions, whose solution in the geometric case is given by a class of integrals of exponentials of one Brownian motion against another, in forms considered by Yor and collaborators. The reduction of the hybrid SDE to a single Brownian motion leads to an SDE of the form considered by Nagahara, which is a type of ‘Pearson diffusion’, or, equivalently, a hyperbolic OU SDE. Various dynamics and equilibria are possible depending on the balance of trades. Under mean-reverting circumstances we arrive naturally at an equilibrium fat-tailed return distribution with a Student or Pearson Type\textbackslashtextasciitildeIV form. Under less-restrictive assumptions, richer dynamics are possible, including time-dependent Johnson-SU distributions and bimodal structures. The phenomenon of variance explosion is identified that gives rise to much larger price movements that might have a priori been expected, so that ‘25σ’ events are significantly more probable. We exhibit simple example solutions of the Fokker–Planck equation that shows how such variance explosion can hide beneath a standard Gaussian facade. These are elementary members of an extended class of distributions with a rich and varied structure, capable of describing a wide range of market behaviors. Several approaches to the density function are possible, and an example of the computation of a hyperbolic VaR is given. The model also suggests generalizations of the Bougerol identity. We touch briefly on the extent to which such a model is consistent with the dynamics of a ‘flash-crash’ event, and briefly explore the statistical evidence for our model. © 2012 Taylor \& Francis.}}, 
pages = {975--998}, 
number = {6}, 
volume = {15}
}
@article{10.1109/iclsim.2010.5461453, 
year = {2010}, 
title = {{The risk control of pledge liquidation in logistics finance based on VaR methodology}}, 
author = {Yongming, Pan and Feifei, Yan}, 
journal = {2010 International Conference on Logistics Systems and Intelligent Management (ICLSIM)}, 
issn = {NA}, 
doi = {10.1109/iclsim.2010.5461453}, 
abstract = {{The ability of pledge liquidation is essential to logistics finance risk control. In the process of evaluating the ability of pledge liquidation, the price stabilization of pledge is what should be mostly considered. VaR (Value at Risk) methodology is an effective measuring method to determine risks in the field of financial risk management. Quantitative analysis of risk degree caused by price fluctuation was made with that method. It can estimate the maximum possible loss in the future, determine an appropriate line of credit and thus could be used as a reference for risk prediction and prevention when banks provide logistics finance service. ©2010 IEEE.}}, 
pages = {135--138}, 
number = {NA}, 
volume = {1}
}
@article{10.1080/15598608.2011.10412039, 
year = {2011}, 
title = {{Using dynamic copulae for modeling dependency in currency denominations of a diversified world stock index}}, 
author = {Ignatieva, Katja and Platen, Eckhard and Rendek, Renata}, 
journal = {Journal of Statistical Theory and Practice}, 
issn = {15598608}, 
doi = {10.1080/15598608.2011.10412039}, 
abstract = {{The aim of this paper is to model the dependency among log-returns when security account prices are expressed in units of a well diversified world stock index. The dependency in log-returns of currency denominations of the index is modeled using time-varying copulae, aiming to identify the best fitting copula family. The Student-t copula turns generally out to be superior to e.g. the Gaussian copula, where the dependence structure relates to the multivariate normal distribution. It is shown that merely changing the distributional assumption for the log-returns of the marginals from normal to Student-t leads to a significantly better fit. The Student-5 copula with Student-t marginals is able to better capture dependent extreme values than the other models considered. Furthermore, the paper applies copulae to the estimation of the Value-at-Risk and the expected shortfall of a portfolio constructed of savings accounts of different currencies. The proposed copula-based approach allows to split market risk into general and specific market risk, as defined in regulatory documents. The paper demonstrates that the approach performs clearly better than the RiskMetrics approach, a widely used methodology for Value-at-Risk estimation. © 2011 Copyright Taylor and Francis Group, LLC.}}, 
pages = {425--452}, 
number = {3}, 
volume = {5}
}
@article{10.1016/j.ijforecast.2003.09.005, 
year = {2004}, 
title = {{Extreme value theory and Value-at-Risk: Relative performance in emerging markets}}, 
author = {Gençay, Ramazan and Selçuk, Faruk}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2003.09.005}, 
abstract = {{In this paper, we investigate the relative performance of Value-at-Risk (VaR) models with the daily stock market returns of nine different emerging markets. In addition to well-known modeling approaches, such as variance-covariance method and historical simulation, we study the extreme value theory (EVT) to generate VaR estimates and provide the tail forecasts of daily returns at the 0.999 percentile along with 95\% confidence intervals for stress testing purposes. The results indicate that EVT-based VaR estimates are more accurate at higher quantiles. According to estimated Generalized Pareto Distribution parameters, certain moments of the return distributions do not exist in some countries. In addition, the daily return distributions have different moment properties at their right and left tails. Therefore, risk and reward are not equally likely in these economies. © 2004 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.}}, 
pages = {287--303}, 
number = {2}, 
volume = {20}
}
@article{10.1016/j.ribaf.2011.03.003, 
year = {2011}, 
title = {{Do ETFs provide effective international diversification?}}, 
author = {Huang, Mei-Yueh and Lin, Jun-Biao}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2011.03.003}, 
abstract = {{Global investments have been a hot issue for years. Investors can diversify risks and obtain benefits from foreign markets by investing directly in the foreign security market or indirectly in Exchange-Trade Funds (ETFs). Because direct investments are not always feasible, we investigate whether indirect investments can replace direct investments. We create different regional optimal portfolios containing ETFs and ensure optimal asset portfolio allocation. In addition to mean-variance approach, the Sharpe index, we also adopt the Campbell et al. (2001) method to have the efficient frontier under control risks, the Value at Risk. We apply both normal and non-normal distributions for comparisons and find that different assumptions of return distributions affect the results of efficient frontier. The results show that international diversification is a reasonable strategy. In addition, when comparing ETFs and target market index portfolios, ETFs have higher Sharpe measures than target market indices especially in the emerging markets. However, there are no significant performance differences between direct and indirect methods even if we use different performance measures. We also find that the diversification benefits are the same before and after the Subprime crisis. We conclude that it is effective for investors to use indirect methods to create internationally diversified portfolios. © 2011 Elsevier B.V.}}, 
pages = {335--344}, 
number = {3}, 
volume = {25}
}
@article{10.1016/j.jeconom.2010.06.003, 
year = {2010}, 
title = {{Specification tests of parametric dynamic conditional quantiles}}, 
author = {Escanciano, Juan Carlos and Velasco, Carlos}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2010.06.003}, 
abstract = {{This article proposes omnibus specification tests of parametric dynamic quantile models. In contrast to the existing procedures, we allow for a flexible specification, where a possible continuum of quantiles is simultaneously specified under fairly weak conditions on the serial dependence in the underlying data-generating process. Since the null limit distribution of tests is not pivotal, we propose a subsampling approximation of the asymptotic critical values. A Monte Carlo study shows that the asymptotic results provide good approximations for small sample sizes. Finally, an application suggests that our methodology is a powerful alternative to standard backtesting procedures in evaluating market risk. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {209--221}, 
number = {1}, 
volume = {159}
}
@article{10.1016/j.amc.2004.10.004, 
year = {2005}, 
title = {{An approach to VaR for capital markets with Gaussian mixture}}, 
author = {Zhang, Ming-Heng and Cheng, Qian-Sheng}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2004.10.004}, 
abstract = {{An approach to VaR (value-at-risk) for capital markets is proposed with Gaussian mixture. Considering the impacts of the components in a Gaussian mixture, an approach to VaR for capital markets is proposed to describe risk structure in capital markets. This approach can be programmed in parallel. Empirical computation of VaR for China securities markets and the Forex markets are provided to demonstrate the proposed method. © 2004 Published by Elsevier Inc.}}, 
pages = {1079--1085}, 
number = {2}, 
volume = {168}
}
@article{10.1016/j.intfin.2017.06.002, 
year = {2018}, 
title = {{More accurate measurement for enhanced controls: VaR vs ES?}}, 
author = {Guegan, Dominique and Hassani, Bertrand K.}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2017.06.002}, 
abstract = {{This paper (this work was achieved through the Laboratory of Excellence on Financial Regulation (Labex ReFi) supported by PRES heSam under the reference ANR-10-LABEX-0095) analyses how risks are measured in financial institutions, for instance Market, Credit, Operational, among others with respect to the choice of risk measures, the choice of distributions used to model them and the level of confidence selected. We discuss and illustrate the characteristics, the paradoxes and the issues observed, comparing the Value-at-Risk and the Expected Shortfall in practice. This paper is built as a differential diagnosis and aims at discussing the reliability of the risk measures and making some recommendations. (This paper has been written in a very particular period of time as most regulatory papers written in the past 20 years are currently being questioned by both practitioners and regulators themselves. Some disarray has been observed among risk managers as most models required by the regulation have not been consistent with their own objective of risk management. The enlightenment brought by this paper is based on an academic analysis of the issues engendered by some pieces of regulation and its purpose is not to create any sort of controversy.) © 2017 Elsevier B.V.}}, 
pages = {152--165}, 
number = {NA}, 
volume = {54}
}
@article{10.3934/jimo.2020103, 
year = {2021}, 
title = {{Optimal Reinsurance with Default Risk: A Reinsurer’s Perspective}}, 
author = {Chen, Tao and Liu, Wei and Tan, Tao and Wu, Lijun and Hu, Yijun}, 
journal = {Journal of Industrial \& Management Optimization}, 
issn = {15475816}, 
doi = {10.3934/jimo.2020103}, 
abstract = {{In this paper, we study the optimal reinsurance design with default risk by minimizing the VaR (value at risk) of the reinsurer’s total risk exposure. The optimal reinsurance treaty is provided. When the reinsurance premium principle is specified to the expected value and exponential premium princi-ples, the explicit expressions for the optimal reinsurance treaties are given, respectively. © 2021. All rights reserved}}, 
pages = {2971}, 
number = {5}, 
volume = {17}
}
@article{10.1080/03085140410001677120, 
year = {2004}, 
title = {{Repoliticizing financial risk}}, 
author = {Goede, Marieke de}, 
journal = {Economy and Society}, 
issn = {03085147}, 
doi = {10.1080/03085140410001677120}, 
abstract = {{This article aims to repoliticize modern financial risk management by offering a genealogical reading of its contested religious and cultural history. While identifying, calculating, and selling risk is at the heart of the modern financial markets, the normative commitments of modern financial risk management remain both hidden from view and politically unquestioned. The article argues that the commercialization of risk in finance should not be understood as a reaction to objectively existing danger, but as a profitable cultural process that rests upon gendered constructions of danger and security. The article examines the role of risk in the recent proposals for a new Capital Accord by the Basle Committee for Banking Supervision, and argues that the that the financial industry is reluctant to accept domains of incalculability in the new Accord. © Copyright 2004 Taylor \& Francis Ltd.}}, 
pages = {197--217}, 
number = {2}, 
volume = {33}
}
@article{10.1007/978-3-030-57524-3_2, 
year = {2020}, 
title = {{Bivariate risk measures and stochastic orders}}, 
author = {Yoshida, Yuji}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-030-57524-3\_2}, 
abstract = {{Bivariate value-at-risks and bivariate average value-at-risks are defined with copula functions, and stochastic orders are induced from these bivariate value-at-risks and bivariate average value-at-risks. Value-at-risk order is almost equivalent to the first-order stochastic dominance, however it is made clear that average value-at-risk order is weaker than the second-order stochastic dominance and it has similar properties. We find that average value-at-risk order is an important criterion as a stochastic order in multi-object case-based reasoning with risk aversity. © Springer Nature Switzerland AG 2020.}}, 
pages = {16--27}, 
number = {NA}, 
volume = {12256 LNAI}
}
@article{10.1080/14697688.2020.1786151, 
year = {2021}, 
title = {{TERES: Tail Event Risk Expectile Shortfall}}, 
author = {Mihoci, Andrija and Härdle, Wolfgang Karl and Chen, Cathy Yi-Hsuan}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2020.1786151}, 
abstract = {{We propose a generalized risk measure for expectile-based expected shortfall estimation. The generalization is designed with a mixture of Gaussian and Laplace densities. Our plug-in estimator is derived from an analytic relationship between expectiles and expected shortfall. We investigate the sensitivity and robustness of the expected shortfall to the underlying mixture parameter specification and the risk level. Empirical results from the US, German and UK stock markets and for selected NASDAQ blue chip companies indicate that expected shortfall can be successfully estimated using the proposed method on a monthly, weekly, daily and intra-day basis using a 1-year or 1-day time horizon across different risk levels. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--12}, 
number = {3}, 
volume = {21}
}
@article{10.1007/s11390-010-9330-4, 
year = {2010}, 
title = {{Towards risk evaluation of denial-of-service vulnerabilities in security protocols}}, 
author = {Cao, Zhen and Guan, Zhi and Chen, Zhong and Hu, Jian-Bin and Tang, Li-Yong}, 
journal = {Journal of Computer Science and Technology}, 
issn = {10009000}, 
doi = {10.1007/s11390-010-9330-4}, 
abstract = {{Denial-of-Service (DoS) attacks are virulent to both computer and networked systems. Modeling and evaluating DoS attacks are very important issues to networked systems; they provide both mathematical foundations and theoretic guidelines to security system design. As defense against DoS has been built more and more into security protocols, this paper studies how to evaluate the risk of DoS in security protocols. First, we build a formal framework to model protocol operations and attacker capabilities. Then we propose an economic model for the risk evaluation. By characterizing the intruder capability with a probability model, our risk evaluation model specifies the "Value-at- Risk" (VaR) for the security protocols. The "Value-at-Risk" represents how much computing resources are expected to lose with a given level of confidence. The proposed model can help users to have a better understanding of the protocols they are using, and in the meantime help designers to examine their designs and get clues of improvement. Finally we apply the proposed model to analyze a key agreement protocol used in sensor networks and identify a DoS flaw there, and we also validate the applicability and effectiveness of our risk evaluation model by applying it to analyze and compare two public key authentication protocols. © 2010 Springer.}}, 
pages = {375--387}, 
number = {2}, 
volume = {25}
}
@article{10.1145/1543834.1543979, 
year = {2009}, 
title = {{Log-optimal portfolio models with risk control of vaR and cVaR using genetic algorithms}}, 
author = {Qin, Sen}, 
journal = {Proceedings of the first ACM/SIGEVO Summit on Genetic and Evolutionary Computation - GEC '09}, 
issn = {NA}, 
doi = {10.1145/1543834.1543979}, 
abstract = {{Value-at-risk (VaR) and conditional value-at-risk (CVaR) have become two very popular measures of market risk during the last decade. Log-optimal portfolio problem with risk control of VaR and CVaR is put forward firstly. Then, we propose the portfolio models with VaR and CVaR and prove the existence and uniqueness of the optimal solutions of these two models. We provide a newly genetic algorithm based on real-code strings of assets' returns to overcome the problem of local optima. Finally, an empirical study is carried out to illustrate the optimal solutions of the log-optimal portfolio models with VaR and CVaR. The numeric results indicate that the optimal portfolio of the log-optimal portfolio model with CVaR gives a balance between the investment risk and the return simultaneously, and is more effective than the corresponding portfolios of the VaR model and the mean-variance model. Copyright 2009 ACM.}}, 
pages = {941--944}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/sef-05-2015-0139, 
year = {2017}, 
title = {{Improved VaR forecasts using extreme value theory with the Realized GARCH model}}, 
author = {Paul, Samit and Sharma, Prateek}, 
journal = {Studies in Economics and Finance}, 
issn = {10867376}, 
doi = {10.1108/sef-05-2015-0139}, 
abstract = {{Purpose: This study aims to forecast daily value-at-risk (VaR) for international stock indices by using the conditional extreme value theory (EVT) with the Realized GARCH (RGARCH) model. The predictive ability of this Realized GARCH-EVT (RG-EVT) model is compared with those of the standalone GARCH models and the conditional EVT specifications with standard GARCH models. Design/methodology/approach: The authors use daily data on returns and realized volatilities for 13 international stock indices for the period from 1 January 2003 to 8 October 2014. One-step-ahead VaR forecasts are generated using six forecasting models: GARCH, EGARCH, RGARCH, GARCH-EVT, EGARCH-EVT and RG-EVT. The EVT models are implemented using the two-stage conditional EVT framework of McNeil and Frey (2000). The forecasting performance is evaluated using multiple statistical tests to ensure the robustness of the results. Findings: The authors find that regardless of the choice of the GARCH model, the two-stage conditional EVT approach provides significantly better out-of-sample performance than the standalone GARCH model. The standalone RGARCH model does not perform better than the GARCH and EGARCH models. However, using the RGARCH model in the first stage of the conditional EVT approach leads to a significant improvement in the VaR forecasting performance. Overall, among the six forecasting models, the RG-EVT model provides the best forecasts of daily VaR. Originality/value: To the best of the authors’ knowledge, this is the earliest implementation of the RGARCH model within the conditional EVT framework. Additionally, the authors use a data set with a reasonably long sample period (around 11 years) in the context of high-frequency data-based forecasting studies. More significantly, the data set has a cross-sectional dimension that is rarely considered in the existing VaR forecasting literature. Therefore, the findings are likely to be widely applicable and are robust to the data snooping bias. © 2017, © Emerald Publishing Limited.}}, 
pages = {238--259}, 
number = {2}, 
volume = {34}
}
@article{10.1080/02102412.2010.10779687, 
year = {2010}, 
title = {{Several risk measures in portfolio selection: Is it worthwhile?}}, 
author = {Baixauli-Soler, J. Samuel and Alfaro-Cid, Eva and Fernández-Blanco, Matilde O.}, 
journal = {Spanish Journal of Finance and Accounting / Revista Española de Financiación y Contabilidad}, 
issn = {02102412}, 
doi = {10.1080/02102412.2010.10779687}, 
abstract = {{This paper is concerned with asset allocation using a set of three widely used risk measures, which are the variance or deviation, Value at Risk and the Conditional Value at Risk. Our purpose is to evaluate whether solving the asset allocation problem under several risk measures is worthwhile or not, given the added computational complexity. The main contribution of the paper is the solution of two models that consider several risk measures: the mean-variance-VaR model and the mean-VaR-CVaR model. The inclusion of Va R as one of the objectives to minimize leads to nonconvex problems, therefore the approach we propose is based on a heuristic: multi-objective genetic algorithms. Our results show the adequacy of the multi-objective approach for the portfolio optimization problem and emphasize the importance of dealing with mean-σ-VaR or mean-VaR-CVaR models as opposed to mean-σ-CVaR, where both risk measures are redundant.}}, 
pages = {421--444}, 
number = {147}, 
volume = {39}
}
@article{10.1007/s10713-006-0557-5, 
year = {2006}, 
title = {{Optimal insurance contract under a value-at-risk constraint}}, 
author = {Huang, Hung-Hsi}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1007/s10713-006-0557-5}, 
abstract = {{This study develops an optimal insurance contract endogenously under a value-at-risk (VaR) constraint. Although Wang et al. [2005] had examined this problem, their assumption implied that the insured is risk neutral. Consequently, this study extends Wang et al. [2005] and further considers a more realistic situation where the insured is risk averse. The study derives the optimal insurance contract as a single deductible insurance when the VaR constraint is redundant or as a double deductible insurance when the VaR constraint is binding. Finally, this study discusses the optimal coverage level from common forms of insurances, including deductible insurance, upper-limit insurance, and proportional coinsurance. © Springer Science + Business Media, LLC 2006.}}, 
pages = {91--110}, 
number = {2}, 
volume = {31}
}
@article{10.21314/jor.2015.303, 
year = {2015}, 
title = {{Historical simulation with component weight and ghosted scenarios}}, 
author = {Liu, Xinyi}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.303}, 
abstract = {{Historical simulation (HS) is a popular value-at-risk (VaR) approach that has the advantage of being intuitive and easy to implement. However, its response to most recent news has been too slow, its two “tails” (upper and lower) cannot learn from each other and it is not robust if there is insufficient data. In this paper, we put forward two strategies for improving HS in these weak areas with only minor additional computational costs. The first strategy is a “ghosted” scenario and the second is a two-component (short-run and long-run) exponentially weighted moving average scheme. The VaR is then calculated according to the empirical distribution of the two-component weighted real and ghosted scenarios. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--25}, 
number = {1}, 
volume = {18}
}
@article{10.1080/14697688.2019.1598568, 
year = {2019}, 
title = {{Simulation-based Value-at-Risk for nonlinear portfolios}}, 
author = {Chen, Junyao and Sit, Tony and Wong, Hoi Ying}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2019.1598568}, 
abstract = {{Value-at-risk (VaR) has been playing the role of a standard risk measure since its introduction. In practice, the delta-normal approach is usually adopted to approximate the VaR of portfolios with option positions. Its effectiveness, however, substantially diminishes when the portfolios concerned involve a high dimension of derivative positions with nonlinear payoffs; lack of closed form pricing solution for these potentially highly correlated, American-style derivatives further complicate the problem. This paper proposes a generic simulation-based algorithm for VaR estimation that can be easily applied to any existing procedures. Our proposal leverages cross-sectional information and applies variable selection techniques to simplify the existing simulation framework. Asymptotic properties of the new approach demonstrate faster convergence due to the additional model selection component introduced. We have also performed sets of numerical results that verify the effectiveness of our approach in comparison with some existing strategies. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--20}, 
number = {10}, 
volume = {19}
}
@article{10.1080/01446191003663264, 
year = {2010}, 
title = {{The risks of unbalanced bidding}}, 
author = {Cattell, David William and Bowen, Paul Anthony and Kaka, Ammar P.}, 
journal = {Construction Management and Economics}, 
issn = {01446193}, 
doi = {10.1080/01446191003663264}, 
abstract = {{Unbalanced bidding models have largely ignored the risk aspect of item pricing. Many researchers have acknowledged that there are considerable risks associated with unbalancing a bid but little has been done to describe these risks, let alone model them. A new framework is proposed by which all of these risks can be assessed. It identifies that these risks comprise the risk of rejection, the risk of reaction, and the risk of being wrong. It is further proposed that the value-at-risk ('VaR') method of measuring risk is a convenientay by which to combine all of these risks into one composite assessment. This quantified assessment serves to describe the extent of risk generated by each level of each item's price. Previous related research has proposed an unbalanced bidding model that has likewise provided a measurement of the expected reward generated by each level of each item's price. By doing a summation of these, keeping in mind that the prices applied to all of a project's component items must add up to the overall bid price, the contractor is able to assess both the risks as well as the rewards of all possible item price combinations. © 2010 Taylor \& Francis.}}, 
pages = {333--344}, 
number = {4}, 
volume = {28}
}
@article{10.1111/risa.12080, 
year = {2014}, 
title = {{Beyond value-at-risk: GlueVaR distortion risk measures}}, 
author = {Belles‐Sampera, Jaume and Guillén, Montserrat and Santolino, Miguel}, 
journal = {Risk Analysis}, 
issn = {02724332}, 
doi = {10.1111/risa.12080}, 
pmid = {23758120}, 
abstract = {{We propose a new family of risk measures, called GlueVaR, within the class of distortion risk measures. Analytical closed-form expressions are shown for the most frequently used distribution functions in financial and insurance applications. The relationship between GlueVaR, value-at-risk, and tail value-at-risk is explained. Tail subadditivity is investigated and it is shown that some GlueVaR risk measures satisfy this property. An interpretation in terms of risk attitudes is provided and a discussion is given on the applicability in nonfinancial problems such as health, safety, environmental, or catastrophic risk management. © 2013 Society for Risk Analysis.}}, 
pages = {121--134}, 
number = {1}, 
volume = {34}
}
@article{10.1016/j.ijpe.2013.06.007, 
year = {2013}, 
title = {{Newsvendor problem with random shortage cost under a risk criterion}}, 
author = {Wu, Meng and Zhu, Stuart X. and Teunter, Ruud H.}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2013.06.007}, 
abstract = {{We study profit maximization vs risk approaches for the standard newsvendor problem with uncertainty in demand as well as a generalized version with uncertainty in the shortage cost (as often applies in practice). We consider two well-known risk approaches: Value-at-Risk (VaR) included as a constraint and Conditional Value-at-Risk (CVaR). We first derive the explicit expressions of the optimal solution with uncertainty of shortage cost under different risk measures and then perform a numerical analysis to quantify the effect of uncertainty in shortage cost and risk measures. For the standard newsvendor problem, we find that the optimal quantity under CVaR is always lower than that under the VaR constraint, which in turn is lower than the order quantity that maximizes the expected profit. Insightful explanations for this result are that: (a) a higher degree of risk aversion drives the newsvendor to order fewer products, increasing the likelihood that all will be sold; (b) this effect is stronger for the CVaR approach as this does not consider the expected profit at all. Another interesting and counter-intuitive observation for the CVaR approach is that a higher retail price may lead to a smaller order quantity, as fewer items need to be sold in order to attain a sufficient profit. These results show that one should be careful in employing the CVaR risk measure for newsvendor type problems. The results remain valid if the shortage cost becomes uncertain. However, increased uncertainty of this type does improve the relative profitability under the CVaR approach by increasing the order quantity under that criterion whilst there is no effect under the other criteria. © 2013 Elsevier B.V. All rights reserved.}}, 
pages = {790--798}, 
number = {2}, 
volume = {145}
}
@article{10.4102/sajems.v21i1.1706, 
year = {2018}, 
title = {{Procyclicality in tradeable credit risk: Consequences for South Africa}}, 
author = {Visser, Dirk and Vuuren, Gary W Van}, 
journal = {South African Journal of Economic and Management Sciences}, 
issn = {10158812}, 
doi = {10.4102/sajems.v21i1.1706}, 
abstract = {{Background: Tradeable credit assets are vulnerable to two varieties of credit risk: default risk (which manifests itself as a binary outcome) and spread risk (which arises as spreads change continuously). Current (2017) regulatory credit risk rules require banks to hold capital for both these risks. Aggregating these capital amounts is non-trivial. Aim: The aim was to implement the bubble value at risk (buVaR) approach, proposed by Wong (2011) to overcome the risk aggregation problem. This method accounts for diversification and for procyclicality and operates by inflating the positive side of the underlying return distribution, in direct proportion to prevailing credit spread levels (usually liquid credit default swap spreads). Setting: The principal setting for the study was the South African credit market which represents a developing market. Previous work by Wong (2011) focussed only on developed markets. Methods: Using South African data, closed form solutions were derived for free parameters of Wong’s formulation, and the relationship between the spread level and the response function was developed and calibrated. Results: The results indicate that the original calibrations and assumptions made by Wong (2011) would result in excessive capital requirement for South African banks. Estimates obtained from this work suggest further calibration is required to cover the unique features of the South African milieu. Considerable differences compared with other markets were also found. Conclusion: The application of buVaR to South African government bond credit default swaps spreads highlighted the metric’s countercyclical properties that would potentially have countered bubble developments had they been implemented during the credit crisis of 2008/2009. Regulatory authorities should take this important metric into account when allocating South African bank’s credit risk capital. © 2018. The Authors.}}, 
number = {1}, 
volume = {21}
}
@article{10.1109/ictai.2010.48, 
year = {2010}, 
title = {{Robust value-at-risk optimization with interval random uncertainty set}}, 
author = {Chen, Wei and Tan, Shaohua}, 
journal = {2010 22nd IEEE International Conference on Tools with Artificial Intelligence}, 
issn = {10823409}, 
doi = {10.1109/ictai.2010.48}, 
abstract = {{This paper addresses a new uncertainty set - interval random uncertainty set for robust Value-at-Risk optimization. The form of interval random uncertainty set makes it suitable for capturing the downside and upside deviations of real-world data. These deviation measures capture distributional asymmetry and lead to better optimization results. We also apply our interval random chance-constrained programming to robust Value-at-Risk optimization under interval random uncertainty sets in the elements of mean vector. Numerical experiments with real market data indicate that our approach results in better portfolio performance. © 2010 IEEE.}}, 
pages = {281--286}, 
number = {NA}, 
volume = {1}
}
@article{10.1142/9789811202391_0072, 
year = {2020}, 
title = {{Non-parametric inference on risk measures for integrated returns}}, 
author = {Lee, Cheng Few and Lee, John C and Tsai, Henghsiu and Ho, Hwai-Chung and Chen, Hung-Yin}, 
issn = {NA}, 
doi = {10.1142/9789811202391\_0072}, 
abstract = {{When evaluating the market risk of long-horizon equity returns, it is always difficult to provide a statistically sound solution due to the limitation of the sample size. To solve the problem for the value-at-risk (VaR) and the conditional tail expectation (CTE), Ho et al. (2016, 2018) introduce a general multivariate stochastic volatility return model from which asymptotic formulas for the VaR and the CTE are derived for integrated returns with the length of integration increasing to infinity. Based on the formulas, simple non-parametric estimators for the two popular risk measures of the long-horizon returns are constructed. The estimates are easy to implement and shown to be consistent and asymptotically normal. In this chapter, we further address the issue of testing the equality of the CTEs of integrated returns. Extensive finite-sample analysis and real data analysis are conducted to demonstrate the efficiency of the test statistics we propose. © 2021 by World Scientific Publishing Co. Pte. Ltd.}}, 
pages = {2485--2497}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.iedeen.2017.05.001, 
year = {2017}, 
title = {{An application of extreme value theory in estimating liquidity risk}}, 
author = {Muela, Sonia Benito and Martín, Carmen López and Sanz, Raquel Arguedas}, 
journal = {European Research on Management and Business Economics}, 
issn = {24448834}, 
doi = {10.1016/j.iedeen.2017.05.001}, 
abstract = {{The last global financial crisis (2007–2008) has highlighted the weaknesses of value at risk (VaR) as a measure of market risk, as this metric by itself does not take liquidity risk into account. To address this problem, the academic literature has proposed incorporating liquidity risk into estimations of market risk by adding the VaR of the spread to the risk price. The parametric model is the standard approach used to estimate liquidity risk. As this approach does not generate reliable VaR estimates, we propose estimating liquidity risk using more sophisticated models based on extreme value theory (EVT). We find that the approach based on conditional extreme value theory outperforms the standard approach in terms of accurate VaR estimates and the market risk capital requirements of the Basel Capital Accord. © 2017 AEDEM}}, 
pages = {157--164}, 
number = {3}, 
volume = {23}
}
@article{10.1016/j.physa.2007.10.023, 
year = {2008}, 
title = {{Application of the Beck model to stock markets: Value-at-Risk and portfolio risk assessment}}, 
author = {Kozaki, M. and Sato, A.-H.}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2007.10.023}, 
abstract = {{We apply the Beck model, developed for turbulent systems that exhibit scaling properties, to stock markets. Our study reveals that the Beck model elucidates the properties of stock market returns and is applicable to practical use such as the Value-at-Risk estimation and the portfolio analysis. We perform empirical analysis with daily/intraday data of the S\&P500 index return and find that the volatility fluctuation of real markets is well-consistent with the assumptions of the Beck model: The volatility fluctuates at a much larger time scale than the return itself and the inverse of variance, or "inverse temperature", β obeys Γ-distribution. As predicted by the Beck model, the distribution of returns is well-fitted by q-Gaussian distribution of Tsallis statistics. The evaluation method of Value-at-Risk (VaR), one of the most significant indicators in risk management, is studied for q-Gaussian distribution. Our proposed method enables the VaR evaluation in consideration of tail risk, which is underestimated by the variance-covariance method. A framework of portfolio risk assessment under the existence of tail risk is considered. We propose a multi-asset model with a single volatility fluctuation shared by all assets, named the single β model, and empirically examine the agreement between the model and an imaginary portfolio with Dow Jones indices. It turns out that the single β model gives good approximation to portfolios composed of the assets with non-Gaussian and correlated returns. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {1225--1246}, 
number = {5-6}, 
volume = {387}
}
@article{10.1016/j.ijforecast.2013.07.014, 
year = {2014}, 
title = {{Evaluating the accuracy of value-at-risk forecasts: New multilevel tests}}, 
author = {Leccadito, Arturo and Boffelli, Simona and Urga, Giovanni}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2013.07.014}, 
abstract = {{We propose independence and conditional coverage tests which are aimed at evaluating the accuracy of Value-at-Risk (VaR) forecasts from the same model at different confidence levels. The proposed procedures are multilevel tests, i.e., joint tests of several quantiles corresponding to different confidence levels. In a comprehensive Monte Carlo exercise, we document the superiority of the proposed tests with respect to existing multilevel tests. In an empirical application, we illustrate the implementation of the tests using several VaR models and daily data for 15 MSCI world indices. © 2013 International Institute of Forecasters.}}, 
pages = {206--216}, 
number = {2}, 
volume = {30}
}
@article{10.1111/itor.12862, 
year = {2021}, 
title = {{A simheuristic algorithm for the stochastic permutation flow-shop problem with delivery dates and cumulative payoffs}}, 
author = {Villarinho, Pedro A. and Panadero, Javier and Pessoa, Luciana S. and Juan, Angel A. and Oliveira, Fernando L. Cyrino}, 
journal = {International Transactions in Operational Research}, 
issn = {09696016}, 
doi = {10.1111/itor.12862}, 
abstract = {{This paper analyzes the permutation flow-shop problem with delivery dates and cumulative payoffs (whenever these dates are met) under uncertainty conditions. In particular, the paper considers the realistic situation in which processing times are stochastic. The main goal is to find the permutation of jobs that maximizes the expected payoff. In order to achieve this goal, the paper first proposes a biased-randomized heuristic for the deterministic version of the problem. Then, this heuristic is extended into a metaheuristic by encapsulating it into a variable neighborhood descent framework. Finally, the metaheuristic is extended into a simheuristic by incorporating Monte Carlo simulations. According to the computational experiments, the level of uncertainty has a direct impact on the solutions provided by the simheuristic. Moreover, a risk analysis is performed using two well-known metrics: the value-at-risk and conditional value-at-risk. © 2020 The Authors. International Transactions in Operational Research © 2020 International Federation of Operational Research Societies}}, 
pages = {716--737}, 
number = {2}, 
volume = {28}
}
@article{10.1016/s0378-4266(03)00130-4, 
year = {2004}, 
title = {{The cyclical behavior of optimal bank capital}}, 
author = {Estrella, Arturo}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(03)00130-4}, 
abstract = {{This paper presents a dynamic model of optimal bank capital in which the bank optimizes over costs associated with failure, holding capital, and flows of external capital. The solution to the infinite-horizon stochastic optimization problem is related to period-by-period value at risk (var) in which the optimal probability of failure is endogenously determined. Over a cycle, var is positively correlated with optimal flows of external capital, but negatively correlated with optimal net changes in capital and the optimal level of total capital. Analysis of this pattern suggests that a regulatory minimum requirement based on var, if binding, is likely to be procyclical. The model points to several ways of reducing this problem. For example, a var-based requirement makes more sense if it is applied to external capital flows than if it is applied to the total level of capital. US commercial bank data since 1984 are generally consistent with the model. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {1469--1498}, 
number = {6}, 
volume = {28}
}
@article{10.1016/j.amc.2014.06.110, 
year = {2014}, 
title = {{Efficient VaR and Expected Shortfall computations for nonlinear portfolios within the delta-gamma approach}}, 
author = {Ortiz-Gracia, Luis and Oosterlee, Cornelis W.}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2014.06.110}, 
abstract = {{We present four numerical methods to compute the Value-at-Risk and Expected Shortfall risk measure values of portfolios with financial options. The numerical methods are based on either wavelets or Fourier cosine approximations and belong to the class of Fourier inversion methods. We show that the risk measures can be efficiently calculated in terms of accuracy and CPU time. Besides, we provide a theoretical result about the shape of the resulting probability density. This a priori knowledge, allows us to enhance the efficiency and effectiveness of the proposed methods. Finally, we assess the accuracy of the approach in the presence of convexity or concavity properties of the financial portfolios. © 2014 Elsevier Inc. All rights reserved.}}, 
pages = {16--31}, 
number = {NA}, 
volume = {244}
}
@article{10.1016/j.insmatheco.2013.11.009, 
year = {2014}, 
title = {{Capital requirements with defaultable securities}}, 
author = {Farkas, Walter and Koch-Medina, Pablo and Munari, Cosimo}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.11.009}, 
eprint = {1203.4610}, 
abstract = {{We study capital requirements for bounded financial positions defined as the minimum amount of capital to invest in a chosen eligible asset targeting a pre-specified acceptability test. We allow for general acceptance sets and general eligible assets, including defaultable bonds. Since the payoff of these assets is not necessarily bounded away from zero, the resulting risk measures cannot be transformed into cash-additive risk measures by a change of numéraire. However, extending the range of eligible assets is important because, as exemplified by the recent financial crisis, assuming the existence of default-free bonds may be unrealistic. We focus on finiteness and continuity properties of these general risk measures. As an application, we discuss capital requirements based on Value-at-Risk and Tail-Value-at-Risk acceptability, the two most important acceptability criteria in practice. Finally, we prove that there is no optimal choice of the eligible asset. Our results and our examples show that a theory of capital requirements allowing for general eligible assets is richer than the standard theory of cash-additive risk measures. © 2013 Elsevier B.V.}}, 
pages = {58--67}, 
number = {1}, 
volume = {55}
}
@article{10.1007/s10479-006-0123-7, 
year = {2007}, 
title = {{A semi-analytical method for VaR and credit exposure analysis}}, 
author = {Prisco, Ben De and Iscoe, Ian and Kreinin, Alexander and Nagi, Ahmed}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-006-0123-7}, 
abstract = {{In this paper, we discuss new analytical methods for computing Value-at-Risk (VaR) and a credit exposure profile. Using a Monte Carlo simulation approach as a benchmark, we find that the analytical methods are more accurate than RiskMetrics delta VaR, and are more efficient than Monte Carlo, for the case of fixed income securities. However the accuracy of the method deteriorates when applied to a portfolio of barrier options. © 2006 Springer Science+Business Media, LLC.}}, 
pages = {23--47}, 
number = {1}, 
volume = {152}
}
@article{10.1080/09603107.2011.631890, 
year = {2012}, 
title = {{The extreme-value dependence between the Chinese and other international stock markets}}, 
author = {Chen, Qian and Giles, David E. and Feng, Hui}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2011.631890}, 
abstract = {{Extreme Value Theory (EVT) measures the behaviour of extreme observations on a random variable. EVT in risk management, an approach to modelling and measuring risks under rare events, has taken on a prominent role in recent years. This article contributes to the literature in two respects by analysing an interesting international financial data set. First, we apply conditional EVT to examine the Value at Risk (VaR) and the Expected Shortfall (ES) for the Chinese and several representative international stock market indices: Hang Seng (Hong Kong), TSEC (Taiwan), Nikkei 225 (Japan), Kospi (Korea), BSE (India), STI (Singapore), S\&P 500 (US), SPTSE (Canada), IPC (Mexico), CAC 40 (France), DAX 30 (Germany), FTSE100 (UK) index. We find that China has the highest VaR and ES for negative daily stock returns. Second, we examine the extreme dependence between these stock markets, and we find that the Chinese market is asymptotically independent of the other stock markets considered. © 2012 Copyright Taylor and Francis Group, LLC.}}, 
pages = {1147--1160}, 
number = {14}, 
volume = {22}
}
@article{10.1109/cisti.2015.7170360, 
year = {2015}, 
title = {{Design of information system for the Liquidity Risk Management in financial institutions [Diseño de um Sistema de Información para la Gestión del Riesgo de Liquidez en las Entidades Financieras]}}, 
author = {Arias, Jaime Alberto Echeverri and Serna, Maria Andrea Arias and Gómez, Juan Guillermo Murillo and Kleine, Collin and Arbeláez, Luis Ceferino Franco}, 
journal = {2015 10th Iberian Conference on Information Systems and Technologies (CISTI)}, 
issn = {NA}, 
doi = {10.1109/cisti.2015.7170360}, 
abstract = {{The financial institutions face risk daily regarding disposition of liquid resources to meet the request from customers, such conflicts found in the information systems potential tools for right decisions. This work present the software architectural design and implementation of information system called Liquidity Risk Management (LRM) helping the collection measures of information in real time for proper risk liquidity management in financial institutions. The paper presents and describe the system architectural components, its functionalities and the implementation of different methods for risk value in a portfolio formed by four portfolios. © 2015 AISTI.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.qref.2010.06.006, 
year = {2010}, 
title = {{Value at risk models for volatile emerging markets equity portfolios}}, 
author = {Dimitrakopoulos, Dimitris N. and Kavussanos, Manolis G. and Spyrou, Spyros I.}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2010.06.006}, 
abstract = {{This paper investigates the issue of market risk quantification for emerging and developed market equity portfolios. A very wide spectrum of popular and widely used in practice Value at Risk (VaR) models are evaluated and compared with Extreme Value Theory (EVT) and adaptive filtered models, during normal, crises, and post-crises periods. The results are interesting and indicate that despite the documented differences between emerging and developed markets, the most successful VaR models are common for both asset classes. Furthermore, in the case of the (fatter tailed) emerging market equity portfolios, most VaR models turn out to yield conservative risk forecasts, in contrast to developed market equity portfolios, where most models underestimate the realized VaR. VaR estimation during periods of financial turmoil seems to be a difficult task, particularly in the case of emerging markets and especially for the higher loss quantiles. VaR models seem to be affected less by crises periods in the case of developed markets. The performance of the parametric (non-parametric) VaR models improves (deteriorates) during post-crises periods due to the inclusion of extreme events in the estimation sample. © 2010 The Board of Trustees of the University of Illinois.}}, 
pages = {515--526}, 
number = {4}, 
volume = {50}
}
@article{10.1007/978-3-642-12990-2_28, 
year = {2010}, 
title = {{Fuzzy material procurement planning with value-at-risk}}, 
author = {Sun, Gao-Ji}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-3-642-12990-2\_28}, 
abstract = {{Based on credibility theory, this paper presents a class of two-stage fuzzy programming with value-at-risk (VaR) to deal with material procurement planning (MPP) problem. Since the MPP problem usually includes continuous fuzzy variable parameters with infinite supports, it is inherently an infinite-dimensional optimization problem that can rarely be solved directly. To overcome this difficultly, this paper introduces an approximation approach (AA) which can turn the infinite-dimensional optimization problem into a finite-dimensional optimization one. Furthermore, in order to solve the proposed MPP model, a hybrid algorithm is designed which combines AA, neural network (NN) and particle swarm optimization (PSO). Additionally, one numerical example is also presented to illustrate the effectiveness of the designed algorithm. © 2010 Springer-Verlag Berlin Heidelberg.}}, 
pages = {245--252}, 
number = {NA}, 
volume = {67 LNEE}
}
@article{10.1109/icife.2010.5609303, 
year = {2010}, 
title = {{A new method for estimating value at risk with EVT and HS}}, 
author = {xiaoping, Wang and xiangxian, Zhang and Hongjian, Qu}, 
journal = {2010 2nd IEEE International Conference on Information and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/icife.2010.5609303}, 
abstract = {{During the past decade, Value at Risk (VaR) has become one of most commonly used Tools in risk measurement. In this paper, We propose a new method for estimating VaR. Our approach combines HS method to estimate the interior and the extreme value theory to estimate the tails. This paper uses a sample of the returns of S\&P 500 daily closing index to test the performance of our VaR procedure. This approach is also backtested for financial data at different confidence level. The results of the backtesting indicate that the new approach is an adequate risk measure. © 2010 IEEE.}}, 
pages = {283--287}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.econmod.2019.09.023, 
year = {2020}, 
title = {{Assessing downside and upside risk spillovers across conventional and socially responsible stock markets}}, 
author = {Ameur, Hachmi Ben and Jawadi, Fredj and Jawadi, Nabila and Cheffou, Abdoulkarim Idi}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2019.09.023}, 
abstract = {{Since the aftermath of the recent global financial crisis, socially responsible (SR) investments have become an alternative form of conventional finance, giving rise to further systemic risk between conventional and SR stock markets. In this paper, we assess this risk transmission using Value at Risk (VaR) modeling for the US, Europe and the Asia-Pacific region, over the period covering January 2004–December 2016. We find that socially responsible stock markets exhibit less risk than do conventional markets in terms of the risk hedging properties induced by the SR screening. Second, contributions to systemic risk vary across market phases and return distribution levels, with a larger contribution and spillover effect during the recent global financial crisis. For example, at the downside of the distribution (CoVaR at 5\%), the conventional European index shows the highest contribution to the world market's systemic risk, while the US stock market shows the highest contribution at the upside of the distribution (CoVaR at 95\%). This finding is justified by the difference in the risk aversion of investors that varies with the market state as well as the disparities in the development of SR markets. © 2019 Elsevier B.V.}}, 
pages = {200--210}, 
number = {NA}, 
volume = {88}
}
@article{10.1177/1354816619828170, 
year = {2020}, 
title = {{Modelling tourism receipts and associated risks, using long-range dependence models}}, 
author = {Pérez-Rodríguez, Jorge V and Santana-Gallego, María}, 
journal = {Tourism Economics}, 
issn = {13548166}, 
doi = {10.1177/1354816619828170}, 
abstract = {{Tourism receipts have important policy implications for destination countries in terms of government revenues and the management of tourism-related policies. This article uses time series models to analyse the risk exposure reflected in the growth rates of tourism revenues. To do so, we apply risk management measures based on value-at-risk (VaR) and the expected shortfall (ES), analysing monthly data for six Spanish regions from January 2004 to March 2017. Two main results were obtained. Firstly, tourism receipt growth rates present negative long-range dependence. In other words, they have intermediate memory or anti-persistence and therefore show signs of dependence between widely separated observations. Moreover, we detected the existence of long-range dependence in these volatilities in one of the six regions considered. Secondly, we show that VaR based on Generalized Autoregressive Conditional Heteroscedasticity (GARCH)-type models is a valid means of analysing the risk exposure of tourism receipt growth rates, doing so by evaluating various in-sample and out-of-sample VaR thresholds and the ES. © The Author(s) 2019.}}, 
pages = {70--96}, 
number = {1}, 
volume = {26}
}
@article{10.1016/j.jebo.2016.10.012, 
year = {2016}, 
title = {{To debt or not to debt: Are Islamic banks less risky than conventional banks?}}, 
author = {Sorwar, Ghulam and Pappas, Vasileios and Pereira, John and Nurullah, Mohamed}, 
journal = {Journal of Economic Behavior \& Organization}, 
issn = {01672681}, 
doi = {10.1016/j.jebo.2016.10.012}, 
abstract = {{We empirically analyze the market risk profiles of Islamic banks with two sets of conventional banks taken from the same geographical locations as Islamic banks and from a random global sample respectively for the period 2000–2013. Moreover, we divided our sample period into pre-financial crisis, during financial and post financial crisis. Estimates of Value-at-Risk (VaR) and Expected Shortfall (ES) which incorporates losses beyond VaR are used as market risk measures for both univariate and multivariate portfolios. Our key input is the share price by market capitalization of publicly traded banks of similar size in Islamic and non-Islamic countries. Univariate analysis finds no discernible differences between Islamic and conventional banks. However, dynamic correlations obtained via a multivariate setting shows Islamic banks to be less riskier for both sets of conventional banks; and especially so during the recent global financial crisis. The policy implications are: (i) that the inclusion of Islamic banks within asset portfolios may mitigate potential risk; (ii) that the Basel committee should consider the ES measure of risk for Islamic banks in preference to the current VaR methodology, which over-estimates the market risk of Islamic banks. © 2016 Elsevier B.V.}}, 
pages = {113--126}, 
number = {NA}, 
volume = {132}
}
@article{10.1007/s10203-015-0167-8, 
year = {2015}, 
title = {{Computing the distribution of the sum of dependent random variables via overlapping hypercubes}}, 
author = {Galeotti, Marcello}, 
journal = {Decisions in Economics and Finance}, 
issn = {15938883}, 
doi = {10.1007/s10203-015-0167-8}, 
abstract = {{The original motivation of this work comes from a classic problem in finance and insurance: that of computing the Value-at-Risk (VaR) of a portfolio of dependent risky positions, i.e., the quantile at a certain level of confidence of the loss distribution. In fact, it is difficult to overestimate the importance of the concept of VaR in modern finance and insurance. It has been recommended, although with several warnings, as a measure of risk and the basis for capital requirement determination by both the guidelines of international committees (such as Basel 2 and 3 and Solvency 2) and the internal models adopted by major banks and insurance companies. However, the actual computation of the VaR of a portfolio constituted by several dependent risky assets is often a hard practical and theoretical task. To this purpose, here we prove the convergence of a geometric algorithm (alternative to Monte Carlo and quasi-Monte Carlo methods) for computing the Value-at-Risk of a portfolio of any dimension, i.e., the distribution of the sum of its components, which can exhibit any dependence structure. Moreover, although the original motivation is financial, our result has a relevant measure-theoretical meaning. What we prove, in fact, is that the H-measure of a d-dimensional simplex (for any \$\$d\textbackslashge 2\$\$d≥2 and any absolutely continuous with respect to Lebesgue measure H) can be approximated by convergent algebraic sums of H-measures of hypercubes (obtained through a self-similar construction). © 2015, Springer-Verlag Italia.}}, 
pages = {231--255}, 
number = {2}, 
volume = {38}
}
@article{10.1504/ijstl.2014.064904, 
year = {2014}, 
title = {{Revealing the freight market risk in Istfix shipping area}}, 
author = {Ünal, Gözde and Köseoğlu, Sinem Derindere}, 
journal = {International Journal of Shipping and Transport Logistics}, 
issn = {17566517}, 
doi = {10.1504/ijstl.2014.064904}, 
abstract = {{This study analyses the market risk of the freight rates of Istfix shipping area, where small coaster size of dry/general cargo mini bulkers ranging from 2,000 dwt to 12,000 dwt operate on Black Sea-Mediterranean-Continent routes. Having used VaR and expected shortfall as the market risk measures, we have found that the market risk is higher on longer routes in this region. However, the results do not suggest a significant relationship between the market risk of freight rates and that of coaster tonnages. Overall, the market risk in Istfix shipping area has been found to be much lower than in the international Baltic handysize index (BHSI), a proxy international index of relatively small sized vessels. The findings provide a better understanding of Istfix shipping area compared to BHSIfor tramp market participants and speculators in freight derivatives markets for hedging and speculative purposes in their investment strategies and financial decision making processes. © 2014 Inderscience Enterprises Ltd.}}, 
pages = {593}, 
number = {6}, 
volume = {6}
}
@article{10.1016/j.jeconom.2005.08.008, 
year = {2007}, 
title = {{Interval estimation of value-at-risk based on GARCH models with heavy-tailed innovations}}, 
author = {Chan, Ngai Hang and Deng, Shi-Jie and Peng, Liang and Xia, Zhendong}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2005.08.008}, 
abstract = {{ARCH and GARCH models are widely used to model financial market volatilities in risk management applications. Considering a GARCH model with heavy-tailed innovations, we characterize the limiting distribution of an estimator of the conditional value-at-risk (VaR), which corresponds to the extremal quantile of the conditional distribution of the GARCH process. We propose two methods, the normal approximation method and the data tilting method, for constructing confidence intervals for the conditional VaR estimator and assess their accuracies by simulation studies. Finally, we apply the proposed approach to an energy market data set. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {556--576}, 
number = {2}, 
volume = {137}
}
@article{10.3233/jifs-17508, 
year = {2018}, 
title = {{Optimal decision for a fuzzy supply chain with shrinkage under VaR criterion}}, 
author = {Song, Qingfeng and Shi, Kai and Lin, Sheng and Xu, Guangping and Yang, Oliver and Wang, Jinsong}, 
journal = {Journal of Intelligent \& Fuzzy Systems}, 
issn = {10641246}, 
doi = {10.3233/jifs-17508}, 
abstract = {{This paper investigates the problems of making optimal decisions on pricing and shelf-space for a fuzzy supply chain with one perishable product, with the help of Radio Frequency Identification Device (RFID) technology to reduce shrinkage. Based on the criterion of Value-at-Risk (VaR) and its minimization, we introduce one centralized decision model and three decentralized decision models to obtain the respective optimal decisions of both the manufacturer and the retailer, by analyzing the fuzzy uncertainties and the relationship among the demand, retail price and shelf-space. The corresponding optimal strategies of the two participants are obtained along with one example. © 2018 - IOS Press and the authors. All rights reserved.}}, 
pages = {1--12}, 
number = {1}, 
volume = {34}
}
@article{10.1080/03461238.2020.1867631, 
year = {2021}, 
title = {{Bowley reinsurance with asymmetric information on the insurer's risk preferences}}, 
author = {Boonen, Tim J. and Cheung, Ka Chun and Zhang, Yiying}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2020.1867631}, 
abstract = {{The Bowley solution refers to the optimal pricing density for the reinsurer and optimal ceded loss for the insurer when there is a monopolistic reinsurer. In a sequential game, the reinsurer first sets the pricing kernel, and thereafter the insurer selects the reinsurance contract given the pricing kernel. In this article, we study Bowley solutions under asymmetric information on the insurer's risk preferences where the identity of the insurer is unknown to the reinsurer. By assuming that the insurer adopts a Value-at-Risk measure or a convex distortion risk measure, the optimal pricing kernel for the insurer and the optimal ceded loss function for the reinsurer are determined. Numerical examples are presented to illustrate the results. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--22}, 
number = {7}, 
volume = {2021}
}
@article{10.1016/j.jempfin.2014.10.001, 
year = {2014}, 
title = {{High-order moments and extreme value approach for value-at-risk}}, 
author = {Lin, Chu-Hsiung and Changchien, Chang-Cheng and Kao, Tzu-Chuan and Kao, Wei-Shun}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2014.10.001}, 
abstract = {{We modify a two-step approach by McNeil and Frey (2000) for forecasting Value-at-Risk (VaR). Our approach combines the asymmetric GARCH (GJR) model that allows the high-order moments (i.e., skewness and kurtosis) of the skewed generalized t (SGT) distribution to rely on the past information set to estimate volatility, and the modified Hill estimator (Huisman et al., 2001) for estimating the innovation distribution tail of the GJR model. Using back-testing of the daily return series of 10 stock markets, the empirical results show that our proposed approach could give better one-day VaR forecasts than McNeil and Frey (2000) and the GJR/GARCH models with alternative distributions. In addition, our proposed approach also provides the accuracy of expected shortfall estimates. The evidence demonstrates that our proposed two-step approach that incorporates the modified Hill estimator into the GJR model based on the SGT density with autoregressive conditional skewness and kurtosis provides consistently accurate VaR forecasts in the short and longer sample periods. © 2014 Elsevier B.V.}}, 
pages = {421--434}, 
number = {NA}, 
volume = {29}
}
@article{10.1016/j.physa.2005.01.031, 
year = {2005}, 
title = {{A risk hedging strategy under the nonparallel-shift yield curve}}, 
author = {Gong, Pu and He, Xubiao}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2005.01.031}, 
abstract = {{Under the assumption of the movement of rigid, a nonparallel-shift model in the term structure of interest rates is developed by introducing Fisher \& Weil duration which is a well-known concept in the area of interest risk management. This paper has studied the hedge and replication for portfolio immunization to minimize the risk exposure. Throughout the experiment of numerical simulation, the risk exposures of the portfolio under the different risk hedging strategies are quantitatively evaluated by the method of value at risk (VaR) order statistics (OS) estimation. The results show that the risk hedging strategy proposed in this paper is very effective for the interest risk management of the default-free bond. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {450--462}, 
number = {1-4}, 
volume = {354}
}
@article{10.1016/j.ecosta.2020.07.001, 
year = {2020}, 
title = {{A nonparametric copula approach to conditional Value-at-Risk}}, 
author = {Geenens, Gery and Dunn, Richard}, 
journal = {Econometrics and Statistics}, 
issn = {24523062}, 
doi = {10.1016/j.ecosta.2020.07.001}, 
abstract = {{Value-at-Risk and its conditional allegory, which takes into account the available information about the economic environment, form the centrepiece of the Basel framework for the evaluation of market risk in the banking sector. A new nonparametric framework for estimating this conditional Value-at-Risk is presented. A nonparametric approach is particularly pertinent as the traditionally used parametric distributions have been shown to be insufficiently robust and flexible in most of the equity-return data sets observed in practice. The method extracts the quantile of the conditional distribution of interest, whose estimation is based on a novel estimator of the density of the copula describing the dynamic dependence observed in the series of returns. Monte-Carlo simulations and real-world back-testing analyses demonstrate the potential of the approach, whose performance may be superior to its industry counterparts. © 2020 EcoSta Econometrics and Statistics}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1103/physreve.80.026131, 
year = {2009}, 
title = {{Improved risk estimation in multifractal records: Application to the value at risk in finance}}, 
author = {Bogachev, Mikhail I. and Bunde, Armin}, 
journal = {Physical Review E}, 
issn = {15393755}, 
doi = {10.1103/physreve.80.026131}, 
pmid = {19792224}, 
abstract = {{We suggest a risk estimation method for financial records that is based on the statistics of return intervals between events above/below a certain threshold Q and is particularly suited for multifractal records. The method is based on the knowledge of the probability WQ (t;Δt) that within the next Δt units of time at least one event above Q occurs, if the last event occurred t time units ago. We propose an analytical estimate of WQ and show explicitly that the proposed method is superior to the conventional precursory pattern recognition technique widely used in signal analysis, which requires considerable fine tuning and is difficult to implement. We also show that the estimation of the Value at Risk, which is a standard tool in finances, can be improved considerably by the method. © 2009 The American Physical Society.}}, 
pages = {026131}, 
number = {2}, 
volume = {80}
}
@article{10.1142/s2010139213500092, 
year = {2013}, 
title = {{Alleviating Coordination Problems and Regulatory Constraints Through Financial Risk Management}}, 
author = {Boyer, Marcel and Boyer, M Martin and Garcia, René}, 
journal = {Quarterly Journal of Finance}, 
issn = {20101392}, 
doi = {10.1142/s2010139213500092}, 
abstract = {{Seeing the firm as a nexus of activities and projects, we propose a characterization of the firm where variations in the market price of risk should induce adjustments in the firm's portfolio of projects. In a setting where managers disagree with respect to what investment maximizes value, changing the portfolio of projects generates coordination costs. We then propose a new role for financial risk management based on the idea that the use of financial derivatives reduces coordination costs by moving the organization's expected cash flows and risks toward a point where coordination in favor of real changes is easier to achieve. We find empirical support for this new rationale for the use of financial derivatives, after controlling for the traditional variables explaining the need for financial risk management. © 2013 World Scientific Publishing Company and Midwest Finance Association.}}, 
pages = {1350009}, 
number = {2}, 
volume = {3}
}
@article{10.1016/j.jeconom.2016.02.013, 
year = {2016}, 
title = {{TENET: Tail-Event driven NETwork risk}}, 
author = {Härdle, Wolfgang Karl and Wang, Weining and Yu, Lining}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2016.02.013}, 
abstract = {{CoVaR is a measure for systemic risk of the networked financial system conditional on institutions being under distress. The analysis of systemic risk is the focus of recent econometric analyses and uses tail event and network based techniques. Here, in this paper we bring tail event and network dynamics together into one context. In order to pursue such joint efforts, we propose a semiparametric measure to estimate systemic interconnectedness across financial institutions based on tail-driven spillover effects in a high dimensional framework. The systemically important institutions are identified conditional to their interconnectedness structure. Methodologically, a variable selection technique in a time series setting is applied in the context of a single-index model for a generalized quantile regression framework. We could thus include more financial institutions into the analysis to measure their tail event interdependencies and, at the same time, be sensitive to non-linear relationships between them. Network analysis, its behaviour and dynamics, allows us to characterize the role of each financial industry group in 2007-2012: the depositories received and transmitted more risk among other groups, the insurers were less affected by the financial crisis. The proposed TENET - Tail Event driven NETwork technique allows us to rank the Systemic Risk Receivers and Systemic Risk Emitters in the US financial market. © 2016 Elsevier B.V. All rights reserved.}}, 
pages = {499--513}, 
number = {2}, 
volume = {192}
}
@article{10.1016/j.jempfin.2009.11.004, 
year = {2010}, 
title = {{Backtesting value-at-risk based on tail losses}}, 
author = {Wong, Woon K.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2009.11.004}, 
abstract = {{Extreme losses caused by leverage and financial derivatives highlight the need to backtest Value-at-Risk (VaR) based on the sizes of tail losses, because the risk measure currently used disregards losses beyond the VaR boundary. While Basel II backtests VaR by counting the number of exceptions, this paper proposes to use the saddlepoint technique by summing the sizes of tail losses. Monte Carlo simulations show that the technique is extremely accurate and powerful, even for small samples. Empirical applications for the proposed backtest find substantial downside tail risks in S\&P 500, and demonstrate that risk models which account for jumps, skewed and fat-tailed distributions failed to capture the tail risk during the 1987 stock market crash. Finally, the saddlepoint technique is used to derive a multiplication factor for any risk capital requirement that is responsive to the sizes of tail losses. © 2009 Elsevier B.V.}}, 
pages = {526--538}, 
number = {3}, 
volume = {17}
}
@article{10.1002/for.1269, 
year = {2013}, 
title = {{International evidence on GFC-robust forecasts for risk management under the Basel Accord}}, 
author = {McAleer, Michael and Jiménez‐Martín, Juan‐Ángel and Pérez‐Amaral, Teodosio}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1269}, 
abstract = {{A risk management strategy designed to be robust to the global financial crisis (GFC), in the sense of selecting a valueat-risk (VaR) forecast that combines the forecasts of different VaR models, was proposed by McAleer and coworkers in 2010. The robust forecast is based on the median of the point VaR forecasts of a set of conditional volatility models. Such a risk management strategy is robust to the GFC in the sense that, while maintaining the same risk management strategy before, during and after a financial crisis, it will lead to comparatively low daily capital charges and violation penalties for the entire period. This paper presents evidence to support the claim that the median point forecast of VaR is generally GFC robust. We investigate the performance of a variety of single and combined VaR forecasts in terms of daily capital requirements and violation penalties under the Basel II Accord, as well as other criteria. In the empirical analysis we choose several major indexes, namely French CAC, German DAX, US Dow Jones, UK FTSE100, Hong Kong Hang Seng, Spanish Ibex 35, Japanese Nikkei, Swiss SMI and US S\&P 500. The GARCH, EGARCH, GJR and RiskMetrics models as well as several other strategies, are used in the comparison. Backtesting is performed on each of these indexes using the Basel II Accord regulations for 2008-10 to examine the performance of the median strategy in terms of the number of violations and daily capital charges, among other criteria. The median is shown to be a profitable and safe strategy for risk management, both in calm and turbulent periods, as it provides a reasonable number of violations and daily capital charges. The median also performs well when both total losses and the asymmetric linear tick loss function are considered. Copyright © 2013 John Wiley \& Sons, Ltd.}}, 
pages = {267--288}, 
number = {3}, 
volume = {32}
}
@article{10.1016/j.asoc.2010.02.004, 
year = {2011}, 
title = {{Two-stage fuzzy stochastic programming with Value-at-Risk criteria}}, 
author = {Wang, Shuming and Watada, Junzo}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2010.02.004}, 
abstract = {{A new class of fuzzy stochastic optimization models - two-stage fuzzy stochastic programming with Value-at-Risk (FSP-VaR) criteria is built in this paper. Some properties of the two-stage FSP-VaR, such as value of perfect information (VPI), value of fuzzy random solution (VFRS), and bounds of the fuzzy random solution, are discussed. An Approximation Algorithm is proposed to compute the VaR by combining discretization method of fuzzy variable, random simulation technique and bisection method. The convergence of the approximation algorithm is proved. To solve the two-stage FSP-VaR, a hybrid mutation-neighborhood-based particle swarm optimization (MN-PSO) which comprises the Approximation Algorithm is proposed to search for the approximate optimal solution. Furthermore, a neural network-based acceleration method is discussed. A numerical experiment illustrates the effectiveness of the proposed hybrid MN-PSO algorithm. The comparison shows that the hybrid MN-PSO exhibits better performance than the one when using other approaches such as hybrid PSO and GA. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {1044--1056}, 
number = {1}, 
volume = {11}
}
@article{10.11130/jei.2015.30.1.172, 
year = {2015}, 
title = {{Measuring risk of portfolio: GARCH-copula model}}, 
author = {Messaoud, Samia Ben and Aloui, Chaker}, 
journal = {Journal of Economic Integration}, 
issn = {1225651X}, 
doi = {10.11130/jei.2015.30.1.172}, 
abstract = {{In this paper, we use the copulas functions in financial application, namely to examine the assumption of asymmetric dependence and to calculate some measures of risk. The first step consists of deducing filtered residuals for each return series by an asymmetric Glosten-Jagannathan-Runkle Generalized Autoregressive conditional Hetero skedasticity (GJR-GARCH) model. For the second step, we use an estimation of a Generalized Pareto Distribution for the upper and lower tails to determine the empirical semiparametric marginal Cumulative Distribution Function. In our approach, we propose to use a portfolio consisting of increments from five countries. The GJR-GARCH copula is then applied to the data and used to reduce correlation between the simulated residuals of each series. The marginal distributions of filtered residuals are fitted with a semi-parametric Cumulative Distribution Function using the copulas’ functions and Generalized Pareto Distribution for tails. For each series, we compute Value-at-Risk and Conditional Value-at-Risk. © 2015-Center for Economic Integration, Sejong Institution, Sejong University, All Rights Reserved.}}, 
pages = {172--205}, 
number = {1}, 
volume = {30}
}
@article{10.3233/jifs-15982, 
year = {2017}, 
title = {{A VaR-based optimization model for crop production planning under imprecise uncertainty}}, 
author = {Yuan, Guoqiang and Tian, Yi and Wang, Shuming}, 
journal = {Journal of Intelligent \& Fuzzy Systems}, 
issn = {10641246}, 
doi = {10.3233/jifs-15982}, 
abstract = {{This paper introduces the fuzzy Value-at-Risk (VaR) into crop production planning and proposes a risk-based decision-making approach to the problem in imprecise or fuzzy parameters environments. In the proposed fuzzy crop production planning VaR model, the profit coefficients are imprecise uncertain and assumed to be fuzzy variables with known possibility distributions. Due to the fuzzy variable parameters with infinite supports, the VaR model is inherently an infinite-dimensional optimization problem that can rarely be solved directly via conventional mathematical programming methods. Therefore, algorithm procedures for solving this optimization problem must rely on approximation schemes and heuristic computing. The paper presents a heuristic algorithm, which integrates approximation approach (AA), neural network (NN) and genetic algorithm (GA), to solve the fuzzy crop production planning VaR model. Finally, a practical example is given to show the feasibility and effectiveness of the proposed model and heuristic algorithm. © 2017 - IOS Press and the authors. All rights reserved.}}, 
pages = {1--14}, 
number = {1}, 
volume = {33}
}
@article{10.1016/j.energy.2019.116516, 
year = {2020}, 
title = {{Risk-based optimal bidding patterns in the deregulated power market using extended Markowitz model}}, 
author = {Ostadi, Bakhtiar and Sedeh, Omid Motamedi and Kashan, Ali Husseinzadeh}, 
journal = {Energy}, 
issn = {03605442}, 
doi = {10.1016/j.energy.2019.116516}, 
abstract = {{Deregulation of power industry has entailed important changes in the energy market. With the power industry being restructured, a generation company (GenCo) sells energy through auctions in a daily market, and submission of the appropriate amount of electricity with the right bidding price is important for a GenCo to maximize their profits and minimize the acceptance risk. The objective of this paper is to propose a novel approach for determination of the optimal biding patterns among GenCos in the deregulated power market using a hybrid of Markowitz Model and Genetic Algorithm (GA). While Markowitz Model as an optimization model considers the risk premium for biding patterns and GA as a search engine, considering the acceptance risk in deregulated market. A case study is used to examine the findings of the proposed approach. Also, to compare the proposed model, neural network by back propagation learning algorithm and real proposed pattern were considered. The numerical results indicate that the proposed model is statistically efficient and offers effective curves and biding patterns by lesser risk and equal profitability in day-ahead market as it is able to achieve better results compared to the neural network. © 2019 Elsevier Ltd}}, 
pages = {116516}, 
number = {NA}, 
volume = {191}
}
@article{10.1007/s11424-014-1262-6, 
year = {2014}, 
title = {{Asymptotic results for over-dispersed operational risk by using the asymptotic expansion method}}, 
author = {Lu, Zhaoyang}, 
journal = {Journal of Systems Science and Complexity}, 
issn = {10096124}, 
doi = {10.1007/s11424-014-1262-6}, 
abstract = {{In this paper, the author considers a new Loss-distribution-approach model, in which the over-dispersed operational risks are modeled by the compound negative binomial process. In the single dimensional case, asymptotic expansion for the quantile of compound negative binomial process is explored for computing the capital charge of a bank for operational risk. Moreover, when the dependence structure between different risk cells is modeled by the Frank copula, this approach is extended to the multi-dimensional setting. A practical example is given to demonstrate the effectiveness of approximation results. © 2014, Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg.}}, 
pages = {524--536}, 
number = {3}, 
volume = {27}
}
@article{10.1108/jm2-07-2019-0178, 
year = {2021}, 
title = {{Multivariate portfolio optimization under illiquid market prospects: a review of theoretical algorithms and practical techniques for liquidity risk management}}, 
author = {Janabi, Mazin A M Al}, 
journal = {Journal of Modelling in Management}, 
issn = {17465664}, 
doi = {10.1108/jm2-07-2019-0178}, 
abstract = {{Purpose: This study aims to examine the theoretical foundations for multivariate portfolio optimization algorithms under illiquid market conditions. In this study, special emphasis is devoted to the application of a risk-engine, which is based on the contemporary concept of liquidity-adjusted value-at-risk (LVaR), to multivariate optimization of investment portfolios. Design/methodology/approach: This paper examines the modeling parameters of LVaR technique under event market settings and discusses how to integrate asset liquidity risk into LVaR models. Finally, the authors discuss scenario optimization algorithms for the assessment of structured investment portfolios and present a detailed operational methodology for computer programming purposes and prospective research design with the backing of a graphical flowchart. Findings: To that end, the portfolio/risk manager can specify different closeout horizons and dependence measures and calculate the necessary LVaR and resulting investable portfolios. In addition, portfolio managers can compare the return/risk ratio and asset allocation of obtained investable portfolios with different liquidation horizons in relation to the conventional Markowitz´s mean-variance approach. Practical implications: The examined optimization algorithms and modeling techniques have important practical applications for portfolio management and risk assessment, and can have many uses within machine learning and artificial intelligence, expert systems and smart financial applications, financial technology (FinTech), and within big data environments. In addition, it provide key real-world implications for portfolio/risk managers, treasury directors, risk management executives, policymakers and financial regulators to comply with the requirements of Basel III best practices on liquidly risk. Originality/value: The proposed optimization algorithms can aid in advancing portfolios selection and management in financial markets by assessing investable portfolios subject to meaningful operational and financial constraints. Furthermore, the robust risk-algorithms and portfolio optimization techniques can aid in solving some real-world dilemmas under stressed and adverse market conditions, such as the effect of liquidity when it dries up in financial and commodity markets, the impact of correlations factors when there is a switching in their signs and the integration of the influence of the nonlinear and non-normal distribution of assets’ returns in portfolio optimization and management. © 2020, Emerald Publishing Limited.}}, 
pages = {288--309}, 
number = {1}, 
volume = {16}
}
@article{10.2143/ast.37.1.2020800, 
year = {2007}, 
title = {{Optimal retention for a stop-loss reinsurance under the VaR and CTE risk measures}}, 
author = {Cai, Jun and Tan, Ken Seng}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.37.1.2020800}, 
abstract = {{We propose practical solutions for the determination of optimal retentions in a stop-loss reinsurance. We develop two new optimization criteria for deriving the optimal retentions by, respectively, minimizing the value-at-risk (VaR) and the conditional tail expectation (CTE) of the total risks of an insurer. We establish necessary and sufficient conditions for the existence of the optimal retentions for two risk models: individual risk model and collective risk model. The resulting optimal solution of our optimization criterion has several important characteristics: (i) the optimal retention has a very simple analytic form; (ii) the optimal retention depends only on the assumed loss distribution and the reinsurer's safety loading factor; (iii) the CTE criterion is more applicable than the VaR criterion in the sense that the optimal condition for the former is less restrictive than the latter; (iv) if optimal solutions exist, then both VaR- and CTE-based optimization criteria yield the same optimal retentions. In terms of applications, we extend the results to the individual risk models with dependent risks and use multivariate phase type distribution, multivariate Pareto distribution and multivariate Bernoulli distribution to illustrate the effect of dependence on optimal retentions. We also use the compound Poisson distribution and the compound negative binomial distribution to illustrate the optimal retentions in a collective risk model. © 2007 by Astin Bulletin. All rights reserved.}}, 
pages = {93--112}, 
number = {1}, 
volume = {37}
}
@article{10.1016/j.compchemeng.2013.03.031, 
year = {2013}, 
title = {{Metrics for evaluating the forest biorefinery supply chain performance}}, 
author = {Mansoornejad, Behrang and Pistikopoulos, Efstratios N. and Stuart, Paul}, 
journal = {Computers \& Chemical Engineering}, 
issn = {00981354}, 
doi = {10.1016/j.compchemeng.2013.03.031}, 
abstract = {{Metrics for evaluating the performance of supply chains, using forest biorefinery as case study, are introduced. A metric of flexibility is introduced to address volume flexibility of processes. A metric of robustness is developed to consider aggregate deviation of downside profits for different market scenarios. A conditional-value-at-risk type parameter is introduced to analyze the risk associated with sales decisions concerning contract and spot markets. The results illustrate how profitability of a supply chain is linked to the developed metrics. For sustainable decision-making regarding biorefinery strategies, different criteria, i.e. economic, environmental, social, should be considered. However, the economic criteria typically do not consider market volatility, whereas today's market involves price and demand volatilities. Biorefinery strategies must be flexible to be robust to market volatility. Therefore, relevant metrics must be developed to quantify the system's performance against volatility. This paper presents metrics of flexibility and robustness which analyze the performance of the supply chain in a dynamic environment, providing additional information along with economic metrics. In this paper, the link between the two metrics, and how profitability and robustness change with flexibility are discussed. The results reveal that, although profitability does not always increase with more flexibility and there is an optimum level of flexibility, the system's robustness is improved by increasing flexibility. Moreover, a "conditional value-at-risk" parameter is introduced to show what patterns of sale lead to highest profit and robustnestness. © 2013 Elsevier Ltd.}}, 
pages = {125--139}, 
number = {NA}, 
volume = {54}
}
@article{10.1016/j.physa.2019.123401, 
year = {2020}, 
title = {{Volatility modeling and the asymmetric effect for China's carbon trading pilot market}}, 
author = {Fu, Yang and Zheng, Zeyu}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.123401}, 
abstract = {{China's carbon trading pilot programs are proceeding gradually, and play a positive role in promoting the development of a domestic carbon market. Because of the late start, deficiencies still exist in the understanding, modeling, and exploration of China's carbon trading pilot market. Our goal is to model the carbon price dynamics and explore the regularities of seven city-based carbon trading pilots. Based on the daily carbon price time series, we analyzed the behavior of the carbon log-returns from 2013 to 2018, modeled them with the ARMA–EGARCH–SGED process, assessed the impact of news on volatility, and carried out the backtesting Value-at-Risk (VaR) analysis. Statistical analysis shows that the log-returns are skewed kurtosis, fat tail, non-normal distribution, and have significantly volatility clustering. The estimated models perform well and capture the asymmetric impact of news on volatility. Positive news increases bigger volatility than a negative one, except for Hubei and Chongqing. The Shenzhen pilot shows the best performance in resisting markets’ risk, while lower-capability in that are found in Hubei and Tianjin. Moreover, the estimated models for seven pilots have adequate capital to cope with large unexpected losses and VaR estimates give a good prediction of market risk at 99\%, 97.5\%, and 95\% confidence level. © 2019 Elsevier B.V.}}, 
pages = {123401}, 
number = {NA}, 
volume = {542}
}
@article{10.1007/s10107-005-0623-2, 
year = {2005}, 
title = {{Calmness of constraint systems with applications}}, 
author = {Henrion, René and Outrata, Jiří V.}, 
journal = {Mathematical Programming}, 
issn = {00255610}, 
doi = {10.1007/s10107-005-0623-2}, 
abstract = {{The paper is devoted to the analysis of the calmness property for constraint set mappings. After some general characterizations, specific results are obtained for various types of constraints, e.g., one single nonsmooth inequality, differentiable constraints modeled by polyhedral sets, finitely and infinitely many differentiable inequalities. The obtained conditions enable the detection of calmness in a number of situations, where the standard criteria (via polyhedrality or the Aubin property) do not work. Their application in the framework of generalized differential calculus is explained and illustrated by examples associated with optimization and stability issues in connection with nonlinear complementarity problems or continuity of the value-at-risk. © Springer-Verlag 2005.}}, 
pages = {437--464}, 
number = {2-3}, 
volume = {104}
}
@article{10.1007/s10260-018-00434-w, 
year = {2019}, 
title = {{Backtesting VaR and expectiles with realized scores}}, 
author = {Bellini, Fabio and Negri, Ilia and Pyatkova, Mariya}, 
journal = {Statistical Methods \& Applications}, 
issn = {16182510}, 
doi = {10.1007/s10260-018-00434-w}, 
abstract = {{Several statistical functionals such as quantiles and expectiles arise naturally as the minimizers of the expected value of a scoring function, a property that is called elicitability (see Gneiting in J Am Stat Assoc 106:746–762, 2011 and the references therein). The existence of such scoring functions gives a natural way to compare the accuracy of different forecasting models, and to test comparative hypotheses by means of the Diebold–Mariano test as suggested in a recent work. In this paper we suggest a procedure to test the accuracy of a quantile or expectile forecasting model in an absolute sense, as in the original Basel I backtesting procedure of value-at-risk. To this aim, we study the asymptotic and finite-sample distributions of empirical scores for normal and uniform i.i.d. samples. We compare on simulated data the empirical power of our procedure with alternative procedures based on empirical identification functions (i.e. in the case of VaR the number of violations) and we find an higher power in detecting at least misspecification in the mean. We conclude with a real data example where both backtesting procedures are applied to AR(1)–Garch(1,1) models fitted to SP500 logreturns for VaR and expectiles’ forecasts. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {119--142}, 
number = {1}, 
volume = {28}
}
@article{10.1142/s1793962321500306, 
year = {2021}, 
title = {{The sum of two independent polynomially-modified hyperbolic secant random variables with application in computational finance}}, 
author = {Zadeh, A A L and Zakerzadeh, Hojatollah and Torabi, Hamzeh}, 
journal = {International Journal of Modeling, Simulation, and Scientific Computing}, 
issn = {17939623}, 
doi = {10.1142/s1793962321500306}, 
abstract = {{In this paper, by reshaping the hyperbolic secant distribution using Hermite polynomial, we devise a polynomially-modified hyperbolic secant distribution which is more flexible than secant distribution to capture the skewness, heavy-tailedness and kurtosis of data. As a portfolio possibly consists of multiple assets, the distribution of the sum of independent polynomially-modified hyperbolic secant random variables is derived. In exceptional cases, we evaluate risk measures such as value at risk and expected shortfall (ES) for the sum of two independent polynomially-modified hyperbolic secant random variables. Finally, using real datasets from four international computers stocks, such as Adobe Systems, Microsoft, Nvidia and Symantec Corporations, the effectiveness of the proposed model is shown by the goodness of Gram-Charlier-like expansion of hyperbolic secant law, for performance of value at risk and ES estimation, both in and out of the sample period. © 2021 World Scientific Publishing Company.}}, 
pages = {2150030}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.irfa.2017.11.007, 
year = {2020}, 
title = {{Risk quantification for commodity ETFs: Backtesting value-at-risk and expected shortfall}}, 
author = {Brio, Esther B. Del and Mora-Valencia, Andrés and Perote, Javier}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2017.11.007}, 
abstract = {{This paper calibrates risk assessment of alternative methods for modeling commodity ETFs. We implement recently proposed backtesting techniques for both value-at-risk (VaR) and expected shortfall (ES) under parametric and semi-nonparametric techniques. Our results indicate that skewed-t and Gram-Charlier distributional assumptions present the best relative performance for individual Commodity ETFs for those confidence levels recommended by Basel Accords. In view of these results, we recommend the application of leptokurtic distributions and semi-nonparametric techniques to mitigate regulation concerns about global financial stability of commodity business. © 2017 Elsevier Inc.}}, 
pages = {101163}, 
number = {NA}, 
volume = {70}
}
@article{10.1146/annurev-economics-080511-110930, 
year = {2012}, 
title = {{Endogenous extreme events and the dual role of prices}}, 
author = {Danielsson, Jon and Shin, Hyun Song and Zigrand, Jean-Pierre}, 
journal = {Annual Review of Economics}, 
issn = {19411383}, 
doi = {10.1146/annurev-economics-080511-110930}, 
abstract = {{Extreme events in financial markets are often generated by shocks that come from within the system, rather than those that arrive from outside the system. The combination of risk-sensitive behavior rules and the coordinated actions implied by market-to-market accounting can result in outcome distributions with fat tails, even if the fundamental shocks are Gaussian. We illustrate such endogenous extreme events through the pricing density resulting from dynamic hedging of options and the flash crash of May 2010. © 2012 by Annual Reviews. All rights reserved.}}, 
pages = {111--129}, 
number = {NA}, 
volume = {4}
}
@article{10.1080/03610926.2014.1002933, 
year = {2017}, 
title = {{Adjusted empirical likelihood for value at risk and expected shortfall}}, 
author = {Yan, Zhen and Zhang, Junjian}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2014.1002933}, 
abstract = {{Value at risk (VaR) and expected shortfall (ES) are widely used risk measures of the risk of loss on a specific portfolio of financial assets. Adjusted empirical likelihood (AEL) is an important non parametric likelihood method which is developed from empirical likelihood (EL). It can overcome the limitation of convex hull problems in EL. In this paper, we use AEL method to estimate confidence region for VaR and ES. Theoretically, we find that AEL has the same large sample statistical properties as EL, and guarantees solution to the estimating equations in EL. In addition, simulation results indicate that the coverage probabilities of the new confidence regions are higher than that of the original EL with the same level. These results show that the AEL estimation for VaR and ES deserves to recommend for the real applications. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {2580--2591}, 
number = {5}, 
volume = {46}
}
@article{10.1007/s10479-016-2251-z, 
year = {2017}, 
title = {{Minimizing value-at-risk in single-machine scheduling}}, 
author = {Atakan, Semih and Bülbül, Kerem and Noyan, Nilay}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-016-2251-z}, 
abstract = {{The vast majority of the machine scheduling literature focuses on deterministic problems in which all data is known with certainty a priori. In practice, this assumption implies that the random parameters in the problem are represented by their point estimates in the scheduling model. The resulting schedules may perform well if the variability in the problem parameters is low. However, as variability increases accounting for this randomness explicitly in the model becomes crucial in order to counteract the ill effects of the variability on the system performance. In this paper, we consider single-machine scheduling problems in the presence of uncertain parameters. We impose a probabilistic constraint on the random performance measure of interest, such as the total weighted completion time or the total weighted tardiness, and introduce a generic risk-averse stochastic programming model. In particular, the objective of the proposed model is to find a non-preemptive static job processing sequence that minimizes the value-at-risk (VaR) of the random performance measure at a specified confidence level. We propose a Lagrangian relaxation-based scenario decomposition method to obtain lower bounds on the optimal VaR and provide a stabilized cut generation algorithm to solve the Lagrangian dual problem. Furthermore, we identify promising schedules for the original problem by a simple primal heuristic. An extensive computational study on two selected performance measures is presented to demonstrate the value of the proposed model and the effectiveness of our solution method. © 2016, Springer Science+Business Media New York.}}, 
pages = {25--73}, 
number = {1-2}, 
volume = {248}
}
@article{10.1016/j.ememar.2011.06.004, 
year = {2011}, 
title = {{Analysing interconnectivity among economies}}, 
author = {Wong, Alfred Y-T. and Fong, Tom Pak Wing}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2011.06.004}, 
abstract = {{As international financial integration gathers pace, interconnectivity has increased tremendously among financial institutions, financial markets and financial systems, a phenomenon to which the recent global financial crisis perhaps provided the best testimony. The interconnectivity among financial entities at various levels is multilateral in dimension and highly complicated with numerous feedback loops. To contribute to the understanding of the complexity of the global financial system, this study shows how the interconnected relationships can be disentangled into simple and quantifiable bilateral interdependence linkages, using 11 Asia-Pacific economies as an example. A major finding is that all these economies register a significantly higher sovereign risk once the condition that another economy is in distress is imposed. © 2011 Elsevier B.V.}}, 
pages = {432--442}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.cor.2016.01.014, 
year = {2016}, 
title = {{Mean-univariate GARCH VaR portfolio optimization: Actual portfolio approach}}, 
author = {Ranković, Vladimir and Drenovak, Mikica and Urosevic, Branko and Jelic, Ranko}, 
journal = {Computers \& Operations Research}, 
issn = {03050548}, 
doi = {10.1016/j.cor.2016.01.014}, 
abstract = {{In accordance with Basel Capital Accords, the Capital Requirements (CR) for market risk exposure of banks is a nonlinear function of Value-at-Risk (VaR). Importantly, the CR is calculated based on a bank's actual portfolio, i.e. the portfolio represented by its current holdings. To tackle mean-VaR portfolio optimization within the actual portfolio framework (APF), we propose a novel mean-VaR optimization method where VaR is estimated using a univariate Generalized AutoRegressive Conditional Heteroscedasticity (GARCH) volatility model. The optimization was performed by employing a Nondominated Sorting Genetic Algorithm (NSGA-II). On a sample of 40 large US stocks, our procedure provided superior mean-VaR trade-offs compared to those obtained from applying more customary mean-multivariate GARCH and historical VaR models. The results hold true in both low and high volatility samples. © 2016 The Authors.}}, 
pages = {83--92}, 
number = {NA}, 
volume = {72}
}
@article{10.1007/978-3-319-50496-4_56, 
year = {2016}, 
title = {{Value at risk for risk evaluation in information retrieval}}, 
author = {Wang, Meijia and Zhang, Peng and Song, Dawei and Wang, Jun}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-50496-4\_56}, 
abstract = {{In Information Retrieval (IR), evaluation metrics continuously play an important role. Recently, some risk measures have been proposed to evaluate the downside performance or the performance variance of an assumingly advanced IR method in comparison with a baseline method. In this paper, we propose a novel risk metric, by applying the Value at Risk theory (VaR, which has been widely used in financial investment) to IR risk evaluation. The proposed metric (VaR IR) is implemented in the light of typical IR effectiveness metrics (e.g. AP) and used to evaluate the participating systems submitted to Session Tracks and compared with other risk metrics. The empirical evaluation has shown that VaR IR is complementary to and can be integrated with the effectiveness metrics to provide a more comprehensive evaluation method. © Springer International Publishing AG 2016.}}, 
pages = {631--638}, 
number = {NA}, 
volume = {10102}
}
@article{10.12693/aphyspola.114.619, 
year = {2008}, 
title = {{Bounds for value at risk for multiasset portfolios}}, 
author = {Jaworski, P}, 
journal = {Acta Physica Polonica A}, 
issn = {05874246}, 
doi = {10.12693/aphyspola.114.619}, 
abstract = {{The theory of copulas provides a useful tool for modeling dependence in risk management. In insurance and finance, as well as in other applications, dependence of extreme events is particularly important, hence there is a need for the detailed study of the tail behaviour of the multivariate copulas. In this paper we investigate the class of copulas being the weighted means of copulas having homogeneous lower tails. We show that having only such information on the structure of dependence of returns from assets is enough to get estimates on value at risk of the multiasset portfolio in ternis of value at risk of one-asset portfolios.}}, 
pages = {619--627}, 
number = {3}, 
volume = {114}
}
@article{10.1016/b978-0-12-811401-8.00008-8, 
year = {2018}, 
title = {{Credit risk in project finance transactions}}, 
author = {Gatti, Stefano}, 
issn = {NA}, 
doi = {10.1016/b978-0-12-811401-8.00008-8}, 
abstract = {{The peculiar characteristics of project finance compared with standard corporate loans pose relevant problems to credit risk managers in terms of methodologies and metrics for credit risk measurement. The issue is even more important as long as capital regulation and Basel II and III rules are concerned. Although theory and practice have clarified the main pillars of credit risk measurement and management for corporate loans, little is still known about the quantification of credit risk for structured transactions in project finance. After a review of the Basel Committee framework for credit risk in specialized lending and of the recent reforms triggered by the financial turmoil of the past few years, the chapter presents empirical data about default and recovery rates for project finance deals and shows that project finance is a very resilient asset class even in periods of severe financial distress. The chapter introduces also the concept of value at risk and presents a specifically designed methodology for the quantification of the value at risk of project finance deals. © 2019 Elsevier Inc. All rights reserved.}}, 
pages = {405--453}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/14697688.2016.1162908, 
year = {2016}, 
title = {{Expected shortfall estimation for apparently infinite-mean models of operational risk}}, 
author = {Cirillo, Pasquale and Taleb, Nassim Nicholas}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2016.1162908}, 
abstract = {{Statistical analyses on actual data depict operational risk as an extremely heavy-tailed phenomenon, able to generate losses so extreme as to suggest the use of infinite-mean models. But no loss can actually destroy more than the entire value of a bank or of a company, and this upper bound should be considered when dealing with tail-risk assessment. Introducing what we call the dual distribution, we show how to deal with heavy-tailed phenomena with a remote yet finite upper bound. We provide methods to compute relevant tail quantities such as the Expected Shortfall, which is not available under infinite-mean models, allowing adequate provisioning and capital allocation. This also permits a measurement of fragility. The main difference between our approach and a simple truncation is in the smoothness of the transformation between the original and the dual distribution. Our methodology is useful with apparently infinite-mean phenomena, as in the case of operational risk, but it can be applied in all those situations involving extreme fat tails and bounded support. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--10}, 
number = {10}, 
volume = {16}
}
@article{10.1016/j.orp.2017.02.001, 
year = {2017}, 
title = {{Portfolio performance evaluation in Mean-CVaR framework: A comparison with non-parametric methods value at risk in Mean-VaR analysis}}, 
author = {Banihashemi, Shokoofeh and Navidi, Sarah}, 
journal = {Operations Research Perspectives}, 
issn = {22147160}, 
doi = {10.1016/j.orp.2017.02.001}, 
abstract = {{As we know, there is a belief in the finance literature that Value at Risk (VaR) and Conditional Value at Risk (CVaR) are new approaches to manage and control the risk. Regard to, value at risk is not a coherent risk measure and it is not sub-additive and convex, so, we have considered conditional value at risk as a risk measure by different confidence level in the Mean-CVaR and multi objective proportional change Mean-CVaR models and compared these models with our previous mean-VaR models. This paper focuses on the performance evaluation process and portfolios selection by using Data Envelopment Analysis (DEA). Conventional DEA models assume non-negative values for inputs and outputs, but many of data take the negative value. Therefore, we have used our models based on Range Directional Measure (RDM) that can take positive and negative values. Here value at risk is obtained by non-parametric methods such as historical simulation and Monte Carlo simulation. Finally, a numerical example in Iran's market is presented. © 2017}}, 
pages = {21--28}, 
number = {NA}, 
volume = {4}
}
@article{10.1016/j.qref.2011.07.002, 
year = {2011}, 
title = {{Risk management of precious metals}}, 
author = {Hammoudeh, Shawkat and Malik, Farooq and McAleer, Michael}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2011.07.002}, 
abstract = {{This paper examines volatility and correlation dynamics in price returns of gold, silver, platinum and palladium, and explores the corresponding risk management implications for market risk and hedging. Value-at-Risk (VaR) is used to analyze the downside market risk associated with investments in precious metals, and to design optimal risk management strategies. We compute the VaR for major precious metals using the calibrated RiskMetrics, different GARCH models, and the semi-parametric Filtered Historical Simulation approach. The best approach for estimating VaR based on conditional and unconditional statistical tests is documented. The economic importance of the results is highlighted by assessing the daily capital charges from the estimated VaRs. © 2011 The Board of Trustees of the University of Illinois.}}, 
pages = {435--441}, 
number = {4}, 
volume = {51}
}
@article{10.1016/j.najef.2014.07.003, 
year = {2014}, 
title = {{Empirical analysis of long memory, leverage, and distribution effects for stock market risk estimates}}, 
author = {Su, Jung-Bin}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2014.07.003}, 
abstract = {{In this study, eight generalized autoregressive conditional heteroskedasticity (GARCH) types of variance specifications and two return distribution settings, the normal and skewed generalized Student's t (SGT) of Theodossiou (1998), totaling nine GARCH-based models, are utilized to forecast the volatility of six stock indices, and then both the out-of-sample-period value-at-risk (VaR) and the expected shortfall (ES) are estimated following the rolling window approach. Moreover, the in-sample VaR is estimated for both the global financial crisis (GFC) period and the non-GFC period. Subsequently, through several accuracy measures, nine models are evaluated in order to explore the influence of long memory, leverage, and distribution effects on the performance of VaR and ES forecasts. As shown by the empirical results of the nine models, the long memory, leverage, and distribution effects subsist in the stock markets. Moreover, regarding the out-of-sample VaR forecasts, long memory is the most important effect, followed by the leverage effect for the low level, whereas the distribution effect is crucial for the high level. As for the three VaR approaches, weighted historical simulation achieves the best VaR forecasting performance, followed by filtered historical simulation, whereas the parametric approach has the worst VaR forecasting performance for all the levels. Furthermore, VaR models underestimate the true risk, whereas ES models overestimate the true risk, indicating that the ES risk measure is more conservative than the VaR risk measure. Additionally, based on back-testing, the VaR provides a better risk forecast than the ES since the ES highly overestimates the true risk. Notably, long memory is important for the ES estimate, whereas both the long memory and the leverage effect are crucial for the VaR estimate. Finally, via in-sample VaR forecasts in regard to the low level, it is found that long memory is important for the non-GFC period, whereas the distribution effect is crucial for the GFC period. On the other hand, with regard to the high level, the distribution effect is crucial for both the non-GFC and the GFC period. These results seem to be consistent with those found in the out-of-sample VaR forecasts. In accordance with these results, several important policy implications are proposed in this study. © 2014 Elsevier Inc.}}, 
pages = {1--39}, 
number = {NA}, 
volume = {30}
}
@article{10.20965/jaciii.2018.p0457, 
year = {2018}, 
title = {{Construction and application of mixed copula model}}, 
author = {Pan, Xueyan and Cai, Guanghui and Chen, Qiulin and {China, School of Economics and Mangement, Anhui Normal Universtity No 189 Jiuhua South Road, Wuhu, Anhui 241002,} and {China, School of Statistics and Mathematics, Zhejiang Gongshang University No 18 Xuezheng Street, Xiasha University Town, Hangzhou 310018,}}, 
journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics}, 
issn = {13430130}, 
doi = {10.20965/jaciii.2018.p0457}, 
abstract = {{The Copula theory has been widely used in many scientific fields. In this study, we discuss model selection for the Copula theory. We derive the mixed Copula model based on the Kendall rank correlation coefficient. This new method is applied to forecast the value at risk of the portfolio in the foreign exchange market. The results show that the proposed mixed Copula model proposed in this paper is better than the other, commonly used, Copula model. © 2018 Fuji Technology Press. All Rights Reserved.}}, 
pages = {457--464}, 
number = {4}, 
volume = {22}
}
@article{10.1016/j.jtbi.2019.01.032, 
year = {2019}, 
title = {{Risk management, signal processing and econometrics: A new tool for forecasting the risk of disease outbreaks}}, 
author = {Hassani, Hossein and Yeganegi, Mohammad Reza and Silva, Emmanuel Sirimal and Ghodsi, Fatemeh}, 
journal = {Journal of Theoretical Biology}, 
issn = {00225193}, 
doi = {10.1016/j.jtbi.2019.01.032}, 
pmid = {30735737}, 
abstract = {{This paper takes a novel approach for forecasting the risk of disease emergence by combining risk management, signal processing and econometrics to develop a new forecasting approach. We propose quantifying risk using the Value at Risk criterion and then propose a two staged model based on Multivariate Singular Spectrum Analysis and Quantile Regression (MSSA-QR model). The proposed risk measure (PLVaR) and forecasting model (MSSA-QR) is used to forecast the worst cases of waterborne disease outbreaks in 22 European and North American countries based on socio-economic and environmental indicators. The results show that the proposed method perfectly forecasts the worst case scenario for less common waterborne diseases whilst the forecasting of more common diseases requires more socio-economic and environmental indicators. © 2019 Elsevier Ltd}}, 
pages = {57--62}, 
number = {NA}, 
volume = {467}
}
@article{10.1016/j.ijforecast.2013.01.007, 
year = {2013}, 
title = {{The two-sided Weibull distribution and forecasting financial tail risk}}, 
author = {Chen, Qian and Gerlach, Richard H.}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2013.01.007}, 
abstract = {{A two-sided Weibull is developed for modelling the conditional financial return distribution, for the purpose of forecasting tail risk measures. For comparison, a range of conditional return distributions are combined with four volatility specifications in order to forecast the tail risk in seven daily financial return series, over a four-year forecast period that includes the recent global financial crisis. The two-sided Weibull performs at least as well as other distributions for Value at Risk (VaR) forecasting, but performs most favourably for conditional VaR forecasting, prior to the crisis as well as during and after it. © 2013 International Institute of Forecasters.}}, 
pages = {527--540}, 
number = {4}, 
volume = {29}
}
@article{10.3846/16111699.2012.744343, 
year = {2014}, 
title = {{VaR and the cross-section of expected stock returns: An emerging market evidence}}, 
author = {Chen, Dar-Hsin and Chen, Chun-Da and Wu, Su-Chen}, 
journal = {Journal of Business Economics and Management}, 
issn = {16111699}, 
doi = {10.3846/16111699.2012.744343}, 
abstract = {{In this paper we investigate the explanatory power of the market beta, firm size, and the book-to-market ratio, as well as Value-at-Risk regarding the cross-sectional expected stock returns in a less developed stock market - Taiwan's stock market. The main purpose is to examine whether the Value-at-Risk factor has marginal explanatory power related to the Fama-French three-factor model. The empirical results show that Value-at-Risk can account for the average stock returns at both 1\% and 5\% significance levels based on cross-sectional regression analysis. Moreover, from the perspective of the time series regression, the Value-at-Risk factor can also demonstrate the variation of the stock market, especially for the larger companies in the Taiwan stock market. © 2014 Copyright © 2014 Vilnius Gediminas Technical University (VGTU) Press Technika.}}, 
pages = {441--459}, 
number = {3}, 
volume = {15}
}
@article{10.1016/j.jbankfin.2004.08.007, 
year = {2005}, 
title = {{Sensitivity analysis of VaR and Expected Shortfall for portfolios under netting agreements}}, 
author = {Fermanian, Jean-David and Scaillet, Olivier}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2004.08.007}, 
abstract = {{In this paper, we characterize explicitly the first derivative of the Value at Risk and the Expected Shortfall with respect to portfolio allocations when netting between positions exists. As a particular case, we examine a simple Gaussian example in order to illustrate the impact of netting agreements in credit risk management. Collateral issues are also dealt with. For practical purposes we further provide nonparametric estimators for sensitivities and derive their asymptotic distributions. An empirical application on a typical banking portfolio is finally provided. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {927--958}, 
number = {4}, 
volume = {29}
}
@article{10.3390/math9050504, 
year = {2021}, 
title = {{Assessing the time-frequency co-movements among the five largest engineering consulting companies: A wavelet-base metrics of contagion and var ratio}}, 
author = {Junior, Marcos Albuquerque and Filipe, José António and Neto, Paulo de Melo Jorge and Silva, Cristiano da Costa da}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math9050504}, 
abstract = {{Diversification in a portfolio is an important tool for the systematic risk management that is inherent to different asset classes. The composition of a portfolio with domestic and international assets is seen as one of the main alternatives for building a diversified portfolio, as this approach tends to reduce portfolio return exposure depending on country factors. However, in scenarios where industry factors are predominant, international diversification can increase systematic risk in a portfolio centered on a single asset class. This study is a pioneer in using wavelet-based methods to identify intersectoral co-movements, based on a portfolio of shares of the world’s top five consulting engineering companies, providing an innovative way to be applied to this phenomenon. Our evidence indicates that companies share a strong pattern of co-movements among themselves, especially in cycles of 32 to 64 days, suggesting a higher exposure to risk for portfolios with an investment horizon in long-term cycles. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {504}, 
number = {5}, 
volume = {9}
}
@article{10.1007/978-3-642-41671-2_23, 
year = {2014}, 
title = {{Efficient data protection scheme in hybrid clouds}}, 
author = {Tung, Der-Kuo and Chen, Wei-Hsiu and Liu, Chiang-Lung}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-3-642-41671-2\_23}, 
abstract = {{The amount of digital data has rapidly increased at a rate of double per year since 2009. Adopting cloud computing storage services will consequently be an alternative approach to gain cost flexibility. Data encryption and access control are common data protection methods in cloud storages. However, traditional full data encryption in clouds can ensure privacy preserving but cause the low analysis efficiency problem. The emerging requirements of data analysis in clouds are balanced between data security and data analysis capacity. An efficient data protection scheme in hybrid clouds is proposed to improve data protection efficiency, rise up data availability and decrease the maintenance cost of cloud services. Data classification and selective data protection mechanisms are two key phases in this paper. According to the functional analysis and comparison results, The proposed scheme is therefore more suitable for efficiency data protection in hybrid clouds than traditional methods. © Springer-Verlag Berlin Heidelberg 2014.}}, 
pages = {173--179}, 
number = {NA}, 
volume = {280 LNEE}
}
@article{10.1002/fut.22017, 
year = {2019}, 
title = {{Panel quantile regressions for estimating and predicting the value-at-risk of commodities}}, 
author = {Čech, František and Baruník, Jozef}, 
journal = {Journal of Futures Markets}, 
issn = {02707314}, 
doi = {10.1002/fut.22017}, 
abstract = {{Using a flexible panel quantile regression framework, we show how the future conditional quantiles of commodities returns depend on both ex post and ex ante uncertainty. Empirical analysis of the most liquid commodities covering main sectors, including energy, food, agriculture, and precious and industrial metals, reveal several important stylized facts. We document common patterns of the dependence between future quantile returns and ex post as well as ex ante volatilities. We further show that the conditional returns distribution is platykurtic. The approach can serve as a useful risk management tool for investors interested in commodity futures contracts. © 2019 Wiley Periodicals, Inc.}}, 
pages = {1167--1189}, 
number = {9}, 
volume = {39}
}
@article{10.1016/s0927-5398(01)00025-1, 
year = {2001}, 
title = {{Testing and comparing value-at-risk measures}}, 
author = {Christoffersen, Peter and Hahn, Jinyong and Inoue, Atsushi}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(01)00025-1}, 
abstract = {{Value-at-Risk (VaR) has emerged as the standard tool for measuring market risk. Currently, more than 80 commercial vendors offer risk management systems that report VaR-like measures. Risk managers are therefore often left with the daunting task of choosing from a plethora of models. This paper develops a framework, first for testing that the VaR measure at hand is properly specified, and second for picking the best among two models in a statistically meaningful way. In an application, competing VaR measures are calculated from either historical or option-price-based volatility measures, and the VaRs are tested and compared. © 2001 Elsevier Science Ltd. All rights reserved.}}, 
pages = {325--342}, 
number = {3}, 
volume = {8}
}
@article{10.1002/ijfe.1618, 
year = {2018}, 
title = {{Enriching the VaR framework to EEMD with an application to the European carbon market}}, 
author = {Zhu, Bangzhu and Wang, Ping and Chevallier, Julien and Wei, Yi‐Ming and Xie, Rui}, 
journal = {International Journal of Finance \& Economics}, 
issn = {10769307}, 
doi = {10.1002/ijfe.1618}, 
abstract = {{Unlike common financial markets, the European carbon market is a typically heterogeneous market, characterized by multiple timescales, and affected by extreme events. The traditional value-at-risk (VaR) with single-timescale fails to deal with the multi-timescale characteristics and the effects of extreme events, which can result in the VaR overestimation for carbon market risk. To measure accurately the risk on the European carbon market, we propose an ensemble empirical mode decomposition (EEMD)-based multiscale VaR approach. First, the EEMD algorithm is utilized to decompose the carbon price return into several intrinsic mode functions (IMFs) with different timescales and a residue, which are modelled, respectively, using the ARMA-Generalized Autoregressive Conditional Heteroscedasticity model to obtain their conditional variances at different timescales. Furthermore, the Iterated Cumulative Sums of Squares algorithm is employed to determine the windows of an extreme event, so as to identify the IMFs influenced by an extreme event and conduct an exponentially weighted moving average on their conditional variations. Finally, the VaRs of various IMFs and the residue are estimated to reconstruct the overall VaR, the validity of which is verified later. Then, we illustrate the results by considering several European carbon futures contracts. Compared with the traditional VaR framework with single timescale, the proposed multiscale VaR-EEMD model can effectively reduce the influences of the heterogeneous environments (such as the influences of extreme events) and obtain a more accurate overall risk measure on the European carbon market. By acquiring the distributions of carbon market risks at different timescales, the proposed multiscale VaR-EEMD estimation is capable of understanding the fluctuation characteristics more comprehensively, which can provide new perspectives for exploring the evolution law of the risks on the European carbon market. Copyright © 2018 John Wiley \& Sons, Ltd.}}, 
pages = {315--328}, 
number = {3}, 
volume = {23}
}
@article{10.1057/s41283-016-0002-8, 
year = {2016}, 
title = {{Estimation of dynamic VaR using JSU and PIV distributions}}, 
author = {Venkataraman, Sree Vinutha and Rao, S. V. D. Nageswara}, 
journal = {Risk Management}, 
issn = {14766930}, 
doi = {10.1057/s41283-016-0002-8}, 
abstract = {{This paper explores the estimation of value at risk (VaR) dynamically for the top three ranked Diversified Equity Schemes, Large Cap Equity Schemes, and Small \& Mid Cap Equity Schemes in India in addition to S\&P CNX Nifty and BSE SENSEX. While volatility clustering is addressed using GARCH model, the non-normality behavior associated with equity returns is handled by employing the Johnson's SU (JSU) or Pearson Type IV (PIV) distributed innovations in the ARMA-GARCH model. We find that both JSU and PIV distributions demonstrate better predictive abilities than Normal distribution. Further, the model incorporates current volatility and administers prior warning to the market participants without disturbing the price discovery process. Regulators can determine capital requirements by implementing the dynamic VaR model. Also, through such a mechanism, additional margins can be imposed in advance during extreme price fluctuations. We propose JSU distribution in conjunction with ARMA-GARCH model based on superior out-sample VaR prediction. ©2016 EEA.}}, 
pages = {111--134}, 
number = {2-3}, 
volume = {18}
}
@article{10.3923/jeasci.2017.2289.2293, 
year = {2017}, 
title = {{Value-at-Risk on different economic sectors in Malaysia}}, 
author = {}, 
issn = {1816949X}, 
doi = {10.3923/jeasci.2017.2289.2293}, 
abstract = {{There are many methods used in determining the performance of a stock. None of them offer the investor a correct risk analysis of their holding and they frequently exposed to a high market risk. However, a sector that is worth investing can also be determined by researching its riskiness in the market. By adapting VaR measurement, this study aims to determine the least risky and the riskiest economic sector for investment in Malaysia. Two approaches used in estimating the VaR for the selected economic sector namely the historical simulation and Monte Carlo simulation. Results of the analysis show that the manufacturing sector have been the least risky sector for investment and the riskiest sector is telecommunication sector. The choice of which to invest depends on the risk appetite of investors. Historical simulation being the most appropriate approach to measure value at risk in this particular study as it results a smaller value of mse and made and also based on backtesting using Kupiec's test. © Medwell Journals, 2017.}}, 
number = {9}, 
volume = {12}
}
@article{10.1016/j.eswa.2021.114893, 
year = {2021}, 
title = {{Value-at-risk backtesting: Beyond the empirical failure rate}}, 
author = {Berger, Theo and Moys, Gunnar}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2021.114893}, 
abstract = {{The quality of Value at Risk (VaR) forecasts is typically determined by the empirical assessment of the frequency of VaR misspecifications. Additionally, the risk of clustered VaR misspecification over time, especially in volatile market times, is usually assessed within a joint testing framework. In this paper, we exclusively focus on the identification of clustered VaR misspecficiations and discuss competing backtesting procedures with respect to their ability to detect inadequate VaR models that are characterized by risk clustering. We present a simulation analysis which comprises different VaR scenarios and we find that the quality of competing backtesting procedures depends on the underlying sample size. Moreover, if sample size is small, it is the parsimonious F-test which describes a sensible choice for applied VaR assessment. © 2021 Elsevier Ltd}}, 
pages = {114893}, 
number = {NA}, 
volume = {177}
}
@article{10.1016/s0378-4371(01)00310-7, 
year = {2001}, 
title = {{Evaluating the RiskMetrics methodology in measuring volatility and Value-at-Risk in financial markets}}, 
author = {Pafka, Szilárd and Kondor, Imre}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/s0378-4371(01)00310-7}, 
eprint = {cond-mat/0103107}, 
abstract = {{We analyze the performance of RiskMetrics, a widely used methodology for measuring market risk. Based on the assumption of normally distributed returns, the RiskMetrics model completely ignores the presence of fat tails in the distribution function, which is an important feature of financial data. Nevertheless, it was commonly found that RiskMetrics performs satisfactorily well, and therefore the technique has become widely used in the financial industry. We find, however, that the success of RiskMetrics is the artifact of the choice of the risk measure. First, the outstanding performance of volatility estimates is basically due to the choice of a very short (one-period ahead) forecasting horizon. Second, the satisfactory performance in obtaining Value-at-Risk by simply multiplying volatility with a constant factor is mainly due to the choice of the particular significance level. © 2001 Elsevier Science B.V. All rights reserved.}}, 
pages = {305--310}, 
number = {1-2}, 
volume = {299}
}
@article{10.1080/10556780410001704911, 
year = {2004}, 
title = {{On the global minimization of the value-at-risk}}, 
author = {†, Jong-shi Pang and Leyffer, Sven}, 
journal = {Optimization Methods and Software}, 
issn = {10556788}, 
doi = {10.1080/10556780410001704911}, 
abstract = {{In this article, we consider the nonconvex minimization problem of the value-at-risk (VaR) that arise from financial risk analysis. By considering this problem as a special linear program (LP) with linear complementarity constraints (a bilevel LP to be more precise), we develop upper and lower bounds for the minimum VaR and show how the combined bounding procedures can be used to compute the latter value to global optimality. A numerical example is provided to illustrate the methodology.}}, 
pages = {611--631}, 
number = {5 SPEC. ISS.}, 
volume = {19}
}
@article{10.1007/s11766-019-3628-9, 
year = {2019}, 
title = {{Econometric modeling of risk measures: A selective review of the recent literature}}, 
author = {Tian, Ding-shi and Cai, Zong-wu and Fang, Ying}, 
journal = {Applied Mathematics-A Journal of Chinese Universities}, 
issn = {10051031}, 
doi = {10.1007/s11766-019-3628-9}, 
abstract = {{Since the financial crisis in 2008, the risk measures which are the core of risk management, have received increasing attention among economists and practitioners. In this review, the concentration is on recent developments in the estimation of the most popular risk measures, namely, value at risk (VaR), expected shortfall (ES), and expectile. After introducing the concept of risk measures, the focus is on discussion and comparison of their econometric modeling. Then, parametric and nonparametric estimations of tail dependence are investigated. Finally, we conclude with insights into future research directions. © 2019, Editorial Committee of Applied Mathematics.}}, 
pages = {205--228}, 
number = {2}, 
volume = {34}
}
@article{10.1155/2021/2839726, 
year = {2021}, 
title = {{The Pareto-Optimal Stop-Loss Reinsurance}}, 
author = {You, Haiyan and Zhou, Xiaoqing}, 
journal = {Mathematical Problems in Engineering}, 
issn = {1024123X}, 
doi = {10.1155/2021/2839726}, 
abstract = {{Reinsurance plays a role of a stabilizer of the insurance industry and can be an effective tool to reduce the risk for the insurer. This paper aims to provide the optimal reinsurance design associated with the stop-loss reinsurance under the criterion of value-at-risk (VaR) risk measure. In this paper, the probability levels in the VaRs used by the both reinsurance parties are assumed to be different and the optimality results of reinsurance are derived by minimizing linear combination of the VaRs of the cedent and the reinsurer. The optimal parameter values of the stop-loss reinsurance policy are formally derived under the expectation premium principle. © 2021 Haiyan You and Xiaoqing Zhou.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {2021}
}
@article{10.21314/jop.2015.165, 
year = {2015}, 
title = {{A comparison of alternative mixing models for external data in operational risk}}, 
author = {Torresetti, Roberto and Pera, Giacomo Le}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2015.165}, 
abstract = {{When measuring its operational value-at-risk, a bank needs to pay attention when including external data in its analysis. Without careful consideration of the specific nature of the bank’s risk there can be relevant systemic risk implications as pointed out by Torresetti and Nordio. Based on real operational risk data, we study alternative mixing models for external data for a particular risk class and show how scaling through a proxy for size for this risk class, as done by Shih, Samad-Khan and Medapa, does not seem to be a sensible technique for incorporating external data. Moving to more sophisticated mixing models, we show how kernel-modified estimators and Bayesian estimators represent an improvement. We also show how the technique outlined by Torresetti and Nordio is capable of further improving the treatment of external data in those instances where the case can be made for a distinct power law governing the tails of the internal and external data. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--21}, 
number = {4}, 
volume = {10}
}
@article{10.1080/03610926.2019.1594304, 
year = {2020}, 
title = {{Multi-modal tempered stable distributions and prosses with applications to finance}}, 
author = {Arefi, Ahmad and Pourtaheri, Reza}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2019.1594304}, 
abstract = {{The assumption of underlying return distribution plays an important role in asset pricing models. While the return distribution used in the traditional theories of asset pricing is the unimodal distribution, numerous studies which have investigated the empirical behavior of asset returns in financial markets use multi-modal distribution. We introduce a new parsimonious multi-modal distribution, referred to as the multi-modal tempered stable (MMTS) distribution. In this article we also generate the exponential Lévy market models and derive the value-at-risk (VaR) induced from them. To demonstrate the advantages, we will present the results of the parameter estimation and the VaRs for financial data. © 2020 Taylor \& Francis Group, LLC.}}, 
pages = {1--17}, 
number = {17}, 
volume = {49}
}
@article{10.1080/03610918.2013.863921, 
year = {2016}, 
title = {{Comparing VaR Approximation Methods that Use the First Four Moments as Inputs}}, 
author = {Lien, Donald and Stroud, Christopher and Ye, Keying}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2013.863921}, 
abstract = {{This article compares four methods used to approximate value at risk (VaR) from the first four moments of a probability distribution: Cornish-Fisher, Edgeworth, Gram-Charlier, and Johnson distributions. Increasing rearrangements are applied to the first three methods. Simulation results suggest that for large sample situations, Johnson distributions yield the most accurate VaR approximation. For small sample situations with small tail probabilities, Johnson distributions yield the worst approximation. A particularly relevant case would be in banking applications for calculating the size of operational risk to cover certain loss types. For this case, the rearranged Gram-Charlier method is recommended. ï¿½ 2016 Taylor and Francis Group, LLC.}}, 
pages = {491--503}, 
number = {2}, 
volume = {45}
}
@article{10.1007/s11156-021-00985-2, 
year = {2021}, 
title = {{Accrual mispricing, value-at-risk, and expected stock returns}}, 
author = {Simlai, Prodosh}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-021-00985-2}, 
abstract = {{We investigate the extent to which a parsimonious measure of maximum likely loss that captures the tail risk of returns—known as value-at-risk (VaR)—explains the relationship between accruals and the cross-sectional dispersion of expected stock returns. We construct portfolios based on Sloan’s (Account Rev 71(3):289–315, 1996) total accruals (TA) measure and individual asset-level VaR, which reflects the dynamic behavior of the asset distribution. We document that VaR is in congruence with portfolio-level accruals and that there is a significant positive relationship between VaR and the cross-section of portfolio returns. Allowing a double-sort involving VaR and TA further suggests that the spread between low- and high-TA portfolios is significantly attenuated after controlling for VaR. We also conduct a firm-level cross-sectional regression analysis and demonstrate that the TA- and VaR-based characteristics—but not the factor-mimicking portfolios—are compensated with higher expected returns, and that VaR neither subsumes nor is subsumed by TA. Finally, our cross-sectional decomposition analysis suggests that the firm-level VaR captures at least 7\% of the accrual premium even in the presence of size and book-to-market. These findings lend support for the mispricing explanation of the accrual anomaly. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1487--1517}, 
number = {4}, 
volume = {57}
}
@article{10.1109/icca.2019.8899534, 
year = {2019}, 
title = {{Genetic Algorithm based Global optimization of VaR}}, 
author = {Xi, Xiangming and Bai, Yu and Wang, Shuning and Lou, Yunjiang}, 
journal = {2019 IEEE 15th International Conference on Control and Automation (ICCA)}, 
issn = {19483449}, 
doi = {10.1109/icca.2019.8899534}, 
abstract = {{As a measure of risks, value at risk (VaR) has been widely used in financial industry, while its optimization is still popular and attracting. In this paper, we will formulate the minimization of VaR as a piecewise linear (PWL) function, i.e. the k-th maximum function, and transform it into a PWL DC problem. In order to address the redundancy and nondifferentiability in the transformed problem, we propose a domain contraction algorithm which solves a series of linear programming problems and converges to a local optimum of the transformed problem. Further, considering that the equivalent problem is concave and multimodal, we carefully design an improved genetic algorithm for global search. Numerical experiments on the datasets collected from the Dow Jones Index confirm the global search capability and efficiency of the proposed algorithms. © 2019 IEEE.}}, 
pages = {1423--1428}, 
number = {NA}, 
volume = {2019-July}
}
@article{10.1016/j.irfa.2005.02.004, 
year = {2006}, 
title = {{The CAPM and value at risk at different time-scales}}, 
author = {Fernandez, Viviana}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2005.02.004}, 
abstract = {{By resorting to wavelet analysis, we estimate the capital asset pricing model (CAPM) at different time-scales for the Chilean stock market. Our sample comprises 24 stocks that were actively traded on the Santiago Stock Exchange over 1997-2002. We find evidence in support of the CAPM at a medium-term horizon. We extend the literature in this area to analyze the impact of time scaling on the computation of value at risk. We conclude that risk is concentrated at higher frequencies of the data. © 2005 Elsevier Inc. All rights reserved.}}, 
pages = {203--219}, 
number = {3}, 
volume = {15}
}
@article{10.1287/opre.2016.1570, 
year = {2017}, 
title = {{Maximizing a class of utility functions over the vertices of a polytope}}, 
author = {Atamtürk, Alper and Gómez, Andrés}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.2016.1570}, 
abstract = {{Given a polytope X, a monotone concave univariate function g, and two vectors c and d, we study the discrete optimization problem of finding a vertex of X that maximizes the utility function c′x + g(d′x). This problem has numerous applications in combinatorial optimization with a probabilistic objective, including estimation of project duration with stochastic times, in reliability models, in multinomial logit models and in robust optimization. We show that the problem is NP-hard for any strictly concave function g even for simple polytopes, such as the uniform matroid, assignment and path polytopes; and propose a 1/2-approximation algorithm for it. We discuss improvements for special cases where g is the square root, log utility, negative exponential utility and multinomial logit probability function. In particular, for the square root function, the approximation ratio is 4/5. We also propose a 1.25-approximation algorithm for a class of minimization problems in which the maximization of the utility function appears as a subproblem. Although the worst-case bounds are tight, computational experiments indicate that the suggested approach finds solutions within 1\%-2\% optimality gap for most of the instances, and can be considerably faster than the existing alternatives. © 2017 INFORMS.}}, 
pages = {433--445}, 
number = {2}, 
volume = {65}
}
@article{10.1080/00207543.2015.1076947, 
year = {2016}, 
title = {{A hybrid risks-informed approach for the selection of supplier portfolio}}, 
author = {Fang, Chao and Liao, Xiangxiang and Xie, Min}, 
journal = {International Journal of Production Research}, 
issn = {00207543}, 
doi = {10.1080/00207543.2015.1076947}, 
abstract = {{In conventional supplier selection approaches, cost consideration is usually emphasised and it renders a vulnerable supply chain with various risks. This article aims to develop a quantitative approach for modelling both supply chain operational risks and disruption risks to support decision-making with regard to order allocation and risk mitigation. We introduce two types of risk evaluation models: value-at-risk (VaR) and conditional value-at-risk (CVaR). Specifically, VaR is used to measure operational risks caused by improper selection and operations of a supplier portfolio to the stochastic demand, which may frequently occur but result in relatively small losses to supply chains; CVaR is used to evaluate disruption risks that are less frequent and tend to cause significant damage. After incorporating risk factors into a probability-based multi-criteria optimisation model, different methods and parameters are compared and tested to determine the factors that may influence the supplier selection process. Computational examples by simulation are presented to illustrate the approach and how decision-makers make trade-offs between costs and hybrid risks. © 2015 Taylor \& Francis.}}, 
pages = {2019--2034}, 
number = {7}, 
volume = {54}
}
@article{10.1109/icnc.2009.94, 
year = {2009}, 
title = {{A multi-objective stochastic programming approach for expressway system planning with risk management}}, 
author = {Huapu, LU and Xinxin, YU and Changzhi, BIAN and Haiwei, WANG and Yue, LI}, 
journal = {2009 Fifth International Conference on Natural Computation}, 
issn = {NA}, 
doi = {10.1109/icnc.2009.94}, 
abstract = {{This study deals with the problem of expressway system planning with risk management. A new multiobjective stochastic programming model is proposed. Transport demand, travel time and future cost are considered as uncertain parameters in expressway investment decision. To develop a robust model, an additional objective junction is added into the traditional SO problem. This multi-objective model includes: (i) the minimization of the sum of current investment cost and the travel time of road users, which is dejined as the expected total cost; (U) the minimization of the VaR (Value-at-Risk) of the total future cost risk. Finally, a numerical example is given to illustrate the ability of the proposed model to expressway system planning. © 2009 IEEE.}}, 
pages = {558--562}, 
number = {NA}, 
volume = {4}
}
@article{10.1016/j.eneco.2018.05.008, 
year = {2018}, 
title = {{Predicting carbon market risk using information from macroeconomic fundamentals}}, 
author = {Jiao, Lei and Liao, Yin and Zhou, Qing}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2018.05.008}, 
abstract = {{Economic theories suggest that carbon price movements are closely related to economic fundamentals. This paper develops an economic state-dependent (SD) approach to evaluate carbon market Value-at-Risk (VaR) that incorporates information from macroeconomic fundamentals into carbon return VaR modeling and forecasting. This method implements an economic SD sampling scheme that utilizes historical carbon return observations from the relevant economic states to predict future carbon market VaR. Applying this SD method to the European Union (EU) carbon market, we confirm that the EU fundamental economy has two distinct states that correspond to “expansion” and “recession” periods and that the carbon returns have different distributions in the two states. We find that the SD method outperforms the traditional non-SD methods in out-of-sample VaR forecasts, and this is particularly evident when the carbon market experiences large-scale economy-driven structural breaks. © 2018 Elsevier B.V.}}, 
pages = {212--227}, 
number = {NA}, 
volume = {73}
}
@article{10.1007/978-3-642-40861-8_58, 
year = {2014}, 
title = {{Global supply chain management using business risk re-alignment via the change of the transfer pricing methodology}}, 
author = {Ahn, Sang Min and Hong, Ki-sung and Lee, Chulung}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-3-642-40861-8\_58}, 
abstract = {{Traditionally, Supply Chain Management (SCM) indicates a strategy enabling an enterprise to achieve optimization of stock level and reduction of lead time through application of information technology. However, such traditional SCM strategy is in contemporary business world required to reflect business economics aspects such as accounting and taxation in order for the enterprise to enjoy maximum benefits. This paper proposes a framework and methodology for optimal profit allocation to participants of multinational enterprises' global supply chain through re-alignment of certain risks assumed by each participant using financial engineering methods. © 2014 Springer-Verlag.}}, 
pages = {413--420}, 
number = {NA}, 
volume = {276 LNEE}
}
@article{10.1007/s10614-011-9256-0, 
year = {2012}, 
title = {{Statistical Inferences for Generalized Pareto Distribution Based on Interior Penalty Function Algorithm and Bootstrap Methods and Applications in Analyzing Stock Data}}, 
author = {Huang, Chao and Lin, Jin-Guan and Ren, Yan-Yan}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-011-9256-0}, 
abstract = {{This paper studies the application of extreme value statistics (EVS) theory on analysis for stock data, based on interior penalty function algorithm and Bootstrap methods. The generalized Pareto distribution (GPD) models are considered in analyzing the closing price data of Shanghai stock market. The maximum likelihood estimates (MLEs) are obtained by using the interior penalty function algorithm. Correspondingly, the bias and standard errors of MLEs, and the hypothesis test on the shape parameter are concerned through Bootstrap methods. Some simulations are performed to demonstrate the efficacy of parameter estimation and the power of the test. The estimates of the tail index in this paper are compared with those obtained via classical methods. At last, the model is diagnosed by numerical and graphical methods and the Value-at-Risk (VaR) is estimated. © 2011 Springer Science+Business Media, LLC.}}, 
pages = {173--193}, 
number = {2}, 
volume = {39}
}
@article{10.1109/fuzz-ieee.2018.8491675, 
year = {2018}, 
title = {{Maximization of returns under value-at-risk constraints in dynamic fuzzy asset allocation}}, 
author = {Yoshida, Yuji}, 
journal = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
issn = {10987584}, 
doi = {10.1109/fuzz-ieee.2018.8491675}, 
abstract = {{Dynamic portfolio allocation is discussed in asset management with fuzziness. By perception-based extension for fuzzy random variables, a dynamic portfolio model with value-at-risks of fuzzy random variables is introduced. By dynamic programming and mathematical programming, this paper derives analytical solutions for the optimization problem. A numerical example is given to discuss the results. © 2018 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {2018-July}
}
@article{10.1063/1.4907448, 
year = {2015}, 
title = {{Risk of portfolio with simulated returns based on copula model}}, 
author = {Razak, Ruzanna Ab and Ismail, Noriszura}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.4907448}, 
abstract = {{The commonly used tool for measuring risk of a portfolio with equally weighted stocks is variance-covariance method. Under extreme circumstances, this method leads to significant underestimation of actual risk due to its multivariate normality assumption of the joint distribution of stocks. The purpose of this research is to compare the actual risk of portfolio with the simulated risk of portfolio in which the joint distribution of two return series is predetermined. The data used is daily stock prices from the ASEAN market for the period January 2000 to December 2012. The copula approach is applied to capture the time varying dependence among the return series. The results shows that the chosen copula families are not suitable to present the dependence structures of each bivariate returns. Exception for the Philippines-Thailand pair where by t copula distribution appears to be the appropriate choice to depict its dependence. Assuming that the t copula distribution is the joint distribution of each paired series, simulated returns is generated and value-at-risk (VaR) is then applied to evaluate the risk of each portfolio consisting of two simulated return series. The VaR estimates was found to be symmetrical due to the simulation of returns via elliptical copula-GARCH approach. By comparison, it is found that the actual risks are underestimated for all pairs of portfolios except for Philippines-Thailand. This study was able to show that disregard of the non-normal dependence structure of two series will result underestimation of actual risk of the portfolio. © 2015 AIP Publishing LLC.}}, 
pages = {219--224}, 
number = {NA}, 
volume = {1643}
}
@article{NA, 
year = {2001}, 
title = {{Optimum strategy in risky challenge model}}, 
author = {}, 
issn = {NA}, 
doi = {NA}, 
abstract = {{The paper deals with modeling of challenge systems which exist in education, science, sport, business etc. The model employes two scenarios outcome: the success, followed by award and zero award failure with given probabilities. A utility function, which takes into account the risk and award has been introduced and the optimum utility maximizing strategy, has been derived. An application, with numerical example, in choosing the optimum education is also included.}}, 
number = {NA}, 
volume = {1}
}
@article{10.1016/j.jeconom.2005.01.008, 
year = {2006}, 
title = {{Volatility comovement: A multifrequency approach}}, 
author = {Calvet, Laurent E. and Fisher, Adlai J. and Thompson, Samuel B.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2005.01.008}, 
abstract = {{We implement a multifrequency volatility decomposition of three exchange rates and show that components with similar durations are strongly correlated across series. This motivates a bivariate extension of the Markov-Switching Multifractal (MSM) introduced in Calvet and Fisher (J. Econ. 105 (2001) 27, J. Financ. Econ. 2 (2004) 49). Bivariate MSM is a stochastic volatility model with a closed-form likelihood. Estimation can proceed by maximum likelihood for state spaces of moderate size, and by simulated likelihood via a particle filter in high-dimensional cases. We estimate the model and confirm its main assumptions in likelihood ratio tests. Bivariate MSM compares favorably to a standard multivariate GARCH both in- and out-of-sample. A parsimonious multifrequency factor structure is finally proposed for multivariate settings with potentially many assets. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {179--215}, 
number = {1-2}, 
volume = {131}
}
@article{10.1111/mafi.12296, 
year = {2021}, 
title = {{Sharing the value-at-risk under distributional ambiguity}}, 
author = {Chen, Zhi and Xie, Weijun}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12296}, 
abstract = {{This paper considers the problem of risk sharing, where a coalition of homogeneous agents, each bearing a random cost, aggregates their costs, and shares the value-at-risk of such a risky position. Due to limited distributional information in practice, the joint distribution of agents' random costs is difficult to acquire. The coalition, being aware of the distributional ambiguity, thus evaluates the worst-case value-at-risk within a commonly agreed ambiguity set of the possible joint distributions. Through the lens of cooperative game theory, we show that this coalitional worst-case value-at-risk is subadditive for the popular ambiguity sets in the distributionally robust optimization literature that are based on (i) convex moments or (ii) Wasserstein distance to some reference distributions. In addition, we propose easy-to-compute core allocation schemes to share the worst-case value-at-risk. Our results can be readily extended to sharing the worst-case conditional value-at-risk under distributional ambiguity. © 2020 Wiley Periodicals LLC}}, 
pages = {531--559}, 
number = {1}, 
volume = {31}
}
@article{10.1109/icps.2019.8733335, 
year = {2019}, 
title = {{Two-stages bidding strategies for residential microgrids based peer-to-peer energy trading}}, 
author = {Zhang, Zhenyuan and Tang, Haoyue and Huang, Qi and Lee, Wei-Jen}, 
journal = {2019 IEEE/IAS 55th Industrial and Commercial Power Systems Technical Conference (I\&CPS)}, 
issn = {NA}, 
doi = {10.1109/icps.2019.8733335}, 
abstract = {{With the promoting of residential Microgrids and transactive energy system, peer-to-peer (P2P) energy transactive mechanism among small household entities have emerged, which allows residents to actively trade energy with their neighbors has appeared in the residential Microgrid. In order to ensure the success of this trading mode, designing an effective bidding strategy in the P2P energy trading has been a hot topic in recent studies. However, in current studies, limitation still exsited such as lack of flexibility and unfair constraints on trading. To solve these problems, this paper proposes a two-stage bidding strategy in the residential Microgrid. In this mechanism, the equilibrim among fair competition in market, economic benefits in participants, and self-sufficiency in Mircogird is achieved. Besides, aiming to help residents make better decisions about their through bidding process, a trading price predictor and a tool for risk analysis, 'Value at Risk' (VaR) are also offered in this process. Moreover, case studies with multiple households involved in a residential Microgrid verifed the effectiveness of the proposed method. © 2019 IEEE.}}, 
pages = {1--9}, 
number = {NA}, 
volume = {2019-May}
}
@article{10.2143/ast.37.2.2024067, 
year = {2007}, 
title = {{The quantitative modeling of operational risk: Between G-and-H and EVT}}, 
author = {Degen, Matthias and Embrechts, Paul and Lambrigger, Dominik D.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.37.2.2024067}, 
abstract = {{Operational risk has become an important risk component in the banking and insurance world. The availability of (few) reasonable data sets has given some authors the opportunity to analyze operational risk data and to propose different models for quantification. As proposed in Dutta and Perry [12], the parametric g-and-h distribution has recently emerged as an interesting candidate. In our paper, we discuss some fundamental properties of the g-and-h distribution and their link to extreme value theory (EVT). We show that for the g-and-h distribution, convergence of the excess distribution to the generalized Pareto distribution (GPD) is extremely slow and therefore quantile estimation using EVT may lead to inaccurate results if data are well modeled by a g-and-h distribution. We further discuss the subadditivity property of Value-at-Risk (VaR) for g-and-h random variables and show that for reasonable g and h parameter values, superadditivity may appear when estimating high quantiles. Finally, we look at the g-and-h distribution in the one-claim-causes-ruin paradigm. © 2007 by Astin Bulletin. All rights reserved.}}, 
pages = {265--291}, 
number = {2}, 
volume = {37}
}
@article{10.1007/978-3-319-02069-3_6, 
year = {2014}, 
title = {{Optimal investment with bounded VaR for power utility functions}}, 
author = {Chouaf, Bénamar and Pergamenchtchikov, Serguei}, 
issn = {NA}, 
doi = {10.1007/978-3-319-02069-3\_6}, 
abstract = {{We consider an optimal investment problem for Black-Scholes type financial market with bounded VaR measure on the whole investment interval [0,T]. The explicit form for the optimal strategies is found. © Springer International Publishing Switzerland 2014. All rights are reserved.}}, 
pages = {103--116}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/mafi.12211, 
year = {2019}, 
title = {{An efficient approach to quantile capital allocation and sensitivity analysis}}, 
author = {Asimit, Vali and Peng, Liang and Wang, Ruodu and Yu, Alex}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/mafi.12211}, 
abstract = {{In various fields of applications such as capital allocation, sensitivity analysis, and systemic risk evaluation, one often needs to compute or estimate the expectation of a random variable, given that another random variable is equal to its quantile at some prespecified probability level. A primary example of such an application is the Euler capital allocation formula for the quantile (often called the value-at-risk), which is of crucial importance in financial risk management. It is well known that classic nonparametric estimation for the above quantile allocation problem has a slower rate of convergence than the standard rate. In this paper, we propose an alternative approach to the quantile allocation problem via adjusting the probability level in connection with an expected shortfall. The asymptotic distribution of the proposed nonparametric estimator of the new capital allocation is derived for dependent data under the setup of a mixing sequence. In order to assess the performance of the proposed nonparametric estimator, AR-GARCH models are proposed to fit each risk variable, and further, a bootstrap method based on residuals is employed to quantify the estimation uncertainty. A simulation study is conducted to examine the finite sample performance of the proposed inference. Finally, the proposed methodology of quantile capital allocation is illustrated for a financial data set. © 2019 Wiley Periodicals, Inc.}}, 
pages = {1131--1156}, 
number = {4}, 
volume = {29}
}
@article{10.1007/978-3-030-79165-0_11, 
year = {2022}, 
title = {{Risk Analysis and Risk Measures Applied to the Furniture Industry}}, 
author = {Brito, Irene and Leão, Celina P. and Rodrigues, Matilde A.}, 
journal = {Lecture Notes in Mechanical Engineering}, 
issn = {21954356}, 
doi = {10.1007/978-3-030-79165-0\_11}, 
abstract = {{In this work we show how concepts and methods from actuarial risk theory can be applied to risk analysis in industry. Risk analysis consists in identifying, quantifying and classifying or ordering risks. In the proposed methodology, the risks identified in industrial setting, are modeled by loss random variables. The loss random variables are used to calculate the expected loss, the loss variance and exceedance probabilities permitting risk quantification. In order to classify and order risks, besides measures of uncertainty, risk measures, such as Value-at Risk and Tail-Value-at-Risk (or Expected Shortfall) are determined and, together with risk quantification, the risk levels are analysed. To exemplify this methodology, a case study for risk analysis and classification of occupational accidents in the furniture industry was carried out. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.}}, 
pages = {113--121}, 
number = {NA}, 
volume = {NA}
}
@article{10.13335/j.1000-3673.pst.2020.0505, 
year = {2021}, 
title = {{Operation Risk Assessment for Power System With Large-scale Wind Power Integration Based on Value at Risk [基于风险价值的大规模风电并网电力系统运行风险评估]}}, 
author = {}, 
issn = {10003673}, 
doi = {10.13335/j.1000-3673.pst.2020.0505}, 
abstract = {{In order to reflect the impact of wind power fluctuation on the overall operation risk of the power system in the future time periods, this paper establishes the load shedding risk change indicators, the line overload risk change indicators and voltage violation risk change indictors, and designs a three-layer assessment indicators with a progressive relationship in the power system with large-scale wind power integration based on the value at risk theory. Firstly, the assessment indicators in the first layer such as the load shedding risk indicator, the line overload risk indicator and the voltage violation risk indicator are calculated based on the wind power prediction output sequence. Secondly, synthesizing the results of the first layer assessment indicators at all moments and using the logistic distribution and exponential distribution to describe the probability distribution of each indicator, the value at risk theory is introduced to calculate the second layer indicators that reflect the overall risk level of the system in the future time periods, and the comprehensive risk indicators that are used as the third layer indicators are introduced to comprehensively evaluate the power system operation risks. Finally, the rationality of the proposed model and method is verified with the analysis in the IEEE-RTS79 system. This paper further analyzes the impact of different wind farm access nodes or access capacities and wind farm centralized access or decentralized access on the system operation risks. © 2021, Power System Technology Press. All right reserved.}}, 
number = {3}, 
volume = {45}
}
@article{10.3390/risks9100184, 
year = {2021}, 
title = {{Cyber risk quantification: Investigating the role of cyber value at risk}}, 
author = {Orlando, Albina}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks9100184}, 
abstract = {{The aim of this paper is to deepen the application of value at risk in the cyber domain, with particular attention to its potential role in security investment valuation. Cyber risk is a fundamental component of the overall risk faced by any organization. In order to plan the size of security investments and to estimate the consequent risk reduction, managers strongly need to quantify it. Accordingly, they can decide about the possibility of sharing residual risk with a third party, such as an insurance company. Recently, cyber risk management techniques are including some risk quantile-based measures that are widely employed in the financial domain. They refer to value at risk that, in the cyber context, takes the name of cyber value at risk (Cy-VaR). In this paper, the main features and challenging issues of Cy-VaR are examined. The possible use of this risk measure in supporting investment decisions in cyber context is discussed, and new risk-based security metrics are proposed. Some simple examples are given to show their potential. © 2021 by the author. Licensee MDPI, Basel, Switzerland.}}, 
pages = {184}, 
number = {10}, 
volume = {9}
}
@article{10.1108/s0276-897620200000020003, 
year = {2020}, 
title = {{Selected multiple criteria supply chain optimization problems}}, 
author = {Sawik, Bartosz}, 
journal = {Applications of Management Science}, 
issn = {02768976}, 
doi = {10.1108/s0276-897620200000020003}, 
abstract = {{Supply chain is an important aspect for all the companies and can affect many aspects of companies. Especially the disruption in supply chain is causing huge impacts and consequences that are difficult to deal with. This chapter presents a review of selected multiple criteria problems used in supply chain optimization. Research analyzed the multiple criteria decision-making methods to tackle the problem of supplier evaluation and selection. It also focuses on the problem of supply chain when a disruption happens and presents strategies to deal with the issue of disruptions in supply chain and how to mitigate the impact of disruptions. Prevention, response, protection, and recovery strategies are explained. Practical part is focused in the risk-averse models to minimize expected worst-case scenario by single sourcing. Computational experiments for practical examples have been solved using CPLEX solver. © 2020 Emerald Publishing Limited.}}, 
pages = {31--58}, 
number = {NA}, 
volume = {20}
}
@article{10.1007/978-3-030-60104-1_7, 
year = {2021}, 
title = {{Using EVT to Assess Risk on Energy Market}}, 
author = {Ganczarek-Gamrot, Alicja and Krężołek, Dominik and Trzpiot, Grażyna}, 
journal = {Studies in Classification, Data Analysis, and Knowledge Organization}, 
issn = {14318814}, 
doi = {10.1007/978-3-030-60104-1\_7}, 
abstract = {{The aim of this paper is to describe and measure the risk of price changes in the energy market. The risk is estimated with Conditional Value-at-Risk (CVaR) and Median Shortfall (MS) based on some types of Value-at-Risk measures: VaR, stress VaR, Incremental Risk Charge (IRC) estimated using Extreme Value Theory (EVT). These measures are calculated for time series of daily and hourly rates of return of electric energy prices from the European Energy Exchange (EEX) spot market. Based on time series from 1st January 2002 to 31st December 2016, we attempt to answer the question: which measure is the most appropriate for risk estimation on the energy market. © 2021, Springer Nature Switzerland AG.}}, 
pages = {57--64}, 
number = {NA}, 
volume = {5}
}
@article{10.1023/b:requ.0000015851.78720.a9, 
year = {2004}, 
title = {{Value-at-risk analysis for Taiwan stock index futures: Fat tails and conditional asymmetries in return innovations}}, 
author = {Huang, Yu Chuan and Lin, Bor-Jing}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1023/b:requ.0000015851.78720.a9}, 
abstract = {{This paper examines the forecasting performance of three value-at-risk (VaR) models (RiskMetrics, Normal APARCH and Student APARCH). We explore and compare two different possible sources of performance improvements: asymmetry in the conditional variance and fat-tailed distributions. Performance is assessed using a range of measures that address the accuracy and efficiency of each model. The TAIFEX and SGX-DT Taiwan stock index futures are studied using daily data. Our results suggest that for asset returns which exhibit fatter tails and volatility clustering, like the TAIFEX and SGX-DT futures, the VaR values produced by the Normal APARCH model are preferred at lower confidence levels. However, at high confidence levels, the VaR forecasts obtained by the Student APARCH model are more accurate than those generated using either the RiskMetrics or Normal APARCH models.}}, 
pages = {79--95}, 
number = {2}, 
volume = {22}
}
@article{10.1109/icicic.2008.351, 
year = {2008}, 
title = {{Measuring fuzzy risk by credibilistic value at risk}}, 
author = {Peng, Jin}, 
journal = {2008 3rd International Conference on Innovative Computing Information and Control}, 
issn = {NA}, 
doi = {10.1109/icicic.2008.351}, 
abstract = {{The value at risk (VaR) methodology is a widely used tool in financial market risk management. In this paper, we present a new method for fuzzy risk analysis. First, we present the new concept of the credibilistic value at risk based on credibility theory. Then, we examine some properties of the proposed credibilistic value at risk. Finally, a kind of fuzzy simulation algorithm is given to show how to calculate the credibilistic value at risk. The proposed credibilistic VaR is suitable for use in many real problems of fuzzy risk analysis. © 2008 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s11425-009-0050-6, 
year = {2009}, 
title = {{Empirical likelihood-based evaluations of value at risk models}}, 
author = {Wei, ZhengHong and Wen, SongQiao and Zhu, LiXing}, 
journal = {Science in China Series A: Mathematics}, 
issn = {10069283}, 
doi = {10.1007/s11425-009-0050-6}, 
abstract = {{Value at Risk (VaR) is a basic and very useful tool in measuring market risks. Numerous VaR models have been proposed in literature. Therefore, it is of great interest to evaluate the efficiency of these models, and to select the most appropriate one. In this paper, we shall propose to use the empirical likelihood approach to evaluate these models. Simulation results and real life examples show that the empirical likelihood method is more powerful and more robust than some of the asymptotic method available in literature. © 2009 Science in China Press and Springer Berlin Heidelberg.}}, 
pages = {1995--2006}, 
number = {9}, 
volume = {52}
}
@article{10.1007/s11579-018-0219-2, 
year = {2019}, 
title = {{Nonlinear equity valuation using conic finance and its regulatory implications}}, 
author = {Madan, Dilip B.}, 
journal = {Mathematics and Financial Economics}, 
issn = {18629679}, 
doi = {10.1007/s11579-018-0219-2}, 
abstract = {{Economic enterprises are modeled to have the return distributions of pure jump limit laws. Specifically the four parameters of a bilateral gamma process synthesize the up and down moves in returns with differing mean and variance rates for the two motions. Prudential capital assessments value a distant terminal payout defined by the accumulated returns. The valuation incorporates risk charges based on measure distortions that generalize the concept of distorted expectations. Particular risk charges are calibrated to data on S\&P 500 index options and their associated time series. On the other hand regulatory capital evaluates extreme loss levels possible over a short time interval. For equity market returns the two calculations yield comparable magnitudes displaying enterprises with both sufficient and insufficient capital. Enterprises invested in Treasury bonds have regulatory capital requirements that are well below their prudential capital levels for long positions. Short positions may have insufficient prudential capital values relative to their regulatory counterparts. The additional prudential and regulatory capital costs of leveraged positions are illustrated. Hedge funds reflect high levels of prudential capital associated with low levels of required regulatory capital reflecting the access of good drifts at low risk levels. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {31--65}, 
number = {1}, 
volume = {13}
}
@article{10.1109/icmse.2008.4668912, 
year = {2008}, 
title = {{The Archimedean copulas measure of the risk characteristic for the tail dependent asset returns}}, 
author = {LU, Jin and TIAN, Wen-ju and ZHANG, Pu}, 
journal = {2008 International Conference on Management Science and Engineering 15th Annual Conference Proceedings}, 
issn = {NA}, 
doi = {10.1109/icmse.2008.4668912}, 
abstract = {{Copulas represent a useful approach to understanding and modeling dependent components of random variables that allows us to focus explicitly on the dependence structure. This paper aims to seek out the most appropriate copula which can model the dependence structure and measure the risk characteristic for the tail dependent asset returns. Based on the empirical data from the financial market, we begin with the analysis of the marginal choice for the copulas by comparing three different Archimedean copulas with respect to nonparametric kernel density estimation, semiparametric estimation and the estimation based on full empirical assumption of the margins, on the basis of which, we conduct the statistical estimation of the copula parameters using Inference functions for margins (IFM) and Canonical maximum likelihood (CML) methods. A procedure is thereafter proposed for identifying the most suitable copula. We then calibrate copula functions to recover the joint tail distribution and to quantify the magnitude of tail dependence by comparing different Archimedean copulas with the nonparametric empirical one. We present in detail from different aspects that Gumbel among three Archimedean members is the most suitable copula that has the desired property which is in accordance with the empirical behavior of our market data. © 2008 IEEE.}}, 
pages = {173--181}, 
number = {NA}, 
volume = {NA}
}
@article{10.1287/moor.1120.0577, 
year = {2013}, 
title = {{External risk measures and basel accords}}, 
author = {Kou, Steven and Peng, Xianhua and Heyde, Chris C}, 
journal = {Mathematics of Operations Research}, 
issn = {0364765X}, 
doi = {10.1287/moor.1120.0577}, 
abstract = {{Choosing a proper external risk measure is of great regulatory importance, as exemplified in the Basel II and Basel III Accords, which use value-at-risk with scenario analysis as the risk measures for setting capital requirements. We argue that a good external risk measure should be robust with respect to model misspecification and small changes in the data. A new class of data-based risk measures called natural risk statistics is proposed to incorporate robustness. Natural risk statistics are characterized by a new set of axioms. They include the Basel II and III risk measures and a subclass of robust risk measures as special cases; therefore, they provide a theoretical framework for understanding and, if necessary, extending the Basel Accords. © 2013 INFORMS.}}, 
pages = {393--417}, 
number = {3}, 
volume = {38}
}
@article{10.3182/20080706-5-kr-1001.2599, 
year = {2008}, 
title = {{On trading of equities: A robust control paradigm}}, 
author = {}, 
issn = {14746670}, 
doi = {10.3182/20080706-5-kr-1001.2599}, 
abstract = {{The objective of this paper is describe a new paradigm for the trading of equities. In our formulation, the control corresponds to a feedback law which modulates the amount invested I(t) in stock over time. The controller also includes a saturation limit Imax corresponding to a limit on the value at risk. The admissible stock price evolution p(t) over time is modelled as a family P of uncertain inputs against which we seek robust returns. Motivated by the fact that back-testing of candidate trading strategies involves significant cost and effort associated with computational simulation over sufficiently diverse markets, our paradigm involves the notion of synthetic prices and some idealizations involving the volatility of prices and trading liquidity. Our point of view is that a robust performance certification in this somewhat idealized market setting serves as a filter to determine if a trading strategy is worthy of the considerable time and expense associated with full-scale back-testing. The paper also includes a description of a so-called saturation reset controller. This controller is used to illustrate how the model works in practice and the attainment of robustness objectives over various sub-classes of P. Copyright © 2007 International Federation of Automatic Control All Rights Reserved.}}, 
number = {1 PART 1}, 
volume = {17}
}
@article{10.1016/j.csda.2009.03.008, 
year = {2010}, 
title = {{Time-varying joint distribution through copulas}}, 
author = {Ausin, M. Concepcion and Lopes, Hedibert F.}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2009.03.008}, 
abstract = {{The analysis of temporal dependence in multivariate time series is considered. The dependence structure between the marginal series is modelled through the use of copulas which, unlike the correlation matrix, give a complete description of the joint distribution. The parameters of the copula function vary through time, following certain evolution equations depending on their previous values and the historical data. The marginal time series follow standard univariate GARCH models. Full Bayesian inference is developed where the whole set of model parameters is estimated simultaneously. This represents an essential difference from previous approaches in the literature where the marginal and the copula parameters are estimated separately in two consecutive steps. Moreover, a Bayesian procedure is proposed for the estimation of several measures of risk, such as the variance, Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) of a portfolio of assets, providing point estimates and predictive intervals. The proposed copula model enables to capture the dependence structure between the individual assets which strongly influences these risk measures. Finally, the problem of optimal portfolio selection based on the estimation of meanvariance, meanVaR and meanCVaR efficient frontiers is also addressed. The proposed approach is illustrated with simulated and real financial time series. © 2009 Elsevier B.V. All rights reserved.}}, 
pages = {2383--2399}, 
number = {11}, 
volume = {54}
}
@article{10.1016/j.ijar.2012.06.007, 
year = {2013}, 
title = {{Four theorems and a financial crisis}}, 
author = {Das, Bikramjit and Embrechts, Paul and Fasen, Vicky}, 
journal = {International Journal of Approximate Reasoning}, 
issn = {0888613X}, 
doi = {10.1016/j.ijar.2012.06.007}, 
abstract = {{In this paper we give an academic assessment of the financial crisis (crises) from our point of view and discuss where quantitative risk management went wrong. We formulate four mathematical theorems/research areas which have relevance for financial crises in general where the underlying theme is model uncertainty. Related to these theorems, key issues that will be discussed are: financial alchemy on Wall street, risk aggregation and diversification, tail dependence for a portfolio of losses, and the significance of correlation bounds. © 2012 Elsevier Inc. All rights reserved.}}, 
pages = {701--716}, 
number = {6}, 
volume = {54}
}
@article{10.1016/j.amc.2020.125109, 
year = {2020}, 
title = {{Modeling right-skewed financial data streams: A likelihood inference based on the generalized Birnbaum–Saunders mixture model}}, 
author = {Naderi, Mehrdad and Hashemi, Farzane and Bekker, Andriette and Jamalizadeh, Ahad}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2020.125109}, 
abstract = {{Finite mixture models have recently been considered for analyzing positive support economical data streams with non-normal features. In this paper, a new mixture model based on the novel class of generalized Birnbaum–Saunders distributions is proposed to enhance strength and flexibility in modeling heterogeneous lifetime data. Some characteristics and properties of this mixture model are outlined. By presenting a convenient hierarchical representation, a mathematically elegant and computationally tractable EM-type algorithm is adopted for computing maximum likelihood estimates. Theoretical formulae of well-known risk measures referring to the class of generalized Birnbaum–Saunders distributions are derived. Finally, the utility of the postulated methodology is illustrated with some real-world data examples. © 2020 Elsevier Inc.}}, 
pages = {125109}, 
number = {NA}, 
volume = {376}
}
@article{10.1080/03610926.2015.1129421, 
year = {2017}, 
title = {{On sufficient conditions for the comparison of some quantile-based measures}}, 
author = {Belzunce, Félix and Martínez-Riquelme, Carolina}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2015.1129421}, 
abstract = {{In this article, we focus on characterizations and sufficient conditions for the comparison of some quantile-based measures. Mainly we focus on crossing conditions and changes of monotonicity of the quantile functions. The results are given for the comparison of the tail value at risk, total time on test, and the expected proportional shortfall measures. © 2017 Taylor \& Francis Group, LLC.}}, 
pages = {0--0}, 
number = {13}, 
volume = {46}
}
@article{10.1007/s00500-018-3094-0, 
year = {2019}, 
title = {{A new bi-objective fuzzy portfolio selection model and its solution through evolutionary algorithms}}, 
author = {Kar, Mohuya B. and Kar, Samarjit and Guo, Sini and Li, Xiang and Majumder, Saibal}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-018-3094-0}, 
abstract = {{In this paper, a new bi-objective fuzzy portfolio selection model is proposed, for which Sharp ratio (SR) and Value at Risk ratio (VR) of a portfolio are chosen as objectives. SR is an important nonsystematic risk measurement that examines the investment risk by aspiring the diversification of the capital allocation. On the other hand, VR measures the systematic risk, which reduces the largest loss of an investment at a given confidence level. The proposed fuzzy portfolio model assumes both SR and VR as maximization objectives for which the associated fuzzy parameters are considered as triangular fuzzy numbers. The proposed model is solved using multi-objective genetic algorithms, namely multi-objective cellular genetic algorithm (MOCell), archive-based hybrid scatter search (AbYSS), and nondominated sorting genetic algorithm II (NSGA-II). We have used a data set from the Shenzhen Stock Exchange to illustrate the performance of the proposed model and algorithms. Finally, a comparative study in terms of five standard performance metrics is presented, among the MOCell, AbYSS, and NSGA-II algorithms that are mentioned extensively in various research articles to exhibit the best suitable algorithm. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {4367--4381}, 
number = {12}, 
volume = {23}
}
@article{10.1108/ijoem-04-2015-0076, 
year = {2017}, 
title = {{Value-at-risk and stock returns: evidence from India}}, 
author = {Aziz, Tariq and Ansari, Valeed Ahmad}, 
journal = {International Journal of Emerging Markets}, 
issn = {17468809}, 
doi = {10.1108/ijoem-04-2015-0076}, 
abstract = {{Purpose: The purpose of this paper is to examine the role of value-at-risk (VaR) in the cross-section of stock returns in the Indian stock market during the period 1999-2014. Design/methodology/approach: The paper follows the methodology of Bali and Cakici (2004) to investigate the relationship between VaR and stock returns and employs Fama and French’s (1993) and Fama and Macbeth’s (1973) methods to find out the predictive power of VaR in time-series and cross-section settings. Further, it follows Fama and French (2008) to estimate separate cross-section regressions for small, medium and big stocks to verify the pervasiveness of the anomaly. Findings: This study finds positive risk premium associated with VaR in the Indian stock market during 2001-2008, the period of short selling constraint for institutional investors. This premium is confined to small stocks and low institutional holdings. The positive premium can be attributed to short selling constraints. Practical implications: The risk-return tradeoff can be utilized by investors and fund managers. As it is confined to small stocks, transaction costs may affect the profitability of the investment strategy. Originality/value: The study contributes to the scanty empirical literature on the role of VaR in the cross-section of expected stock returns. Moreover, this is the first study that explores the relationship between VaR and stock returns in the asset pricing context for the Indian stock market. © 2017, © Emerald Publishing Limited.}}, 
pages = {384--399}, 
number = {2}, 
volume = {12}
}
@article{10.1109/icsmc.2009.5346757, 
year = {2009}, 
title = {{Fuzzy portfolio selection based on value-at-risk}}, 
author = {Wang, Bo and Wang, Shuming and Watada, Junzo}, 
journal = {2009 IEEE International Conference on Systems, Man and Cybernetics}, 
issn = {1062922X}, 
doi = {10.1109/icsmc.2009.5346757}, 
abstract = {{In this paper, using Value-at-Risk, a new fuzzy portfolio selection model named VaR-FPSM is proposed. The Value-at-Risk is the measure of risk, which describes the greatest loss of an investment with some confidence level. When security returns are same kind of fuzzy variable, we derive two crisp equivalent forms of the VaR-FPSM. Furthermore, in general situations, we designed a fuzzy simulation based particle swarm optimization (PSO) algorithm to find an approximately optimal result. To illustrate the proposed model and hybrid PSO algorithm, a numerical example is provided and some discussions on the results are given. ©2009 IEEE.}}, 
pages = {1840--1845}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icbdaci.2017.8070810, 
year = {2017}, 
title = {{Comparative study and analysis of innovative approaches in financial assets identical allocation and genetic algorithm implications}}, 
author = {Neelima, Ch. and Sharma, S.S.V.N}, 
journal = {2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)}, 
issn = {NA}, 
doi = {10.1109/icbdaci.2017.8070810}, 
abstract = {{The financial data characteristically formed with series of cost, prices, bid, asset, ask, volume, element, time, date and so on. In financial, trading, asset distribution fields, the various levels and kinds of averages can be derived for representing form based indicators. There are no much definitive prescription on how to build a successful trading model and how to define the indicators for understanding banking organisations, financial based markets, trading industries. Some of the auto tuning search mechanism and optimization based techniques for understanding the financial situations. Financial optimization problem has become a standard financial engineering problem. Banking and industrial organisations are more profitable when they have the main goals to discover the best possible allocation of capital based assets between along with a set of possible and flexible assets by concurrently reducing the risk factors and improving the return of the financial-investments. Asset gathering is easy task than the distributing concern asset equally to required sectors for more profitable. Soft computing technology tools such as neural networks, fuzzy inference based system, genetic based algorithms are suitable for solvating the uncertainty problems in financial based asset distribution values. Genetic based algorithms implications are most useful for complex based problems such as asset equal distribution in finance based organisations like as banking, year wise budgets and so on. Genetic algorithm stages are more effective in asset distribution because of the initial population will be eliminated when new active generation has been invented and treated it as the basic element of future generation. The progression of reproduction, crossover and mutation implemented, repeated and finally future generations progress. This total procedure holds the relationship between estimation of nature and the algorithm. Most of the financial based companies, industries and banking sectors are puzzled in their financial transitions for distributing assets. This paper proposed the tabular form of comparative analysis and detail description of past designed innovative approaches for financial assets with equal distributions. This tabular and graphical form of analysis can help the banking, industry, government budget issue to equally distribute the capital asset values for various levels of industry needs and service cum profit modes. © 2017 IEEE.}}, 
pages = {61--66}, 
number = {NA}, 
volume = {NA}
}
@article{10.2143/ast.41.2.2136986, 
year = {2011}, 
title = {{Optimal reinsurance under VaR and CVaR risk measures: A simplified approach}}, 
author = {Chi, Yichun and Tan, Ken Seng}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.41.2.2136986}, 
abstract = {{In this paper, we study two classes of optimal reinsurance models by minimizing the total risk exposure of an insurer under the criteria of value at risk (VaR) and conditional value at risk (CVaR). We assume that the reinsurance premium is calculated according to the expected value principle. Explicit solutions for the optimal reinsurance policies are derived over ceded loss functions with increasing degrees of generality. More precisely, we establish formally that under the VaR minimization model, (i) the stop-loss reinsurance is optimal among the class of increasing convex ceded loss functions; (ii) when the constraints on both ceded and retained loss functions are relaxed to increasing functions, the stop-loss reinsurance with an upper limit is shown to be optimal; (iii) and finally under the set of general increasing and left-continuous retained loss functions, the truncated stop-loss reinsurance is shown to be optimal. In contrast, under CVaR risk measure, the stop-loss reinsurance is shown to be always optimal. These results suggest that the VaR-based reinsurance models are sensitive with respect to the constraints imposed on both ceded and retained loss functions while the corresponding CVaR-based reinsurance models are quite robust. © 2011 by Astin Bulletin.}}, 
pages = {487--509}, 
number = {2}, 
volume = {41}
}
@article{10.3182/20050703-6-cz-1902.02237, 
year = {2005}, 
title = {{The hidden risks of optimizing bond portfolios under var}}, 
author = {Winker, Peter and Maringer, Dietmar}, 
journal = {IFAC Proceedings Volumes}, 
issn = {14746670}, 
doi = {10.3182/20050703-6-cz-1902.02237}, 
abstract = {{Value at risk (VaR) has become a standard measure of portfolio risk over the last decade. It even became one of the corner stones in the Basel II accord about banks' equity requirements. Nevertheless, the practical application of the VaR concept suffers from two problems: how to estimate VaR and how to optimize a portfolio for a given level of VaR? This application to bond portfolios shows that a solution to the two aforementioned problems gives raise to a third one: the actual VaR of bond portfolios optimized under a VaR constraint might exceed its nominal level to a large extent. Thus, optimizing bond portfolios under a VaR constraint might increase risk. This finding is of relevance not only for investors, but even more so for bank regulation authorities. Copyright © 2005 IFAC.}}, 
pages = {6--11}, 
number = {NA}, 
volume = {16}
}
@article{10.1109/icmse.2010.5719853, 
year = {2010}, 
title = {{Asymmetric price adjustment between gasoline market and crude oil market when price is up and down}}, 
author = {Wen-dong, LV and Hui-feng, PAN}, 
journal = {2010 International Conference on Management Science \& Engineering 17th Annual Conference Proceedings}, 
issn = {NA}, 
doi = {10.1109/icmse.2010.5719853}, 
abstract = {{This article analyses the crude oil and gasoline price adjustments due to extreme price rise and price fall in the economic environment without market power. Using daily data for the period May 2000 to May 2005, this article adopts GARCH model with generalized error distribution to estimate the time-varying Value at Risk in both upside and downside directions at confidence level of 90 \% and 95 \%, and then uses granger causality in risk to investigate the extreme co-movement in both upside and downside directions between crude oil markets and gasoline markets from three continents in the world. The empirical results show that oil price adjustment exhibits asymmetric relationship, namely, the rise of price will transmit to another market within one day, while the decline of price will transmit to another market with delay. This means that the efficiency of information transmission when price are rising is higher than the instance when price are falling. Our results do not support the popular opinion that the asymmetric gasoline price adjustment is due to the market power. © 2010 IEEE.}}, 
pages = {527--532}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.pacfin.2020.101373, 
year = {2020}, 
title = {{Increasing the risk management effectiveness from higher accuracy: A novel non-parametric method}}, 
author = {Huang, Jinbo and Ding, Ashley and Li, Yong and Lu, Dong}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2020.101373}, 
abstract = {{The importance of effective risk management has never been greater in recent decades. However, the estimate accuracy of risk measures, such as Value-at-Risk, remains a challenge. In this paper, we propose a novel non-parametric method to efficiently enhance the accuracy of risk estimation as our method can avoid model misspecification and fully explore the tail information contained in asset returns. The non-parametric Value-at-Risk measure is embedded into a risk hedge model to increase the effectiveness of risk management. Simulations show that our new method outperforms existing methods in terms of accuracy. Empirical findings support that the improved estimation is helpful for more effectively managing risk in weather-sensitive markets. © 2020 Elsevier B.V.}}, 
pages = {101373}, 
number = {NA}, 
volume = {62}
}
@article{10.3844/jmssp.2016.99.106, 
year = {2016}, 
title = {{Testing VaR accuracy for CDS portfolios using historical simulation and delta-normal models}}, 
author = {Naimy, Viviane Y}, 
journal = {Journal of Mathematics and Statistics}, 
issn = {15493644}, 
doi = {10.3844/jmssp.2016.99.106}, 
abstract = {{This paper studies the accuracy of the VaR using the Delta- Normal and Historical approaches in measuring the risk of CDS portfolios in three different zones, US, Europe and Asia, for the period March 2013- November 2015. The portfolios consist exclusively of CDS of high rated banks and financial institutions. We found that at the 95\% confidence level both approaches were accurate using equal weights over 500 days. However, at the 99\% level, the Delta-Normal method underestimated the VaR except for Aisa portfolio even with the use of exponential weights for different values of λ. The Historical Simulation approach was more accurate because it accounted for fat tails and therefore avoided nonstationarity. Both approaches failed in accurately measuring the VaR over 50 days. © 2016 Viviane Y. Naimy.}}, 
pages = {99--106}, 
number = {2}, 
volume = {12}
}
@article{10.1016/j.jempfin.2006.10.003, 
year = {2008}, 
title = {{Volatility of stock price as predicted by patent data: An MGARCH perspective}}, 
author = {Chow, William W. and Fung, Michael K.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2006.10.003}, 
abstract = {{This paper proposes to model stock price volatility and variations in innovation effort using a Multivariate GARCH structure designed to extract information for risk prediction. The salient feature is that the model order, alongside other parameters, is endogenously determined by the estimation procedures. Using stock prices of U.S. computer firms, it is found that the model can pick up the correlation between the two variables and aid in producing accurate Value-at-Risk estimates. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {64--79}, 
number = {1}, 
volume = {15}
}
@article{10.1016/j.insmatheco.2013.09.012, 
year = {2013}, 
title = {{Optimal reinsurance in the presence of counterparty default risk}}, 
author = {Asimit, Alexandru V. and Badescu, Alexandru M. and Cheung, Ka Chun}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.09.012}, 
abstract = {{The optimal reinsurance arrangement is identified whenever the reinsurer counterparty default risk is incorporated in a one-period model. Our default risk model allows the possibility for the reinsurer to fail paying in full the promised indemnity, whenever it exceeds the level of regulatory capital. We also investigate the change in the optimal solution if the reinsurance premium recognises or not the default in payment. Closed form solutions are elaborated when the insurer's objective function is set via some well-known risk measures. It is also discussed the effect of reinsurance over the policyholder welfare. If the insurer is Value-at-Risk regulated, then the reinsurance does not increase the policyholder's exposure for any possible reinsurance transfer, even if the reinsurer may default in paying the promised indemnity. Numerical examples are also provided in order to illustrate and conclude our findings. It is found that the optimal reinsurance contract does not usually change if the counterparty default risk is taken into account, but one should consider this effect in order to properly measure the policyholders exposure. In addition, the counterparty default risk may change the insurer's ideal arrangement if the buyer and seller have very different views on the reinsurer's recovery rate. © 2013 Elsevier B.V.}}, 
pages = {690--697}, 
number = {3}, 
volume = {53}
}
@article{10.1080/14697680601155516, 
year = {2007}, 
title = {{Conditional tail behaviour and value at risk}}, 
author = {Bellini, Fabio and Figà-talamanca, Gianna}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680601155516}, 
abstract = {{In this paper we study the tail behaviour of eight major market indexes stratifying data according to the violation of a high threshold on the previous day. The distributional differences found can be exploited to improve VaR calculations in several settings, giving rise to what we call 'MCVaR'. We compare the performance of MCVaR with unconditioned VaR calculation methods and with GARCH VaR by means of several back-testing techniques that take into account not only the number of violations but also their magnitude and clustering.}}, 
pages = {599--607}, 
number = {6}, 
volume = {7}
}
@article{10.1016/j.jbankfin.2021.106248, 
year = {2021}, 
title = {{Forecasting VaR and ES using a joint quantile regression and its implications in portfolio allocation}}, 
author = {Merlo, Luca and Petrella, Lea and Raponi, Valentina}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2021.106248}, 
eprint = {2106.06518}, 
abstract = {{In this paper, we propose a multivariate quantile regression framework to forecast Value at Risk (VaR) and Expected Shortfall (ES) of multiple financial assets simultaneously, extending Taylor (2019). We generalize the Multivariate Asymmetric Laplace (MAL) joint quantile regression of Petrella and Raponi (2019) to a time-varying setting, which allows us to specify a dynamic process for the evolution of both the VaR and ES of each asset. The proposed methodology accounts for the dependence structure among asset returns. By exploiting the properties of the MAL distribution, we propose a new portfolio optimization method that minimizes portfolio risk and controls for well-known characteristics of financial data. We evaluate the advantages of the proposed approach on both simulated and real data, using weekly returns on three major stock market indices. We show that our method outperforms other existing models and provides more accurate risk measure forecasts than univariate methods. © 2021 Elsevier B.V.}}, 
pages = {106248}, 
number = {NA}, 
volume = {133}
}
@article{10.1109/iccoins.2018.8510576, 
year = {2018}, 
title = {{A Fuzzy Index Tracking Multi-Objective Approach to Stock Data Analytics}}, 
author = {Zhang, Huiming and Watada, Junzo}, 
journal = {2018 4th International Conference on Computer and Information Sciences (ICCOINS)}, 
issn = {NA}, 
doi = {10.1109/iccoins.2018.8510576}, 
abstract = {{Index tracking is an passive strategy in portfolio management, it mimics the performance of a benchmark index to construct portfolios for obtaining the average return of the target market. Index tracking has become popular in investors because it possesses the advantages of low cost, high liquidity and lower risk. This paper introduced sensitivity analysis to construct a fuzzy multi-objective index tracking portfolio model with value at risk (SA-IT-VAR-FMOPM) when return rate was set as parabolic fuzzy variable, the sensitivity and VaR factors were considered in the model. An improved particle swarm optimization (IPSO) algorithm was used to search optimal solution for multi-objective problem. To verify the effective of the proposed model, Dow 30 index data were selected to the empirical experiment, the results show the fuzzy multi-objective index tracking portfolio model which considered the sensitivity and VaR factors can obtain more stable portfolio and achieve the average return of target market. © 2018 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1111/j.1467-842x.2007.00483.x, 
year = {2007}, 
title = {{Semiparametric estimation of the error distribution in multivariate regression using copulas}}, 
author = {Kim, Gunky and Silvapulle, Mervyn J. and Silvapulle, Paramsothy}, 
journal = {Australian \& New Zealand Journal of Statistics}, 
issn = {13691473}, 
doi = {10.1111/j.1467-842x.2007.00483.x}, 
abstract = {{Summary A semiparametric method is developed to estimate the dependence parameter and the joint distribution of the error term in the multivariate linear regression model. The nonparametric part of the method treats the marginal distributions of the error term as unknown, and estimates them using suitable empirical distribution functions. Then the dependence parameter is estimated by either maximizing a pseudolikelihood or solving an estimating equation. It is shown that this estimator is asymptotically normal, and a consistent estimator of its large sample variance is given. A simulation study shows that the proposed semiparametric method is better than the parametric ones available when the error distribution is unknown, which is almost always the case in practice. It turns out that there is no loss of asymptotic efficiency as a result of the estimation of regression parameters. An empirical example on portfolio management is used to illustrate the method. © 2007 Australian Statistical Publishing Association Inc.}}, 
pages = {321--336}, 
number = {3}, 
volume = {49}
}
@article{10.1016/j.insmatheco.2015.06.005, 
year = {2015}, 
title = {{Convex ordering for insurance preferences}}, 
author = {Cheung, K.C. and Chong, W.F. and Yam, S.C.P.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.06.005}, 
abstract = {{In this article, we study two broad classes of convex order related optimal insurance decision problems, in which the objective function or the premium valuation is a general functional of the expectation, Value-at-Risk and Average Value-at-Risk of the loss variables. These two classes of problems include many existing and contemporary optimal insurance problems as interesting examples being prevalent in the literature. To solve these problems, we apply the Karlin-Novikoff-Stoyan-Taylor multiple-crossing conditions, which is a useful sufficient criterion in the theory of convex ordering, to replace an arbitrary insurance indemnity by a more favorable one in convex order sense. The convex ordering established provides a unifying approach to solve the special cases of the problem classes. We show that the optimal indemnities for these problems in general take the double layer form. © 2015 Elsevier B.V.}}, 
pages = {409--416}, 
number = {NA}, 
volume = {64}
}
@article{10.1016/j.frl.2016.03.019, 
year = {2016}, 
title = {{The performance of the switching forecast model of value-at-risk in the Asian stock markets}}, 
author = {Chiu, Yen-Chen and Chuang, I-Yuan}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2016.03.019}, 
abstract = {{This paper examines a comparative risk forecast experiment for Asian stock markets. Apart from the literature, this work extends previous methods to propose a Switching forecast model to increase forecast effectiveness. The Switching forecast model is explicitly designed to estimate the forecasting problem faced by the risk manager who does not rely on a specific Value-at-Risk (VaR) model and allows for the VaR model to change over time. It is found that the Switching forecast model is not only capable of capturing the characteristics of Asian stock markets but also provides a satisfactorily accurate measurement based on coverage tests. Additionally, the superiority test indicates statistically that the Switching forecast model is more effective than alternative models based on quadratic loss function. © 2016 Elsevier Inc.}}, 
pages = {43--51}, 
number = {NA}, 
volume = {18}
}
@article{10.1109/cso.2009.350, 
year = {2009}, 
title = {{A statistical neural network approach for value-at-risk analysis}}, 
author = {Chen, Xiaoliang and Lai, Kin Keung and Yen, Jerome}, 
journal = {2009 International Joint Conference on Computational Sciences and Optimization}, 
issn = {NA}, 
doi = {10.1109/cso.2009.350}, 
abstract = {{This study develops a new methodology based on ANN for Value-at-Risk (VaR) modeling. Specifically, we propose a statistical procedure for ANN model selection. The statistical ANN deals with each layer individually and estimates the weights of subsequent layer with those of preceding layers fixed. This allows the derivation of statistical theory for model selection, which reduces the need to fit a comprehensive set of models. Experiment results show that the statistical ANN approach performs well on stock index return series compared to traditional forecasting methods. © 2009 IEEE.}}, 
pages = {17--21}, 
number = {NA}, 
volume = {2}
}
@article{10.1142/s0219024906003548, 
year = {2006}, 
title = {{Does the application of innovative internal models diminish regulatory capital?}}, 
author = {KALYVAS, LAMPROS and SFETSOS, ATHANASIOS}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024906003548}, 
abstract = {{The broad spectrum and the increased complexity of financial products that compose modern portfolios have forced credit and financial institutions to focus on innovative and more effective ways of estimating market risks. These new approaches, very often, prove to be more conservative compared to traditional approaches in terms of market risk quantification. On the other hand, according to the Basel Committee evaluation framework, this conservatism is rewarded with lower multiplication factors when calculations of capital requirements take place. The present study elaborates on the comparison of several Value-at-Risk (VaR) methodologies based on the capital requirements they provide according to the Basel Committee regulatory framework. © World Scientific Publishing Company.}}, 
pages = {217--226}, 
number = {2}, 
volume = {9}
}
@article{10.1016/j.iref.2018.12.016, 
year = {2019}, 
title = {{Choosing expected shortfall over VaR in Basel III using stochastic dominance}}, 
author = {Chang, Chia-Lin and Jimenez-Martin, Juan-Angel and Maasoumi, Esfandiar and McAleer, Michael and Pérez-Amaral, Teodosio}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2018.12.016}, 
abstract = {{In this paper we use stochastic dominance to evaluate the consequences of moving from Value-at-Risk (VaR) to Expected Shortfall (ES) from a policy maker's perspective. In particular, we compare VaR at the 99\% level (VaR99) and ES at the 97.5\% level (ES97.5). We contemplate VaR99 and ES97.5 as two alternative risk metrics according to the capital adequacy bank regulation, as suggested by Basel III. Moving from VaR99 to ES97.5 will have effects in terms of the quantity and quality of the capital required to banks. According to the Basel Committee on Banking Supervision (2013, page 18): “the Committee believes that moving to a confidence level of 97.5\% (relative to the 99th percentile confidence level for the current VaR measure) is appropriate.” Stochastic dominance of the capital requirement distribution after the change in bank regulations, as suggested by Basel III, over the capital requirement under the old regulation, implies that a policy maker with any positive marginal utility of capital requirements (and a negative second derivative for risk aversion) would prefer it. SD tests examines if the rankings of outcomes are utility function specific, or uniform, over all decision makers with preferences in the class considered. © 2018 Elsevier Inc.}}, 
pages = {95--113}, 
number = {NA}, 
volume = {60}
}
@article{10.1515/jtse-2017-0011, 
year = {2018}, 
title = {{Methods for computing numerical standard errors: Review and application to value-at-risk estimation}}, 
author = {Ardia, David and Bluteau, Keven and Hoogerheide, Lennart F.}, 
journal = {Journal of Time Series Econometrics}, 
issn = {21946507}, 
doi = {10.1515/jtse-2017-0011}, 
abstract = {{Numerical standard error (NSE) is an estimate of the standard deviation of a simulation result if the simulation experiment were to be repeated many times. We review standard methods for computing NSE and perform a Monte Carlo experiments to compare their performance in the case of high/extreme autocorrelation. In particular, we propose an application to risk management where we assess the precision of the value-at-risk measure when the underlying risk model is estimated by simulation-based methods. Overall, heteroscedasticity and autocorrelation estimators with prewhitening perform best in the presence of large/extreme autocorrelation. © 2018 Walter de Gruyter GmbH, Berlin/Boston 2018.}}, 
pages = {20170011}, 
number = {2}, 
volume = {10}
}
@article{10.1016/j.eneco.2014.02.019, 
year = {2014}, 
title = {{Modelling the general dependence between commodity forward curves}}, 
author = {Zolotko, Mikhail and Okhrin, Ostap}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2014.02.019}, 
abstract = {{This study proposes a novel framework for the joint modelling of commodity forward curves. Its key contribution is twofold. First, we introduce a family of dynamic conditional correlation models based on hierarchical Archimedean copulae (HAC-DCC), which are flexible but parsimonious instruments that capture a wide range of dynamic dependencies. Second, we apply these models in the context of commodity forward curves as part of the framework. An extensive Value-at-Risk analysis shows that certain HAC-DCC models consistently outperform other introduced benchmarks in terms of the preciseness of their out-of-sample distribution forecasts of the returns of various commodity futures portfolios. This shows that the proposed modelling framework, as one of its possible applications, can be a useful and convenient risk management tool. © 2014 Elsevier B.V.}}, 
pages = {284--296}, 
number = {NA}, 
volume = {43}
}
@article{10.1016/j.irfa.2016.12.005, 
year = {2017}, 
title = {{The price of shelter - Downside risk reduction with precious metals}}, 
author = {Bredin, Don and Conlon, Thomas and Potì, Valerio}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2016.12.005}, 
abstract = {{Investor aversion to extreme losses may motivate them to seek out investments perceived to function as a safe haven during times of crisis. In this study, we consider the potential for precious metals to mitigate downside risk when combined with equities, and evaluate the impact on portfolio risk-adjusted returns. Each of gold, silver and platinum are found to contribute to downside risk reduction at short horizons, but diversification into silver and platinum may result in increased long horizon portfolio risk. The price of sheltering an equity portfolio from downside risk is a relative reduction in portfolio risk-adjusted returns. Variance and kurtosis properties of precious metals are identified as marginal contributors to downside risk reduction. Futures markets on precious metals are also shown to present an interesting and viable diversification alternative to physical metals. © 2016}}, 
pages = {48--58}, 
number = {NA}, 
volume = {49}
}
@article{10.4028/www.scientific.net/amr.204-210.537, 
year = {2011}, 
title = {{Risk asset portfolio choice models under three risk measures}}, 
author = {Wang, Yu Ling and Ma, Jun Hai and Xu, Yu Hua}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.204-210.537}, 
abstract = {{Mean-variance model, value at risk and Conditional Value at Risk are three chief methods to measure financial risk recently. The demonstrative research shows that three optional questions are equivalence when the security rates have a multivariate normal distribution and the given confidence level is more than a special value. Applications to real data provide empirical support to this methodology. This result has provided new methods for us about further research of risk portfolios.}}, 
pages = {537--540}, 
number = {NA}, 
volume = {204-210}
}
@article{10.1016/j.physa.2013.01.032, 
year = {2013}, 
title = {{Measuring daily Value-at-Risk of SSEC index: A new approach based on multifractal analysis and extreme value theory}}, 
author = {Wei, Yu and Chen, Wang and Lin, Yu}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2013.01.032}, 
abstract = {{Recent studies in the econophysics literature reveal that price variability has fractal and multifractal characteristics not only in developed financial markets, but also in emerging markets. Taking high-frequency intraday quotes of the Shanghai Stock Exchange Component (SSEC) Index as example, this paper proposes a new method to measure daily Value-at-Risk (VaR) by combining the newly introduced multifractal volatility (MFV) model and the extreme value theory (EVT) method. Two VaR backtesting techniques are then employed to compare the performance of the model with that of a group of linear and nonlinear generalized autoregressive conditional heteroskedasticity (GARCH) models. The empirical results show the multifractal nature of price volatility in Chinese stock market. VaR measures based on the multifractal volatility model and EVT method outperform many GARCH-type models at high-risk levels. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {2163--2174}, 
number = {9}, 
volume = {392}
}
@article{10.1007/s00500-018-3492-3, 
year = {2020}, 
title = {{Tail value-at-risk in uncertain random environment}}, 
author = {Liu, Yuhan and Ralescu, Dan A. and Xiao, Chen and Lio, Waichon}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-018-3492-3}, 
abstract = {{Chance theory is a rational tool to be used in the systems which contain not only uncertainty but also randomness. In this paper, the concept of tail value-at-risk in uncertain random risk analysis is proposed and some theorems are provided for its calculation. Moreover, the tail value-at-risk is applied as the right-tail in the parallel system, series system, standby system, k-out-of-n system and structural system. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {2495--2502}, 
number = {4}, 
volume = {24}
}
@article{10.1109/cybconf47073.2019.9436572, 
year = {2019}, 
title = {{Value at risk performance in BRICS countries}}, 
author = {Wu, Desheng and Thanyaluckpark, Chatpailin}, 
journal = {2019 4th IEEE International Conference on Cybernetics (Cybconf)}, 
issn = {NA}, 
doi = {10.1109/cybconf47073.2019.9436572}, 
abstract = {{This study aims to verify the actuary of the VaR model to find out whether the VaR model can provide accurate risk measurement results for market risk, especially the stock exchange market of BRICS, particularly historical VaR and delta normal VaR model. We used the POF test and independence test of Kupiec - Christoffersen (1998), which is commonly used to determine the accuracy of VaR for the backtesting VaR model. The result of the risk measurement performance for historical VaR provides a reasonably reliable VaR over delta normal by using Kupiec's POF-test for VaR model accuracy. The independence test of Christoffersen(1998) indicates exceptions to dependency (failures) in the historical VaR and delta normal VaR model. © 2019 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ijepes.2009.09.018, 
year = {2010}, 
title = {{Strategic evaluation of bilateral contract for electricity retailer in restructured power market}}, 
author = {Karandikar, R.G. and Khaparde, S.A. and Kulkarni, S.V.}, 
journal = {International Journal of Electrical Power \& Energy Systems}, 
issn = {01420615}, 
doi = {10.1016/j.ijepes.2009.09.018}, 
abstract = {{In a competitive market scenario, consumers make payments for the consumption of electricity to retailers at fixed tariff. The retailers buy power at the Market Clearing Price (MCP) in spot market and/or through bilateral contract at agreed upon price. Due to these different modes at buying and selling ends, the retailers are faced with an involved task of estimating their payoffs along with the risk-quantification. The methodology presented in this paper gives a range of bilateral quantity and associated price for a retailer to ensure risk-constrained payoff. The exercise is carried out with a single retailer in the market as well as for a case of competition amongst two retailers. Risk is quantified using Risk Adjusted Recovery on Capital (RAROC). The problem is evaluated to get a range of bilateral quantity to be quoted for a particular bilateral price at fixed tariff of loyal load and fixed value of switching load. This summary combined with risk-averseness of the retailer leads him to make a judicial choice about bilateral transactions such that it leads to a risk-constrained payoff. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {457--463}, 
number = {5}, 
volume = {32}
}
@article{10.1016/j.ijepes.2007.05.003, 
year = {2007}, 
title = {{Risk management in a competitive electricity market}}, 
author = {Liu, Min and Wu, Felix F.}, 
journal = {International Journal of Electrical Power \& Energy Systems}, 
issn = {01420615}, 
doi = {10.1016/j.ijepes.2007.05.003}, 
abstract = {{In a competitive electricity market, it is necessary and important to develop an appropriate risk management scheme for trade with full utilization of the multi-market environment in order to maximize participants' benefits and minimize the corresponding risks. Based on the analyses to trading environments and risks in the electricity market, a layered framework of risk management for electric energy trading is proposed in this paper. Simulation results confirmed that trading among multiple markets is helpful to reduce the complete risk, and VaR provides a useful approach to judge whether the formed risk-control scheme is acceptable. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {690--697}, 
number = {9}, 
volume = {29}
}
@article{10.1080/1351847x.2010.481467, 
year = {2011}, 
title = {{Translation-invariant and positive-homogeneous risk measures and optimal portfolio management}}, 
author = {Landsman, Z. and Makov, U.}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2010.481467}, 
abstract = {{The problem of risk portfolio optimization with translation-invariant and positive-homogeneous risk measures, which includes value-at-risk (VaR) and tail conditional expectation (TCE), leads to the problem of minimizing a combination of a linear functional and a square root of a quadratic functional for the case of elliptical multivariate underlying distributions. In this paper, we provide an explicit closed-form solution of this minimization problem, and the condition under which this solution exists. The results are illustrated using the data of 10 stocks from NASDAQ/Computers. The distance between the VaR and TCE optimal portfolios has been investigated. © 2011 Taylor \& Francis.}}, 
pages = {307--320}, 
number = {4}, 
volume = {17}
}
@article{10.1287/mnsc.1070.0769, 
year = {2008}, 
title = {{Incorporating asymmetric distributional information in robust value-at-risk optimization}}, 
author = {Natarajan, Karthik and Pachamanova, Dessislava and Sim, Melvyn}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1070.0769}, 
abstract = {{Value-at-Risk (VaR) is one of the most widely accepted risk measures in the financial and insurance industries, yet efficient optimization of VaR remains a very difficult problem. We propose a computationally tractable approximation method for minimizing the VaR of a portfolio based on robust optimization techniques. The method results in the optimization of a modified VaR measure, Asymmetry-Robust VaR (ARVaR), that takes into consideration asymmetries in the distributions of returns and is coherent, which makes it desirable from a financial theory perspective. We show that ARVaR approximates the Conditional VaR of the portfolio as well. Numerical experiments with simulated and real market data indicate that the proposed approach results in lower realized portfolio VaR, better efficient frontier, and lower maximum realized portfolio loss than alternative approaches for quantile-based portfolio risk minimization. © 2008 INFORMS.}}, 
pages = {573--585}, 
number = {3}, 
volume = {54}
}
@article{10.1016/j.insmatheco.2015.06.009, 
year = {2015}, 
title = {{Risk concentration based on Expectiles for extreme risks under FGM copula}}, 
author = {Mao, Tiantian and Yang, Fan}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2015.06.009}, 
abstract = {{Risk concentration is used as a measurement of diversification benefits in the context of risk aggregation. Expectiles, which are known to possess many good properties, have attracted increasing interest in recent years. In this paper, we aim to study the asymptotic properties of risk concentration based on Expectiles. Firstly, we extend the results on the second-order asymptotics of Expectiles in Mao et al. (2015). Secondly, we investigate the second-order asymptotics of tail probabilities and then apply them to risk concentrations based on Expectiles as well as on VaR. © 2015 Elsevier B.V.}}, 
pages = {429--439}, 
number = {NA}, 
volume = {64}
}
@article{10.1016/j.insmatheco.2012.10.007, 
year = {2013}, 
title = {{Optimal reinsurance with concave ceded loss functions under VaR and CTE risk measures}}, 
author = {Lu, ZhiYi and Liu, LePing and Meng, ShengWang}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2012.10.007}, 
abstract = {{Most of the studies on optimal reinsurance are from the viewpoint of the insurer and the optimal ceded functions always turn out to be convex. However reinsurance contracts always involve a limit on the ceded loss function in practice, thus it may not be enough to confine the analysis to the class of convex functions only. In this paper, we study the problem of optimal reinsurance under VaR and CTE optimization criteria when the ceded loss functions are in the class of increasing concave functions. By using a simple geometric approach, we prove that under the VaR optimization criterion, the quota-share reinsurance with a policy limit is always optimal, while the full reinsurance with a policy limit is optimal under the CTE optimization criterion. Some illustrative examples are presented. © 2012 Elsevier B.V.}}, 
pages = {46--51}, 
number = {1}, 
volume = {52}
}
@article{10.1016/j.knosys.2018.06.007, 
year = {2018}, 
title = {{A fuzzy credibility model to estimate the Operational Value at Risk using internal and external data of risk events}}, 
author = {Peña, Alejandro and Bonet, Isis and Lochmuller, Christian and Patiño, Héctor Alejandro and Chiclana, Francisco and Góngora, Mario}, 
journal = {Knowledge-Based Systems}, 
issn = {09507051}, 
doi = {10.1016/j.knosys.2018.06.007}, 
abstract = {{Operational Risk (OpR) refers to the possibility of suffering losses resulting from inadequate or failure of processes and/or technology, inadequate behaviour of people or external events. OpR was one of the main risks that led to the 2008 global financial crisis. Limitations of the analytical models that are applied in estimating this risk surface when qualitative information, frequently associated with OpR events, is used. To determine the magnitude of OpR in financial organisations, qualitative data and also historical data from risk events can be used. Current research trends that focus on the development of analytical models, by using different databases, to estimate the Operational Value at Risk (OpVaR) still lack models based on qualitative information, risk management profiles and the ability to integrate different databases of OpR events. In this paper we present a fuzzy model to estimate the OpVaR of an organisation by working with two different databases that contain internal available data and external or observed data. The proposed model considers: (1) the intrinsic properties of the data as fuzzy sets related to the linguistic variables of the observed data (external) and the data from available databases (internal), and (2) a series of management profiles to mitigate the effect that external data usually causes in estimating the OpVaR of an organisation. The results obtained with the proposed model allow an organisation to estimate and determine the behaviour of the OpVaR over time by using different risk profiles. The integration of qualitative information, different risk profiles (ranging from weak to strong risk management), and internal and external databases contributes to the advancement of estimating the OpVaR in risk management. © 2018}}, 
pages = {98--109}, 
number = {NA}, 
volume = {159}
}
@article{10.4028/www.scientific.net/amm.135-136.1051, 
year = {2012}, 
title = {{The dynamics of VaRs with skew t distribution for A300 index in China}}, 
author = {Zheng, Cheng Li and Huang, Dong Dong}, 
journal = {Applied Mechanics and Materials}, 
issn = {16609336}, 
doi = {10.4028/www.scientific.net/amm.135-136.1051}, 
abstract = {{The high frequency of financial crises make the risk management of financial asset become the focus of the financial investors and scholars. The traditional VaR based on assumption about the normal distribution of yield is not suitable for the real thing in China. The characteristics of stock market yield in China is fat tail and non-symmetry. In this paper, the dynamics of VaRs based on TARCH model with the skew t distribution are computed and analysed. And then failure rate of VaRs are compared under normal distribution, student's-t distribution and GED distribution. The results show that the most accurate VaR is the one with the skew t distribution, which describes the reality of the stock market better than others.}}, 
pages = {1051--1056}, 
number = {NA}, 
volume = {135-136}
}
@article{10.1007/978-3-319-26416-5_4, 
year = {2015}, 
title = {{CyVar: Extending Var-At-Risk to ICT}}, 
author = {Baiardi, Fabrizio and Tonelli, Federico and Bertolini, Alessandro}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-319-26416-5\_4}, 
abstract = {{CyVar extends the Value-At-Risk statistics to ICT systems under attack by intelligent, goal oriented agents. CyVar is related to the time it takes an agent to acquire some access privileges and to the one it owns these privileges. To evaluate the former time, we use the security stress, a synthetic measure of the robustness of an ICT system. We approximate this measure through the Haruspex suite, an integrated set of tools that supports ICT risk assessment and management. After defining CyVar, we show how it supports the evaluation of three versions of an industrial control system. © Springer International Publishing Switzerland 2015.}}, 
pages = {49--62}, 
number = {NA}, 
volume = {9488}
}
@article{10.1016/j.irfa.2012.06.001, 
year = {2013}, 
title = {{Forecasting value-at-risk and expected shortfall using fractionally integrated models of conditional volatility: International evidence}}, 
author = {Degiannakis, Stavros and Floros, Christos and Dent, Pamela}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2012.06.001}, 
abstract = {{The present study compares the performance of the long memory FIGARCH model, with that of the short memory GARCH specification, in the forecasting of multi-period value-at-risk (VaR) and expected shortfall (ES) across 20 stock indices worldwide. The dataset is composed of daily data covering the period from 1989 to 2009. The research addresses the question of whether or not accounting for long memory in the conditional variance specification improves the accuracy of the VaR and ES forecasts produced, particularly for longer time horizons. Accounting for fractional integration in the conditional variance model does not appear to improve the accuracy of the VaR forecasts for the 1-day-ahead, 10-day-ahead and 20-day-ahead forecasting horizons relative to the short memory GARCH specification. Additionally, the results suggest that underestimation of the true VaR figure becomes less prevalent as the forecasting horizon increases. Furthermore, the GARCH model has a lower quadratic loss between actual returns and ES forecasts, for the majority of the indices considered for the 10-day and 20-day forecasting horizons. Therefore, a long memory volatility model compared to a short memory GARCH model does not appear to improve the VaR and ES forecasting accuracy, even for longer forecasting horizons. Finally, the rolling-sampled estimated FIGARCH parameters change less smoothly over time compared to the GARCH models. Hence, the parameters' time-variant characteristic cannot be entirely due to the news information arrival process of the market; a portion must be due to the FIGARCH modelling process itself. © 2012 Elsevier Inc.}}, 
pages = {21--33}, 
number = {NA}, 
volume = {27}
}
@article{10.1080/00036846.2015.1133897, 
year = {2016}, 
title = {{Estimating multi-period Value at Risk of oil futures prices}}, 
author = {Zhou, Chunyang and Qin, Xiao and Diao, Xundi and He, Yingchen}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2015.1133897}, 
abstract = {{In this study, we estimate the multi-period Value at Risk (VaR) of oil future prices under a generalized autoregressive conditional heteroscedasticity with a skewed- (Formula presented.) residuals (GARCH-ST) model, which is developed to account for the stylized facts of oil futures returns, such as serial correlation, volatility clustering, asymmetry and heavy tails. An efficient approximation algorithm based on the moment calibration method is developed to compute the multi-period VaR, and the numerical experiments show that the algorithm can yield good approximation quality. In the empirical analysis, we find that the GARCH-ST model can yield superior out-of-sample performance to a GARCH-normal model or a GARCH- (Formula presented.) model, especially when measuring the extreme tail risk. Meanwhile, the square root of time rule (SRTR) tends to underestimate the multi-period tail risk, and cannot produce a better performance than the GARCH family models. © 2016 Taylor \& Francis.}}, 
pages = {2994--3004}, 
number = {32}, 
volume = {48}
}
@article{10.3233/978-1-61499-828-0-79, 
year = {2017}, 
title = {{VaR and tail dependence between the US and Asian stock exchange indices - An EGARCH-copula approach}}, 
author = {}, 
issn = {09226389}, 
doi = {10.3233/978-1-61499-828-0-79}, 
abstract = {{This paper uses the eGARCH-Copula model to examine the tail dependence and Value at Risk (VaR) of the log returns of the US and Asian exchange indices as pairs of portfolio in three periods: before, in and after finance crisis. The results indicated that the eGARCH-Copula model works well on measuring the tail dependence and VaR between the US and Asian stock market; and after finance crisis, the dependence structure changed including on tail dependence and VaR. © 2017 The authors and IOS Press. All rights reserved.}}, 
number = {NA}, 
volume = {299}
}
@article{10.1016/j.insmatheco.2013.05.002, 
year = {2013}, 
title = {{Optimal reinsurance subject to vajda condition}}, 
author = {Chi, Yichun and Weng, Chengguo}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2013.05.002}, 
abstract = {{In this paper, we study optimal reinsurance design by minimizing the risk-adjusted value of an insurer's liability, where the valuation is carried out by a cost-of-capital approach based either on the value at risk or the conditional value at risk. To prevent moral hazard and to be consistent with the spirit of reinsurance, we follow Vajda (1962) and assume that both the insurer's retained loss and the proportion paid by a reinsurer are increasing in indemnity. We analyze the optimal solutions for a wide class of reinsurance premium principles which satisfy three axioms (law invariance, risk loading and preserving convex order) and encompass ten of the eleven widely used premium principles listed in Young (2004). Our results show that the optimal ceded loss functions are in the form of three interconnected line segments. Further simplified forms of the optimal reinsurance are obtained for the premium principles under an additional mild constraint. Finally, to illustrate the applicability of our results, we derive the optimal reinsurance explicitly for both the expected value principle and Wang's principle. © 2013 Elsevier B.V.}}, 
pages = {179--189}, 
number = {1}, 
volume = {53}
}
@article{10.1109/ssp.2016.7551835, 
year = {2016}, 
title = {{A skewed exponential power distribution to measure value at risk in electricity markets}}, 
author = {Thibault, Aymeric and Bandon, Pascal}, 
journal = {2016 IEEE Statistical Signal Processing Workshop (SSP)}, 
issn = {NA}, 
doi = {10.1109/ssp.2016.7551835}, 
abstract = {{Interest in risk measurement for spot price has increased since the worldwide deregulation and liberalization of electricity started in the early 90's. This paper is focused on quantifying risk for the Nordic Power Exchange (Nord Pool) system price. Our analysis is based on a generalized autoregressive conditional heteroskedastic (GARCH) process with skewed exponential power innovations to model the stochastic component of the system price. Value at risk (VaR) backtesting procedures are presented and our model performance is compared to commonly used distributions in risk measurement. We show that the skewed exponential power distribution outperforms the competitors for the upside risk (95\%, 97.5\% and 99\% VaR), which is of high interest as electricity spot prices are positively skewed. © 2016 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {2016-August}
}
@article{10.1080/14697688.2018.1436765, 
year = {2018}, 
title = {{On the price of risk in a mean-risk optimization model}}, 
author = {Dentcheva, Darinka and Stock, Gregory J.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2018.1436765}, 
abstract = {{We investigate a mean-risk model for portfolio optimization where the risk quantifier is selected as a semi-deviation or as a standard deviation of the portfolio return. We analyse the existence of solutions to the problem under general assumptions. When the short positions are not constrained, we establish a lower bound on the cost of risk associated with optimizing the mean–standard deviation model and show that optimal solutions do not exist for any positive price of risk which is smaller than that bound. If the investment allocations are constrained, then we obtain a lower bound on the price of risk in terms of the shadow prices of said constraints and the data of the problem. A Value-at-Risk constraint in the model implies an upper bound on the price of risk for all feasible portfolios. Furthermore, we provide conditions under which using this upper bound as the cost of risk parameter in the model provides a non-dominated optimal portfolio with respect to the second-order stochastic dominance. Additionally, we study the relationship between minimizing the mean–standard deviation objective and maximizing the coefficient of variation and show that both problems are equivalent when the upper bound is used as the cost of risk. Additional relations between the Value-at-Risk constraint and the coefficient of variation are discussed as well. We illustrate the results numerically. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--15}, 
number = {10}, 
volume = {18}
}
@article{10.1080/14697688.2013.847280, 
year = {2014}, 
title = {{Copula dynamics in CDOs}}, 
author = {Choroś-Tomczyk, Barbara and Härdle, Wolfgang Karl and Overbeck, Ludger}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2013.847280}, 
abstract = {{Values of tranche spreads of collateralized debt obligations (CDOs) are driven by the joint default performance of the assets in the collateral pool. The dependence between the entities in the portfolio mainly depends on current economic conditions. Therefore, a correlation implied from tranches can be seen as a measure of the general situation of the credit market. We analyse the European market of standardized CDOs using tranches of the iTraxx index in the periods before and during the global financial crisis. We investigate the evolution of the correlations using different copula models: the standard Gaussian, the NIG, the double-t, and the Gumbel copula model. After calibration of these models, one obtains a time varying vector of parameters. We analyse the dynamic pattern of these coefficients. That enables us to forecast future parameters and consequently calculate Value-at-Risk measures for iTraxx Europe tranches. © 2014 Copyright Taylor \& Francis Group, LLC.}}, 
pages = {1573--1585}, 
number = {9}, 
volume = {14}
}
@article{10.3390/en13215852, 
year = {2020}, 
title = {{An optimal phase arrangement of distribution transformers under risk assessment}}, 
author = {Tu, Chia-Sheng and Yang, Chung-Yuen and Tsai, Ming-Tang}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en13215852}, 
abstract = {{This paper presents a phase arrangement procedure for distribution transformers to improve system unbalance and voltage profile of distribution systems, while considering the location and uncertainties of the wind turbine (WT) and photovoltaics (PV). Based on historical data, the Monte Carlo method is used to calculate the power generation value-at-risk (VAR) of WTs/PVs installed under a given level of confidence. The main target of this paper is to reduce the line loss and unbalance factor during 24-hour intervals. Assessing the various confidence levels of risk, a feasible particle swarm optimization (FPSO) is proposed to solve the optimal location of WTs/PVs installed and transformer load arrangement. A three-phase power flow with equivalent current injection (ECI) is analyzed to demonstrate the operating efficiency of the FPSO in a Taipower feeder. Simulation results will support the planner in the proper location of WTs/PVs installed to reduce system losses and maintain the voltage profile. They can also provide more risk information for handing uncertainties when the renewable energy is connected to the distribution system. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {5852}, 
number = {21}, 
volume = {13}
}
@article{10.1016/j.eswa.2010.09.024, 
year = {2011}, 
title = {{Activity-based divergent supply chain planning for competitive advantage in the risky global environment: A DEMATEL-ANP fuzzy goal programming approach}}, 
author = {Hung, Shih-Jieh}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2010.09.024}, 
abstract = {{Supply chain management allows modern enterprises to relax their own capacities and produce in a more flexible manner for diversified consumer demands. However, for an enterprise with divergent supply chain (DSC) and multiple product lines, to plan the production allocation for higher competitive advantage in the risky global market is a challenging problem. The existing literature still has not address this problem very well. This paper is aimed to treat this problem by using an integrated approach of activity based costing (ABC) and management, five forces analysis, risk and value-at-risk analysis, decision making trial and evaluation laboratory (DEMATEL), analytic network process (ANP), and fuzzy goal programming (FGP). The proposed model can effectively incorporate the key factors of precise costing, managerial constraints, competitive advantage analysis, and risk management into DSC forecasting and multi-objective production planning. A case study of a consumer-oriented cell phone DSC is also presented. The sensitivity analysis shows that identifying and relaxing crucial constraints can play an important role in DSC planning for higher competitive advantage and lower risk. © 2011 Published by Elsevier Ltd.}}, 
pages = {9053--9062}, 
number = {8}, 
volume = {38}
}
@article{10.21314/jor.2017.364, 
year = {2017}, 
title = {{Comparing multivariate volatility forecasts by direct and indirect approaches}}, 
author = {Amendola, Alessandra and Candila, Vincenzo}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2017.364}, 
abstract = {{Multivariate volatility models can be evaluated via direct and indirect approaches. The former uses statistical loss functions (LFs) and a proxy to provide consistent estimates of the unobserved volatility. The latter uses utility LFs or other instruments, such as value-at-risk and its backtesting procedures. Existing studies commonly employ these procedures separately, focusing mostly on the multivariate generalized autoregressive conditional heteroscedasticity (MGARCH) models. This work investigates and compares the two approaches in a model selection context. An extensive Monte Carlo simulation experiment is carried out, including MGARCH models based on daily returns and, extending the current literature, models that directly use the realized covariance, obtained from intraday returns. With reference to the direct approach, we rank the set of competing models empirically by means of four consistent statistical LFs and by reducing the quality of the volatility proxy. For the indirect approach, we use standard backtesting procedures to evaluate whether the number of value-at-risk violations is acceptable, and whether these violations are independently distributed over time. © 2017 Incisive Risk Information (IP) Limited.}}, 
pages = {33--57}, 
number = {6}, 
volume = {19}
}
@article{10.1016/j.resourpol.2020.101737, 
year = {2020}, 
title = {{Do bitcoin and precious metals do any good together? An extreme dependence and risk spillover analysis}}, 
author = {Rehman, Mobeen Ur}, 
journal = {Resources Policy}, 
issn = {03014207}, 
doi = {10.1016/j.resourpol.2020.101737}, 
abstract = {{The speculative nature of Bitcoin over the last 5 years documents high returns for investors and can also provide optimal returns with a good mix of some assets with hedging abilities. We analyze extreme dependence and risk spillover between Bitcoin and a sample of precious metal commodities comprising of gold, silver, copper, wheat, platinum and palladium over a sample period of April 2013–January 2018 based on daily data. We test the long memory properties of our sampled assets using ARFIMA-FIGARCH model followed by followed by time varying copula framework. Later we use VaR, CoVaR and ΔCoVaR tests to measure risk spillover and resulting asymmetries. Our results contribute towards the existing literature by reporting the hedging ability of gold for Bitcoin and the effect that have on precious metal returns. In this way our results provide insights in both the direction. Our results also document spillover from Bitcoin to precious metal market however in terms of directional spillover from precious metals to Bitcoin, silver remains insensitive to any downside risk spillover. We also report asymmetries in upside and downside ΔCoVaR values, suggesting that extreme changes in returns in either of the market has the potential to affect extreme returns in the other market. Our results have implications for individual investor and fund managers in formulating an optimal portfolio yielding returns with hedge against extreme downward price movements. © 2020 Elsevier Ltd}}, 
pages = {101737}, 
number = {NA}, 
volume = {68}
}
@article{10.2143/ast.42.2.2182807, 
year = {2012}, 
title = {{Reinsurance arrangements minimizing the risk-adjusted value of an insurer's liability}}, 
author = {Chi, Yichun}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.42.2.2182807}, 
abstract = {{In this paper, we investigate the problem of purchasing a reinsurance policy that minimizes the risk-adjusted value of an insurer's liability, where the valuation is carried out using a cost-of-capital approach. In order to exclude the moral hazard, we assume that both the insurer and reinsurer are obligated to pay more for larger loss in a typical reinsurance treaty. Moreover, the reinsurance premium principle is assumed to satisfy three axioms: law invariance, risk loading and preserving convex order. The proposed class of premium principles is quite general in the sense that it contains all the widely used premium principles except Esscher principle listed in Young (2004). When capital at risk is measured by the value at risk (VaR) or conditional value at risk (CVaR), we fi nd it is optimal for the insurer to cede two separate layers over the prescribed premium principles. By imposing an additional weak constraint on the premium principle, we further get that the reinsurance in the form of a layer is optimal. Finally, to illustrate the applicability of our results, we derive explicitly the optimal one-layer reinsurance for expected value principle and Wang's premium principle, and show that two-layer reinsurance may be optimal for Dutch premium principle. © 2012 by Astin Bulletin. All rights reserved.}}, 
pages = {529--557}, 
number = {2}, 
volume = {42}
}
@article{10.1016/j.ribaf.2017.07.181, 
year = {2018}, 
title = {{Banks’ systemic risk in the Tunisian context: Measures and Determinants}}, 
author = {Khiari, Wided and Nachnouchi, Jamila}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2017.07.181}, 
abstract = {{This paper proposes a systemic Risk cartography for Tunisian banks in order to shed the light on the key drivers of the Tunisian banks’ involvement in systemic risk. The aim of this analysis is to better understand the determining factors of the systemic risk levels. We intend to find the most influential vital few variables that are impacting the degree of systemic risk implications. In this perspective, we will develop a proper statistical framework based on a muli-level approach in order to explore, assess and explain the systemic risk incurred by the Tunisian banks. As a first step, we will provide a systemic risk cartography for the Tunisian banks using, and for the first time, a unified approach that combines CoES and Multi Dimensional Scaling (MDS) techniques. The recovered map revealed that public banks are taking the lead as the most involved banks in the systemic risk, followed closely by the two most important private banks BIAT and UBCI. As a second step, we will indicate how the use of a Systemic Risk Implication Composite Index (SRICI) provides the key drivers analysis of banks in systemic risk implication and shows that the implication of the Tunisian banks is highly dependent on the size of the financial institution, its liquidity, its technical efficiency, and its direct exposure to the interbank lending market. The present research offers two main insights. First, the ranking of the banks in terms of systemic implication, and second, the factors from which stems systemic importance. The first insight could complement the micro prudential scope of the regulation by providing a broader perspective that includes contagion and subsequently formulates a macro prudential vision and reinforces the regulation policy. The second one deserves higher attention on on-going basis in any systemic risk analysis. Supervisors could detect potentially systemic banks by keeping track of these factors and imposing a close monitoring for this type of institution. © 2017}}, 
pages = {620--631}, 
number = {NA}, 
volume = {45}
}
@article{10.1080/15427560.2016.1238370, 
year = {2016}, 
title = {{Predicting Equity Markets with Digital Online Media Sentiment: Evidence from Markov-switching Models}}, 
author = {Nooijen, Steven J. and Broda, Simon A.}, 
journal = {Journal of Behavioral Finance}, 
issn = {15427560}, 
doi = {10.1080/15427560.2016.1238370}, 
abstract = {{The authors examine the predictive capabilities of online investor sentiment for the returns and volatility of MSCI U.S. Equity Sector Indices by including exogenous variables in the mean and volatility specifications of a Markov-switching model. As predicted by the semistrong efficient market hypothesis, they find that the Thomson Reuters Marketpsych Indices (TRMI) predict volatility to a greater extent than they do returns. The TRMI derived from equity specific digital news are better predictors than similar sentiment from social media. In the two-regime setting, there is evidence supporting the hypothesis of emotions playing a more important role during stressed markets compared to calm periods. The authors also find differences in sentiment sensitivity between different industries: it is greatest for financials, whereas the energy and information technology sectors are scarcely affected by sentiment. Results are obtained with the R programming language. Code is available from the authors upon request. © 2016 The Institute of Behavioral Finance.}}, 
pages = {321--335}, 
number = {4}, 
volume = {17}
}
@article{10.1016/j.ejor.2012.12.013, 
year = {2013}, 
title = {{Multistage optimization of option portfolio using higher order coherent risk measures}}, 
author = {Matmoura, Yassine and Penev, Spiridon}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2012.12.013}, 
abstract = {{Choosing a suitable risk measure to optimize an option portfolio's performance represents a significant challenge. This paper is concerned with illustrating the advantages of Higher order coherent risk measures to evaluate option risk's evolution. It discusses the detailed implementation of the resulting dynamic risk optimization problem using stochastic programming. We propose an algorithmic procedure to optimize an option portfolio based on minimization of conditional higher order coherent risk measures. Illustrative examples demonstrate some advantages in the performance of the portfolio's levels when higher order coherent risk measures are used in the risk optimization criterion. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {190--198}, 
number = {1}, 
volume = {227}
}
@article{10.1109/bcgin.2012.54, 
year = {2012}, 
title = {{Risk management: VaR model for information disclosure}}, 
author = {Zheng, Yuhua}, 
journal = {2012 Second International Conference on Business Computing and Global Informatization}, 
issn = {NA}, 
doi = {10.1109/bcgin.2012.54}, 
abstract = {{This article discusses how to quantify the risk with Value at Risk model, and then proposes some problems in the application of this model. © 2012 IEEE.}}, 
pages = {183--186}, 
number = {NA}, 
volume = {NA}
}
@article{10.2143/ast.42.1.2160745, 
year = {2012}, 
title = {{Conditional tail expectation and premium calculation}}, 
author = {Heras, Antonio and Balbás, Beatriz and Vilar, José Luis}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.42.1.2160745}, 
abstract = {{In this paper we calculate premiums which are based on the minimization of the Expected Tail Loss or Conditional Tail Expectation (CTE) of absolute loss functions. The methodology generalizes well known premium calculation procedures and gives sensible results in practical applications. The choice of the absolute loss becomes advisable in this context since its CTE is easy to calculate and to understand in intuitive terms. The methodology also can be applied to the calculation of the VaR and CTE of the loss associated with a given premium. © 2012 by Astin Bulletin. All rights reserved.}}, 
pages = {325--342}, 
number = {1}, 
volume = {42}
}
@article{10.1016/j.frl.2017.07.005, 
year = {2017}, 
title = {{Robust multivairiate extreme value at risk allocation}}, 
author = {Belhajjam, A. and Belbachir, M. and Ouardirhi, S. El}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2017.07.005}, 
abstract = {{Recent research in economy, especially in finance, is using the assumption that the returns distribution is normal for determining optimal allocation for different models in finance optimization like Markowitz model. However this assumption is not already exact because the problem of the asymmetric distribution has a big impact. Our paper proposes a Multivariate Extreme Value at Risk (MEVaR) approach to find the optimal allocation of a portfolio based on the extreme value theory. A detailed procedure and implementation on two different portfolio (the first, for an emerging market as Morocco and the second on a Canadian portfolio of a very liquid market. Are given for demonstrating the consistency of the new approach? Results are compared with the Worst-Case-Value at Risk (WCVaR) proposed by El Ghaoui (2003), and Partitioned Value at Risk (PVaR) approach Joel Goh et al. (2011). They establish that the MEVaR performs well and is used in prediction model. © 2017 Elsevier Inc.}}, 
pages = {1--11}, 
number = {NA}, 
volume = {23}
}
@article{10.1080/03610920902822670, 
year = {2010}, 
title = {{Revisiting the edge, Ten years on}}, 
author = {Chavez-Demoulin, Valérie and Embrechts, Paul}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610920902822670}, 
abstract = {{When these lines are written, it is January 21, 2008, a further Black Monday on the international markets. Stock indices have fallen between 5 and 10\%. Which statistical tools help in describing such events and may help in understanding the consequences? In this article we update our knowledge on the modeling of extremal events, in particular with a view toward applications to finance, insurance, and risk management. Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {1674--1688}, 
number = {8-9}, 
volume = {39}
}
@article{10.1057/jam.2015.33, 
year = {2015}, 
title = {{Time lag dependence, cross-correlation and risk analysis of US energy and non-energy stock portfolios}}, 
author = {Hernandez, Jose Arreola and Janabi, Mazin A M Al and Hammoudeh, Shawkat and Nguyen, Duc Khuong}, 
journal = {Journal of Asset Management}, 
issn = {14708272}, 
doi = {10.1057/jam.2015.33}, 
abstract = {{This study estimates the time lag cross-correlation matrix, the Sharpe ratio and the Value-at-Risk (VaR) for three 36-stock energy, IT-computer and medicine-biotechnology sector portfolios derived from the US stock market during a post- global financial crisis period. We specifically look at the cause-effect dependence relationship, market risk and investment features of the sector portfolios. Our results uncover unidirectional time lag dependence between the IT-computer and medicine-biotechnology sector portfolios, stating that the price and return values of the former are dependent on the past price and return values of the latter. The IT-computer sector portfolio appears to be the best investment choice in terms of diversification, risk and return. Finally, the energy sector portfolio is found to have the highest VaR values and the lowest return relative to risk. The empirical results regarding the unveiled risk and dependence characteristics of the sectors are promising in terms of theory and practical financial applications. © 2015 Macmillan Publishers Ltd.}}, 
pages = {467--483}, 
number = {7}, 
volume = {16}
}
@article{10.1016/j.frl.2019.03.025, 
year = {2019}, 
title = {{Tail risk and the consumption CAPM}}, 
author = {Kwon, Ji Ho}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2019.03.025}, 
abstract = {{I examine if the market tail risk can be the conditioning information for consumption-based asset pricing model. While “cay”, Lettau and Ludvigson's (2001) conditioning variable, no longer works in the extended sample period, I find that Value at Risk (VaR) is the conditioning variable that enables consumption CAPM to explain substantial variation of cross-section of stock returns. Asset's riskiness is determined by the correlation with consumption growth conditional on the tail risk of the aggregate market. © 2019}}, 
pages = {69--75}, 
number = {NA}, 
volume = {30}
}
@article{10.1109/soli.2014.6960694, 
year = {2014}, 
title = {{A financial risk evaluation service for integrating private portfolios securely}}, 
author = {Watanabe, Yuji and Koseki, Akira}, 
journal = {Proceedings of 2014 IEEE International Conference on Service Operations and Logistics, and Informatics}, 
issn = {NA}, 
doi = {10.1109/soli.2014.6960694}, 
abstract = {{Value at Risk (VaR) is a widely accepted measure of counting the risk of loss on a specific portfolio of a collection of financial assets such as stocks, bonds and cash, held by a financial institution or individual. It is quite useful to compute "Integrated VaR", a VaR for a portfolio which combines all of the portfolios in the group. However, the downside of the integration is that every financial institution needs to disclose its own portfolio to others to obtain integrated portfolio. We propose a novel approach to realize an efficient and secure protocol for N participants who hold each portfolio s1,⋯,sN to compute Integrated VaR for integrated portfolio s=s1+⋯+sN collaboratively without disclosing the value of each portfolio. This allows the financial institutions or individuals to compute cross-institutional risk more accurately to make the financial risk more transparently. Our protocol achieves high efficiency by specializing message structure and exchange to compute an integrated VaR in a privacy preserving manner. © 2014 IEEE.}}, 
pages = {59--64}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.insmatheco.2016.11.005, 
year = {2017}, 
title = {{Modeling operational risk incorporating reputation risk: An integrated analysis for financial firms}}, 
author = {Eckert, Christian and Gatzert, Nadine}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2016.11.005}, 
abstract = {{It has been shown in the empirical literature that operational losses of financial firms can cause severe reputational losses, which, however, are typically not taken into account when modeling and assessing operational risk. The aim of this paper is to fill this gap by assessing the consequences of operational risk for a financial firm including reputational losses. Toward this end, we extend current operational risk models by incorporating reputation losses. We propose three different models for reputation risk: a simple deterministic approach, a stochastic model using distributional assumptions, and an extension of the second model by taking into account a firm's ability to deal with reputation events. Our results emphasize that reputational losses can by far exceed the original operational loss and that neglecting reputational losses may lead to a severe underestimation of certain operational risk types and especially fraud events. © 2016}}, 
pages = {122--137}, 
number = {NA}, 
volume = {72}
}
@article{10.1016/j.jbankfin.2007.04.014, 
year = {2007}, 
title = {{Financial prediction with constrained tail risk}}, 
author = {Trindade, A. Alexandre and Uryasev, Stan and Shapiro, Alexander and Zrazhevsky, Grigory}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2007.04.014}, 
abstract = {{A new class of asymmetric loss functions derived from the least absolute deviations or least squares loss with a constraint on the mean of one tail of the residual error distribution, is introduced for analyzing financial data. Motivated by risk management principles, the primary intent is to provide "cautious" forecasts under uncertainty. The net effect on fitted models is to shape the residuals so that on average only a prespecified proportion of predictions tend to fall above or below a desired threshold. The loss functions are reformulated as objective functions in the context of parameter estimation for linear regression models, and it is demonstrated how optimization can be implemented via linear programming. The method is a competitor of quantile regression, but is more flexible and broader in scope. An application is illustrated on prediction of NDX and SPX index returns data, while controlling the magnitude of a fraction of worst losses. © 2007 Elsevier B.V. All rights reserved.}}, 
pages = {3524--3538}, 
number = {11}, 
volume = {31}
}
@article{10.1504/gber.2015.070304, 
year = {2015}, 
title = {{On the devolatised returns and dynamic conditional correlations GARCH modelling in selected European indices}}, 
author = {Stavroyiannis, Stavros and Zarangas, Leonidas}, 
journal = {Global Business and Economics Review}, 
issn = {10974954}, 
doi = {10.1504/gber.2015.070304}, 
abstract = {{Typical issues of multivariate GARCH models are dimensionality, which is time-consuming both in terms of computations and their programming, and the availability of very few distributional schemes, since linear correlations are a natural dependence measure, only if the joint distribution of the variables is elliptical. We consider the new approach of devolatised returns, computed as returns standardised by realised volatilities rather than by GARCH-type volatilities estimates. As a case study, we examine several European indices, and the methodology incorporates a multivariate t-student version of the dynamic conditional correlations GARCH model. The time series under consideration and the results are subjected to several diagnostic tests, including the temporal volatilities and correlations of the asset returns, the validity of the t-DCC model using value-at-risk, the empirical cumulative distribution function of the probability integral transform variable, and forecasts of the conditional volatility and correlations. The concluding remarks are consistent, and in agreement with the new devolatised returns concept. Copyright © 2015 Inderscience Enterprises Ltd.}}, 
pages = {256}, 
number = {3}, 
volume = {17}
}
@article{10.15199/48.2016.12.63, 
year = {2016}, 
title = {{Using Markowitz’s idea to the valuable estimation of the IT projects risk [Zastosowanie metody Markowitza do wartościowej oceny ryzyka projektów informatycznych]}}, 
author = {ROZENBERG, Leonard}, 
journal = {PRZEGLĄD ELEKTROTECHNICZNY}, 
issn = {00332097}, 
doi = {10.15199/48.2016.12.63}, 
abstract = {{Realization of information projects require planning, execution, testing, monitoring and control in all phases and aspects. This activities are made by project’s members. It is also necessary to estimate the resources and costs associated with the project. The starting point is to determine the tasks structure which leads to calculating the value of the loading of work.The cost is an important information before starting the project, especially when analyzing the relationship between benefits and costs. This knowledge is also necessary because of limited budget of organization involved in the projects. © 2016, Wydawnictwo SIGMA - N O T Sp. z o.o. All rights reserved.}}, 
pages = {247--250}, 
number = {12}, 
volume = {92}
}
@article{10.2143/ast.42.2.2182809, 
year = {2012}, 
title = {{Average value-at-risk minimizing reinsurance under Wang's premium principle with constraints}}, 
author = {Cheung, K.C. and Liu, F. and Yam, S.C.P.}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.2143/ast.42.2.2182809}, 
abstract = {{In the present work, we study the optimal reinsurance decision problem in which the Average Value-at-Risk of the retained loss is minimized under Wang's premium principle and is also subject to either (1) a budget constraint on reinsurance premium, or (2) a reinsurer's probabilistic benchmark constraint of his potential loss. We show that the optimal reinsurance is a single-insurance layer under Constraint (1), and a cap insurance or a double-insurance layer under Constraint (2); moreover, under Constraint (2), we further establish that under most common circumstances (see Remark after Theorem 3), a cap insurance will suffi ce to be optimal. Finally, some numerical illustrations will be provided. © 2012 by Astin Bulletin. All rights reserved.}}, 
pages = {575--600}, 
number = {2}, 
volume = {42}
}
@article{10.1080/02102412.2010.10779678, 
year = {2010}, 
title = {{Efficiency in market risk measures techniques face to crisis situations}}, 
author = {Sánchez, Mariano González and Pineda, Juan M. Nave}, 
journal = {Spanish Journal of Finance and Accounting / Revista Española de Financiación y Contabilidad}, 
issn = {02102412}, 
doi = {10.1080/02102412.2010.10779678}, 
abstract = {{These days the estimation of risk is one of the real arts of business management; with a scientifi c aspect provided by the techniques employed. In this context, and due to regulatory changes and the recent critical market situations, decision makers have not only increased the sophistication of the mechanisms used but have also increased the demands imposed on risk measurement. There is a wide range of possible techniques that can be used to measure market risk (parametric VaR, historical simulation, «shortfall», among others). the choice is conditioned fi rstly by the behaviour of the stock, which is known as «stylized facts», and secondly by the results of «backtesting» of these techniques. The objetive of this study is to analyse the results of these techniques using a market sample (IBEX-35), in order to be able to discriminate in critical situations such as those that have ocurred recently and thus facilitate the choice of te technique. © 2001 Asociación Española de Contabilidad.}}, 
pages = {41--64}, 
number = {145}, 
volume = {39}
}
@article{10.1016/s0378-4266(02)00277-7, 
year = {2003}, 
title = {{The effects of estimation error on measures of portfolio credit risk}}, 
author = {Löffler, Gunter}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00277-7}, 
abstract = {{This paper uses Monte Carlo simulations to assess the impact of noisy input parameters on the accuracy of estimated portfolio credit risk. Assumptions about input quality are derived from the distribution of historical sample statistics commonly used in default risk modelling. The resulting estimation error in the distribution of portfolio losses is considerable. Losses that are judged to occur with a probability of 0.3\% may actually occur with a probability of 1\%. The paper also shows that estimation error leads to biases in value at risk estimates and significance levels of backtests. The biases can be corrected by analysing predictive distributions which average over the unknown parameter values. © 2003 Elsevier Science B.V. All rights reserved.}}, 
pages = {1427--1453}, 
number = {8}, 
volume = {27}
}
@article{10.18488/journal.aefr.2019.93.339.352, 
year = {2019}, 
title = {{Systemic risk in Vietnam stock market}}, 
author = {Vu, Thi Thuy Van and Tran, Dang Kham}, 
journal = {Asian Economic and Financial Review}, 
issn = {23052147}, 
doi = {10.18488/journal.aefr.2019.93.339.352}, 
abstract = {{Systemic risk is one of the issues currently being paid attention to in ensuring the stability and sustainability of the global financial system in general and the securities market of countries in particular. The paper studied the systemic risk of enterprises listed on Ho Chi Minh City Stock Exchange in the period from the first quarter of 2010 to the second quarter of 2017. The authors have applied the VaR and ∆CoVaR method to compare the loss level of businesses to the systemic risk of the whole market upon an unstable event. The study also found a disadvantage of using VaR in measuring systemic risk in that it was still "individual" and "single" and didn’t consider the spread among various entities in the market. In addition, the sensitivity of listed companies varied under normal and volatile conditions. The results showed that ∆CoVaR is a more suitable measure in considering the contribution level of companies to the systemic risk of the whole market. Calculated results were proposed as an indicator for investors and market managers in order to limit systemic risks in the future. © 2019 AESS Publications. All Rights Reserved.}}, 
pages = {339--352}, 
number = {3}, 
volume = {9}
}
@article{10.1016/j.frl.2013.07.005, 
year = {2014}, 
title = {{Contagion effect on bond portfolio risk measures in a hybrid credit risk model}}, 
author = {Boudreault, Mathieu and Gauthier, Geneviève and Thomassin, Tommy}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2013.07.005}, 
abstract = {{This paper illustrates how modelling the contagion effect among assets of a given bond portfolio changes the risk perception associated to it. This empirical work is developed in a hybrid credit risk framework that incorporates recovery rate risk. Dependence structures among firms and between external shocks affecting firms together are considered. The presence of correlations among firm leverage ratios and the interrelation between default probabilities and recovery rates produces clusters of defaults with low recovery rates. This has a major impact on standard risk measures such as Value-at-Risk and conditional tail expectation. Consequently, an appropriate measurement of the contagion has a tremendous effect on the capital requirement of many financial institutions. © 2013 Elsevier Inc.}}, 
pages = {131--139}, 
number = {2}, 
volume = {11}
}
@article{10.1109/is.2016.7737477, 
year = {2016}, 
title = {{Value at Risk backtesting techniques: Intuitionistic fuzzy approach and InterCriteria Analysis}}, 
author = {Tisheva, Diana and Netov, Nikolay}, 
journal = {2016 IEEE 8th International Conference on Intelligent Systems (IS)}, 
issn = {NA}, 
doi = {10.1109/is.2016.7737477}, 
abstract = {{There are various approaches for development of Value at Risk (VaR) forecasting models but the final steps of a building procedure usually incorporate different validations of the models like adequacy tests and backtesting. These validations provide solid reasons to reject those models which do not pass the tests. However there are situations when several models are deemed to provide good forecasts and for a practical application only one of them should be selected. We suggest a methodology to select the VaR forecasting model which approximates the actual market risk the most closely in cases when there is more than one candidate for a best model. We have used also InterCriteria Analysis to do an additional stability test on the best model. © 2016 IEEE.}}, 
pages = {552--559}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.pacfin.2016.01.003, 
year = {2017}, 
title = {{Financial tail risks in conventional and Islamic stock markets: A comparative analysis}}, 
author = {Mwamba, John W. Muteba and Hammoudeh, Shawkat and Gupta, Rangan}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2016.01.003}, 
abstract = {{This paper makes use of two types of extreme value distributions, namely: the generalized extreme value distribution often referred to as the block of maxima method (BMM), and the peak-over-threshold method (POT) of the extreme value distributions, to model the financial tail risks associated with the empirical daily log-return distributions of the Dow Jones Islamic market (DJIM), the U.S. S\&P 500, the S\&P Europe (SPEU), and the Asian S\&P (SPAS50) indexes during the period between 01/01/1998 and 16/09/2015. Using both the maximum likelihood (ML) method and the bootstrap simulations to estimate the parameters of these extreme value distributions in the left and right tails separately, we find that the empirical distributions of conventional stock markets are characterized by a fat-left tail behaviour, which implies high probability of price drops during a financial crisis, and by a right-tail characterized by a truncation. This finding implies the existence of an upper bound on possible profit during an extreme event. The empirical distribution of the Islamic market is characterized by a thin-left tail behaviour, implying moderately low probability of price drops during a financial crisis, and by a right-tail without truncation implying large probability of positive returns during an extreme event. We divide our sample period into three equal sub-periods in order avoid the impact of outliers and structural breaks. The results in each sub-period remain the same and also suggest that for all stock returns the BMM method performs better than the POT method, and that the Islamic stock market is less risky than the conventional stock markets during extreme events. © 2016 Elsevier B.V.}}, 
pages = {60--82}, 
number = {NA}, 
volume = {42}
}
@article{10.1016/j.iref.2019.02.014, 
year = {2019}, 
title = {{Asymmetric jump beta estimation with implications for portfolio risk management}}, 
author = {Alexeev, Vitali and Urga, Giovanni and Yao, Wenying}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2019.02.014}, 
abstract = {{We evaluate the impact of extreme market shifts on equity portfolios and study the difference in negative and positive reactions to market jumps with implications for portfolio risk management. Employing high-frequency data for the constituents of the S\&P500 index over the period 2 January 2003 to 30 December 2017, we investigate to what extent the portfolio exposure to the downside and upside jumps can be mitigated. We contrast the risk exposure of individual stocks with those of the portfolios as the number of holdings increases. Varying the jump identification threshold, we show that the number of holdings required to stabilise portfolios’ sensitivities to negative jumps is higher than when positive jumps are considered and that the asymmetry is more prominent for more extreme events. Ignoring this asymmetry results in under-diversification of portfolios and increases exposure to sudden extreme negative market shifts. © 2019}}, 
pages = {20--40}, 
number = {NA}, 
volume = {62}
}
@article{10.4028/www.scientific.net/amr.562-564.1990, 
year = {2012}, 
title = {{Risk analysis of distributed energy source in the smart grid}}, 
author = {Shen, Jing Shuang and Jiang, Chuan Wen}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.562-564.1990}, 
abstract = {{This paper reviews the distributed energy source in the smart grid and it's advantage. The risk of distributed energy source is analyzed. The value of risk (VaR) is studied in this paper. It is important to improve the new energy and build the smart grid in the future. © (2012) Trans Tech Publications, Switzerland.}}, 
pages = {1990--1993}, 
number = {NA}, 
volume = {562-564}
}
@article{10.1109/iccae.2010.5451372, 
year = {2010}, 
title = {{Resolving multi objective stock portfolio optimization problem using genetic algorithm}}, 
author = {Hoklie and Zuhal, Lavi Rizki}, 
journal = {2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)}, 
issn = {NA}, 
doi = {10.1109/iccae.2010.5451372}, 
abstract = {{Portfolio optimization is an important research field in modern finance. The most important characteristic within this optimization problem is the risk and the returns. Modern portfolio theory provides a well-developed paradigm to form a portfolio with the highest expected return for a given level of risk tolerance. Multi objective portfolio optimization problem is the portfolio selection process that result highest expected return and smallest identified risk among the various financial assets. In this paper, we propose to identify expected return (mean profit) and risk using historical data of stock prices. The downside values of the variance of each stock are considered to be the identified risk in first case. The Value at Risk of each stock that we obtained using parametric and historical simulation methodology are considered to be the identified risk in second case. One of method being widely used lately for optimization need is Genetic Algorithm or GA. This method adapted the mechanism of biology mechanism involving natural selection. With many advantages it has, numerical process of a portfolio optimization in both case are attempted to be coupled with Genetic Algorithm. The values of expected return and risk of each stock will be used as inputs into a fitness function in Genetic Algorithm. Performance evaluation is done by examining the parameters of GA, such as population size, stall generation and number of elitism. ©2010 IEEE.}}, 
pages = {40--45}, 
number = {NA}, 
volume = {2}
}
@article{10.1016/j.asoc.2021.107948, 
year = {2021}, 
title = {{Robust mean-risk portfolio optimization using machine learning-based trade-off parameter}}, 
author = {Min, Liangyu and Dong, Jiawei and Liu, Jiangwei and Gong, Xiaomin}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2021.107948}, 
abstract = {{Conservatism is the notorious problem of the worst-case robust portfolio optimization, and this issue has raised broad discussion in academia. To this end, we propose the hybrid robust portfolio models under ellipsoidal uncertainty sets in this paper, where both the best-case and the worst-case counterparts are involved. In the suggested models, we introduce a trade-off parameter to adjust the portfolio optimism level. Machine learning algorithms including Long Short-Term Memory (LSTM) and eXtreme Gradient Boosting (XGBoost) are used to evaluate the potential market movements and provide forecasting information to generate the hyperparameter for modeling. Additionally, we develop a clustering-based algorithm for properly constructing joint ellipsoidal uncertainty sets to reduce conservatism further. In the modeling phase, we design the hybrid portfolios based on variance (HRMV) and value at risk (VaR) and prove the equivalent relationship between the hybrid robust mean-VaR model (HRMVaR) and the hybrid robust mean-CVaR (conditional value at risk) according to the existing research. The US 12 industry portfolio data set retrieved from Kenneth R. French is employed for the in-sample and out-of-sample numerical experiments. The experimental results demonstrate the effectiveness and robustness of the proposed portfolios, where HRMV models have better Sharpe ratios and Calmar ratios than the corresponding nominal mean–variance model, and HRMVaR models outperform the baseline VaR-based portfolios in terms of returns. Sensitivity analysis supports the superiority of the joint ellipsoidal uncertainty set Uδ2, where the proposed portfolios constrained with Uδ2 show stable risk characteristics. © 2021 Elsevier B.V.}}, 
pages = {107948}, 
number = {NA}, 
volume = {113}
}
@article{10.1080/13504851.2017.1307928, 
year = {2018}, 
title = {{Backtesting expected shortfall: evidence from European securitized real estate}}, 
author = {Almudhaf, Fahad}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2017.1307928}, 
abstract = {{Events such as the European sovereign debt crisis, terrorism and Brexit cause more uncertainty and volatility in capital markets. This encourages us to use both conditional and unconditional forecasts (backtests) for expected shortfall (ES) in 8 indices of listed European real estate securities and Real estate investment trusts (REITs). Using the method proposed by Du and Escanciano, we find that ES is generally superior to Value-at-Risk in describing and capturing risk during extreme events such as the financial crisis. Our results are important to regulators, risk managers and investors. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--7}, 
number = {3}, 
volume = {25}
}
@article{10.1142/9789813278387_0008, 
year = {2020}, 
title = {{Applied models of heavy tails and skewness in energy prices with an application to electricity price risk}}, 
author = {Goutte, Stéphane and Nguyen, Duc Khuong and Walls, W D and Zhang, Wei}, 
issn = {NA}, 
doi = {10.1142/9789813278387\_0008}, 
abstract = {{Energy prices are characterized by distributions that are asymmetric and that have heavier than Gaussian tails. Yet, many researchers continue to employ statistical methods that do not explicitly account for heavy tails and skewness in energy prices. In this chapter, we explicitly account for heavy tails and skewness with an application to electricity price risk using the Generalized Pareto Distribution and the Generalized Extreme Value Distribution. Specifically, we model valueat- risk (VaR) which is a widely-used measure of the maximum potential change in value of a portfolio of financial assets with a given probability over a given time horizon. VaR has become a standard measure of market risk and a common practice is to compute VaR by assuming that changes in value of the portfolio are conditionally normally distributed. However, assets returns usually come from heavy-tailed distributions, so computing VaR under the assumption of conditional normality can be an important source of error. We illustrate in our application to electric power, that VaR estimates based on extreme value theory models - in particular the generalized Pareto distribution - are more accurate than those produced by alternative models such as normality or historical simulation. © 2019 by World Scientific Publishing Co. Pte. Ltd.}}, 
pages = {185--213}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/risks7010032, 
year = {2019}, 
title = {{A genetic algorithm for investment–consumption optimization with value-at-risk constraint and information-processing cost}}, 
author = {Jin, Zhuo and Yang, Zhixin and Yuan, Quan}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks7010032}, 
abstract = {{This paper studies the optimal investment and consumption strategies in a two-asset model. A dynamic Value-at-Risk constraint is imposed to manage the wealth process. By using Value at Risk as the risk measure during the investment horizon, the decision maker can dynamically monitor the exposed risk and quantify the maximum expected loss over a finite horizon period at a given confidence level. In addition, the decision maker has to filter the key economic factors to make decisions. Considering the cost of filtering the factors, the decision maker aims to maximize the utility of consumption in a finite horizon. By using the Kalman filter, a partially observed system is converted to a completely observed one. However, due to the cost of information processing, the decision maker fails to process the information in an arbitrarily rational manner and can only make decisions on the basis of the limited observed signals. A genetic algorithm was developed to find the optimal investment, consumption strategies, and observation strength. Numerical simulation results are provided to illustrate the performance of the algorithm. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {32}, 
number = {1}, 
volume = {7}
}
@article{10.1007/s11156-006-8541-9, 
year = {2006}, 
title = {{Evaluating effects of excess kurtosis on VaR estimates: Evidence for international stock indices}}, 
author = {Baixauli, J. Samuel and Alvarez, Susana}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-006-8541-9}, 
abstract = {{The calculus of VaR involves dealing with the confidence level, the time horizon and the true underlying conditional distribution function of asset returns. In this paper, we shall examine the effects of using a specific distribution function that fits well the low-tail data of the observed distribution of asset returns on the accuracy of VaR estimates. In our analysis, we consider some distributional forms characterized by capturing the excess kurtosis characteristic of stock return distributions and we compare their performance using some international stock indices. © Springer Science+Business Media, LLC 2006.}}, 
pages = {27--46}, 
number = {1}, 
volume = {27}
}
@article{10.1016/j.jedc.2016.05.002, 
year = {2016}, 
title = {{Quantifying market risk with Value-at-Risk or Expected Shortfall? - Consequences for capital requirements and model risk}}, 
author = {Kellner, Ralf and Rösch, Daniel}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/j.jedc.2016.05.002}, 
abstract = {{The Basel Committee on Banking Supervision recently proposed fundamental changes in the regulatory treatment of financial institutions' trading book positions. Among others, a replacement of Value-at-Risk (α = 0.99) by Expected Shortfall (α = 0.975) for the quantification of market risk is recommended. While this increases capital requirements for heavy tailed risks, its consequences for model risk related to the estimation process have not been explored. Hence, the aim of this paper is to analyze how both risk measures react to different sources of model risk in order to better understand the impact of the intended change in risk measures. Our results show that the Expected Shortfall (α = 0.975) is more sensitive towards regulatory arbitrage and parameter misspecification. We find that this is based on a trade-off between a model[U+05F3]s ability to better capture the heavy tailed behavior of risks and a higher vulnerability to model risk. These new aspects should be taken into account in the regulatory decision for Expected Shortfall (α = 0.975). © 2016 Elsevier B.V.}}, 
pages = {45--63}, 
number = {NA}, 
volume = {68}
}
@article{10.1007/s10693-009-0075-6, 
year = {2010}, 
title = {{The impact of downward rating momentum}}, 
author = {Güttler, Andre and Raupach, Peter}, 
journal = {Journal of Financial Services Research}, 
issn = {09208550}, 
doi = {10.1007/s10693-009-0075-6}, 
abstract = {{Rating downgrades are known to make subsequent downgrades more likely. We analyze the impact of this "downward momentum" on credit portfolio risk and bond portfolio management. Using Standard\&Poor's ratings from 1996 to 2005, we apply a novel approach to estimate a transition matrix that is sensitive to previous downgrades and contrast it with an insensitive benchmark matrix. First, we find that, under representative economic conditions, investors who rely on insensitive transition matrices underestimate the momentum-sensitive Value-at-Risk (VaR), on average, by 107 basis points. Second, we show that bond portfolio managers should use our downgrade-sensitive probabilities of default as they seem to be better calibrated than momentum-insensitive estimates. © Springer Science+Business Media, LLC 2009.}}, 
pages = {1}, 
number = {1}, 
volume = {37}
}
@article{10.1007/978-3-319-73150-6_55, 
year = {2018}, 
title = {{Portfolio selection with stock, gold and bond in Thailand under vine copulas functions}}, 
author = {Pastpipatkul, Pathairat and Yamaka, Woraphon and Sriboonchitta, Songsak}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-319-73150-6\_55}, 
abstract = {{The paper aims to measure the risk and find the optimal weights of portfolio containing three instruments: Stock Exchange of Thailand, Thai Baht gold, and Treasury 10-year bond yield. The study employs the C-D vine copulas approach to construct the dependency of each pair instruments and uses the Monte Carlo simulation technique to generate the simulated data to compute Value at Risk (VaR) and Expected Shortfall (ES). Our results show that there exists a weak significant dependency between Stock Exchange of Thailand index and Thai Baht gold and dependency between Treasury 10-year bond yield and Thai Baht gold. Moreover, we find that the desired portfolio allocation is 49.8\% of SET, 18.8\% of Bond, and 31.4\% of Gold where risk and return of the portfolio are 2.7\% and 0.05\%, respectively. © 2018, Springer International Publishing AG.}}, 
pages = {698--711}, 
number = {NA}, 
volume = {760}
}
@article{10.1016/j.jbankfin.2004.08.005, 
year = {2005}, 
title = {{On the significance of expected shortfall as a coherent risk measure}}, 
author = {Inui, Koji and Kijima, Masaaki}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2004.08.005}, 
abstract = {{This article shows that any coherent risk measure is given by a convex combination of expected shortfalls, and an expected shortfall (ES) is optimal in the sense that it gives the minimum value among the class of plausible coherent risk measures. Hence, it is of great practical interest to estimate the ES with given confidence level from the market data in a stable fashion. In this article, we propose an extrapolation method to estimate the ES of interest. Some numerical results are given to show the efficiency of our method. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {853--864}, 
number = {4}, 
volume = {29}
}
@article{10.1093/ajae/aaw049, 
year = {2016}, 
title = {{Estimating expected and unexpected losses for agricultural mortgage portfolios}}, 
author = {Dressler, Jonathan B. and Tauer, Loren W.}, 
journal = {American Journal of Agricultural Economics}, 
issn = {00029092}, 
doi = {10.1093/ajae/aaw049}, 
abstract = {{The financial crisis that began in 2008 placed renewed emphasis and responsibility on financial institutions to assess financial risks and provide evidence of adequate capital to accommodate those risks. Financial regulators in the United States are now requiring financial institutions with mortgage portfolios to show they have set aside sufficient capital reserves to meet expected and unexpected losses caused by credit defaults. Various methods exist to estimate the loss risk of agricultural loan portfolios, and those methods were used to estimate the economic capital necessary to cover loan losses for a financial institution. Combining estimates of probability of delinquency, probability of loss, loss given default, and exposure at default from various models, one-year-ahead expected loss estimates were derived. Value-at-Risk and expected shortfall estimates were obtained from loss distributions to arrive at unexpected loss estimates. Previous empirical studies reported in the literature often only measured the probability of default, but the other components are essential to determine the necessary economic capital to meet expected and unexpected loan losses. Results show that measures of liquidity, solvency, profitability, and controls for unobserved heterogeneity are important when modeling delinquency, while measures of leverage, state-level economic output, and controls for unobserved heterogeneity are important when modeling loss given default. Predicted portfolio expected losses were greatest for the regression model combination that included the loss given default linear regression static variable models, and least for the regression model combination using static and time-varying variables. These results should prove useful in determining and assessing capital reserve requirements for financial institutions with mortgage portfolios. © 2016 The Authors 2016. Published by Oxford University Press on behalf of the Agricultural and Applied Economics Association.}}, 
pages = {1470--1485}, 
number = {5}, 
volume = {98}
}
@article{10.1080/14697688.2020.1812701, 
year = {2021}, 
title = {{The dependence structure between equity and foreign exchange markets and tail risk forecasts of foreign investments}}, 
author = {Kim, Minjoo and Yang, Junhong and Song, Pengcheng and Zhao, Yang}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2020.1812701}, 
abstract = {{Motivated by the importance of the dependence structure between equity and foreign exchange rates in international financial markets, we investigate whether modelling the dependence structure can help forecast the tail risk of foreign investments. We propose a new time-varying asymmetric copula for modelling the dependence structure and forecasting the tail risk. We conduct backtesting on our tail risk forecasts for 12 major developed and emerging markets. We find that modelling the dependence structure can improve the tail risk forecast and make risk management of foreign investments more robust. © 2020 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--21}, 
number = {5}, 
volume = {21}
}
@article{10.1109/ipdps.2009.5161146, 
year = {2009}, 
title = {{Advanced risk analytics on the cell broadband engine}}, 
author = {Docan, Ciprian and Parashar, Manish and Marty, Christopher}, 
journal = {2009 IEEE International Symposium on Parallel \& Distributed Processing}, 
issn = {NA}, 
doi = {10.1109/ipdps.2009.5161146}, 
abstract = {{This paper explores the effectiveness of using the CBE platform for Value-at-Risk (VaR) calculations. Specifically, it focuses on the design, optimization and evaluation of pricing European and American stock options across Monte-Carlo VaR scenarios. This analysis is performed on two distinct platforms with CBE processors, i.e., IBM Q22 blade server and the Playstation3 gaming console. © 2009 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.inteco.2019.03.002, 
year = {2019}, 
title = {{Vine copula-based dependence and portfolio value-at-risk analysis of the cryptocurrency market}}, 
author = {Boako, Gideon and Tiwari, Aviral Kumar and Roubaud, David}, 
journal = {International Economics}, 
issn = {21107017}, 
doi = {10.1016/j.inteco.2019.03.002}, 
abstract = {{In this paper, we use vine copula approaches to model the co-dependence and portfolio value-at-risk (VaR)of six cryptocurrencies using data of daily periodicity from September 2015 to June 2018. We establish evidence of strong dependencies among the virtual currencies with a dynamic dependency structure. We find that among the class of cryptocurrencies examined, Ethereum offers the best optimal and economically risk-reward trade-off subject to a no-shorting constraint for portfolio investors using the efficient frontier. Given the paucity of empirical research on the cryptocurrency markets, this paper provides new insights, which could be useful in developing dependence and risk strategies for investment and hedging purposes, especially during more volatile periods in the markets. © 2019 CEPII (Centre d'Etudes Prospectives et d'Informations Internationales), a center for research and expertise on the world economy}}, 
pages = {77--90}, 
number = {NA}, 
volume = {158}
}
@article{10.19030/jabr.v33i2.9899, 
year = {2017}, 
title = {{Extreme risk in resource indices and the generalized logistic distribution}}, 
author = {Huang, Chun-Kai and Pather, Venelle and Hammujuddy, Jahvaid and Chinhamu, Knowledge}, 
journal = {Journal of Applied Business Research (JABR)}, 
issn = {08927626}, 
doi = {10.19030/jabr.v33i2.9899}, 
abstract = {{The resource sector accounts for a substantial proportion of market capitalization on the US and South African stock exchanges. Hence, severe movements in related stock prices can drastically affect the risk profile of the entire market. Extreme value theory provides a basis for evaluating and forecasting such sporadic occurrences. In this article, we compare performances of classical extreme value models against the recently suggested generalized logistic distribution, for estimating value-at-risk and expected shortfall in resource indices. Our results suggest a significant difference in risk behavior between the two markets and the generalized logistic distribution does not always outperform classical models, as previous work may have suggested. © by author(s); CC-BY.}}, 
pages = {283--296}, 
number = {2}, 
volume = {33}
}
@article{10.1111/1475-3995.00378, 
year = {2002}, 
title = {{Portfolio Value at Risk Bounds}}, 
author = {Luciano, Elisa and Marena, Marina}, 
journal = {International Transactions in Operational Research}, 
issn = {09696016}, 
doi = {10.1111/1475-3995.00378}, 
abstract = {{This paper develops value at risk (VAR) measures for portfolios of correlated financial assets, without assuming normal returns. The approach can cope with any distribution for marginal returns, the fat–tailed ones included. We provide VAR bounds which hold independently of the joint distribution of returns and their dependence structure. The lower bound can be interpreted as a worst–case scenario VAR. We show that it not only requires little information, but is also easy to compute. In this sense, we suggest it as a practical device for portfolio managers. An application to portfolios of stock–market indices is provided. Comparisons with the VAR values under the normality assumption on returns are discussed. International Federation of Operational Research Societies 2002.}}, 
pages = {629--641}, 
number = {5}, 
volume = {9}
}
@article{10.1109/icgec.2010.19, 
year = {2010}, 
title = {{A hybrid BPSO approach for fuzzy facility location problems with VaR}}, 
author = {Wang, Shuming and Watada, J}, 
journal = {2010 Fourth International Conference on Genetic and Evolutionary Computing}, 
issn = {NA}, 
doi = {10.1109/icgec.2010.19}, 
abstract = {{In this paper, a fuzzy facility location model with Value at Risk (VaR) is proposed, which is a two-stage fuzzy zero-one integer programming. Since the fuzzy parameters of the location problem are continuous fuzzy variables with an infinite support, the computation of VaR is inherently an infinite-dimensional optimization problem, which can not be solved analytically. In order to solve the model, first of all, the objective function VaR is approximated through discretization method of fuzzy variables. Therefore, the original problem is converted to the task of a finite-dimensional optimization. Then, a hybrid heuristic algorithm integrating binary particle swarm optimization (BPSO), simplex algorithm and the approximation approach is designed to solve the location model. Finally, a numerical example is provided. © 2010 IEEE.}}, 
pages = {43--46}, 
number = {NA}, 
volume = {NA}
}
@article{10.1504/ijmef.2008.019222, 
year = {2008}, 
title = {{An empirical comparison of alternative models in estimating Value-at-Risk: evidence and application from the LSE}}, 
author = {Dockery, Everton and Efentakis, Miltos}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2008.019222}, 
abstract = {{This paper compares a select number of Value-at-Risk (VaR) models using daily data from the London stock exchange for estimating the model-based VaR. The period covers volatile market conditions triggered by a host of events that induced market uncertainty. Our results provide an indication of the degree of accuracy of the various methods and discuss issues of model selection. The empirical findings suggest that the Equally Weighted Moving Average (EWMA) model can furnish more accurate estimated VaR than the GARCH methods, including the popular Historical Simulation (HS) approach, by altering the estimation horizon. © 2008 Inderscience Enterprises Ltd.}}, 
pages = {201}, 
number = {2}, 
volume = {1}
}
@article{10.1007/s10436-014-0249-6, 
year = {2014}, 
title = {{Does value-at-risk encourage diversification when losses follow tempered stable or more general Lévy processes?}}, 
author = {Grabchak, Michael}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-014-0249-6}, 
abstract = {{In this paper, we address the question of when portfolio selection based on Value-at-risk encourages diversification [in the sense of Ibragimov (Quant Financ 9(5):565–580, 2009)]. Specifically, we give sufficient conditions for the case when losses follow a Lévy process. When the process has finite variation, these conditions are also necessary. We then specialize our results to the case when losses have tempered stable distributions. © 2014, Springer-Verlag Berlin Heidelberg.}}, 
pages = {553--568}, 
number = {4}, 
volume = {10}
}
@article{10.1016/j.jhydrol.2017.03.074, 
year = {2017}, 
title = {{Study on optimization of the short-term operation of cascade hydropower stations by considering output error}}, 
author = {Wang, Liping and Wang, Boquan and Zhang, Pu and Liu, Minghao and Li, Chuangang}, 
journal = {Journal of Hydrology}, 
issn = {00221694}, 
doi = {10.1016/j.jhydrol.2017.03.074}, 
abstract = {{The study of reservoir deterministic optimal operation can improve the utilization rate of water resource and help the hydropower stations develop more reasonable power generation schedules. However, imprecise forecasting inflow may lead to output error and hinder implementation of power generation schedules. In this paper, output error generated by the uncertainty of the forecasting inflow was regarded as a variable to develop a short-term reservoir optimal operation model for reducing operation risk. To accomplish this, the concept of Value at Risk (VaR) was first applied to present the maximum possible loss of power generation schedules, and then an extreme value theory-genetic algorithm (EVT-GA) was proposed to solve the model. The cascade reservoirs of Yalong River Basin in China were selected as a case study to verify the model, according to the results, different assurance rates of schedules can be derived by the model which can present more flexible options for decision makers, and the highest assurance rate can reach 99\%, which is much higher than that without considering output error, 48\%. In addition, the model can greatly improve the power generation compared with the original reservoir operation scheme under the same confidence level and risk attitude. Therefore, the model proposed in this paper can significantly improve the effectiveness of power generation schedules and provide a more scientific reference for decision makers. © 2017 Elsevier B.V.}}, 
pages = {326--339}, 
number = {NA}, 
volume = {549}
}
@article{10.1016/j.jimonfin.2014.12.002, 
year = {2015}, 
title = {{Systemic risk in European sovereign debt markets: A CoVaR-copula approach}}, 
author = {Reboredo, Juan C. and Ugolini, Andrea}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2014.12.002}, 
abstract = {{We studied systemic risk in European sovereign debt markets before and after the onset of the Greek debt crisis, taking the conditional value-at-risk (CoVaR) as a systemic risk measure, characterized and computed using copulas. We found that, before the debt crisis, sovereign debt markets were all coupled and systemic risk was similar for all countries. However, with the onset of the Greek crisis, debt markets decoupled and the systemic risk of the countries in crisis (excepting Spain) for the European debt market as a whole decreased, whereas that of the non-crisis countries increased to a small degree. The systemic risk of the Greek debt market for other countries in difficulties increased, especially for Portugal where systemic risk tripled after the onset of the crisis, whereas the systemic impact on the non-crisis countries decreased. © 2014 Elsevier Ltd.}}, 
pages = {214--244}, 
number = {NA}, 
volume = {51}
}
@article{10.1007/s10693-005-1802-2, 
year = {2005}, 
title = {{Deposit insurance and risk management of the U.S. banking system: What is the loss distribution faced by the FDIC?}}, 
author = {Kuritzkes, Andrew and Schuermann, Til and Weiner, Scott M.}, 
journal = {Journal of Financial Services Research}, 
issn = {09208550}, 
doi = {10.1007/s10693-005-1802-2}, 
abstract = {{We examine the question of deposit insurance through the lens of risk management by constructing the loss distribution faced by the Federal Deposit Insurance Corporation (FDIC). We take a novel approach by arguing that the risk management problem faced by the FDIC is similar to that of a bank managing a loan portfolio, only in the FDIC's case the risk arises from the potential for loss of the individual banks in its portfolio. We explicitly estimate the cumulative loss distribution of FDIC insured banks using two variations of the Merton model and find that reserves are sufficient to cover roughly 99.85\% of the loss distribution, corresponding to about a BBB+ rating. However, under different stress scenarios (higher correlations, fat-tailed bank returns, increased loss severity) that level can be much lower: approximately 96\% corresponding to about a B+ rating. © 2005 Springer Science + Business Media, Inc.}}, 
pages = {217--242}, 
number = {3}, 
volume = {27}
}
@article{10.1002/csr.2180, 
year = {2021}, 
title = {{Forecasting volatility by integrating financial risk with environmental, social, and governance risk}}, 
author = {Capelli, Paolo and Ielasi, Federica and Russo, Angeloantonio}, 
journal = {Corporate Social Responsibility and Environmental Management}, 
issn = {15353958}, 
doi = {10.1002/csr.2180}, 
abstract = {{The study aims to verify whether the consideration of a risk measure based on environmental, social, and governance (ESG) factors can reduce the difference between the ex-ante financial risk and ex-post volatility of financial assets. The statistical models are run on 17,996 firm-year observations (3332 active firms from 55 countries and 10 industries, listed on the ECPI Global Ethical Equity index) in 2007–2015. According to our main results, the forecasting effectiveness of traditional financial risk measures can be improved by integrating financial risk with an ESG risk measure that considers the ESG entropy. We found that the dispersion of ESG scores within a country, sector and year is a risk factor that would be helpful in predicting the volatility of financial assets. Other similar long-run risk measures, such as issuers' credit ratings, do not reveal the same forecasting power. By reducing unexpected volatility, especially in the medium term, the ESG risk measure provides investors and fund managers with a useful metric for decision making. © 2021 The Authors. Corporate Social Responsibility and Environmental Management published by ERP Environment and John Wiley \& Sons Ltd.}}, 
pages = {1483--1495}, 
number = {5}, 
volume = {28}
}
@article{10.1007/s10064-017-1176-3, 
year = {2019}, 
title = {{Comparison of data-driven models of loess landslide runout distance estimation}}, 
author = {Xu, Qiang and Li, Huajin and He, Yusen and Liu, Fangzhou and Peng, Dalei}, 
journal = {Bulletin of Engineering Geology and the Environment}, 
issn = {14359529}, 
doi = {10.1007/s10064-017-1176-3}, 
abstract = {{Irrigation-induced landslides with long runout distances endanger local communities. Estimating runout distance of landslides may contribute to the mitigation of potential hazards. Conventional mechanism-related methods require a series of experiments and/or numerical simulations that are commonly time-consuming and expensive, yet data-driven models reduce the experimental workload and require less prior knowledge in the geological history as well as mechanical behavior of the material. A data-driven model is proposed to forecast landslide runout distance using geometrical characteristics of the landslide. The geometrical dataset of the shallow loess landslides and loess-bedrock landslides occurred in Heifangtai terrace, China, was employed to develop the model. All geometrical datasets were obtained from field investigation and monitoring. Seven data-mining techniques were used and compared for runout estimation, among which the most optimal technique was integrated in the estimation model for loess slope failures. The multi-layer perceptron method outperforms other algorithms, and thus it was selected for the runout distance estimation model. Parametric models are constructed to fit runout distance based on the estimation. Hazard analysis measurements, including value-at-risk (VaR) and tail-value-at-risk (TVaR), are computed for the parametric distributions, which shows the potential area of impact and number of residential clusters at risk. © 2017, Springer-Verlag GmbH Germany.}}, 
pages = {1281--1294}, 
number = {2}, 
volume = {78}
}
@article{10.1016/j.cam.2011.01.044, 
year = {2011}, 
title = {{Comparison of certain value-at-risk estimation methods for the two-parameter Weibull loss distribution}}, 
author = {Gebizlioglu, Omer L. and Şenoğlu, Birdal and Kantar, Yeliz Mert}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2011.01.044}, 
abstract = {{The Weibull distribution is one of the most important distributions that is utilized as a probability model for loss amounts in connection with actuarial and financial risk management problems. This paper considers the Weibull distribution and its quantiles in the context of estimation of a risk measure called Value-at-Risk (VaR). VaR is simply the maximum loss in a specified period with a pre-assigned probability level. We attempt to present certain estimation methods for VaR as a quantile of a distribution and compare these methods with respect to their deficiency (Def) values. Along this line, the results of some Monte Carlo simulations, that we have conducted for detailed investigations on the efficiency of the estimators as compared to MLE, are provided. © 2011 Elsevier B.V. All rights reserved.}}, 
pages = {3304--3314}, 
number = {11}, 
volume = {235}
}
@article{10.1080/03610926.2019.1584312, 
year = {2020}, 
title = {{Asymptotic analysis of tail distortion risk measure under the framework of multivariate regular variation}}, 
author = {Xing, Guo-dong and Gan, Xiaoli}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2019.1584312}, 
abstract = {{Under the framework of multivariate regular variation, we obtain the asymptotic ratio between the tail distortion risk measure for portfolio loss and the sum of value-at-risk for single loss by a different method from the one in Zhu and Li when the confidence level tends to one. In order to illustrate the derived result, a relevant example is given and the corresponding numerical simulation is also carried out. © 2019, © 2019 Taylor \& Francis Group, LLC.}}, 
pages = {1--11}, 
number = {12}, 
volume = {49}
}
@article{10.1007/978-981-10-7901-6_59, 
year = {2018}, 
title = {{Performance Analysis of Different Models to Find Value at Risk in the Indian Market Using a Bi-Portfolio Allocation}}, 
author = {Verma, Om Prakash and Agarwal, Eshwar and Agrawal, Cherry and Gupta, Avanti}, 
journal = {Lecture Notes in Electrical Engineering}, 
issn = {18761100}, 
doi = {10.1007/978-981-10-7901-6\_59}, 
abstract = {{Risk analysis is one of the most important components of any financial decision and has been a subject of extensive research. We present here a comparative analysis of different methods to calculate value at risk (VaR) in the Indian Market. The models that have been explored are filtered historical simulations, Monte Carlo, historical simulation, and variance–covariance. The implementation was carried out in MATLAB, and the stock indices of NIFTY 50 and BSE SENSEX were used as a representative of the Indian Stock Market. The results obtained were first compared on the basis of time taken to calculate the risk incurred using six different datasets. They were also compared on the basis of whether the actual losses were within the calculated VaRs. A theoretical comparison on the various methods was also performed. © 2018, Springer Nature Singapore Pte Ltd.}}, 
pages = {539--551}, 
number = {NA}, 
volume = {462}
}
@article{10.1504/ijbaaf.2012.048320, 
year = {2012}, 
title = {{Extreme value theory performance in the event of major financial crises}}, 
author = {Rossignolo, Adrian F and Fethi, Meryem Duygun and Shaban, Mohamed}, 
journal = {International Journal of Banking, Accounting and Finance}, 
issn = {17553830}, 
doi = {10.1504/ijbaaf.2012.048320}, 
abstract = {{Current directives issued by the Basel Committee have established value-at-risk (VaR) as the standard measure to quantify market risk. In view of the wide range of applications and regulatory requirements, the development of accurate techniques becomes a topic of prime importance. VaR should protect market participants against sudden jerks in financial markets. While most models achieve that purpose for common everyday movements, they fail to account for unexpected crises. Extreme value theory (EVT) provides a method to estimate VaR at high quantiles of the distribution focusing on unusual circumstances. This paper employs EVT to calculate VaR for ten stock market indices belonging to developed and emerging markets in two different ways: unconditional EVT on raw returns and conditional EVT through quasi-maximum likelihood. The performance of EVT representations during the 2008 turmoil reveals that this methodology could have helped institutions to avoid huge losses arising from market disasters. A simple exercise on the constitution of Regulatory Capital illustrates the advantages of EVT. Copyright © 2012 Inderscience Enterprises Ltd.}}, 
pages = {94}, 
number = {2}, 
volume = {4}
}
@article{10.1109/tsmca.2010.2043947, 
year = {2010}, 
title = {{Optimal pricing and stocking decisions for newsvendor problem with value-at-risk consideration}}, 
author = {Chiu, Chun-Hung and Choi, Tsan-Ming}, 
journal = {IEEE Transactions on Systems, Man, and Cybernetics—Part A: Systems and Humans}, 
issn = {10834427}, 
doi = {10.1109/tsmca.2010.2043947}, 
abstract = {{Motivated by the popularity of VaR measure in financial applications, we study the classical newsvendor problem with Value-at-Risk (VaR) consideration and price-dependent demands. We first investigate the problem's structural properties and derive analytically the optimal joint stocking and pricing decisions. We then explore the difference between the optimal decisions under the VaR formulation and the classical expected profit-maximization model. Finally, we reveal an interesting analytical relationship between the inventory service level and the VaR measure. Insights are generated. © 2010 IEEE.}}, 
pages = {1116--1119}, 
number = {5}, 
volume = {40}
}
@article{10.3390/math9040394, 
year = {2021}, 
title = {{Optimisation of time-varying asset pricing models with penetration of value at risk and expected shortfall}}, 
author = {Nasir, Adeel and Khan, Kanwal Iqbal and Mata, Mário Nuno and Mata, Pedro Neves and Martins, Jéssica Nunes}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math9040394}, 
abstract = {{This study aims to apply value at risk (VaR) and expected shortfall (ES) as time-varying systematic and idiosyncratic risk factors to address the downside risk anomaly of various asset pricing models currently existing in the Pakistan stock exchange. The study analyses the significance of high minus low VaR and ES portfolios as a systematic risk factor in one factor, three-factor, and five-factor asset pricing model. Furthermore, the study introduced the six-factor model, deploying VaR and ES as the idiosyncratic risk factor. The theoretical and empirical alteration of traditional asset pricing models is the study’s contributions. This study reported a strong positive relationship of traditional market beta, value at risk, and expected shortfall. Market beta pertains its superiority in estimating the time-varying stock returns. Furthermore, value at risk and expected shortfall strengthen the effects of traditional beta impact on stock returns, signifying the proposed six-factor asset pricing model. Investment and profitability factors are redundant in conventional asset pricing models. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {394}, 
number = {4}, 
volume = {9}
}
@article{10.1109/tpwrs.2007.894858, 
year = {2007}, 
title = {{Negotiating bilateral contracts in electricity markets}}, 
author = {Khatib, S.E. and Galiana, F.D.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2007.894858}, 
abstract = {{In mixed pool/bilateral electricity markets, participants can sign forward bilateral contracts several months in advance of its delivery. In addition, generators may sell to and loads may buy from the pool at the spot price through the day- ahead or balancing markets. Forward bilateral contracts have the advantage of price predictability in comparison with the uncertain spot price. However, the risk is that such a contract commits the partners to a price that may be disadvantageous compared to the spot price. Here, we propose a systematic negotiation scheme through which a generator and load can reach a mutually beneficial and risk tolerable forward bilateral contract, either physical or financial. Under this approach, the generator and load respond rationally to a stream of bilateral bids/counter-bids and offers/ counter-offers considering their respective benefits while accounting for the risks incurred by the prediction uncertainty in the pool spot price and other market parameters over the length of the contract. Each negotiating party can choose its own definition of risk which can be influenced by regret, value-at-risk or dispersion from the mean. Numerical tests show that this flexible negotiating approach can be readily put into practice. © 2007 IEEE.}}, 
pages = {553--562}, 
number = {2}, 
volume = {22}
}
@article{10.1016/j.irfa.2016.10.008, 
year = {2017}, 
title = {{Multiple-days-ahead value-at-risk and expected shortfall forecasting for stock indices, commodities and exchange rates: Inter-day versus intra-day data}}, 
author = {Degiannakis, Stavros and Potamia, Artemis}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2016.10.008}, 
abstract = {{In order to provide reliable Value-at-Risk (VaR) and Expected Shortfall (ES) forecasts, this paper attempts to investigate whether an inter-day or an intra-day model provides accurate predictions. We investigate the performance of inter-day and intra-day volatility models by estimating the AR(1)-GARCH(1,1)-skT and the AR(1)-HAR-RV-skT frameworks, respectively. This paper is based on the recommendations of the Basel Committee on Banking Supervision. Regarding the forecasting performances, the exploitation of intra-day information does not appear to improve the accuracy of the VaR and ES forecasts for the 10-steps-ahead and 20-steps-ahead for the 95\%, 97.5\% and 99\% significance levels. On the contrary, the GARCH specification, based on the inter-day information set, is the superior model for forecasting the multiple-days-ahead VaR and ES measurements. The intra-day volatility model is not as appropriate as it was expected to be for each of the different asset classes; stock indices, commodities and exchange rates. The multi-period VaR and ES forecasts are estimated for a range of datasets (stock indices, commodities, foreign exchange rates) in order to provide risk managers and financial institutions with information relating the performance of the inter-day and intra-day volatility models across various markets. The inter-day specification predicts VaR and ES measures adequately at a 95\% confidence level. Regarding the 97.5\% confidence level that has been recently proposed in the revised 2013 version of Basel III, the GARCH-skT specification provides accurate forecasts of the risk measures for stock indices and exchange rates, but not for commodities (that is Silver and Gold). In the case of the 99\% confidence level, we do not achieve sufficiently accurate VaR and ES forecasts for all the assets. © 2016 Elsevier Inc.}}, 
pages = {176--190}, 
number = {NA}, 
volume = {49}
}
@article{10.1145/3209108.3209176, 
year = {2018}, 
title = {{Conditional Value-at-Risk for Reachability and Mean Payoff in Markov Decision Processes}}, 
author = {Křetínský, Jan and Meggendorfer, Tobias}, 
journal = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science}, 
issn = {10436871}, 
doi = {10.1145/3209108.3209176}, 
abstract = {{We present the conditional value-at-risk (CVaR) in the context of Markov chains and Markov decision processes with reachability and mean-payoff objectives. CVaR quantifies risk by means of the expectation of the worst p-quantile. As such it can be used to design risk-averse systems. We consider not only CVaR constraints, but also introduce their conjunction with expectation constraints and quantile constraints (value-at-risk, VaR). We derive lower and upper bounds on the computational complexity of the respective decision problems and characterize the structure of the strategies in terms of memory and randomization. © 2018 ACM.}}, 
pages = {609--618}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.qref.2020.09.005, 
year = {2020}, 
title = {{Revisiting the accuracy of standard VaR methods for risk assessment: Using the Copula–EVT multidimensional approach for stock markets in the MENA region}}, 
author = {Chebbi, Ali and Hedhli, Amel}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2020.09.005}, 
abstract = {{The aim of this study is twofold. First, it aims to show how to overcome some of the shortcomings of the standard risk measurement methods using value-at-risk (VaR) in the context of extreme events and propose an alternative empirical method. Second, it aims to fill the research gap in analysis of risk in the Middle East and North Africa (MENA) region. In this regard, for six daily stock indices in the MENA region, from January 3, 2005 to December 31, 2014, we employ a vine copula-based generalized autoregressive conditional heteroskedastic (GARCH) method and the extreme value theory (EVT) to model the dependence between the marginal distributions of returns and forecast the VaR. Based on backtesting, we assess the efficiency of the standard risk measurement models from the following families—the exponentially weighted moving average, the historical simulation, and the GARCH. By implementing the GARCH-EVT-C-vine method in the MENA region, we find that, empirically, standard methods overestimate (underestimate) the violation ratio, implying an underestimation (overestimation) of the risk, and therefore a misallocation of the capital covering the risk. The method also provides better VaR estimates for the MENA stock markets than that of the standard methods. We also verify that the greater the openness of the capital accounts and the flexibility of the exchange rate regimes, the greater will be the conditional dependence of the MENA countries on the developed markets. Finally, the empirical method we propose has an important implication for the MENA countries in that it can be adopted by these countries to forecast VaR effectively. © 2020 Board of Trustees of the University of Illinois}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/13504851.2016.1200174, 
year = {2017}, 
title = {{SEC FRR No. 48 and analyst forecast accuracy}}, 
author = {Lin, Bing-Xuan and Lin, Chen-Miao}, 
journal = {Applied Economics Letters}, 
issn = {13504851}, 
doi = {10.1080/13504851.2016.1200174}, 
abstract = {{SEC FRR No. 48 requires that all firms report their market risk exposures by choosing among three alternative formats: sensitivity analysis, tabular and value at risk (VaR). In this article, we examine how different methods affect analyst forecast accuracy. By regressing analyst forecast errors on a company’s choice of disclosure method, we find that analyst forecast errors are smaller for firms using VaR and tabular than for firms using sensitivity analysis. Our findings suggest that VaR and tabular approaches are more informative than sensitivity analysis. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--6}, 
number = {6}, 
volume = {24}
}
@article{10.1111/j.1468-036x.2011.00611.x, 
year = {2013}, 
title = {{An international perspective on risk management quality}}, 
author = {Mira, Svetlana and Taylor, Nicholas}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/j.1468-036x.2011.00611.x}, 
abstract = {{This paper introduces an alternative method for assessing the quality of risk management models. Specifically, using the forecast efficiency notion that forecast errors should be independent of a pertinent information set, we consider the extent to which unanticipated downside risk (extreme risk) is independent of overseas extreme risk. This is achieved using a bootstrap version of the non-causality test recently introduced by Hong et al., data covering 45 international equity markets, and by measuring extreme risk via a class of risk management models recently introduced by Xiao and Koenker. In doing this, we find significant evidence of transmission (causality) across national borders. Moreover, we discuss how risk managers in developed and emerging markets can parsimoniously incorporate such information (international dependency) into their risk management models to produce measures of downside risk that have more desirable ex post properties (viz. forecast efficiency properties). © 2011 John Wiley \& Sons Ltd.}}, 
pages = {935--955}, 
number = {5}, 
volume = {19}
}
@article{10.1002/apj.567, 
year = {2011}, 
title = {{A study on the financial approach of risk assessment using chemical accident records in chemical process industries}}, 
author = {Jang, Namjin and Koo, Jamin and Kim, Hosoo and Shin, Dongil and Yoon, En Sup}, 
journal = {Asia‐Pacific Journal of Chemical Engineering}, 
issn = {19322135}, 
doi = {10.1002/apj.567}, 
abstract = {{Usually, risk assessment is a combination of risk analysis and risk appraisal to evaluate the consequences and frequencies of hazardous events. The acceptability of the exposed risk is also judged in the process. In this regard, the financial risk matrix could be a useful tool based on the frequency and expected loss of damage. Thus, it is adopted in this study where a financial approach has been taken in risk assessment; the proposed methodology uses frequencies from chemical accident records and value at risk (VaR). The methodology consists of mainly five steps. It starts with hazard identification, and is followed by adjustment of history-based accident frequencies by using severity ratios. Then, accident probability and expected damage loss are estimated. The results obtained in the previous two steps are combined to compute VaR for the target process; this value is mapped with financial risk matrix to re-evaluate accident frequency. As an illustrative case, Texas BP refinery accident in 2005 is studied according to the proposed methodology. The results indicate that the financial risk increased from a low level to a medium high level after the occurrence of the tragic accident. As similar accidents frequently occurred for the same process, the risk of the process should have been increased. This proposed method can reflect this dynamic change in risks with the help of accident records and their impacts. Copyright © 2011 Curtin University of Technology and John Wiley \& Sons, Ltd. Copyright © 2011 Curtin University of Technology and John Wiley \& Sons, Ltd.}}, 
pages = {509--517}, 
number = {3}, 
volume = {6}
}
@article{10.1016/j.ememar.2017.06.002, 
year = {2017}, 
title = {{Diversification potential of Asian frontier, BRIC emerging and major developed stock markets: A wavelet-based value at risk approach}}, 
author = {Mensi, Walid and Shahzad, Syed Jawad Hussain and Hammoudeh, Shawkat and Zeitun, Rami and Rehman, Mobeen Ur}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2017.06.002}, 
abstract = {{This study examines the portfolio risk and the co-movements between each of the BRIC emerging and South Asian frontier stock markets and each of the major developed stock markets (U.S., UK and Japan), using the wavelet squared coherence approach as well as the wavelet-based Value at Risk (VaR) method. The results show that the co-movements and diversification benefits between these markets vary over time and across frequencies. Additionally, the co-movements are intensified in the wake of the recent global financial crisis (GFC) and the Eurozone sovereign debt crisis (ESDC). More precisely, the wavelet-based VaR ratio indicates that including a BRIC or a South Asian (particularly Pakistan and Sri Lanka at both the short- and long-term) stock market in a portfolio of the developed stock markets reduces the resulting portfolio's VaR. Specifically, adding China in the medium term to this portfolio reduces risk in the pre- and during both the GFC and ESDC periods. By assigning optimal weights to the different market assets in the portfolio formulation, the analysis thus has implications for international investors. © 2017 Elsevier B.V.}}, 
pages = {130--147}, 
number = {NA}, 
volume = {32}
}
@article{10.1057/s41274-016-0133-z, 
year = {2017}, 
title = {{Estimating and forecasting portfolio's Value-at-Risk with wavelet-based extreme value theory: Evidence from crude oil prices and US exchange rates}}, 
author = {Jammazi, Rania and Nguyen, Duc Khuong}, 
journal = {Journal of the Operational Research Society}, 
issn = {01605682}, 
doi = {10.1057/s41274-016-0133-z}, 
abstract = {{This article proposes a wavelet-based extreme value theory (W-EVT) approach to estimate and forecast portfolio's Value-at-Risk (VaR) given the stylized facts and complex structure of financial data. Our empirical application to portfolios of crude oil prices and US dollar exchange rates shows that the W-EVT models provide an effective and powerful tool for gauging extreme moments and improving the accuracy of portfolio's VaR estimates and forecasts after noise is removed from the original data. © 2017 The Operational Research Society.}}, 
pages = {1352--1362}, 
number = {11}, 
volume = {68}
}
@article{10.1145/2396761.2398443, 
year = {2012}, 
title = {{Model the complex dependence structures of financial variables by using canonical vine}}, 
author = {Wei, Wei and Fan, Xuhui and Li, Jinyan and Cao, Longbing}, 
journal = {Proceedings of the 21st ACM international conference on Information and knowledge management - CIKM '12}, 
issn = {NA}, 
doi = {10.1145/2396761.2398443}, 
abstract = {{Financial variables such as asset returns in the massive market contain various hierarchical and horizontal relationships forming complicated dependence structures. Modeling and mining of these structures is challenging due to their own high structural complexities as well as the stylized facts of the market data. This paper introduces a new canonical vine dependence model to identify the asymmetric and non-linear dependence structures of asset returns without any prior independence assumptions. To simplify the model while maintaining its merit, a partial correlation based method is proposed to optimize the canonical vine. Compared with the original canonical vine, the new model can still maintain the most important dependence but many unimportant nodes are removed to simplify the canonical vine structure. Our model is applied to construct and analyze dependence structures of European stocks as case studies. Its performance is evaluated by measuring portfolio of Value at Risk, a widely used risk management measure. In comparison to a very recent canonical vine model and the 'full' model, our experimental results demonstrate that our model has a much better quality of Value at Risk, providing insightful knowledge for investors to control and reduce the aggregation risk of the portfolio. © 2012 ACM.}}, 
pages = {1382--1391}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0378-4266(99)00068-0, 
year = {2000}, 
title = {{Value at risk models for Dutch bond portfolios}}, 
author = {Vlaar, Peter J.G.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(99)00068-0}, 
abstract = {{This study investigates the consequences of dynamics in the term structure of Dutch interest rates for the accurateness of value-at-risk models. Therefore, value-at-risk measures are calculated using both historical simulation, variance-covariance and Monte Carlo simulation methods. For a ten days holding period, the best results were obtained for a combined variance-covariance Monte Carlo method using a term structure model with a normal distribution and GARCH specification. Term structure models with a t-distribution or with cointegration performed much worse. © 2000 Elsevier Science B.V.}}, 
pages = {1131--1154}, 
number = {7}, 
volume = {24}
}
@article{10.1080/14697688.2011.595731, 
year = {2014}, 
title = {{Haar wavelets-based approach for quantifying credit portfolio losses}}, 
author = {Masdemont, Josep J. and Ortiz-Gracia, Luis}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2011.595731}, 
abstract = {{This paper proposes a new methodology to compute Value at Risk (VaR) for quantifying losses in credit portfolios. We approximate the cumulative distribution of the loss function by a finite combination of Haar wavelet basis functions and calculate the coefficients of the approximation by inverting its Laplace transform. The Wavelet Approximation (WA) method is particularly suitable for non-smooth distributions, often arising in small or concentrated portfolios, when the hypothesis of the Basel II formulas are violated. To test the methodology we consider the Vasicek one-factor portfolio credit loss model as our model framework. WA is an accurate, robust and fast method, allowing the estimation of the VaR much more quickly than with a Monte Carlo (MC) method at the same level of accuracy and reliability. © 2014 Copyright Taylor \& Francis Group, LLC.}}, 
pages = {1587--1595}, 
number = {9}, 
volume = {14}
}
@article{10.1080/00949655.2013.812092, 
year = {2015}, 
title = {{A new algorithm for maximum likelihood estimation in normal scale-mixture generalized autoregressive conditional heteroskedastic models}}, 
author = {Seo, Byungtae and Lee, Taewook}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {00949655}, 
doi = {10.1080/00949655.2013.812092}, 
abstract = {{In this paper, we propose a new generalized autoregressive conditional heteroskedastic (GARCH) model using infinite normal scale-mixtures which can suitably avoid order selection problems in the application of finite normal scale-mixtures. We discuss its theoretical properties and develop a two-stage algorithm for the maximum likelihood estimator to estimate the mixing distribution non-parametric maximum likelihood estimator (NPMLE) as well as GARCH parameters (two-stage MLE). For the estimation of a mixing distribution, we employ a fast computational algorithm proposed by Wang [On fast computation of the non-parametric maximum likelihood estimate of a mixing distribution. J R Stat Soc Ser B. 2007;69:185–198] under the gradient characterization of the non-parametric mixture likelihood. The GARCH parameters are then estimated either using the expectation-mazimization algorithm or general optimization scheme. In addition, we propose a new forecasting algorithm of value-at-risk (VaR) using the two-stage MLE and the NPMLE. Through a simulation study and real data analysis, we compare the performance of the two-stage MLE with the existing ones including quasi-maximum likelihood estimator based on the standard normal density and the finite normal mixture quasi maximum estimated-likelihood estimator (cf. Lee S, Lee T. Inference for Box–Cox transformed threshold GARCH models with nuisance parameters. Scand J Stat. 2012;39:568–589) in terms of the relative efficiency and accuracy of VaR forecasting. © 2013, © 2013 Taylor \& Francis.}}, 
pages = {202--215}, 
number = {1}, 
volume = {85}
}
@article{10.1016/j.physa.2015.12.161, 
year = {2016}, 
title = {{Multifractal Value at Risk model}}, 
author = {Lee, Hojin and Song, Jae Wook and Chang, Woojin}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2015.12.161}, 
abstract = {{In this paper new Value at Risk (VaR) model is proposed and investigated. We consider the multifractal property of financial time series and develop a multifractal Value at Risk (MFVaR). MFVaR introduced in this paper is analytically tractable and not based on simulation. Empirical study showed that MFVaR can provide the more stable and accurate forecasting performance in volatile financial markets where large loss can be incurred. This implies that our multifractal VaR works well for the risk measurement of extreme credit events. © 2016 Elsevier B.V. All rights reserved.}}, 
pages = {113--122}, 
number = {NA}, 
volume = {451}
}
@article{10.1016/j.physa.2008.02.055, 
year = {2008}, 
title = {{Copula-based measures of dependence structure in assets returns}}, 
author = {Fernandez, Viviana}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2008.02.055}, 
abstract = {{Copula modeling has become an increasingly popular tool in finance to model assets returns dependency. In essence, copulas enable us to extract the dependence structure from the joint distribution function of a set of random variables and, at the same time, to isolate such dependence structure from the univariate marginal behavior. In this study, based on US stock data, we illustrate how tail-dependency tests may be misleading as a tool to select a copula that closely mimics the dependency structure of the data. This problem becomes more severe when the data is scaled by conditional volatility and/or filtered out for serial correlation. The discussion is complemented, under more general settings, with Monte Carlo simulations and portfolio management implications. © 2008 Elsevier Ltd. All rights reserved.}}, 
pages = {3615--3628}, 
number = {14}, 
volume = {387}
}
@article{10.15611/aoe.2021.1.01, 
year = {2021}, 
title = {{Industry standard and econometric standard: The search for powerful approach to evaluate var models}}, 
author = {Małecka, Marta}, 
journal = {Argumenta Oeconomica}, 
issn = {12335835}, 
doi = {10.15611/aoe.2021.1.01}, 
abstract = {{Under the Basel III and Basel IV accords, risk model validation remains based on the VaR measure. According to the industry practice, VaR backtesting procedures rely on two likelihood ratio tests, which, in light of the academic research, have been criticized for their unsatisfactory power. This paper aims to show the differences between VaR model evaluation based on the standard likelihood ratio approach and backtesting by means of other econometric methods applicable to the binary VaR failure process. The author decomposed the model evaluation into testing the unconditional coverage, replaced the likelihood ratio with a normal statistic, and in the next stage in order to verify the conditional coverage, employed the Ljung-Box statistic. The study experimentally confirmed the superiority of the proposed procedures over the industry standards. The main contribution, however, is the empirical study designed to demonstrate the practical differences in risk analysis attributable to the choice of the backtesting method. Using data on leading stock market indexes, from various periods, the author showed that the practical conclusions from backtesting diverge markedly due to the test choice. The proposed, more powerful tests, contrary to the standard procedures, allowed for distinguishing distinct models of index behaviour connected with undergoing the financial crises. © 2021, Wroclaw University of Economics. All rights reserved.}}, 
pages = {5--30}, 
number = {1}, 
volume = {46}
}
@article{10.1007/978-3-030-77876-7_15, 
year = {2021}, 
title = {{Sample Approximations of Bilevel Stochastic Programming Problems with Probabilistic and Quantile Criteria}}, 
author = {Ivanov, Sergey V. and Ignatov, Aleksei N.}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-030-77876-7\_15}, 
abstract = {{In this paper, bilevel stochastic programming problems with probabilistic and quantile criteria are considered. The lower level problem is assumed to be linear for fixed leader’s (upper level) variables and fixed realizations of the random parameters. The objective function and the constraints of the lower level problem depend on the leader’s strategy and random parameters. The objective function of the upper level problem is defined as the value of the probabilistic or quantile functional of the random losses on the upper level. We suggest conditions guaranteeing that the objective function of the upper level is a normal integrand. It is shown that these conditions are satisfied for a class of problems with positive coefficients of the lower level problem. This allows us to suggest sufficient conditions of the existence of a solution to the considered problem. We construct sample approximations of these problems. These approximations reduce to mixed integer nonlinear programming problems. We describe sufficient conditions of the convergence of the sample approximations to the original problems. © 2021, Springer Nature Switzerland AG.}}, 
pages = {221--234}, 
number = {NA}, 
volume = {12755 LNCS}
}
@article{10.1007/s11156-012-0274-3, 
year = {2013}, 
title = {{Returns transmission, value at risk, and diversification benefits in international REITs: Evidence from the financial crisis}}, 
author = {Lu, Chiuling and Tse, Yiuman and Williams, Michael}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-012-0274-3}, 
abstract = {{We examine daily cross-market return interactions and downside risk between a US REIT returns index and the return indexes of twelve international REIT markets. These relationships are investigated for a period of normal REIT market conditions as well as for periods of inflating and collapsing REIT prices. We find that US REIT returns are contemporaneously correlated with other REITs most strongly during the bubble and crash market conditions where the US REIT market is an almost unilateral transmitter of returns. We also find that the Value at Risk (VaR) of the least capitalized REIT markets is proportionally higher during base/normal market conditions but that the largest REIT markets have the highest VaR contribution during the crash (financial crisis) period. Overall, our evidence indicates that REIT market risk shifted to the largest REIT markets and that diversification benefits eroded considerably during turbulent market conditions. © 2012 Springer Science+Business Media, LLC.}}, 
pages = {293--318}, 
number = {2}, 
volume = {40}
}
@article{10.1080/1350486x.2014.962182, 
year = {2015}, 
title = {{A New Variance Reduction Technique for Estimating Value-at-Risk}}, 
author = {Korn, Ralf and Pupashenko, Mykhailo}, 
journal = {Applied Mathematical Finance}, 
issn = {1350486X}, 
doi = {10.1080/1350486x.2014.962182}, 
abstract = {{Abstract: In this article we present a new variance reduction technique for estimating the Value-at-Risk (VaR) of a portfolio of various securities via Monte Carlo (MC) simulation. The technique can be applied for any type of distribution of the risk factors, no matter if light- or heavy-tailed. It consists of a particular variant of importance sampling where the change of measure is obtained by using an approximation to an optimal importance sampling density. Any approximation of the portfolio loss function (such as the popular Delta–Gamma approximation) can be used. An in-depth numerical study in the case of risk factors with light-tailed distributions exhibits a great variance reduction when estimating the probability of large portfolio losses outperforming other known methods. © 2014 Taylor \& Francis.}}, 
pages = {83--98}, 
number = {1}, 
volume = {22}
}
@article{10.1108/raf-01-2017-0006, 
year = {2018}, 
title = {{The predictive power of log-likelihood of GARCH volatility}}, 
author = {Handika, Rangga and Chalid, Dony Abdul}, 
journal = {Review of Accounting and Finance}, 
issn = {14757702}, 
doi = {10.1108/raf-01-2017-0006}, 
abstract = {{Purpose: This paper aims to investigate whether the best statistical model also corresponds to the best empirical performance in the volatility modeling of financialized commodity markets. Design/methodology/approach: The authors use various p and q values in Value-at-Risk (VaR) GARCH(p, q) estimation and perform backtesting at different confidence levels, different out-of-sample periods and different data frequencies for eight financialized commodities. Findings: They find that the best fitted GARCH(p,q) model tends to generate the best empirical performance for most financialized commodities. Their findings are consistent at different confidence levels and different out-of-sample periods. However, the strong results occur for both daily and weekly returns series. They obtain weak results for the monthly series. Research limitations/implications: Their research method is limited to the GARCH(p,q) model and the eight discussed financialized commodities. Practical implications: They conclude that they should continue to rely on the log-likelihood statistical criteria for choosing a GARCH(p,q) model in financialized commodity markets for daily and weekly forecasting horizons. Social implications: The log-likelihood statistical criterion has strong predictive power in GARCH high-frequency data series (daily and weekly). This finding justifies the importance of using statistical criterion in financial market modeling. Originality/value: First, this paper investigates whether the best statistical model corresponds to the best empirical performance. Second, this paper provides an indirect test for evaluating the accuracy of volatility modeling by using the VaR approach. © 2018, Emerald Publishing Limited.}}, 
pages = {482--497}, 
number = {4}, 
volume = {17}
}
@article{10.1080/14697680903419701, 
year = {2010}, 
title = {{The impact of the choice of VaR models on the level of regulatory capital according to Basel II}}, 
author = {Hermsen, Oliver}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680903419701}, 
abstract = {{The Basel II framework allows the calculation of the capital requirements for market risk with Value-at-Risk models. Since no special model is prescribed in the framework, banks may use simple models with questionable assumptions concerning their underlying distributions. Our numerical analysis reveals that simple VaR models that perform noticeably worse than comparable simple models with more realistic assumptions may lead to a lower level of regulatory capital for banks. For this reason, banks have a major incentive to implement bad models. This is obviously contrary to the interests of regulatory authorities. © 2010 Taylor \& Francis.}}, 
pages = {1215--1224}, 
number = {10}, 
volume = {10}
}
@article{10.1007/s10589-005-2054-7, 
year = {2005}, 
title = {{Strategic long-term financial risks: Single risk factors}}, 
author = {Embrechts, Paul and Kaufmann, Roger and Patie, Pierre}, 
journal = {Computational Optimization and Applications}, 
issn = {09266003}, 
doi = {10.1007/s10589-005-2054-7}, 
abstract = {{The question of the measurement of strategic long-term financial risks is of considerable importance. Existing modelling instruments allow for a good measurement of market risks of trading books over relatively small time intervals. However, these approaches may have severe deficiencies if they are routinely applied to longer time periods. In this paper we give an overview on methodologies that can be used to model the evolution of risk factors over a one-year horizon. Different models are tested on financial time series data by performing backtesting on their expected shortfall predictions. © 2005 Springer Science + Business Media, Inc.}}, 
pages = {61--90}, 
number = {1-2}, 
volume = {32}
}
@article{10.1002/for.2255, 
year = {2013}, 
title = {{Exponentially smoothing the skewed Laplace distribution for value-at-risk forecasting}}, 
author = {Gerlach, Richard and Lu, Zudi and Huang, Hai}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2255}, 
abstract = {{Value-at-risk (VaR) is a standard measure of market risk in financial markets. This paper proposes a novel, adaptive and efficient method to forecast both volatility and VaR. Extending existing exponential smoothing as well as GARCH formulations, the method is motivated from an asymmetric Laplace distribution, where skewness and heavy tails in return distributions, and their potentially time-varying nature, are taken into account. The proposed volatility equation also involves novel time-varying dynamics. Back-testing results illustrate that the proposed method offers a viable, and more accurate, though conservative, improvement in forecasting VaR compared to a range of popular alternatives. Copyright © 2013 John Wiley \& Sons, Ltd. Copyright © 2013 John Wiley \& Sons, Ltd.}}, 
pages = {534--550}, 
number = {6}, 
volume = {32}
}
@article{10.1016/j.jeconom.2014.11.003, 
year = {2015}, 
title = {{Through the looking glass: Indirect inference via simple equilibria}}, 
author = {Calvet, Laurent E. and Czellar, Veronika}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2014.11.003}, 
abstract = {{This paper develops an indirect inference (Gourieroux et al.; 1993; Smith, 1993) estimation method for a large class of dynamic equilibria. Our approach consists of constructing econometrically tractable auxiliary equilibria, obtained by simplifying the economic primitives of the structural equilibrium model, via which estimation can proceed. We use this approach to develop an accurate estimator for the long-run risk model of Bansal and Yaron (2004). We demonstrate the method in Monte Carlo simulations and implement it on U.S. data. We also illustrate the good performance of the proposed methodology on an equilibrium model with investor learning. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {343--358}, 
number = {2}, 
volume = {185}
}
@article{10.21314/jor.2017.359, 
year = {2017}, 
title = {{How risk managers should fix tracking error volatility and value-at-risk constraints in asset management}}, 
author = {Riccetti, Luca}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2017.359}, 
abstract = {{Investors usually assign part of their funds to asset managers, who are given the task of beating a benchmark. Asset managers usually face a constraint on maximum tracking error volatility (TEV), which is imposed by the risk management office. In the mean– variance space, Jorion, in his 2003 paper “Portfolio optimization with tracking-error constraints”, shows that this constraint determines an ellipse containing all admissible portfolios. However, many admissible portfolios have problems in mean–variance terms, for example, because of an overly high variance. To overcome this problem, Jorion also fixes a constraint on variance, while, in their 2008 paper “Active portfolio management with benchmarking: adding a value-at-risk constraint”, Alexander and Baptista fix a constraint on value-at-risk (VaR). In this paper, I determine an optimal value for a set of limits composed of the lower limit on TEV, the upper limit on TEV and the upper limit on VaR. To fix the upper limit on VaR, I use the TEV constrained efficient frontier developed in Palomba and Riccetti’s 2013 paper “Asset management with TEV and VAR constraints: the constrained efficient frontiers”, which is the set of portfolios that is on Jorion’s ellipse and not dominated from the mean–variance perspective. In particular, I develop a strategy to impose on asset managers a set of portfolios that contains as many TEV constrained efficient portfolios and as few inefficient portfolios as possible. Moreover, I show that a limit on maximum VaR is usually better than a limit on maximum variance. © 2017 Incisive Risk Information (IP) Limited.}}, 
number = {4}, 
volume = {19}
}
@article{10.3934/jimo.2019021, 
year = {2020}, 
title = {{Model Selection Based On Value-At-Risk Backtesting Approach For Garch-Type Models}}, 
author = {Tay, Hao-Zhe and Ng, Kok-Haur and Koh, You-Beng and Ng, Kooi-Huat}, 
journal = {Journal of Industrial \& Management Optimization}, 
issn = {15475816}, 
doi = {10.3934/jimo.2019021}, 
abstract = {{This paper aims to investigate the efficiency of the value-at-risk (VaR) backtests in the model selection from different types of generalised au-toregressive conditional heteroskedasticity (GARCH) models with skewed and non-skewed innovation distributions. Extensive simulation is carried out to compare the model selection based on VaR backtests and Akaike Information Criteria (AIC). When the model is given but the innovation distribution is one of the six selected distributions which may be skewed or non-skewed, the sim-ulation results show that both AIC and the VaR backtests succeed in selecting the correct innovation distribution from the set of six distributions under con-sideration. This indicates that both AIC and the VaR backtests are able to distinguish between skewed and non-skewed distributions when the innovation distribution is misspeciffied. Using an empirical data from NASDAQ index, we observe that the selected combination of model and innovation distribu-tion based on the smallest AIC does not agree with that selected by using the in-sample VaR backtests. Examination of confidence limits for VaR and the expected shortfall forecasts under various loss functions provides evidence that the selected combination of model and innovation distribution using the VaR backtests tends to possess smaller mean absolute percentage error and logarithmic loss. © 2020, Journal of Industrial and Management Optimization. All Rights Reserved.}}, 
pages = {1635--1654}, 
number = {4}, 
volume = {16}
}
@article{10.1016/j.ejor.2012.08.022, 
year = {2013}, 
title = {{Risk neutral and risk averse Stochastic Dual Dynamic Programming method}}, 
author = {Shapiro, Alexander and Tekaya, Wajdi and Costa, Joari Paulo da and Soares, Murilo Pereira}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2012.08.022}, 
abstract = {{In this paper we discuss risk neutral and risk averse approaches to multistage (linear) stochastic programming problems based on the Stochastic Dual Dynamic Programming (SDDP) method. We give a general description of the algorithm and present computational studies related to planning of the Brazilian interconnected power system. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {375--391}, 
number = {2}, 
volume = {224}
}
@article{10.1016/j.jbankfin.2015.11.006, 
year = {2016}, 
title = {{Unexpected shortfalls of Expected Shortfall: Extreme default profiles and regulatory arbitrage}}, 
author = {Koch-Medina, Pablo and Munari, Cosimo}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2015.11.006}, 
abstract = {{The purpose of this paper is to dispel some common misunderstandings about capital adequacy rules based on Expected Shortfall. We establish that, from a theoretical perspective, Expected Shortfall based regulation can provide a misleading assessment of tail behavior, does not necessarily protect liability holders' interests much better than Value-at-Risk based regulation, and may also allow for regulatory arbitrage when used as a global solvency measure. We also show that, for a value-maximizing financial institution, the benefits derived from protecting its franchise may not be sufficient to disincentivize excessive risk taking. We further interpret our results in the context of portfolio risk measurement. Our results do not invalidate the possible merits of Expected Shortfall as a risk measure but instead highlight the need for its cautious use in the context of capital adequacy regimes and of portfolio risk control. © 2015 Elsevier B.V.}}, 
pages = {141--151}, 
number = {NA}, 
volume = {62}
}
@article{10.1109/ieem.2017.8290168, 
year = {2018}, 
title = {{Financial risk measurement in colombian system of mining royalties}}, 
author = {Bustos-González, A.M. and Ramírez-Domínzuez, L.F. and Mosquera-López, S. and Duque, D. F. Manotas}, 
journal = {2017 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
issn = {21573611}, 
doi = {10.1109/ieem.2017.8290168}, 
abstract = {{The objective of risk management is to mitigate the probability of financial losses due to different types of risk that could affect supply chain. This paper presents a methodological approach to estimate the financial risk of steam coal royalties received by Colombian government considering risk factors such as the commodity price, freight price and Exchange rate. These risk factors are considered in the model for estimating the Colombian steam coal royalties. We used robust risk indicators such as VaR (Value at Risk) and ES (Expected Shortfall). These measures were obtained through three approaches: parametric, semi-parametric and non-parametric. The coal royalties are a very important income for Colombia. The royalties obtained of steam coal exports represented USD 894 million in 2014. In this framework, it is very important to estimate the volatility of tax incomes related to steam coal exports. © 2017 IEEE.}}, 
pages = {1626--1630}, 
number = {NA}, 
volume = {2017-December}
}
@article{10.1177/0312896211410795, 
year = {2011}, 
title = {{The collapse of pasminco: Misjudgment, misfortune and miscalculation}}, 
author = {Brown, Christine and Ma, James}, 
journal = {Australian Journal of Management}, 
issn = {03128962}, 
doi = {10.1177/0312896211410795}, 
abstract = {{In September 2001 when Pasminco Ltd was placed in voluntary administration, it was the world's biggest zinc producer with over 10\% of global output. This paper examines the hedging strategies that Pasminco had in place in the years leading up to the firm being declared insolvent and documents the financial factors that contributed to the corporate collapse. We build a valuation model that illustrates in stark terms that the company's hedging strategies materially reduced neither the exposure to market risk nor the probability of financial distress, and were a major contributing factor to the company's downfall. The analysis in this case study contains valuable lessons for companies facing quantity and exchange rate risk. © The Author(s) 2011.}}, 
pages = {287--312}, 
number = {2}, 
volume = {36}
}
@article{10.1109/pct.2007.4538282, 
year = {2007}, 
title = {{Value at risk method for asset management of power transmission systems}}, 
author = {Schreiner, Andrej and Balzer, Gerd}, 
journal = {2007 IEEE Lausanne Power Tech}, 
issn = {NA}, 
doi = {10.1109/pct.2007.4538282}, 
abstract = {{The value at risk (VaR) is a popular method in financial world. VaR measures the worst expected loss over a given horizon under normal market conditions at a given confidence level. This procedure summarizes objectively the exposure to market risk and the probability of adverse move. So VaR measurement can be applied as benchmark of different portfolio, as control instrument for asset owners and as regulation instrument for asset management. The simplicity and objectivity of VaR concludes to the idea, to apply this method for power systems risk management. Basing on the simple example concerning transmission network, a VaR derivation model for power transmission and distribution systems is presented. The discussion concerning advantages of this model in relationship to the conventional reliability methods is closing the paper. ©2007 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10898-011-9656-7, 
year = {2011}, 
title = {{Low order-value approach for solving VaR-constrained optimization problems}}, 
author = {Birgin, E. G. and Bueno, L. F. and Krejić, N. and Martínez, J. M.}, 
journal = {Journal of Global Optimization}, 
issn = {09255001}, 
doi = {10.1007/s10898-011-9656-7}, 
abstract = {{In Low Order-Value Optimization (LOVO) problems the sum of the r smallest values of a finite sequence of q functions is involved as the objective to be minimized or as a constraint. The latter case is considered in the present paper. Portfolio optimization problems with a constraint on the admissible Value at Risk (VaR) can be modeled in terms of a LOVO problem with constraints given by Low order-value functions. Different algorithms for practical solution of this problem will be presented. Using these techniques, portfolio optimization problems with transaction costs will be solved. © 2011 Springer Science+Business Media, LLC.}}, 
pages = {715--742}, 
number = {4}, 
volume = {51}
}
@article{10.1080/14697680903159240, 
year = {2010}, 
title = {{(Non-)robustness of maximum likelihood estimators for operational risk severity distributions}}, 
author = {Huber, Sonja}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680903159240}, 
abstract = {{The quality of operational risk data sets suffers from missing or contaminated data points. This may lead to implausible characteristics of the estimates. Outliers, especially, can make a modeler's task difficult and can result in arbitrarily large capital charges. Robust statistics provides ways to deal with these problems as well as measures for the reliability of estimators. We show that using maximum likelihood estimation can be misleading and unreliable assuming typical operational risk severity distributions. The robustness of the estimators for the Generalized Pareto distribution, and the Weibull and Lognormal distributions is measured considering both global and local reliability, which are represented by the breakdown point and the influence function of the estimate. © 2010 Taylor \& Francis.}}, 
pages = {871--882}, 
number = {8}, 
volume = {10}
}
@article{10.1016/j.pacfin.2011.05.002, 
year = {2011}, 
title = {{Style analysis and Value-at-Risk of Asia-focused hedge funds}}, 
author = {Weng, Haijie and Trück, Stefan}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2011.05.002}, 
abstract = {{In this paper we identify risk factors for Asia-focused hedge funds through a modified style analysis technique. Using an Asian hedge fund index, we find that Asian hedge funds show significant positive exposures to emerging equity markets. For both a static and rolling period style analysis, our model provides a high explanatory power for returns of the considered hedge fund index. We further conduct a Value-at-Risk analysis using the results of a rolling window style analysis as inputs. Our findings suggest that the considered parametric models outperform a simple historical simulation that is purely based on past return observations. © 2011 Elsevier B.V.}}, 
pages = {491--510}, 
number = {5}, 
volume = {19}
}
@article{10.1016/j.intfin.2006.12.001, 
year = {2008}, 
title = {{Evidence of non-stationary bias in scaling by square root of time: Implications for Value-at-Risk}}, 
author = {Saadi, Samir and Rahman, Abdul}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2006.12.001}, 
abstract = {{In this paper, we show that scaled conditional volatilities obtained by the square root formula applied to i.i.d residuals from a sample of Canadian stock market data for various time horizons and error distributions, typically underestimate the true conditional volatility; consistently have a higher standard deviation and exhibit non-stationary kurtosis. Furthermore, the bias produced by volatility scaling is non-stationary in mean and standard deviation and its magnitude is likely influenced by monetary policy regime shifts. Moreover, while VaR is risk-coherence for elliptical distributions, this bias remains even for this class of distributions. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {272--289}, 
number = {3}, 
volume = {18}
}
@article{10.21314/jrmv.2016.151, 
year = {2016}, 
title = {{Value-at-risk estimation with the Carr-Geman-Madan-Yor process: An empirical study on foreign exchange rates}}, 
author = {Choi, Sun-Yong}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2016.151}, 
abstract = {{To discover whether the Carr-Geman-Madan-Yor (CGMY) distribution, a Lévy process that can have an infinite number of jumps in a finite time interval, is appropriate for describing the fat tails of the distribution of foreign exchange (FX) rate returns, we investigate its performance in estimating the risk of FX rates. We estimate valueat- risk (VaR), one of the most popular concepts in the area of risk management, for FX rate returns. To enhance the robustness of the estimation results, we use other VaR models, such as historical simulation, the generalized autoregressive conditional heteroscedasticity (GARCH) model and conditional extreme value theory (EVT). Our four methodologies are applied to three major foreign exchange rates (EUR/JPY, EUR/USD and USD/JPY) by using a different size estimation windows for each model to forecast one-day-ahead VaR. We propose a new procedure to find the best size of estimation window for each VaR model, with which we compare the respective VaR estimates. The conditional EVT and CGMY distributions provide superior forecasting performance for all three rates's daily returns, respectively. Furthermore, the performance of the other methods can be significantly improved by adjusting the size of the estimation window. © 2016 Incisive Risk Information (IP) Limited.}}, 
number = {2}, 
volume = {10}
}
@article{10.2139/ssrn.2967412, 
year = {2017}, 
title = {{Conduct risk: Distribution models with very thin tails}}, 
author = {Mitic, Peter}, 
journal = {SSRN Electronic Journal}, 
issn = {NA}, 
doi = {10.2139/ssrn.2967412}, 
abstract = {{Regulatory requires dictate that financial institutions must calculate risk capital (funds that must be retained to cover future losses) at least annually. Procedures for doing this have been well-established for many years, but recent developments in the treatment of conduct risk (the risk of loss due to the relationship between a financial institution and its customers) have cast doubt on 'standard' procedures. Regulations require that operational risk losses should be aggregated by originating event. The effect is that a large number of small and medium-sized losses are aggregated into a small number of very large losses, such that a risk capital calculation produces a hugely inflated result. To solve this problem, a novel distribution based on a probability density with an exp(-x4/(2s2)) component is proposed, where s is a parameter to be estimated. Symbolic computation is used to derive the necessary analytical expressions with which to formulate the problem, and is followed by numeric calculations in R. Goodnessof-fit and parameter estimation are both determined by using a novel method developed specifically for use with probability distribution functions. The results compare favourably with an existing model that used a LogGamma Mixture density, for which it was necessary to limit the frequency and severity of the losses. No such limits were needed using the exp(-x4/2) density. © ECCOMAS, Portugal.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10346-018-1020-2, 
year = {2018}, 
title = {{Prediction of landslide displacement with an ensemble-based extreme learning machine and copula models}}, 
author = {Li, Huajin and Xu, Qiang and He, Yusen and Deng, Jiahao}, 
journal = {Landslides}, 
issn = {1612510X}, 
doi = {10.1007/s10346-018-1020-2}, 
abstract = {{Research on the dynamics of landslide displacement forms the basis for landslide hazard prevention. This paper proposes a novel data-driven approach to monitor and predict the landslide displacement. In the first part, autoregressive moving average time series models are constructed to analyze the autocorrelation of landslide triggering factors. A linear ensemble-based extreme learning machine using the least absolute shrinkage and selection operator is applied in predicting the displacement of landslides. Five benchmarking data-driven models, the support vector machine, neural network, random forest, k-nearest neighbor, and the classical extreme learning machine, are considered as baseline models for validating the ensemble-based extreme learning machines. Numerical experiments demonstrated that the proposed prediction model produces the smallest prediction errors among all the algorithms tested. In the second part, parametric copula models are fitted on the predicted displacement, to investigate the relationship between the triggering factors and landslide displacement values. The Gumbel-Hougaard copula model performs best, which indicates strong upper tail correlation between the triggering factors and displacement values. Thresholds for the triggering factors can be obtained by monitoring the landslide moving patterns with large displacement values. The effectiveness and utility of the proposed data-driven approach have been confirmed with the landslide case study in the region of the Three Gorges Reservoir. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {2047--2059}, 
number = {10}, 
volume = {15}
}
@article{10.19030/jabr.v30i4.8675, 
year = {2014}, 
title = {{Assessing the relative performance of heavy-tailed distributions: Empirical evidence from the Johannesburg stock exchange}}, 
author = {Huang, Chun-Sung and Huang, Chun-Kai and Chinhamu, Knowledge}, 
journal = {Journal of Applied Business Research (JABR)}, 
issn = {08927626}, 
doi = {10.19030/jabr.v30i4.8675}, 
abstract = {{It has been well documented that the empirical distribution of daily logarithmic returns from financial market variables is characterized by excess kurtosis and skewness. In order to capture such properties in financial data, heavy-tailed and asymmetric distributions are required to overcome shortfalls of the widely exhausted classical normality assumption. In the context of financial forecasting and risk management, the accuracy in modeling the underlying returns distribution plays a vital role. For example, risk management tools such as value-at-risk (VaR) are highly dependent on the underlying distributional assumption, with particular focus being placed at the extreme tails. Hence, identifying a distribution that best captures all aspects of the given financial data may provide vast advantages to both investors and risk managers. In this paper, we investigate major financial indices on the Johannesburg Stock Exchange (JSE) and fit their associated returns to classes of heavy tailed distributions. The relative adequacy and goodness-of-fit of these distributions are then assessed through the robustness of their respective VaR estimates. Our results indicate that the best model selection is not only variant across the indices, but also across different VaR levels and the dissimilar tails of return series.}}, 
pages = {1263--1286}, 
number = {4}, 
volume = {30}
}
@article{10.1080/02664760902846114, 
year = {2010}, 
title = {{The exchange rate risk of Chinese yuan: Using VaR and ES based on extreme value theory}}, 
author = {Wang, Zongrun and Wu, Weitao and Chen, Chao and Zhou, Yanju}, 
journal = {Journal of Applied Statistics}, 
issn = {02664763}, 
doi = {10.1080/02664760902846114}, 
abstract = {{This paper applies extreme value theory (EVT) to estimate the tails of return series of Chinese yuan (CNY) exchange rates. We find that the degree of fitting Pareto distribution to the data of the tail of return series is extremely high. The empirical results indicate that expected shortfall cannot improve the tail risk problem of value-at-risk (VaR). The evidence of back testing indicates that EVT-based VaR values underestimate the risks of exchange rates such as USD/CNY and HKD/CNY, which may be caused by the continuous appreciation of CNY against USD and HKD. However, compared with VaR values calculated by historical simulation and variance-covariance method, VaR values calculated by EVT can measure the risk more accurately for the exchange rates of JPY/CNY and EUR/CNY. © 2010 Taylor \& Francis.}}, 
pages = {265--282}, 
number = {2}, 
volume = {37}
}
@article{10.1023/a:1010077117199, 
year = {2000}, 
title = {{Statistical methodologies for the market risk measurement}}, 
author = {Miura, Ryozo and Oue, Shingo}, 
journal = {Asia-Pacific Financial Markets}, 
issn = {13872834}, 
doi = {10.1023/a:1010077117199}, 
abstract = {{This paper classifies statistical methodologies available for the market risk measurement. With the help of the weighted likelihood, a broad class of non-normal distributions, which are not generally considered so far, are applied to possibly hetero-scedastic financial variables. The approach is compared with popular procedures such as GARCH and J. P. Morgan's using daily data of 12 financial variables. © 2001 Kluwer Academic Publishers.}}, 
pages = {305--319}, 
number = {4}, 
volume = {7}
}
@article{10.1108/jrf-05-2019-0078, 
year = {2020}, 
title = {{Market risk assessment: Evidence from packaged retail and insurance-based investment products}}, 
author = {Kokoris, Athanasios and Archontakis, Fragiskos and Grose, Christos}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-05-2019-0078}, 
abstract = {{Purpose: This study aims to examine whether the methodology proposed by the European Supervisory Authorities (ESAs) within Delegated Regulation (European Union) 2017/653 for the calculation of market risk of certain packaged retail and insurance-based investment products (PRIIPs) is the most appropriate. Design/methodology/approach: Risk models are put into effect to validate the appropriateness of the methodology announced by ESAs. ESAs have announced that the unit-linked (UL) products, labeled as Category II PRIIPs, will be subject to the Cornish–Fisher value-at-risk (CFVaR) methodology for their market risk assessment. We test CFVaR at 97.5\% confidence level on 70 UL products, and we test Cornish–Fisher expected shortfall (CFES) at the same confidence level, which acts as a counter methodology for CFVaR. Findings: The paper provides empirical insights about the Cornish-Fisher (CF) expansion being a method that incorporates the possibility of financial instability. When CFVaR by ESAs is calculated, it is shown that CF is in general a more robust risk model than the simpler historical ones. However, when CFES is applied, important points are derived. First, only in half of the occasions the CF expansion can be considered as a reliable method. Second, the CFES is a more coherent risk measure than CFVaR. We conclude that the CF expansion is unable to accurately estimate the market risk of UL products when excessive fat-tailed or non-symmetrical distributions are present. Hence, we suggest that a different methodology could also be considered by the regulatory bodies which will capture the excessive values of products in financial distress. Originality/value: Literature, both theoretical and applied, regarding PRIIPs, is not extended. Although business and regulators research has begun to intensify in the last two years, to our knowledge this is one of the first studies that uses the CFES methodology for market risk assessment of Category II PRIIPs. In addition, we use a unique data set from a country in the headwinds of the recent financial crisis. This research contributes both to the academic and business community by enriching the existing literature and aiding risk managers in assessing the market risk of certain Category II PRIIPs. Considering the recent efforts of the regulatory authorities at the beginning of 2020 to implement certain amendments to the PRIIPs, we indicate relative risks related with the calculation of the market risk of the aforementioned products. Our findings could contribute to regulatory authorities’ persistent efforts in wrapping up this ongoing project. © 2020, Emerald Publishing Limited.}}, 
pages = {111--126}, 
number = {2}, 
volume = {21}
}
@article{10.17010/ijf/2018/v12i1/120739, 
year = {2018}, 
title = {{Comparison of VaR methods: The case of Indian equities}}, 
author = {Bedi, Prateek and Shankar, Devesh and Agnihotri, Shalini and Kalra, Jappanjyot Kaur}, 
journal = {Indian Journal of Finance}, 
issn = {09738711}, 
doi = {10.17010/ijf/2018/v12i1/120739}, 
abstract = {{Different approaches to calculate VaR are based on different assumptions. This study dealt with a comparative evaluation of four Value-at-Risk models namely, historical VaR, normal VaR, GARCH (1,1) VaR, and volatility weighted historical simulation (VWHS) VaR in terms of their prediction accuracy for an active portfolio of Indian equities. Daily NAVs of 34 Indian equity growth mutual fund schemes for a period of 10 years were used to calculate 95\% VaR and backtest the results using Kupiec's POF test for all four VaR models. To identify the better performing VaR methods accurately, the analysis was performed in two phases: pre-crisis analysis and post crisis analysis. We concluded that there was a significant (insignificant) difference in performance of different VaR models if market conditions during VaR calculation and VaR backtesting periods were in contrast (congruence) to each other. The study found VWHS to be a better methodology for measuring VaR of an active portfolio of Indian equity stocks in both phases of the analysis. The results are relevant for traders \& retail and institutional investors who hold stocks of Indian companies in their portfolio and need to calculate VaR as a measure of market risk for their positions.}}, 
pages = {24--36}, 
number = {1}, 
volume = {12}
}
@article{10.4028/www.scientific.net/amr.361-363.1887, 
year = {2012}, 
title = {{A comparative study of VAR measure using different frequency data of China's fuel oil futures market}}, 
author = {Wang, Feng}, 
journal = {Advanced Materials Research}, 
issn = {10226680}, 
doi = {10.4028/www.scientific.net/amr.361-363.1887}, 
abstract = {{By using datas of Chinese fuel oil futures market, this pater establishes VAR model based on low frequency, high frequency and ultra-high frequency data, to measure the value at risk, and compares the prediction accuracy of different frequency. The research results show that the high frequency and ultra-high frequency data have better accuracy in the VAR measuring, as they contain more intraday information and can reflect the futures market microstructure better. © (2012) Trans Tech Publications, Switzerland.}}, 
pages = {1887--1891}, 
number = {NA}, 
volume = {361-363}
}
@article{10.1109/icams.2010.5553214, 
year = {2010}, 
title = {{RETRACTED ARTICLE: Risk measurement of Chinese SMEB stock market: An APARCH-SKST approach}}, 
author = {Chen, Yanxiang and Luo, Jianying}, 
journal = {2010 IEEE International Conference on Advanced Management Science(ICAMS 2010)}, 
issn = {NA}, 
doi = {10.1109/icams.2010.5553214}, 
abstract = {{This paper use the APARCH(1,1) to model the conditional volatility of Small \& Medium Enterprise Board stock market, and use normal, student t and skewed student t distribution to model the standardized residual of conditional loss of SMEB, and then compute dynamic value-at-risk, at last, we use Kupiec's LR statistic to test the accuracy of risk measurement model. Our results show that the APARCH(1,1) model with skew student t distribution exhibits outperform ability of VaR of SMEB, skew student t distribution fits to residual series well. © 2010 IEEE.}}, 
pages = {400--403}, 
number = {NA}, 
volume = {3}
}
@article{10.21314/jor.2018.379, 
year = {2018}, 
title = {{Valuing streams of risky cashflows with risk-value models}}, 
author = {Dorfleitner, Gregor and Gleißner, Werner}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2018.379}, 
abstract = {{Based on risk-value models, we introduce a multiperiod approach to the valuation of streams of risky cashflows. The valuation is based on the (expected) value of the output’s or input’s magnitude and the risk of the output cashflow, as captured by a risk measure. We derive three formulas for valuing single cashflows and utilize the principles of separate valuation and of cumulating the cashflows to derive a multiperiod valuation method. In an axiomatic way, the paper sets out the foundations for a new approach and suggests several directions for its further development. © 2018 Infopro Digital Risk (IP) Limited.}}, 
pages = {1--27}, 
number = {3}, 
volume = {20}
}
@article{10.1111/1467-9965.00141, 
year = {2002}, 
title = {{Portfolio value-at-risk with heavy-tailed risk factors}}, 
author = {Glasserman, Paul and Heidelberger, Philip and Shahabuddin, Perwez}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/1467-9965.00141}, 
abstract = {{This paper develops efficient methods for computing portfolio value-at-risk (VAR) when the underlying risk factors have a heavy-tailed distribution. In modeling heavy tails, we focus on multivariate t distributions and some extensions thereof. We develop two methods for VAR calculation that exploit a quadratic approximation to the portfolio loss, such as the delta-gamma approximation. In the first method, we derive the characteristic function of the quadratic approximation and then use numerical transform inversion to approximate the portfolio loss distribution. Because the quadratic approximation may not always yield accurate VAR estimates, we also develop a low variance Monte Carlo method. This method uses the quadratic approximation to guide the selection of an effective importance sampling distribution that samples risk factors so that large losses occur more often. Variance is further reduced by combining the importance sampling with stratified sampling. Numerical results on a variety of test portfolios indicate that large variance reductions are typically obtained. Both methods developed in this paper overcome difficulties associated with VAR calculation with heavy-tailed risk factors. The Monte Carlo method also extends to the problem of estimating the conditional excess, sometimes known as the conditional VAR.}}, 
pages = {239--269}, 
number = {3}, 
volume = {12}
}
@article{10.1007/s10440-007-9128-8, 
year = {2007}, 
title = {{Evaluating currency risk in emerging markets}}, 
author = {Novak, S. Y. and Dalla, V. and Giraitis, L.}, 
journal = {Acta Applicandae Mathematicae}, 
issn = {01678019}, 
doi = {10.1007/s10440-007-9128-8}, 
abstract = {{We present a systematic approach to the problem of evaluating currency risk. The approach involves a test for stationarity, and a method of estimating Value-at-Risk (VaR) and Expected Shortfall (ES) from dependent heavy-tailed data. Various estimation methods are compared and the accuracy of the approach is discussed. An application of the technique to the Mexican peso/US dollar exchange rate reveals the level of currency risk foreign investors face in Mexico. © Springer Science + Business Media B.V. 2007.}}, 
pages = {163--175}, 
number = {1-3}, 
volume = {97}
}
@article{10.1007/978-3-642-39165-1_3, 
year = {2013}, 
title = {{Copulas, tail dependence and applications to the analysis of financial time series}}, 
author = {Durante, Fabrizio}, 
journal = {Advances in Intelligent Systems and Computing}, 
issn = {16153871}, 
doi = {10.1007/978-3-642-39165-1\_3}, 
abstract = {{Tail dependence is an important property of a joint distribution function that has a huge impact on the determination of risky quantities associated to a stochastic model (Value-at-Risk, for instance). Here we aim at presenting some investigations about tail dependence including the following aspects: the determination of suitable stochastic models to be used in extreme scenarios; the notion of threshold copula, that helps in describing the tail of a joint distribution. Possible applications of the introduced concepts to the analysis of financial time series are presented with particular emphasis on cluster methods and determination of possible contagion effects among markets. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {17--22}, 
number = {NA}, 
volume = {228}
}
@article{10.5325/transportationj.52.4.0441, 
year = {2013}, 
title = {{A portfolio approach to allocating airline seats}}, 
author = {Leon, Steve M and Szmerekovsky, Joseph G and Tolliver, Denver D}, 
journal = {Transportation Journal}, 
issn = {00411612}, 
doi = {10.5325/transportationj.52.4.0441}, 
abstract = {{What else can airline managers do to reduce the likelihood of financial losses? The US global airline industry is characterized by highly cyclical and inconsistent operating profits, razor-thin profit margins, and unimpressive passenger yields. The objective of this research is to explore a new approach to airline seat allocation in global markets by employing a risk mitigation model, using portfolio theory to diversity an airline's route network. A portfolio of available seat miles distributed to global regions is determined using the Mean-Variance approach, followed by a second portfolio approach, the Mean-Value-at-Risk (VaR) approach. Last, a comparison is made between the two approaches in terms of actual airline operating profits. Given the financial improvements shown by the employed techniques, there is promise in pursing these methods for airline seat allocation. Copyright © 2013 The Pennsylvania State University, University Park, PA.}}, 
pages = {441}, 
number = {4}, 
volume = {52}
}
@article{10.1016/j.ijinfomgt.2018.12.013, 
year = {2020}, 
title = {{Measuring extreme risk of sustainable financial system using GJR-GARCH model trading data-based}}, 
author = {Ma, Xiaomeng and Yang, Ruixian and Zou, Dong and Liu, Rui}, 
journal = {International Journal of Information Management}, 
issn = {02684012}, 
doi = {10.1016/j.ijinfomgt.2018.12.013}, 
abstract = {{This paper investigates the role of gold as a safe haven for stock markets and the US dollar by examining the extreme risk spillovers. The extreme risk is measured by Value at Risk (VaR), which is estimated by GJR-GARCH model based on skewed t distribution. Two test statistics of one-way and two-way Granger causality in risk are used to detect extreme risk spillovers. In general, the empirical results show that there are negative extreme risk spillovers between gold and stock markets and between gold and foreign exchange markets of US dollar, which indicate that gold can act as an effective safe haven against extreme stock and US dollar exchange rate movements. In addition, the global financial crisis can affect the safe haven role of gold. © 2019 Elsevier Ltd}}, 
pages = {526--537}, 
number = {NA}, 
volume = {50}
}
@article{10.3390/risks2010025, 
year = {2014}, 
title = {{An academic response to Basel 3.5}}, 
author = {Embrechts, Paul and Puccetti, Giovanni and Rüschendorf, Ludger and Wang, Ruodu and Beleraj, Antonela}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks2010025}, 
abstract = {{Recent crises in the financial industry have shown weaknesses in the modeling of Risk-Weighted Assets (RWAs). Relatively minor model changes may lead to substantial changes in the RWA numbers. Similar problems are encountered in the Value-at-Risk (VaR)-aggregation of risks. In this article, we highlight some of the underlying issues, both methodologically, as well as through examples. In particular, we frame this discussion in the context of two recent regulatory documents we refer to as Basel 3.5. © 2014 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {25--48}, 
number = {1}, 
volume = {2}
}
@article{10.1007/978-3-642-35494-6_18, 
year = {2013}, 
title = {{Correlation, tail dependence and diversification}}, 
author = {Pfeifer, Dietmar}, 
issn = {NA}, 
doi = {10.1007/978-3-642-35494-6\_18}, 
abstract = {{In this contribution we analyze the interplay between correlation, tail dependence and diversification between risks which has a great impact on the calculation of the Solvency Capital Requirement under the Solvency II directive. The analysis shows that the prevailing assumption of an economic relationship between diversification and correlation or stochastic dependence is misleading under the risk measure VaR (Value at Risk) used under Solvency II and can lead to an underestimation of the necessary economic capital. © Springer-Verlag Berlin Heidelberg 2013.}}, 
pages = {301--314}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/cisp.2010.5646828, 
year = {2010}, 
title = {{Application of VaR based on stable distribution}}, 
author = {Wang, Yuling and Ma, Junhai and Xu, Yuhua and Wang, Jing}, 
journal = {2010 3rd International Congress on Image and Signal Processing}, 
issn = {NA}, 
doi = {10.1109/cisp.2010.5646828}, 
abstract = {{Value at risk is widely applied to estimated market risks .But it lacks a convincing technique capturing the observed phenomena in financial data such as heavy-tails, time -varying and short and long-range dependence. The stable distribution can deal with these problems well. The demonstrative research shows that VaR based on stable distribution can solve these problems better than the traditional normal distribution. ©2010 IEEE.}}, 
pages = {4193--4196}, 
number = {NA}, 
volume = {9}
}
@article{10.1109/ieem.2013.6962686, 
year = {2014}, 
title = {{A practical supply chain risk management approach using VaR}}, 
author = {Lim, Jasmine J. and Zhang, Allan N. and Tan, P. S.}, 
journal = {2013 IEEE International Conference on Industrial Engineering and Engineering Management}, 
issn = {21573611}, 
doi = {10.1109/ieem.2013.6962686}, 
abstract = {{In these recent years, the complexity of supply chain has been increasing with globalisation and as such, disruptions are happening more frequently with the impacts getting more severe. Thus, there is a growing need to deal with such risks arising from disruptions to the supply chain. The objective of this paper is to present a practical method for supply chain risk management using Value at Risk (VaR), a technique to identify, assess and mitigate these disruption risks. The method aims to tackle possible risks through the application of VaR and provides deterrent solutions. Through this method, firms will be able to conduct 'what-if' analysis to manage potential risks and thus minimizing the impact of the risks to their supply chains during the sudden occurrence of disruptions. © 2013 IEEE.}}, 
pages = {1631--1635}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10479-015-1822-8, 
year = {2016}, 
title = {{Robust scenario-based value-at-risk optimization}}, 
author = {Romanko, Oleksandr and Mausser, Helmut}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-015-1822-8}, 
abstract = {{This paper develops and tests a heuristic algorithm for scenario-based value-at-risk (VaR) optimization. Due to the high computational complexity of VaR optimization, conditional value-at-risk-based proxies are utilized for VaR objectives. It is shown that our heuristic algorithm obtains robust results with low computational complexity. © 2015, IBM Corp.}}, 
pages = {203--218}, 
number = {1-2}, 
volume = {237}
}
@article{10.1088/1742-6596/1320/1/012004, 
year = {2019}, 
title = {{Value at risk in the black litterman portfolio with stock selection through cluster analysis}}, 
author = {Subekti, R and Sari, E Ratna and Kusumawati, R and Pintari, H O and Renggani, P}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/1320/1/012004}, 
abstract = {{The aim of this paper is to support the new strategy in the previous research where Black Litterman procedure is assisted by clustering for the process of given views. We provide the empirical result and focus on how to compute the Value at Risk (VaR) to illustrate the risk measure on Black Litterman as a reference portfolio. © 2019 IOP Publishing Ltd. All rights reserved.}}, 
pages = {012004}, 
number = {1}, 
volume = {1320}
}
@article{10.3390/su12197911, 
year = {2020}, 
title = {{Modeling the risk of extreme value dependence in Chinese regional carbon emission markets}}, 
author = {Qiu, Hong and Hu, Genhua and Yang, Yuhong and Zhang, Jeffrey and Zhang, Ting}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su12197911}, 
abstract = {{In this study, we analyze the risk of extreme value dependence in Chinese regional carbon emission markets. After filtering the daily return data of six carbon markets in China using a generalized autoregressive conditional heteroscedasticity (GARCH) model, we obtain the standardized residual series. Next, the dependence structures in the markets are captured by the Copula function and the Extreme Value theory (EVT). We report high peaks, heavy tails and fluctuation aggregation in the logarithm return series of the markets, as well as significant dependent structures. There are significant extreme value risks in Chinese regional carbon markets, but the risks can be mitigated through appropriate portfolio diversification. © 2020 by the authors.}}, 
pages = {7911}, 
number = {19}, 
volume = {12}
}
@article{10.1109/iceee.2010.5660186, 
year = {2010}, 
title = {{MCMC algorithm and simulation of a class of jump VaR estimation}}, 
author = {Wang, Jingyong and Xue, Lida}, 
journal = {2010 International Conference on E-Product E-Service and E-Entertainment}, 
issn = {NA}, 
doi = {10.1109/iceee.2010.5660186}, 
abstract = {{This paper develops a class of jump stochastic volatility threshold model of VaR Estimation from a Bayesian viewpoint. Bayesian inferences of the unknown parameters are obtained with respect to a subjective prior distribution via Markov chain Monte Carlo(MCMC) method, MCMC algorithm and the value at risk(VaR) predictive are also developed. Based on simulation, if the jump is not Considered, the value at risk is overestimated. The precision of value at risk estimation is increased. ©2010 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1504/ijmef.2019.102956, 
year = {2019}, 
title = {{Modelling and forecasting long memory time series with exponential and switching GARCH models}}, 
author = {Amiri, Esmail}, 
journal = {International Journal of Monetary Economics and Finance}, 
issn = {17520479}, 
doi = {10.1504/ijmef.2019.102956}, 
abstract = {{There is some evidence that structural change or stochastic regime switching and long memory are intimately related concepts. However, long memory and regime switching in a stochastic process are properties that could be easily confused in a financial study. Using a modelling approach, the aim is to distinguish regime switching behaviour from long memory for the financial time series. In an empirical study the forecasting performance of symmetric, asymmetric, long memory and Markov switching GARCH model are compared using Tehran stock market daily returns. The results indicate that in out of sample performance, long memory exponential GARCH (FIEGARCH) model outperforms the competing models. To ensure the validity of the results, the value at risk (VaR) forecasts are obtained for each model and a loss function is calculated. A simple rule for distinguishing between long memory and structural break in financial and economic time series is suggested. © 2019 Inderscience Enterprises Ltd.}}, 
pages = {407--425}, 
number = {5}, 
volume = {12}
}
@article{10.1002/sam.10114, 
year = {2011}, 
title = {{Smoothing Methods for Histogram-Valued Time Series: An Application to Value-at-Risk}}, 
author = {Arroyo, Javier and González‐Rivera, Gloria and Maté, Carlos and Roque, Antonio Muñoz San}, 
journal = {Statistical Analysis and Data Mining}, 
issn = {19321872}, 
doi = {10.1002/sam.10114}, 
abstract = {{We adapt smoothing methods to histogram-valued time series (HTS) by introducing a barycentric histogram that emulates the "average" operation, which is the key to any smoothing filter. We show that, due to its linear properties, only the Mallows-barycenter is acceptable if we wish to preserve the essence of any smoothing mechanism. We implement a barycentric exponential smoothing to forecast the HTS of daily histograms of intradaily returns to both the SP500 and the IBEX 35 indexes. We construct a one-step-ahead histogram forecast, from which we retrieve a desired γ-value-at-risk (VaR) forecast. In the case of the SP500 index, a barycentric exponential smoothing delivers a better forecast, in the MSE sense, than those derived from vector autoregression models, especially for the 5\% VaR. In the case of IBEX35, the forecasts from both methods are equally good. Copyright © 2011 Wiley Periodicals, Inc., A Wiley Company.}}, 
pages = {216--228}, 
number = {2}, 
volume = {4}
}
@article{10.1016/j.najef.2012.06.005, 
year = {2013}, 
title = {{Forecasting volatility via stock return, range, trading volume and spillover effects: The case of Brazil}}, 
author = {Asai, Manabu and Brugal, Ivan}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2012.06.005}, 
abstract = {{For the purpose of developing alternative approach for forecasting volatility, we consider heterogeneous VAR (HVAR) model which accommodates the market effects of different horizons, namely, daily, weekly and monthly effects, and examine the interdependence of stock markets in Brazil and the US, based on information of daily return, range and trading volume. To compare with the new approach, we also work with the univariate and multivariate GARCH models with asymmetric effects, trading volumes and fat-tails. The heteroskedasticity-corrected Granger causality tests based on the HVAR show the strong evidence of such spillover effects. We assess the value-at-risk thresholds for Brazil, based on the out-of-sample forecasts of the HVAR model, finding the new approach works satisfactory for the periods including the global financial crisis, without assuming heavy-tailed conditional distributions. © 2012 Elsevier Inc.}}, 
pages = {202--213}, 
number = {NA}, 
volume = {25}
}
@article{10.1007/978-3-030-04263-9_25, 
year = {2018}, 
title = {{Value at risk of set returns based on Bayesian markov-switching garch approach}}, 
author = {Boonyakunakorn, Petchaluck and Pastpipatkul, Pathairat and Sriboonchitta, Songsak}, 
journal = {Studies in Computational Intelligence}, 
issn = {1860949X}, 
doi = {10.1007/978-3-030-04263-9\_25}, 
abstract = {{This study aims to investigate the forecasting ability of volatility of Set return based on Bayesian Markov-Switching GARCH models in VaR estimation. We examine whether the Bayesian MSGARCH models with two-regime improve the forecasting volatility of VaR model by comparing with their single-regime counterpart. The empirical results show that Bayesian two-regime MS-GJR-GARCH model with a GED distribution is the best fit to the data based on DIC. The model confirms that the two regimes are different in both unconditional volatility levels and the persistence of the volatility process. The backtesting VaR at 5\% risk level results also confirms that the Bayesian two-regime MSGARCH model outperforms their single-regime counterpart. Therefore, this study provides an empirical evidence supporting that Bayesian two-regime MSGARCH model appears to improve forecasting SET volatility. © Springer Nature Switzerland AG 2019.}}, 
pages = {329--341}, 
number = {NA}, 
volume = {808}
}
@article{10.21314/j0r.2016.326, 
year = {2016}, 
title = {{Model uncertainty in risk capital measurement}}, 
author = {Bignozzi, Valeria and Tsanakas, Andreas}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/j0r.2016.326}, 
abstract = {{The required solvency capital for a financial portfolio is typically given by a tail risk measure such as value-at-risk. Estimating the value of that risk measure from a limited, often small, sample of data gives rise to potential errors in the selection of the statistical model and the estimation of its parameters. We propose to quantify the effectiveness of a capital estimation procedure via the notions of residual estimation risk and estimated capital risk. It is shown that, for capital estimation procedures that do not require the specification of a model (eg, historical simulation) or for worst-case scenario procedures, the impact of model uncertainty is substantial, while capital estimation procedures that allow for multiple candidate models using Bayesian methods partially eliminate model error. In the same setting, we propose a way of quantifying model error that allows us to disentangle the impact of model uncertainty from that of parameter uncertainty. We illustrate these ideas by simulation examples considering standard loss and return distributions used in banking and insurance. © 2016 Incisive Risk Information (IP) Limited.}}, 
pages = {1--24}, 
number = {3}, 
volume = {18}
}
@article{10.1515/jbnst-2014-0502, 
year = {2014}, 
title = {{Liquidity and the value at risk}}, 
author = {Großmaß, Lidan}, 
journal = {Jahrbücher für Nationalökonomie und Statistik}, 
issn = {00214027}, 
doi = {10.1515/jbnst-2014-0502}, 
abstract = {{We introduce an intuitive method of enhancing low-frequency volatility measures used to compute Value-at-Risk (VaR) by incorporating intradaily liquidity information from the limit order book. Using the quote slope of Hasbrouck and Seppi (2001), a compound liquidity measure comprising the dimensions of bid-ask spread and log depths, as a proxy for latent liquidity, we assign states of liquidity that the asset instantaneously resides in to allow only extremal liquidity shocks to influence volatility. To forecast the liquidity states, we use the autoregressive conditional multinomial model of Liesenfeld et al. (2006). We test the method on a number of stocks and find that (1) for stocks in financial and technological sectors, only the extremal shocks to liquidity affect volatility significantly and such a liquidity-state adjusted volatility is likely to improve VaR forecasts; (2) the volatility of stock returns in most other sectors are less affected by extremal shocks to liquidity but the continuous liquidity proxy is able to explain some of the dynamics of volatility and (3) the inclusion of liquidity in VaR becomes increasingly important as the quantile under consideration becomes more extreme. © 2014, Walter de Gruyter GmbH. All rights reserved.}}, 
pages = {572--602}, 
number = {5}, 
volume = {234}
}
@article{10.1137/17m1126138, 
year = {2018}, 
title = {{Worst-case range value-at-risk with partial information}}, 
author = {Li, Lujun and Shao, Hui and Wang, Ruodu and Yang, Jingping}, 
journal = {SIAM Journal on Financial Mathematics}, 
issn = {1945497X}, 
doi = {10.1137/17m1126138}, 
abstract = {{In this paper, we study the worst-case scenarios of a general class of risk measures, the range value-at-risk (RVaR), in single and aggregate risk models with given mean and variance, as well as symmetry and/or unimodality of each risk. For different types of partial information settings, sharp bounds for RVaR are obtained for single and aggregate risk models, together with the corresponding worst-case scenarios of marginal risks and the corresponding copula functions (dependence structure) among them. Different from the existing literature, the sharp bounds under different partial information settings in this paper are obtained via a unified method combining convex order and the recently developed notion of joint mixability. As particular cases, bounds for value-at-risk and tail value-at-risk are derived directly. Numerical examples are also provided to illustrate our results. © 2018 Society for Industrial and Applied Mathematics.}}, 
pages = {190--218}, 
number = {1}, 
volume = {9}
}
@article{10.2514/6.2019-0392, 
year = {2019}, 
title = {{A dynamic risk form of entropic value at risk}}, 
author = {Axelrod, Allan and Chowdhary, Girish}, 
journal = {AIAA Scitech 2019 Forum}, 
issn = {NA}, 
doi = {10.2514/6.2019-0392}, 
abstract = {{In this work, we show that the entropic value at risk (EVaR), which is not a dynamic risk measure in general, can be a finitely-valued dynamic risk measure for at least one scenario. Specifically, for any distribution where the moment generating function (mgf) exists, then there exists a confidence parameter such that EVaR can be a finitely-valued dynamic risk measure. This separates EVaR from other risk measures, such as the value at risk (VaR), which may not be a finitely-valued dynamic risk for any allowable constant confidence parameter value. This result is not unique to EVaR, as we also show that there exists a confidence parameter such that the average value at risk (AVaR)–which does not require the mgf to exist–is also a finite dynamic risk measure. The main focus of this work is EVaR as it has a stronger stochastic ordering guarantee than AVaR. Future work will examine whether EVaR can be a dynamic risk measure for any other cases. © 2019, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.compfluid.2013.10.021, 
year = {2014}, 
title = {{Extreme scenarios for the evolution of a soft bed interacting with a fluid using the Value at Risk of the bed characteristics}}, 
author = {Mohammadi, Bijan and Bouchette, Frederic}, 
journal = {Computers \& Fluids}, 
issn = {00457930}, 
doi = {10.1016/j.compfluid.2013.10.021}, 
abstract = {{We show how to introduce the Value at Risk (VaR) concept in the analysis of the adaptation of the shape of a soft bed to a flow knowing the Probability Density Function (PDF) of the responses of the shape to flow perturbations (bed receptivity). Our aim is to quantify our confidence on simulation scenarios by an available morphodynamic model for the shape. The approach permits to perform this task at low complexity as it does not require any sampling of the bed receptivity parameter space. The paper goes beyond stationarity for the variability of the bed receptivity by linking its dynamics to the bottom morphodynamics through an original transport equation for the local bed receptivity standard deviation. The approach has been applied to the analysis of bed morphodynamics based on minimization principles. The results show the importance of including uncertainty information during the coupling and not only eventually through simple margins on the results. © 2013 Elsevier Ltd.}}, 
pages = {78--87}, 
number = {NA}, 
volume = {89}
}
@article{10.2495/miit131842, 
year = {2014}, 
title = {{Research on the evaluation and trade-off models of the value and risk of companies’ patent}}, 
author = {Du, Z and Zhou, Zixiong and Xie, Kefan}, 
journal = {Management Innovation and Information Technology}, 
issn = {17433517}, 
doi = {10.2495/miit131842}, 
abstract = {{This paper conducts a research on decision making about patents development of a company. Patents have similar financial features to call option, which makes it possible to apply option methods to patent value and risk evaluation. By employing the Black-Scholes model and Value at Risk model, this paper takes the earnings of patent investment and patent risks loss into compositive consideration, and aims to make optimal patent decisions of patent reward and risk status. The paper provides analytical support for patent decision and appraisal and carries out a typical case analysis. © 2014 WIT Press.}}, 
pages = {1461--1468}, 
number = {NA}, 
volume = {61}
}
@article{10.1109/tpwrs.2009.2032233, 
year = {2010}, 
title = {{Optimization of electricity retailer's contract portfolio subject to risk preferences}}, 
author = {Kettunen, J. and Salo, A. and Bunn, D.W.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2009.2032233}, 
abstract = {{When an electricity retailer faces volume risk in meeting load and spot price risk in purchasing from the wholesale market, conventional risk management optimization methods can be quite inefficient. For the management of an electricity contract portfolio in this context, we develop a multistage stochastic optimization approach which accounts for the uncertainties of both electricity prices and loads, and which permits the specification of conditional-value-at-risk requirements to optimize hedging across intermediate stages in the planning horizons. Our experimental results, based on real data from Nordpool, suggest that the modeling of price and load correlations is particularly important. The sensitivity analysis is extended to characterize the behavior of retailers with different risk attitudes. Thus, we observe that a risk neutral retailer is more susceptible to price-related than load-related uncertainties in terms of the expected cost of satisfying the load, and that a risk averse retailer is especially sensitive to the drivers of the forward risk premium. © 2009 IEEE.}}, 
pages = {117--128}, 
number = {1}, 
volume = {25}
}
@article{10.1504/aajfa.2017.084222, 
year = {2017}, 
title = {{Robust value-at-risk forecasting of Karachi Stock Exchange}}, 
author = {Iqbal, Farhat}, 
journal = {Afro-Asian J. of Finance and Accounting}, 
issn = {17516447}, 
doi = {10.1504/aajfa.2017.084222}, 
abstract = {{A class of robust M-estimators for generalised autoregressive conditional heteroscedastic (GARCH) type models are used for the prediction of value-at-risk (VaR) of Karachi Stock Exchange (KSE). To better understand the impact of global financial crisis on KSE, the daily stock return data is divided into three sub-periods: the pre-crisis period (3 January 2005 to 31 December 2007), the crisis period (2 January 2008 to 30 June 2009), and the post-crisis period (1 July 2009 to 31 December 2013). Symmetric and asymmetric GARCH models that capture the most common stylised facts about index returns such as volatility clustering and leverage effect are fitted to these time periods and in-sample and out-of-sample estimates of VaR are obtained. Our results show that M-estimators provide accurate and reliable estimates of VaR in low and high volatile time. Our findings also show that the asymmetric model provides better fit than the symmetric model for the KSE. © 2017 Inderscience Enterprises Ltd.}}, 
pages = {130}, 
number = {2}, 
volume = {7}
}
@article{10.1016/j.insmatheco.2011.05.002, 
year = {2011}, 
title = {{Asymptotics for risk capital allocations based on Conditional Tail Expectation}}, 
author = {Asimit, Alexandru V. and Furman, Edward and Tang, Qihe and Vernic, Raluca}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2011.05.002}, 
abstract = {{An investigation of the limiting behavior of a risk capital allocation rule based on the Conditional Tail Expectation (CTE) risk measure is carried out. More specifically, with the help of general notions of Extreme Value Theory (EVT), the aforementioned risk capital allocation is shown to be asymptotically proportional to the corresponding Value-at-Risk (VaR) risk measure. The existing methodology acquired for VaR can therefore be applied to a somewhat less well-studied CTE. In the context of interest, the EVT approach is seemingly well-motivated by modern regulations, which openly strive for the excessive prudence in determining risk capitals. © 2011 Elsevier B.V.}}, 
pages = {310--324}, 
number = {3}, 
volume = {49}
}
@article{10.1016/s0378-4266(02)00283-2, 
year = {2002}, 
title = {{On the coherence of expected shortfall}}, 
author = {Acerbi, Carlo and Tasche, Dirk}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(02)00283-2}, 
abstract = {{Expected shortfall (ES) in several variants has been proposed as remedy for the deficiencies of value-at-risk (VaR) which in general is not a coherent risk measure. In fact, most definitions of ES lead to the same results when applied to continuous loss distributions. Differences may appear when the underlying loss distributions have discontinuities. In this case even the coherence property of ES can get lost unless one took care of the details in its definition. We compare some of the definitions of ES, pointing out that there is one which is robust in the sense of yielding a coherent risk measure regardless of the underlying distributions. Moreover, this ES can be estimated effectively even in cases where the usual estimators for VaR fail. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {1487--1503}, 
number = {7}, 
volume = {26}
}
@article{10.1016/j.spl.2005.02.001, 
year = {2005}, 
title = {{VaR is subject to a significant positive bias}}, 
author = {Inui, Koji and Kijima, Masaaki and Kitano, Atsushi}, 
journal = {Statistics \& Probability Letters}, 
issn = {01677152}, 
doi = {10.1016/j.spl.2005.02.001}, 
abstract = {{This article shows that value-at-risk (VaR), the most popular risk measure in financial practice, has a considerable positive bias when used for a portfolio with fat-tail distribution. The bias increases with higher confidence level, heavier tails, and smaller sample size. Also, the Harrell-Davis quantile estimator and its simulation counterpart, called the bootstrap estimator, tend to have a more significant positive bias for fat-tail distributions. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {299--311}, 
number = {4}, 
volume = {72}
}
@article{10.1109/wicom.2008.1392, 
year = {2008}, 
title = {{A Markov chain to estimate stochastic risk of enterprise structure for value innovation of China construction enterprises}}, 
author = {Zhang, Jing-Xiao and Li, Hui and Jin, Wei-Xing}, 
journal = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing}, 
issn = {NA}, 
doi = {10.1109/wicom.2008.1392}, 
abstract = {{One of the main problems in the enterprise structure is the actual disorder of the three hierarchical schemes for the enterprise management China construction industry, which brings the stochastic risk of enterprise structure for the value innovation of China construction corporations. The principal reason is that corporation are being involved in a low-interest and severe circle competition, due to the quality management which the authority wants to use to restructure the construction industry. However, there is not the relatively recent definition of this type of risk, especially for the value innovation in China construction industry. Considering this drawback, the employment of Markov methods and simulation tools could be a natural solution to the problem. The use of Markov methods allows us to estimate the parameters about mutual transition for the value innovation of construction enterprises, and calculate the stochastic risk for the total construction enterprises in the freedom market. Besides, an original proposal is a Markov approach for modeling the stochastic risk in the China construction enterprise structure and for the computation of the type of risk to cover the three hierarchical schemes. Besides this methodological innovation a positive research, based on Markov chain, is required. In particular, the study reveals that the running of current Construction Enterprise structure is a real block for the evolution of the value innovation of China Construction Enterprises according to the stimulation value in the different hierarchical choices for the construction enterprise distributions. © 2008 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jmva.2014.04.023, 
year = {2014}, 
title = {{A note on the computation of sharp numerical bounds for the distribution of the sum, product or ratio of dependent risks}}, 
author = {Cossette, Hélène and Côté, Marie-Pier and Mailhot, Mélina and Marceau, Etienne}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2014.04.023}, 
abstract = {{In this paper, an approximation method for computing numerically the cumulative distribution function of the sum of d random variables is developed. The method leads to numerical bounds for the distribution of the sum of dependent risks. The bounds are fast to compute and converge to the exact value if the joint probability density function exists. They also allow to evaluate sharp numerical bounds on the Value-at-Risk measure. Moreover, the fact that the approximation is deterministic, hence without uncertainty on the resulting values, is an advantage over MC simulation techniques. Applications in actuarial science and finance illustrate the accuracy of the procedure. We also present analogous bounds for the distribution of the product or the ratio of two random variables, which can be useful for actuarial or financial applications. © 2014 Elsevier Inc.}}, 
pages = {1--20}, 
number = {NA}, 
volume = {130}
}
@article{10.15728/bbr.2019.16.6.6, 
year = {2019}, 
title = {{Comparison of VAR models to the Brazilian stock market under the hypothesis of serial independence in higher orders: Are GARCH models really indispensable?}}, 
author = {Maluf, Luiz and Asano, Jéssica}, 
journal = {Brazilian Business Review}, 
issn = {18082386}, 
doi = {10.15728/bbr.2019.16.6.6}, 
abstract = {{Our objective in this article was to verify which models for the Value at Risk (VaR), among those that do not consider conditional volatility (Extreme Values Theory and the traditional Historical Simulation), and those that do consider it (GARCH and IGARCH), are adequate for the main index of the Brazilian stock market, the IBOVESPA. For this purpose, backtesting of adherence and the independence of first and higher orders were implemented for the four models mentioned, over forecast horizons of 1 and 10 days. The contribution is based on a the more rigorous criteria than those used in the literature for validating VaR models, as we performed backtesting for violation independence of higher orders on forecast horizons of 10 days. The results show that only GARCH family models were adequate. Thus, it is recommended to entities of the National Financial System that keep relevant positions in the Brazilian stock market, the utilization of internal risk models based on conditional volatility, in order to minimize the occurrence of violation clusters. © 2019 FUCAPE Business School. All rights reserved.}}, 
pages = {626--645}, 
number = {6}, 
volume = {16}
}
@article{10.1016/s1058-3300(03)00009-0, 
year = {2003}, 
title = {{The Basel Committee proposals for a new capital accord: Implications for Italian banks}}, 
author = {Sironi, Andrea and Zazzara, Cristiano}, 
journal = {Review of Financial Economics}, 
issn = {10583300}, 
doi = {10.1016/s1058-3300(03)00009-0}, 
abstract = {{Following a few general considerations on the recently proposed revision of the Basel Agreement on capital adequacy, this paper focuses on the first pillar of the Basel Committee proposals, the handling of capital requirements for credit risk in the banking book. The Basel Committee envisages an approach alternatively based on external ratings or on internal rating systems for the determination of the minimum capital requirement related to bank loan portfolios. This approach supports a system of capital requirements that is more sensitive to credit risk. On the basis of specific assumptions, these requirements provide a measure of the value at risk (VaR) produced by models used by major international banks. We first address the impact of the standardised and (internal ratings-based) IRB foundation approach using general data on Italian banks loans' portfolios default rates. We then simulate the impact of the proposed new rules on the corporate loan portfolios of Italian banks, using the unique data set of mortality rates recently published by the Bank of Italy. Three main conclusions emerge from the analysis: (i) the standardised approach implicitly penalizes Italian banks in their interbank funding as their rating is generally below AA/Aa, (ii) the average default rate experienced by Italian banks is higher than the one implied in the benchmark risk weight (BRW) proposed by the Basel Committee for the IRB foundation approach, thereby potentially leading to an increase in the regulatory risk weights, and (iii) the risk-weight is based on an average asset correlation that is significantly higher than the one historically recorded within the Italian banks' corporate borrowers. These findings support the need for a significant revision of the basic inputs and assumptions of the Basel proposals. Finally, in relation to the conditions that allow the capital market to effectively discipline banks, we comment on the proposals advanced in relation to the third pillar of the new capital adequacy scheme. © 2003 Elsevier Science Inc. All rights reserved.}}, 
pages = {99--126}, 
number = {1}, 
volume = {12}
}
@article{10.1016/s0047-259x(03)00100-3, 
year = {2004}, 
title = {{Asymptotic behavior of tails and quantiles of quadratic forms of Gaussian vectors}}, 
author = {Jaschke, Stefan and Klüppelberg, Claudia and Lindner, Alexander}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/s0047-259x(03)00100-3}, 
abstract = {{We derive results on the asymptotic behavior of tails and quantiles of quadratic forms of Gaussian vectors. They appear in particular in delta-gamma models in financial risk management approximating portfolio returns. Quantile estimation corresponds to the estimation of the Value-at-Risk, which is a serious problem in high dimension. © 2003 Elsevier Inc. All rights reserved.}}, 
pages = {252--273}, 
number = {2}, 
volume = {88}
}
@article{10.1016/j.renene.2021.03.065, 
year = {2021}, 
title = {{Usage of the net present value-at-risk to design ground-coupled heat pump systems under uncertain scenarios}}, 
author = {Dusseault, Bernard and Pasquier, Philippe}, 
journal = {Renewable Energy}, 
issn = {09601481}, 
doi = {10.1016/j.renene.2021.03.065}, 
abstract = {{Hybrid ground-coupled heat pump systems can efficiently heat and cool buildings by exchanging heat with geological materials. Their sizing is, however, complex and their financial profitability is hard to establish during the design phase due to uncertainties that taint important design parameters. Historically, impacts of uncertainties are assessed after sizing completion using sensitivity analyses. Unfortunately, these analyses cannot mitigate design risks, should the consequences of uncertainties be significant. Here, we show how the net present value-at-risk, a stochastic financial indicator inspired by a metric used in the financial sector, can weave the impacts of uncertainties throughout the design phase. The net present value-at-risk is compared to the net present value in a case study that considers uncertainty of construction costs, building's heat load, energy tariffs and ground thermal conductivity. Results show that this financial indicator, although it comes at a higher computational price than traditional net present value indicator, leads to shorter payback periods, greatly reduces the risks of unforeseen financial losses and does not require further sensitivity analysis. By describing uncertain parameters with statistical distributions during sizing, the proposed designs are more conservative, still efficient and financially viable while being shielded from worst case scenarios. © 2021 Elsevier Ltd}}, 
pages = {953--971}, 
number = {NA}, 
volume = {173}
}
@article{10.1016/s0927-5398(02)00070-1, 
year = {2003}, 
title = {{Disturbing extremal behavior of spot rate dynamics}}, 
author = {Bali, Turan G. and Neftci, Salih N.}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(02)00070-1}, 
abstract = {{This paper presents a study of extreme interest rate movements in the US federal funds market over almost a half century of daily observations from the mid 1950s through the end of 2000. We analyze the fluctuations of the maximal and minimal changes in short-term interest rates and test the significance of time-varying paths followed by the mean and volatility of extremes. We formally determine the relevance of introducing trend and serial correlation in the mean and of incorporating the level and GARCH effects in the volatility of extreme changes in the federal funds rate. The empirical findings indicate the existence of volatility clustering in the standard deviation of extremes and a significantly positive relationship between the level and the volatility of extremes. The results point to the presence of an autoregressive process in the means of both local maxima and local minima values. The paper proposes a conditional extreme value approach to calculating value at risk (VaR) by specifying the location and scale parameters of the generalized Pareto distribution (GPD) as a function of past information. Based on the estimated VaR thresholds, the statistical theory of extremes is found to provide more accurate estimates of the rate of occurrence and the size of extreme observations. © 2003 Elsevier Science B.V. All rights reserved.}}, 
pages = {455--477}, 
number = {4}, 
volume = {10}
}
@article{10.1198/jbes.2009.07063, 
year = {2010}, 
title = {{Backtesting parametric value-at-risk with estimation risk}}, 
author = {Escanciano, J Carlos and Olmo, Jose}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1198/jbes.2009.07063}, 
abstract = {{One of the implications of the creation of the Basel Committee on Banking Supervision was the implementation of Value-at-Risk (VaR) as the standard tool for measuring market risk. Since then, the capital requirements of commercial banks with trading activities are based on VaR estimates. Therefore, appropriately constructed tests for assessing the out-of-sample forecast accuracy of the VaR model (backtesting procedures) have become of crucial practical importance. In this article we show that the use of the standard unconditional and independence backtesting procedures to assess VaR models in out-of-sample composite environments can be misleading. These tests do not consider the impact of estimation risk, and therefore, may use wrong critical values to assess market risk. The purpose of this article is to quantify such estimation risk in a very general class of dynamic parametric VaR models and to correct standard backtesting procedures to provide valid inference in out-of-sample analyses. A Monte Carlo study illustrates our theoretical findings in finite-samples and shows that our corrected unconditional test can provide more accurately sized and more powerful tests than the uncorrected one. Finally, an application to the S\&P 500 Index shows the importance of this correction and its impact on capital requirements as imposed by the Basel Accord. © 2010 American Statistical Association.}}, 
pages = {36--51}, 
number = {1}, 
volume = {28}
}
@article{10.1109/hicss.2004.1265497, 
year = {2004}, 
title = {{Innovator or owner? information sharing, incomplete contracts and governance in financial risk management systems}}, 
author = {Han, Kunsoo and Kauffman, Robert J. and Nault, Barrie R.}, 
journal = {37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings}, 
issn = {10603425}, 
doi = {10.1109/hicss.2004.1265497}, 
abstract = {{Financial risk management has become increasingly important in the financial industry during the last decade, especially due to the financial disasters in the mid-1990s. Value-at-risk (VAR), developed by J. P. Morgan in the late 1980s, has been the most widely accepted risk measurement methodology because it provides a single number summary of an institution's portfolio risk. To promote VAR, Morgan provided RiskMetrics, a VAR-based risk management service, for its clients (borrowers). Later, RiskMetrics was spun off as an independent company providing fee-based services. Building upon the theory of incomplete contracts and other related theories, we develop a game theoretic model to explain why adoption of the free RiskMetrics service stalled and how the change in ownership structure of the service led to wider adoption. In particular, we examine a situation where the borrowers have heterogeneous portfolio riskiness and this differential riskiness affects the value they can gain from the service and the impact of the service provider exploiting their private risk information. Our results suggest that the most important roadblock to borrowers' adoption of the free service might have been the potential damage from the service provider's information exploitation. When the service was spun off, borrowers' concern was reduced due to the multi-party independent ownership structure, which led to wider adoption. The spin-off was Morgan's strategic move to maximize long-term profits from RiskMetrics.}}, 
pages = {1--10}, 
number = {NA}, 
volume = {37}
}
@article{10.1007/978-94-007-4839-2_1, 
year = {2012}, 
title = {{The value of information in index insurance for farmers in Africa}}, 
author = {Osgood, Daniel and Shirley, Kenneth E.}, 
issn = {NA}, 
doi = {10.1007/978-94-007-4839-2\_1}, 
abstract = {{Index insurance is a relatively new approach for providing climate risk protection to low-income farmers in developing countries. Because this insurance is implemented in data-poor environments, information constraints and uncertainty substantially affect the products. Since insurance is a tool that can be used to exchange uncertainty in the market, the level of information available directly alters prices, with insurance protection for climate risk and insurance protection for information uncertainties about climate risks both being components of the final price. Using data, methodologies, and contracts for index insurance applications in Africa, the chapter presents this concrete component of the value of information by quantifying the value of improved data in lowering insurance prices. It provides a brief overview of index insurance in developing countries and discusses the value of remote sensing in informing the index and the role of climate trends. © Springer Science+Business Media Dordrecht 2012.}}, 
pages = {1--18}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/for.2627, 
year = {2020}, 
title = {{On the use of power transformations in CAViaR models}}, 
author = {Tsiotas, Georgios}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2627}, 
abstract = {{Value at risk (VaR) is a risk measure widely used by financial institutions in allocating risk. VaR forecast estimation involves the conditional evaluation of quantiles based on the currently available information. Recent advances in VaR evaluation incorporate a proxy for conditional variance, yielding the conditional autoregressive VaR (CAViaR) models. However, early work in finance literature has shown that the introduction of power transformations has resulted in improvements in volatility forecasting. Having a direct association between volatility and conditional VaR, we adopt power-transformed CAViaR models. We investigate whether the flexible conditional VaR dynamics associated with power-transformed CAViaR models can result in better forecasting results than those assumed by the nontransformed CAViaR models. Estimation in CAViaR models is based on an early-rejection Markov chain Monte Carlo algorithm. We illustrate our forecasting evaluation results using simulated and financial daily return data series. The results demonstrate that there is strong evidence that supports the use of power-transformed CAViaR models when forecasting VaR. © 2019 John Wiley \& Sons, Ltd.}}, 
pages = {296--312}, 
number = {2}, 
volume = {39}
}
@article{10.1109/bife.2009.73, 
year = {2009}, 
title = {{Robustness analysis and algorithm of expected shortfall based on extreme-value block minimum model}}, 
author = {Ou, Shide and Yi, Danhui}, 
journal = {2009 International Conference on Business Intelligence and Financial Engineering}, 
issn = {NA}, 
doi = {10.1109/bife.2009.73}, 
abstract = {{To measure effectively the risk of stock market, the algorithm of expected shortfall is presented by using the extreme-value block minimum method. By transforming the distribution of standardized minimal return in an interval into the distribution of ordinary minimal return, the formula of expected shortfall is derived. By simulation and statistical analysis, an appropriate interval length is found out to make this algorithm robust. The simulation results show that the robustness of value at risk and expected shortfall based on this method is very good when the interval length isn't more than 30. This algorithm measures effectively the expected shortfall of stock market. © 2009 IEEE.}}, 
pages = {288--292}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jedc.2011.10.005, 
year = {2012}, 
title = {{Improving the value at risk forecasts: Theory and evidence from the financial crisis}}, 
author = {Halbleib, Roxana and Pohlmeier, Winfried}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/j.jedc.2011.10.005}, 
abstract = {{The recent financial crisis has raised numerous questions about the accuracy of value-at-risk (VaR) as a tool to quantify extreme losses. In this paper we develop data-driven VaR approaches that are based on the principle of optimal combination and that provide robust and precise VaR forecasts for periods when they are needed most, such as the recent financial crisis. Within a comprehensive comparative study we provide the latest piece of empirical evidence on the performance of a wide range of standard VaR approaches and highlight the overall outperformance of the newly developed methods. © 2012 Elsevier B.V..}}, 
pages = {1212--1228}, 
number = {8}, 
volume = {36}
}
@article{10.1016/j.orl.2012.09.002, 
year = {2012}, 
title = {{Bounds for nested law invariant coherent risk measures}}, 
author = {Xin, Linwei and Shapiro, Alexander}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2012.09.002}, 
abstract = {{With every law invariant coherent risk measure is associated its conditional analogue. In this paper we discuss lower and upper bounds for the corresponding nested (composite) formulations of law invariant coherent risk measures. In particular, we consider the Average Value-at-Risk and comonotonic risk measures. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {431--435}, 
number = {6}, 
volume = {40}
}
@article{10.1016/j.jimonfin.2015.07.016, 
year = {2015}, 
title = {{The tail risk premia of the carry trades}}, 
author = {Dupuy, Philippe}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/j.jimonfin.2015.07.016}, 
abstract = {{We study the relationship between the excess returns of portfolios invested in carry trade positions and an innovative global tail risk factor. We find that high interest rate currencies are related to innovations in global currency tail risk. They deliver low returns in times of unexpected high tail risk and high returns in times of unexpected low tail risk suggesting a standard Asset Pricing Theory approach to explain the returns to the carry trade. Our tail risk factor can be understood as the interaction of moment-based factors such as volatility, skewness and kurtosis. Our results tend to indicate that the interaction of moments, i.e. tail risk, rather than the moments alone drives investor behavior. This makes sense since the ultimate risk for carry traders is to reach their funding limits which are set, because of the regulations, on the back of tail risk statistics (Value at Risk) and not simply on the back of the volatility, the skewness or the kurtosis alone. The result holds in other cross-sections of currencies and whether the global tail risk indicators are estimated in the currency, the equity or the bond market. © 2015 Elsevier Ltd.}}, 
pages = {123--145}, 
number = {NA}, 
volume = {59}
}
@article{10.1016/j.insmatheco.2020.06.001, 
year = {2020}, 
title = {{Characterizing optimal allocations in quantile-based risk sharing}}, 
author = {Wang, Ruodu and Wei, Yunran}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.06.001}, 
abstract = {{Unlike classic risk sharing problems based on expected utilities or convex risk measures, quantile-based risk sharing problems exhibit two special features. First, quantile-based risk measures (such as the Value-at-Risk) are often not convex, and second, they ignore some part of the distribution of the risk. These features create technical challenges in establishing a full characterization of optimal allocations, a question left unanswered in the literature. In this paper, we address the issues on the existence and the characterization of (Pareto-)optimal allocations in risk sharing problems for the Range-Value-at-Risk family. It turns out that negative dependence, mutual exclusivity in particular, plays an important role in the optimal allocations, in contrast to positive dependence appearing in classic risk sharing problems. As a by-product of our main finding, we obtain some results on the optimization of the Value-at-Risk (VaR) and the Expected Shortfall, as well as a new result on the inf-convolution of VaR and a general distortion risk measure. © 2020 Elsevier B.V.}}, 
pages = {288--300}, 
number = {NA}, 
volume = {93}
}
@article{10.1016/j.ins.2010.02.014, 
year = {2012}, 
title = {{A hybrid modified PSO approach to VaR-based facility location problems with variable capacity in fuzzy random uncertainty}}, 
author = {Wang, Shuming and Watada, Junzo}, 
journal = {Information Sciences}, 
issn = {00200255}, 
doi = {10.1016/j.ins.2010.02.014}, 
abstract = {{This paper studies a facility location model with fuzzy random parameters and its swarm intelligence approach. A Value-at-Risk (VaR) based fuzzy random facility location model (VaR-FRFLM) is built in which both the costs and demands are assumed to be fuzzy random variables, and the capacity of each facility is unfixed but a decision variable assuming continuous values. Under this setting, the VaR-FRFLM is inherently a two-stage mixed 0-1 integer fuzzy random programming problem, to which analytical nonlinear programming methods are not applicable. A hybrid modified particle swarm optimization (MPSO) approach is proposed to solve the VaR-FRFLM. In this hybrid mechanism, an approximation algorithm is utilized to compute the fuzzy random VaR objective, a continuous Nbest-Gbest-based PSO and a genotype-phenotype-based binary PSO vehicles are designed to deal with the continuous capacity decisions and the binary location decisions, respectively, and two mutation operators are incorporated into the PSO to further decrease the possibility of becoming trapped in the local optima. A numerical experiment illustrates the application of the proposed hybrid MPSO algorithm and lays out its robustness to the system parameter settings. The comparison shows that the hybrid MPSO exhibits better performance than that when hybrid regular continuous-binary PSO and genetic algorithm (GA) are used to solve the VaR-FRFLM. © 2010 Elsevier Inc. All rights reserved.}}, 
pages = {3--18}, 
number = {NA}, 
volume = {192}
}
@article{10.1080/09720502.2011.10700736, 
year = {2011}, 
title = {{Bivariate Value-at-risk in the emerging Malaysian sectoral markets}}, 
author = {Cheong, Chin Wen and Isa, Zaidi}, 
journal = {Journal of Interdisciplinary Mathematics}, 
issn = {09720502}, 
doi = {10.1080/09720502.2011.10700736}, 
abstract = {{This study examines the transmission of price changes and volatility among the Malaysian economic barometer (FTSE Bursa Malaysia Kuala Lumpur Composite Index- FBMKLCI) and four sectoral markets after the Asian financial crisis. The preliminary structural break identification provides an optimal sample size for the cross-markets transmission mechanism analysis. In order to reveal the hidden intention of interactions among the sectoral markets, the pair-wise markets which consist of CI-IND, CI-PLN, CI-PRP and CI-FIN are evaluated using a bivariate asymmetric BEKK model. The major intention of this study focuses on the cross-market hedging and market risk evaluations in terms of shocks and volatility. © 2011, Taru Publications.}}, 
pages = {67--94}, 
number = {1}, 
volume = {14}
}
@article{10.1111/j.1538-4616.2007.00081.x, 
year = {2007}, 
title = {{A generalized extreme value approach to financial risk measurement}}, 
author = {BALI, TURAN G.}, 
journal = {Journal of Money, Credit and Banking}, 
issn = {00222879}, 
doi = {10.1111/j.1538-4616.2007.00081.x}, 
abstract = {{This paper develops an unconditional and conditional extreme value approach to calculating value at risk (VaR), and shows that the maximum likely loss of financial institutions can be more accurately estimated using the statistical theory of extremes. The new approach is based on the distribution of extreme returns instead of the distribution of all returns and provides good predictions of catastrophic market risks. Both the in-sample and out-of-sample performance results indicate that the Box-Cox generalized extreme value distribution introduced in the paper performs surprisingly well in capturing both the rate of occurrence and the extent of extreme events in financial markets. The new approach yields more precise VaR estimates than the normal and skewed t distributions. © 2007 The Ohio State University.}}, 
pages = {1613--1649}, 
number = {7}, 
volume = {39}
}
@article{10.1016/j.jbankfin.2005.04.004, 
year = {2006}, 
title = {{Master funds in portfolio analysis with general deviation measures}}, 
author = {Rockafellar, R. Tyrrell and Uryasev, Stan and Zabarankin, Michael}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2005.04.004}, 
abstract = {{Generalized measures of deviation are considered as substitutes for standard deviation in a framework like that of classical portfolio theory for coping with the uncertainty inherent in achieving rates of return beyond the risk-free rate. Such measures, derived for example from conditional value-at-risk and its variants, can reflect the different attitudes of different classes of investors. They lead nonetheless to generalized one-fund theorems in which a more customized version of portfolio optimization is the aim, rather than the idea that a single "master fund" might arise from market equilibrium and serve the interests of all investors. The results that are obtained cover discrete distributions along with continuous distributions. They are applicable therefore to portfolios involving derivatives, which create jumps in distribution functions at specific gain or loss values, well as to financial models involving finitely many scenarios. Furthermore, they deal rigorously with issues that come up at that level of generality, but have not received adequate attention, including possible lack of differentiability to th deviation expression with respect to the portfolio weights, and the potential nonuniqueness of optimal weights. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {743--778}, 
number = {2}, 
volume = {30}
}
@article{10.1515/snde-2018-0112, 
year = {2020}, 
title = {{Bayesian analysis of periodic asymmetric power GARCH models}}, 
author = {Aknouche, Abdelhakim and Demmouche, Nacer and Dimitrakopoulos, Stefanos and Touche, Nassim}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.1515/snde-2018-0112}, 
abstract = {{In this paper, we set up a generalized periodic asymmetric power GARCH (PAP-GARCH) model whose coefficients, power, and innovation distribution are periodic over time. We first study its properties, such as periodic ergodicity, finiteness of moments and tail behavior of the marginal distributions. Then, we develop an MCMC algorithm, based on the Griddy-Gibbs sampler, under various distributions of the innovation term (Gaussian, Student-t, mixed Gaussian-Student-t). To assess our estimation method we conduct volatility and Value-at-Risk forecasting. Our model is compared against other competing models via the Deviance Information Criterion (DIC). The proposed methodology is applied to simulated and real data. © 2020 Walter de Gruyter GmbH, Berlin/Boston 2020.}}, 
pages = {20180112}, 
number = {4}, 
volume = {24}
}
@article{10.1016/j.qref.2019.08.006, 
year = {2020}, 
title = {{How informative are variance risk premium and implied volatility for Value-at-Risk prediction? International evidence}}, 
author = {Slim, Skander and Dahmene, Meriam and Boughrara, Adel}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2019.08.006}, 
abstract = {{The aim of this paper is to examine the information embedded in the implied volatility index and the variance risk premium in terms of quantifying market risk for developed and emerging stock markets. The backtesting results indicate that incorporating the relative variance risk premium into the GARCH model, greatly enhances the forecasts of one-day-ahead Value-at-Risk (VaR) for a long trading position in developed markets, while the standard GARCH is the most relevant specification in capturing risk in emerging markets. Results are found to be robust against distressed financial markets and alternative measures of the variance risk premium. Moreover, the empirical evidence shows that the superior performance of these models cannot completely reduce the scope of implied volatility as a risk management tool. Including implied volatility into the GARCH model incurs substantial savings in terms of efficient regulatory capital provisions. © 2019 Board of Trustees of the University of Illinois}}, 
pages = {22--37}, 
number = {NA}, 
volume = {76}
}
@article{10.1007/s40822-017-0067-z, 
year = {2017}, 
title = {{Value at risk (VaR) analysis for fat tails and long memory in returns}}, 
author = {Günay, Samet}, 
journal = {Eurasian Economic Review}, 
issn = {1309422X}, 
doi = {10.1007/s40822-017-0067-z}, 
abstract = {{In this study, different value at risk models (VaR), which are used to measure downside investment risk, have been analyzed under different methods and stylized facts of financial time series. Downside investment risk of a single asset and of a hypothetical portfolio have first been measured by conventional VaR models (Parametrical VaR, Historical VaR, Historical Simulation VaR and Monte Carlo Simulation VaR) and then by alternative simulation models that consider fat tails (Alpha-Stable Simulation VaR) in return distributions and long memory in returns (Long Memory Simulation VaR). Empirical findings and the Duration Based Backtesting procedure indicate that the largest VaR value is obtained under Long Memory Simulation VaR that is based on the long memory in returns. This result is consistent with the findings of Mandelbrot’s various studies. © 2017, Eurasia Business and Economics Society.}}, 
pages = {215--230}, 
number = {2}, 
volume = {7}
}
@article{10.3390/su12218849, 
year = {2020}, 
title = {{Jump aggregation, volatility prediction, and nonlinear estimation of banks’ sustainability risk}}, 
author = {Wang, Zhouwei and Zhao, Qicheng and Zhu, Min and Pang, Tao}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su12218849}, 
abstract = {{Extreme financial events usually lead to sharp jumps in stock prices and volatilities. In addition, jump clustering and stock price correlations contribute to the risk amplification acceleration mechanism during the crisis. In this paper, four Jump-GARCH models are used to forecast the jump diffusion volatility, which is used as the risk factor. The linear and asymmetric nonlinear effects are considered, and the value at risk of banks is estimated by support vector quantile regression. There are three main findings. First, in terms of the volatility process of bank stock price, the Jump Diffusion GARCH model is better than the Continuous Diffusion GARCH model, and the discrete jump volatility is significant. Secondly, due to the difference of the sensitivity of abnormal information shock, the jump behavior of bank stock price is heterogeneous. Moreover, CJ-GARCH models are suitable for most banks, while ARJI-R2-GARCH models are more suitable for small and medium sized banks. Thirdly, based on the jump diffusion volatility information, the performance of the support vector quantile regression is better than that of the parametric quantile regression and nonparametric quantile regression. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {8849}, 
number = {21}, 
volume = {12}
}
@article{10.1002/asmb.684, 
year = {2007}, 
title = {{Medium-term horizon volatility forecasting: A comparative study}}, 
author = {Hawkes, Richard and Date, Paresh}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.684}, 
abstract = {{In this paper, volatility is estimated and then forecast using unobserved components-realized volatility (UC-RV) models as well as constant volatility and GARCH models. With the objective of forecasting medium-term horizon volatility, various prediction methods are employed: multi-period prediction, variable sampling intervals and scaling. The optimality of these methods is compared in terms of their forecasting performance. To this end, several UC-RV models are presented and then calibrated using the Kalman filter. Validation is based on the standard errors on the parameter estimates and a comparison with other models employed in the literature such as constant volatility and GARCH models. Although we have volatility forecasting for the computation of Value-at-Risk in mind the methodology presented has wider applications. This investigation into practical volatility forecasting complements the substantial body of work on realized volatility-based modelling in business. Copyright © 2007 John Wiley \& Sons, Ltd.}}, 
pages = {465--481}, 
number = {6}, 
volume = {23}
}
@article{10.1109/wicom.2008.2320, 
year = {2008}, 
title = {{Optimal portfolio model based on value-at-risk and two-fund separation}}, 
author = {Lili, MA and Xusong, XU}, 
journal = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing}, 
issn = {NA}, 
doi = {10.1109/wicom.2008.2320}, 
abstract = {{This paper uses value-at-risk to measure the risk of portfolios and develop an optimal portfolio model by minimizing the VaR subject to the constraint that the final wealth should meet the minimal acceptable limits. And we find that the optimal portfolio model based on VaR generates two-fund separation, so the portfolio can be replaced with two mutual funds, a risky asset and a risk-free asset. Finally we use the simplified model that only exists two assets to have empirical studies on how investors make his optimal portfolio choice between the two mutual funds in Shanghai stock market. The results show that as the acceptable return rises, the investor saves less, and the amount of wealth invested in stocks increases. © 2008 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1287/mnsc.44.12.1650, 
year = {1998}, 
title = {{Financial data and the skewed generalized t distribution}}, 
author = {Theodossiou, Panayiotis}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.44.12.1650}, 
abstract = {{This paper develops a skewed extension of the generalized t (GT) distribution, introduced by McDonald and Newey (1988). In particular, the paper derives the mathematical moments and other properties of the distribution and assesses its ability to fit the empirical distribution of several financial series characterized by skewness and excess kurtosis. In all cases the skewed GT provides an excellent fit to the empirical distribution of data.}}, 
pages = {1650--1661}, 
number = {12 PART 1}, 
volume = {44}
}
@article{10.1016/j.jbankfin.2013.08.002, 
year = {2013}, 
title = {{Forecasting the return distribution using high-frequency volatility measures}}, 
author = {Hua, Jian and Manzan, Sebastiano}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2013.08.002}, 
abstract = {{The aim of this paper is to forecast (out-of-sample) the distribution of financial returns based on realized volatility measures constructed from high-frequency returns. We adopt a semi-parametric model for the distribution by assuming that the return quantiles depend on the realized measures and evaluate the distribution, quantile and interval forecasts of the quantile model in comparison to a benchmark GARCH model. The results suggest that the model outperforms an asymmetric GARCH specification when applied to the S\&P 500 futures returns, in particular on the right tail of the distribution. However, the model provides similar accuracy to a GARCH (1,. 1) model when the 30-year Treasury bond futures return is considered. © 2013 Elsevier B.V.}}, 
pages = {4381--4403}, 
number = {11}, 
volume = {37}
}
@article{10.12011/1000-6788(2017)08-2052-08, 
year = {2017}, 
title = {{The estimating method of VaR based on PGARCH model with high-frequency data}}, 
author = {}, 
issn = {10006788}, 
doi = {10.12011/1000-6788(2017)08-2052-08}, 
abstract = {{The frequency data plays more important role in VaR estimation. This paper presents an M-estimator of a proxy periodic GARCH (p,q) scaling model, and gives corresponding estimation method of VaR. Simultaneously we apply it to forecast out-of-time VaR with 5-min high frequency data of CSI 300 Index and HSI. The empirical analysis shows that the estimation method of VaR based on M-estimator with high-frequency data performs better than the results without the use of high-frequency data and QMLE with high-frequency data. © 2017, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.}}, 
number = {8}, 
volume = {37}
}
@article{10.1109/wicom.2008.2414, 
year = {2008}, 
title = {{Margin-setting in Chinese commodity futures markets: A VaR approach}}, 
author = {Zhang, Xiaoyan and Chen, Zhiding}, 
journal = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing}, 
issn = {NA}, 
doi = {10.1109/wicom.2008.2414}, 
abstract = {{Statistical theory is used to help select a margin level. This paper present a prudent margin-setting models to protect futures positions from extreme price movement. Five methods based GARCH models to estimate the current volatility are proposed to estimate Value at Risk describing the tail of the conditional or unconditional distributions of two financial return series. Using backtesting of historical daily return series we show that our procedure gives better 1-day estimates than methods which ignore the heavy tails of the innovations or the stochastic nature of the volatility. Furthermore, the empirical results show that the default risk of GPD distribution is judged to the most prudential method among the three fat fail distributions. © 2008 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/icebe.2009.52, 
year = {2009}, 
title = {{Value at risk management in multi-period supply inventory coordination}}, 
author = {Cai, Zheng-Ying and Xin, Rong and Xiao, Renbin}, 
journal = {2009 IEEE International Conference on e-Business Engineering}, 
issn = {NA}, 
doi = {10.1109/icebe.2009.52}, 
abstract = {{Inventory coordination problem of multi supplies and its risks management are rarely studied in the past. Here a multi-period supply inventory coordination operation between suppliers and manufacturers and a risk management strategy are presented. All inventory coordination risks are divided into holding risk and shortage risk and analyzed on the basis of value at risk (VaR). Then a risk-control model is established to lessen the risks including both holding risk and shortage risk and gain better supply coordination. Finally, a car supply inventory problem is simulated to illustrate the proposed model. © 2009 IEEE.}}, 
pages = {335--339}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.amc.2014.12.042, 
year = {2015}, 
title = {{Ruin probabilities and optimal investment when the stock price follows an exponential Lévy process}}, 
author = {Li, Ping and Zhao, Wu and Zhou, Wei}, 
journal = {Applied Mathematics and Computation}, 
issn = {00963003}, 
doi = {10.1016/j.amc.2014.12.042}, 
abstract = {{This paper investigates the infinite and finite time ruin probability under the condition that the company is allowed to invest a certain amount of money in some stock market, and the remaining reserve in the bond with constant interest force. The total insurance claim amount is modeled by a compound Poisson process and the price of the risky asset follows a general exponential Lévy process. Exponential type upper bounds for the ultimate ruin probability are derived when the investment is a fixed constant, which can be calculated explicitly. This constant investment strategy yields the optimal asymptotic decay of the ruin probability under some mild assumptions. Finally, we provide an approximation of the optimal investment strategy, which maximizes the expected wealth of the insurance company under a risk constraint on the Value-at-Risk. © 2015 Published by Elsevier Inc.}}, 
pages = {1030--1045}, 
number = {NA}, 
volume = {259}
}
@article{10.1016/j.matcom.2011.06.008, 
year = {2011}, 
title = {{Modeling the yearly Value-at-Risk for operational risk in Chinese commercial banks}}, 
author = {Lu, Zhaoyang}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2011.06.008}, 
abstract = {{In this paper, we explore the loss data collection exercise for operational risk in Chinese commercial banks from 1999 to first half of 2006. Firstly, the above data are bootstrapped to analyze the capital allocation for a medium-scaled commercial bank in China. Secondly, for every selected cell, we calibrate two truncated distributions to fit the loss severity, one for 'normal' losses and the other for the 'extreme' losses. Moreover, a more realistic dependence structure - multivariate t copula function is used to measure the relation among the selected cells. In the final, the simulation results suggest that substantial savings can be achieved through measuring the dependence by means of multivariate t copula function than by means of perfect positive dependence. © 2011 Published by Elsevier B.V. on behalf of IMACS.}}, 
pages = {604--616}, 
number = {4}, 
volume = {82}
}
@article{10.1108/sef-10-2015-0254, 
year = {2017}, 
title = {{Commodities returns’ volatility in financialization era}}, 
author = {Handika, Rangga and Putra, Iswahyudi Sondi}, 
journal = {Studies in Economics and Finance}, 
issn = {10867376}, 
doi = {10.1108/sef-10-2015-0254}, 
abstract = {{Purpose: This paper aims to indirectly evaluate the accuracy of various volatility models using a value-at-risk (VaR) approach and to investigate the relationship between the accuracy of volatility modelling and investments performance in the financialized commodity markets. Design/methodology/approach: This paper uses the VaR back-testing approach at six different commodities, seven different volatility models and five different time horizons. Findings: This paper finds that the moving average (MA) VaR model tends to be the best for oil, copper, wheat and corn (long horizon) whereas the exponential generalized autoregressive conditional heteroscedastic (E-GARCH) VaR model tends to be the best for gold, silver and corn (short horizon). Our findings indicate that MA volatility model should be used for oil, copper, wheat and corn (for longer time horizons) commodities whereas E-GARCH volatility model should be used for gold, silver and corn (for short time horizons) commodities. We also find that there is a positive relationship between an accurate VaR performance and commodity return. This indicates that a good job in modelling volatility will be rewarded by higher returns in financialized commodity markets. Originality/value: This paper indirectly evaluates the accuracy of volatility model via VaR measure and investigates the relationship between the accuracy of volatility and investments performance in financialized commodity markets. This paper contributes to the literature by offering VaR approach in evaluating volatility model performance and reporting the importance of performing accurate volatility modelling in financialized commodity markets. © 2017, © Emerald Publishing Limited.}}, 
pages = {344--362}, 
number = {3}, 
volume = {34}
}
@article{10.1016/j.jpolmod.2015.01.006, 
year = {2015}, 
title = {{Value at Risk of the main stock market indexes in the European Union (2000-2012)}}, 
author = {Iglesias, Emma M.}, 
journal = {Journal of Policy Modeling}, 
issn = {01618938}, 
doi = {10.1016/j.jpolmod.2015.01.006}, 
abstract = {{We analyze extreme movements of the main stocks market indexes in the European Union. We find that the Sweden and UK markets are the preferred ones for risk averse investors since they present the best risk-return performance. Moreover, the UK market is found to have a very low dependence with the rest of the European financial cycles, being the best one (in terms of risk-return) available for investors among the ones studied in this paper. Greece and Holland have the worst performance in terms of risk-return. Austria has the highest average return although the VaR is also high. Moreover, all markets are found to be linked: Austria, Belgium, Germany, Ireland and UK are the markets that are less dependent; while France, Greece, Holland, Italy, Spain and Sweden are more dependent on the rest of the European financial cycles. We find a very strong dependence of France from Belgium. Our results have very important policy implications with respect to the appropriate monetary policy that countries should adopt. In special, countries that experience unstable financial markets should consider similar macroeconomic policies to the UK and Sweden. © 2015 Society for Policy Modeling.}}, 
pages = {1--13}, 
number = {1}, 
volume = {37}
}
@article{10.1007/s10287-009-0099-2, 
year = {2009}, 
title = {{DC programming and DCA for globally solving the value-at-risk}}, 
author = {Dinh, Tao Pham and Nam, Nguyen Canh and Thi, Hoai An Le}, 
journal = {Computational Management Science}, 
issn = {1619697X}, 
doi = {10.1007/s10287-009-0099-2}, 
abstract = {{The value-at-risk is an important risk measure that has been used extensively in recent years in portfolio selection and in risk analysis. This problem, with its known bilevel linear program, is reformulated as a polyhedral DC program with the help of exact penalty techniques in DC programming and solved by DCA. To check globality of computed solutions, a global method combining the local algorithm DCA with a well adapted branch-and-bound algorithm is investigated. An illustrative example and numerical simulations are reported, which show the robustness, the globality and the efficiency of DCA. © Springer-Verlag 2009.}}, 
pages = {477--501}, 
number = {4}, 
volume = {6}
}
@article{10.1111/j.0306-686x.2004.00578.x, 
year = {2004}, 
title = {{FOREX risk: Measurement and evaluation using value-at-risk}}, 
author = {Bredin, Don and Hyde, Stuart}, 
journal = {Journal of Business Finance \& Accounting}, 
issn = {0306686X}, 
doi = {10.1111/j.0306-686x.2004.00578.x}, 
abstract = {{We measure and evaluate the performance of a number of Valueat- Risk (VaR) methods using a portfolio based on the foreign exchange exposure of a small open economy (Ireland) among its trading partners. The sample period highlights the changing nature of Ireland's exposure to risk over the past decade in the run-up to EMU. Our results offer an indication of the level of accuracy of the various approaches and discuss the issues of models ensuring statistical accuracy or more conservative leanings. Our findings suggest that the Orthogonal GARCH model is the most accurate methodology while the EWMA specification is the more conservative approach. © Blackwell Publishing Ltd. 2004.}}, 
pages = {1389--1417}, 
number = {9-10}, 
volume = {31}
}
@article{10.11118/actaun201765051687, 
year = {2017}, 
title = {{Using HMM approach for assessing quality of value at risk estimation: Evidence from PSE listed company}}, 
author = {Konderla, Tomáš and Klepáč, Václav}, 
journal = {Acta Universitatis Agriculturae et Silviculturae Mendelianae Brunensis}, 
issn = {12118516}, 
doi = {10.11118/actaun201765051687}, 
abstract = {{The article points out the possibilities of using Hidden Markov model (abbrev. HMM) for estimation of Value at Risk metrics (abbrev. VaR) in sample. For the illustration we use data of the company listed on Prague Stock Exchange in range from January 2011 to June 2016. HMM approach allows us to classify time series into different states based on their development characteristic. Due to a deeper shortage of existing domestic results or comparison studies with advanced volatility governed VaR forecasts we tested HMM with univariate ARMA-GARCH model based VaR estimates. The common testing via Kupiec and Christoffersen procedures offer generalization that HMM model performs better that volatility based VaR estimation technique in terms of accuracy, even with the simpler HMM with normal-mixture distribution against previously used GARCH with many types of non-normal innovations. © 2017 Mendel University of Agriculture and Forestry Brno. All rights reserved.}}, 
pages = {1687--1694}, 
number = {5}, 
volume = {65}
}
@article{10.1142/s0217595910002697, 
year = {2010}, 
title = {{Estimating bivariate garch-jump model based on high frequency data: The case of revaluation of the chinese yuan in july 2005}}, 
author = {LU, XINHONG and KAWAI, KEN-ICHI and MAEKAWA, KOICHI}, 
journal = {Asia-Pacific Journal of Operational Research}, 
issn = {02175959}, 
doi = {10.1142/s0217595910002697}, 
abstract = {{This paper analyzes the behavior of one-minute high-frequency time-series data of exchange rates for five currencies (Japanese Yen, Australian Dollar, Canadian Dollar, Euro, and Pound Sterling) against the US Dollar when the Chinese Yuan was revalued on July 21st, 2005. The data show the following distinctive features: (1) There is a large jump in the exchange rates time series at the time of the Yuan revaluation. (2) Large volatility in the returns of exchange rates is observed for a while after the jump. (3) There are many other jumps, possibly correlated, in each exchange rate time series. To capture these features we fit the following models to the data: (i) a univariate GARCH-Jump model with a large jump that is influential on volatility, and (ii) a bivariate GARCH-Jump model with correlated Poisson jumps. For comparison, we also estimate these GARCH models without the associated jumps. The model performance is evaluated based on Value-at-Risk (VaR). © World Scientific Publishing Co. \& Operational Research Society of Singapore.}}, 
pages = {287--300}, 
number = {2}, 
volume = {27}
}
@article{10.1287/opre.1080.0531, 
year = {2009}, 
title = {{Estimating quantile sensitivities}}, 
author = {Hong, L Jeff}, 
journal = {Operations Research}, 
issn = {0030364X}, 
doi = {10.1287/opre.1080.0531}, 
abstract = {{Quantiles of a random performance serve as important alternatives to the usual expected value. They are used in the financial industry as measures of risk and in the service industry as measures of service quality. To manage the quantile of a performance, we need to know how changes in the input parameters affect the output quantiles, which are called quantile sensitivities. In this paper, we show that the quantile sensitivities can be written in the form of conditional expectations. Based on the conditional-expectation form, we first propose an infinitesimal-perturbation-analysis (IPA) estimator. The IPA estimator is asymptotically unbiased, but it is not consistent. We then obtain a consistent estimator by dividing data into batches and averaging the IPA estimates of all batches. The estimator satisfies a central limit theorem for the i.i.d. data, and the rate of convergence is strictly slower than n- 1/3. The numerical results show that the estimator works well for practical problems. © 2009 INFORMS.}}, 
pages = {118--130}, 
number = {1}, 
volume = {57}
}
@article{10.2202/1558-3708.1522, 
year = {2008}, 
title = {{Markov-switching GARCH modelling of value-at-risk}}, 
author = {Sajjad, Rasoul and Coakley, Jerry and Nankervis, John C}, 
journal = {Studies in Nonlinear Dynamics \& Econometrics}, 
issn = {10811826}, 
doi = {10.2202/1558-3708.1522}, 
abstract = {{This paper proposes an asymmetric Markov regime-switching (MS) GARCH model to estimate value-at-risk (VaR) for both long and short positions. This model improves on existing VaR methods by taking into account both regime change and skewness or leverage effects. The performance of our MS model and single-regime models is compared through an innovative backtesting procedure using daily data for UK and US market stock indices. The findings from exceptions and regulatory-based tests indicate the MS-GARCH specifications clearly outperform other models in estimating the VaR for both long and short FTSE positions and also do quite well for S\&P positions. We conclude that ignoring skewness and regime changes has the effect of imposing larger than necessary conservative capital requirements. Copyright ©2008 The Berkeley Electronic Press. All rights reserved.}}, 
number = {3}, 
volume = {12}
}
@article{10.1016/j.iref.2011.08.007, 
year = {2012}, 
title = {{Forecasting the volatility of S\&P depositary receipts using GARCH-type models under intraday range-based and return-based proxy measures}}, 
author = {Liu, Hung-Chun and Chiang, Shu-Mei and Cheng, Nick Ying-Pin}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2011.08.007}, 
abstract = {{We employ four various GARCH-type models, incorporating the skewed generalized t (SGT) errors into those returns innovations exhibiting fat-tails, leptokurtosis and skewness to forecast both volatility and value-at-risk (VaR) for Standard \& Poor's Depositary Receipts (SPDRs) from 2002 to 2008. Empirical results indicate that the asymmetric EGARCH model is the most preferable according to purely statistical loss functions. However, the mean mixed error criterion suggests that the EGARCH model facilitates option buyers for improving their trading position performance, while option sellers tend to favor the IGARCH/EGARCH model at shorter/longer trading horizon. For VaR calculations, although these GARCH-type models are likely to over-predict SPDRs' volatility, they are, nevertheless, capable of providing adequate VaR forecasts. Thus, a GARCH genre of model with SGT errors remains a useful technique for measuring and managing potential losses on SPDRs under a turbulent market scenario. © 2011 Elsevier Inc.}}, 
pages = {78--91}, 
number = {1}, 
volume = {22}
}
@article{10.12733/jics20105294, 
year = {2015}, 
title = {{Using information entropy to measure bond risk: An empirical investigation}}, 
author = {Zhou, Rongxi}, 
journal = {Journal of Information and Computational Science}, 
issn = {15487741}, 
doi = {10.12733/jics20105294}, 
abstract = {{In the security market, there are some bonds with the same structure and properties, namely the same duration and credit level, but their prices are in distinct difference. In this paper, we firstly explore information entropy to measure the risk of such kind of bonds, and discuss the difference of the four kinds of methods, namely entropy, convexity, variance and VaR, then make empirical comparisons among them by using the bond data from Shanghai Stock Exchange (SSE) and Shenzhen Stock Exchange (SZSE) in China. The results show that the information entropy method can be a better risk measurement of such bonds than other methods, and the bond risk in SSE is lower than that in SZSE in China. ©, 2015, Binary Information Press. All right reserved.}}, 
pages = {1089--1100}, 
number = {3}, 
volume = {12}
}
@article{10.1007/s10479-010-0761-7, 
year = {2010}, 
title = {{An algorithm for sequential tail value at risk for path-independent payoffs in a binomial tree}}, 
author = {Roorda, Berend}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-010-0761-7}, 
abstract = {{We present an algorithm that determines Sequential Tail Value at Risk (STVaR) for path-independent payoffs in a binomial tree. STVaR is a dynamic version of Tail-Value-at-Risk (TVaR) characterized by the property that risk levels at any moment must be in the range of risk levels later on. The algorithm consists of a finite sequence of backward recursions that is guaranteed to arrive at the solution of the corresponding dynamic optimization problem. The algorithm makes concrete how STVaR differs from TVaR over the remaining horizon, and from recursive TVaR, which amounts to Dynamic Programming. Algorithmic aspects are compared with the cutting-plane method. Time consistency and comonotonicity properties are illustrated by applying the algorithm on elementary examples. © 2010 The Author(s).}}, 
pages = {463--483}, 
number = {1}, 
volume = {181}
}
@article{10.1016/j.jbef.2020.100333, 
year = {2020}, 
title = {{Clarifying managerial biases using a probabilistic framework}}, 
author = {Ellina, Polina and Mascarenhas, Briance and Theodossiou, Panayiotis}, 
journal = {Journal of Behavioral and Experimental Finance}, 
issn = {22146350}, 
doi = {10.1016/j.jbef.2020.100333}, 
abstract = {{A unifying probabilistic framework is developed to analyze and compare the impact of the psychological biases of overconfidence and underconfidence on managerial perceptions about the expected value, overall risk, downside risk, value-at-risk and expected shortfall of decision-making economic variables. The results depict that overconfident managers overestimate their expected values and underestimate downside risk, VaR and ES of decision-making variables. Underconfident managers, on the other hand, underestimate their expected values and overestimate downside risk, value-at-risk, and expected shortfall. © 2020 Elsevier B.V.}}, 
pages = {100333}, 
number = {NA}, 
volume = {27}
}
@article{10.1080/24725854.2019.1674464, 
year = {2020}, 
title = {{Risk aversion to parameter uncertainty in Markov decision processes with an application to slow-onset disaster relief}}, 
author = {Meraklı, Merve and Küçükyavuz, Simge}, 
journal = {IISE Transactions}, 
issn = {24725854}, 
doi = {10.1080/24725854.2019.1674464}, 
abstract = {{In classic Markov Decision Processes (MDPs), action costs and transition probabilities are assumed to be known, although an accurate estimation of these parameters is often not possible in practice. This study addresses MDPs under cost and transition probability uncertainty and aims to provide a mathematical framework to obtain policies minimizing the risk of high long-term losses due to not knowing the true system parameters. To this end, we utilize the risk measure value-at-risk associated with the expected performance of an MDP model with respect to parameter uncertainty. We provide mixed-integer linear and nonlinear programming formulations and heuristic algorithms for such risk-averse models of MDPs under a finite distribution of the uncertain parameters. Our proposed models and solution methods are illustrated on an inventory management problem for humanitarian relief operations during a slow-onset disaster. The results demonstrate the potential of our risk-averse modeling approach for reducing the risk of highly undesirable outcomes in uncertain/risky environments. © 2019, Copyright © 2019 “IISE”.}}, 
pages = {1--21}, 
number = {8}, 
volume = {52}
}
@article{10.1080/14697680902968013, 
year = {2011}, 
title = {{Multi-regime nonlinear capital asset pricing models}}, 
author = {Chen, Cathy W. S. and Gerlach, Richard H. and Lin, Ann M. H.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680902968013}, 
abstract = {{A multiple-regime threshold generalized autoregressive conditionally heteroskedastic capital asset pricing model is introduced. The model captures asymmetric risk through allowing market beta to change discretely between regimes that are driven by market information. Asymmetric volatility and mean equation dynamics are also captured. We confirm the time varying nature of market risk, in response to changes in the market, and that this discrete time variation can differ across assets. These findings could have important implications for optimizing investment decisions: e.g. in risk assessment, portfolio selection and hedging decisions. © 2011 Taylor \& Francis.}}, 
pages = {1421--1438}, 
number = {9}, 
volume = {11}
}
@article{10.1016/j.physa.2019.123360, 
year = {2020}, 
title = {{Risk forecasting in the crude oil market: A multiscale Convolutional Neural Network approach}}, 
author = {Zou, Yingchao and Yu, Lean and Tso, Geoffrey K.F. and He, Kaijian}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2019.123360}, 
abstract = {{As the crude oil price movement is influenced by increasingly diverse range of risk factors in the crude oil markets, the crude oil price exhibits more complex nonlinear behavior and poses higher level of risk for investors than ever before. To model the crude oil risk changes at higher level of accuracy, we proposed a new multiscale approach to estimate Value at Risk. It takes advantage of Variational Mode Decomposition (VMD) model to extract and model the main risk factors in the multiscale domain, where the individual characteristics of these risk factors are modeled using ARMA-GARCH models. The Convolutional Neural Network (CNN) based nonlinear ensemble model is employed to aggregate these risk forecasts as the ensemble members and produce the optimal ensemble forecasts. Empirical evaluation of the performance of the proposed model has been conducted using the extensive dataset, constructed with daily price observations in the major crude oil markets, Experiment results confirm that the proposed risk forecasting models produce an improved forecasting accuracy for the typical risk measures such as Value at Risk. © 2019 Elsevier B.V.}}, 
pages = {123360}, 
number = {NA}, 
volume = {541}
}
@article{10.1109/ccdc.2009.5194855, 
year = {2009}, 
title = {{A portfolio model based on the minimax risk and return factors}}, 
author = {Lin, Pingping and Liu, Shu-an and Wang, Qing}, 
journal = {2009 Chinese Control and Decision Conference}, 
issn = {NA}, 
doi = {10.1109/ccdc.2009.5194855}, 
abstract = {{Based on the analysis of minimax portfolio selection model proposed by Yong, a new risk function is designed with risk factor and extra return factor. The risk factor is used to adjust the effect of asset return level on the investment decision, and the extra return factor is for controlling the effect of portfolio promising profits on the investment decision. Furthermore, a portfolio selection model is proposed based on the new risk measure. The model is solved by transforming it into a linear programming model. By experiments with actual data of the stock market, this proposed model shows its effectiveness and practicability. © 2009 IEEE.}}, 
pages = {4767--4771}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/ijfs8010009, 
year = {2020}, 
title = {{The Bayesian approach to capital allocation at operational risk: A combination of statistical data and expert opinion}}, 
author = {Habachi, Mohamed and Benbachir, Saâd}, 
journal = {International Journal of Financial Studies}, 
issn = {22277072}, 
doi = {10.3390/ijfs8010009}, 
abstract = {{Operational risk management remains a major concern for financial institutions. Indeed, institutions are bound to manage their own funds to hedge this risk. In this paper, we propose an approach to allocate one’s own funds based on a combination of historical data and expert opinion using the loss distribution approach (LDA) and Bayesian logic. The results show that internal models are of great importance in the process of allocating one’s own funds, and the use of the Delphi method for modelling expert opinion is very useful in ensuring the reliability of estimates. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {9}, 
number = {1}, 
volume = {8}
}
@article{10.1080/17509653.2010.10671113, 
year = {2010}, 
title = {{A two echelon location-routing model with considering value-at-risk measure}}, 
author = {Azad, Nader and Davoudpour, Hamid}, 
journal = {International Journal of Management Science and Engineering Management}, 
issn = {17509653}, 
doi = {10.1080/17509653.2010.10671113}, 
abstract = {{In this paper, we present a distribution network design problem in supply chain system which optimizes location, and routing decisions. We propose a two-stage supply chain model which involves determining simultaneously the best sites for the distribution centers and determining capacity for distribution centers and the best strategy for distributing the product from the suppliers to the distribution centers and from distribution centers to the customers and the routes of the vehicles. Each customer’s demand is uncertain and follows a normal distribution. For the first time in the location-routing problem, we introduce a new solution methodology that adopts from the finance literature, to optimize the Value-at-Risk (VaR) measure in our problem. A heuristic method is developed for solving the problem. The heuristic method is decomposed into two phases: distribution center location-allocation phase, and routing phase. First of all, an initial solution is determined, then a Tabu search algorithm is used to improve the initial solution for each phase separately and alternatively. © 2010 Taylor \& Francis Group, LLC.}}, 
pages = {235--240}, 
number = {3}, 
volume = {5}
}
@article{10.3969/j.issn.1001-0505.2009.06.031, 
year = {2009}, 
title = {{Study of Chinese industry risk based on stock index analysis of dynamic EVT-VaR}}, 
author = {}, 
issn = {10010505}, 
doi = {10.3969/j.issn.1001-0505.2009.06.031}, 
abstract = {{Chinese industry stock price index is analyzed and empirical studies on China's industrial risk under different market conditions are made with GARCH-EVT (generalized autoregressive conditional heteroskedasticity-extreme value theory) VaR (value at risk) model. On the basis of the GARCH model and extreme value theory presented by McNei and other scholars, this paper modifies the above method of risk evaluation and constructs a GARCH-EVT VaR model by applying VaR method put forward by Huisman and others. Taking time-varying property of the risk factor into consideration while constructing the model, this paper models the fat-tail of the risk factor using EVT, thus simplifying the evaluation process of VaR and improving the accuracy of evaluation as a result. The GARCH-EVT VaR model can better describe the tail of the distribution, and more accurately measure the market risk and industrial risk within studied period as well. The results indicate that risks of different industries vary in different market conditions. The above conclusions are validated by using Kruskal-Wallis Consistency tests. It is essential to adjust the distributions of assets in accordance with market conditions and features of industrial risks in the process of access combination management.}}, 
number = {6}, 
volume = {39}
}
@article{10.1111/eufm.12256, 
year = {2020}, 
title = {{Estimating portfolio risk for tail risk protection strategies}}, 
author = {Happersberger, David and Lohre, Harald and Nolte, Ingmar}, 
journal = {European Financial Management}, 
issn = {13547798}, 
doi = {10.1111/eufm.12256}, 
abstract = {{We forecast portfolio risk for managing dynamic tail risk protection strategies, based on extreme value theory, expectile regression, copula-GARCH and dynamic generalized autoregressive score models. Utilizing a loss function that overcomes the lack of elicitability for expected shortfall, we propose a novel expected shortfall (and value-at-risk) forecast combination approach, which dominates simple and sophisticated standalone models as well as a simple average combination approach in modeling the tail of the portfolio return distribution. While the associated dynamic risk targeting or portfolio insurance strategies provide effective downside protection, the latter strategies suffer less from inferior risk forecasts, given the defensive portfolio insurance mechanics. © 2020 The Authors. European Financial Management Published by John Wiley \& Sons Ltd.}}, 
pages = {1107--1146}, 
number = {4}, 
volume = {26}
}
@article{10.1007/978-3-319-47172-3_7, 
year = {2017}, 
title = {{The calibration of market risk measures during period of economic downturn: Market risks and measures}}, 
author = {Mwamba, John Weirstrass Muteba}, 
journal = {Contributions to Management Science}, 
issn = {14311941}, 
doi = {10.1007/978-3-319-47172-3\_7}, 
abstract = {{This study explores the calibration of market risk measures during period of economic downturn. This calibration is done in two frameworks: firstly individual profit and loss distribution is modelled using two different types of extreme value distribution namely: the generalized extreme value (GEV) distribution, and the generalized Pareto distribution (GPD). The resulting shape parameters are all positive indicating that these distributions can in fact capture the negative skewness and excess kurtosis of the profit and loss (P\&L) distribution during period of economic downturn. We show that the presence of such positive shape parameters indicates the existence of large probabilities of extreme price drops in the left tail of the P\&L distribution. Based on these results the second framework used in this study builds two multivariate copula distributions with GEV and GPD marginals. This procedure captures the dependence structure of stock markets during periods of financial crisis. To illustrate the computation of market risk measures; we consider one elliptical copula (student t copula) and one Archimedean copula (Gumbel copula). Using two stock market indices we compute what we refer to as EVT based mark risk measures and the copula based market risk measures for both the left and right tails of the P\&L distribution. Our results suggest that copula based risk measures are more reliable in predicting the behavior of market risks during period of economic downturn. © Springer International Publishing AG 2017.}}, 
pages = {89--102}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.tcs.2007.02.038, 
year = {2007}, 
title = {{A new algorithm based on copulas for VaR valuation with empirical calculations}}, 
author = {Cheng, Gang and Li, Ping and Shi, Peng}, 
journal = {Theoretical Computer Science}, 
issn = {03043975}, 
doi = {10.1016/j.tcs.2007.02.038}, 
abstract = {{This paper concerns the application of copula functions in VaR valuation. The copula function is used to model the dependence structure of multivariate assets. After the introduction of the traditional Monte Carlo simulation method and the pure copula method we present a new algorithm based on mixture copula functions and the dependence measure, Spearman's rho. This new method is used to simulate daily returns of two stock market indices in China, Shanghai Stock Composite Index and Shenzhen Stock Composite Index, and then empirically calculate six risk measures including VaR and conditional VaR. The results are compared with those derived from the traditional Monte Carlo method and the pure copula method. From the comparison we show that the dependence structure between asset returns plays a more important role in valuating risk measures comparing with the form of marginal distributions. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {190--197}, 
number = {2}, 
volume = {378}
}
@article{10.1016/j.matcom.2013.08.001editorial, 
year = {2013}, 
title = {{Risk modelling and management: An overview}}, 
author = {}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2013.08.001editorial}, 
abstract = {{The papers in this special issue of Mathematics and Computers in Simulation are substantially revised versions of the papers that were presented at the 2011 Madrid International Conference on "Risk Modelling and Management" (RMM2011). The papers cover the following topics: currency hedging strategies using dynamic multivariate GARCH, risk management of risk under the Basel Accord: A Bayesian approach to forecasting value-at-risk of VIX futures, fast clustering of GARCH processes via Gaussian mixture models, GFC-robust risk management under the Basel Accord using extreme value methodologies, volatility spillovers from the Chinese stock market to economic neighbours, a detailed comparison of Value-at-Risk estimates, the dynamics of BRICS's country risk ratings and domestic stock markets, U.S. stock market and oil price, forecasting value-at-risk with a duration-based POT method, and extreme market risk and extreme value theory. © 2013 IMACS.}}, 
number = {NA}, 
volume = {94}
}
@article{10.3390/e22060663, 
year = {2020}, 
title = {{Maximum varma entropy distribution with conditional value at risk constraints}}, 
author = {Liu, Chang and Chang, Chuo and Chang, Zhe}, 
journal = {Entropy}, 
issn = {10994300}, 
doi = {10.3390/e22060663}, 
pmid = {33286435}, 
pmcid = {PMC7517197}, 
abstract = {{It is well known that Markowitz's mean-variance model is the pioneer portfolio selection model. The mean-variance model assumes that the probability density distribution of returns is normal. However, empirical observations on financial markets show that the tails of the distribution decay slower than the log-normal distribution. The distribution shows a power law at tail. The variance of a portfolio may also be a random variable. In recent years, the maximum entropy method has been widely used to investigate the distribution of return of portfolios. However, the mean and variance constraints were still used to obtain Lagrangian multipliers. In this paper, we use Conditional Value at Risk constraints instead of the variance constraint to maximize the entropy of portfolios. Value at Risk is a financial metric that estimates the risk of an investment. Value at Risk measures the level of financial risk within a portfolio. The metric is most commonly used by investment bank to determine the extent and occurrence ratio of potential losses in portfolios. Value at Risk is a single number that indicates the extent of risk in a given portfolio. This makes the risk management relatively simple. The Value at Risk is widely used in investment bank and commercial bank. It has already become an accepted standard in buying and selling assets. We show that the maximum entropy distribution with Conditional Value at Risk constraints is a power law. Algebraic relations between the Lagrangian multipliers and Value at Risk constraints are presented explicitly. The Lagrangian multipliers can be fixed exactly by the Conditional Value at Risk constraints. © 2020 by the authors.}}, 
pages = {663}, 
number = {6}, 
volume = {22}
}
@article{10.1140/epjb/e2010-00199-9, 
year = {2010}, 
title = {{Accounting for risk of non linear portfolios : A novel Fourier approach}}, 
author = {Bormetti, G. and Cazzola, V. and Delpini, D. and Livan, G.}, 
journal = {The European Physical Journal B}, 
issn = {14346028}, 
doi = {10.1140/epjb/e2010-00199-9}, 
eprint = {1002.4817}, 
abstract = {{The presence of non linear instruments is responsible for the emergence of non Gaussian features in the price changes distribution of realistic portfolios, even for Normally distributed risk factors. This is especially true for the benchmark Delta Gamma Normal model, which in general exhibits exponentially damped power law tails. We show how the knowledge of the model characteristic function leads to Fourier representations for two standard risk measures, the Value at Risk and the Expected Shortfall, and for their sensitivities with respect to the model parameters. We detail the numerical implementation of our formulae and we emphasize the reliability and efficiency of our results in comparison with Monte Carlo simulation. © 2010 EDP Sciences, SIF, Springer-Verlag Berlin Heidelberg.}}, 
pages = {157--165}, 
number = {1}, 
volume = {76}
}
@article{10.1080/00036846.2015.1037439, 
year = {2015}, 
title = {{Value at risk estimation by threshold stochastic volatility model}}, 
author = {Huang, Alex YiHou}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2015.1037439}, 
abstract = {{This article proposes a threshold stochastic volatility model that generates volatility forecasts specifically designed for value at risk (VaR) estimation. The method incorporates extreme downside shocks by modelling left-tail returns separately from other returns. Left-tail returns are generated with a t-distributional process based on the historically observed conditional excess kurtosis. This specification allows VaR estimates to be generated with extreme downside impacts, yet remains empirically widely applicable. This article applies the model to daily returns of seven major stock indices over a 22-year period and compares its forecasts to those of several other forecasting methods. Based on back-testing outcomes and likelihood ratio tests, the new model provides reliable estimates and outperforms others. © 2015 Taylor \& Francis.}}, 
pages = {4884--4900}, 
number = {45}, 
volume = {47}
}
@article{10.1109/netwks.2014.6958530, 
year = {2014}, 
title = {{Risk-awareness in resilient networks design: Value-at-risk is enough}}, 
author = {Cholda, Piotr and Guzik, Piotr and Rusek, Krzysztof}, 
journal = {2014 16th International Telecommunications Network Strategy and Planning Symposium (Networks)}, 
issn = {NA}, 
doi = {10.1109/netwks.2014.6958530}, 
abstract = {{The paper discusses business impact analysis in the context of resilient networks. The emphasis is put on risk evaluation using Value-at-Risk (VaR) and Conditional VaR measures. The concerns known for VaR are shown, especially the theoretical lack of subadditivity. The study shows that in practice the disadvantages do not appear in resilient networks design, and there is no need to apply the more complex CVaR. © 2014 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1088/1742-6596/909/1/012040, 
year = {2017}, 
title = {{Estimating the Value-at-Risk for some stocks at the capital market in Indonesia based on ARMA-FIGARCH models}}, 
author = {Sukono and Lesmana, E. and Susanti, D. and Napitupulu, H. and Hidayat, Y.}, 
journal = {Journal of Physics: Conference Series}, 
issn = {17426588}, 
doi = {10.1088/1742-6596/909/1/012040}, 
abstract = {{Value-at-Risk has already become a standard measurement that must be carried out by the financial institution for both internal interest and regulatory. In this paper, the estimation of Value-at-Risk of some stocks with econometric models approach is analyzed. In this research, we assume that the stock return follows the time series model. To do the estimation of mean value we are using ARMA models, while to estimate the variance value we are using FIGARCH models. Furthermore, the mean value estimator and the variance are used to estimate the Value-at-Risk. The result of the analysis shows that from five stock PRUF, BBRI, MPPA, BMRI, and INDF, the Value-at-Risk obtained are 0.01791, 0.06037, 0.02550, 0.06030, and 0.02585 respectively. Since Value-at-Risk represents the maximum risk size of each stock at a 95\% level of significance, then it can be taken into consideration in determining the investment policy on stocks. © Published under licence by IOP Publishing Ltd.}}, 
pages = {012040}, 
number = {1}, 
volume = {909}
}
@article{10.1016/j.eswa.2012.02.090, 
year = {2012}, 
title = {{A random fuzzy minimum spanning tree problem through a possibility-based value at risk model}}, 
author = {Katagiri, Hideki and Kato, Kosuke and Hasuike, Takashi}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2012.02.090}, 
abstract = {{This paper considers a minimum spanning tree problem under the situation where costs for constructing edges in a network include both fuzziness and randomness. In particular, this article focuses on the case that the edge costs are expressed by random fuzzy variables. A new decision making model based on a possibility measure and a value at risk measure is proposed in order to find a solution which fully reflects random and fuzzy information. It is shown that an optimal solution of the proposed model is obtained by a polynomial-time algorithm. © 2012 Elsevier Ltd. All rights reserved.}}, 
pages = {10639--10646}, 
number = {12}, 
volume = {39}
}
@article{10.1002/for.1224, 
year = {2012}, 
title = {{A study of value-at-risk based on M-estimators of the conditional heteroscedastic models}}, 
author = {Iqbal, Farhat and Mukherjee, Kanchan}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.1224}, 
abstract = {{In this paper, we investigate the performance of a class of M-estimators for both symmetric and asymmetric conditional heteroscedastic models in the prediction of value-at-risk. The class of estimators includes the least absolute deviation (LAD), Huber's, Cauchy and B-estimator, as well as the well-known quasi maximum likelihood estimator (QMLE). We use a wide range of summary statistics to compare both the in-sample and out-of-sample VaR estimates of three well-known stock indices. Our empirical study suggests that in general Cauchy, Huber and B-estimator have better performance in predicting one-step-ahead VaR than the commonly used QMLE. Copyright © 2011 John Wiley \& Sons, Ltd.}}, 
pages = {377--390}, 
number = {5}, 
volume = {31}
}
@article{10.1007/11496199_41, 
year = {2005}, 
title = {{An algorithm for portfolio's Value at Risk based on principal factor analysis}}, 
author = {Xue, Honggang and Xu, Chengxian and Hu, Chunping}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/11496199\_41}, 
abstract = {{In this paper, we propose principle factor analysis method to reduce the dimensions of a high dimensional random vector in calculating portfolio's Value at Risk. The theoretical foundation, algorithm and numerical example of the method are given. This method outperforms the principle component analysis method. Especially, the advantages of the method are marked, while the factors F's multicollinearity is serious. © Springer-Verlag Berlin Heidelberg 2005.}}, 
pages = {381--391}, 
number = {NA}, 
volume = {3521}
}
@article{10.1057/grir.2010.10, 
year = {2011}, 
title = {{Properties of a risk measure derived from ruin theory}}, 
author = {Trufin, Julien and Albrecher, Hansjoerg and Denuit, Michel M}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1057/grir.2010.10}, 
abstract = {{This paper studies a risk measure inherited from ruin theory and investigates some of its properties. Specifically, we consider a value-at-risk (VaR)-type risk measure defined as the smallest initial capital needed to ensure that the ultimate ruin probability is less than a given level. This VaR-type risk measure turns out to be equivalent to the VaR of the maximal deficit of the ruin process in infinite time. A related Tail-VaR-type risk measure is also discussed. © 2011 The International Association for the Study of Insurance Economics 1554-964X/11.}}, 
pages = {174--188}, 
number = {2}, 
volume = {36}
}
@article{10.17159/2222-3436/2016/v19n1a8, 
year = {2016}, 
title = {{Trading book risk metrics: A South African perspective}}, 
author = {Visser, Dirk and Vuuren, Gary van}, 
journal = {South African Journal of Economics and Management Sciences}, 
issn = {10158812}, 
doi = {10.17159/2222-3436/2016/v19n1a8}, 
abstract = {{The regulatory market risk metric – Value at Risk – has remained virtually unchanged since its introduction by JP Morgan in 1996. Many prominent examples of market risk underestimation have undermined the credibility of VaR, prompting the search for better, more robust measures. Expected shortfall and procyclical capital buffers have been proposed by regulatory authorities, but neither is without problems. Bubble VaR – a coherent measure which avoids many of the pitfalls to which other measures have succumbed – was designed to be both forward-looking and countercyclical. Although tested on other markets, here it is applied to various South African prices and the results compared with both international observations and other market risk measures. Bubble VaR is found to perform consistently and reliably under all market conditions. © 2016, University of Pretoria. All rights reserved.}}, 
pages = {118--138}, 
number = {1}, 
volume = {19}
}
@article{10.1201/b18660-48, 
year = {2015}, 
title = {{Operational risk measurement based on POT-copula}}, 
issn = {NA}, 
doi = {10.1201/b18660-48}, 
abstract = {{According to the sample data of the operational risks of the Chinese commercial banks during 2003 to 2013, this paper constructs a risk measurement model based on POT-Copula. Using Hill graph and the excess mean function diagram determining the model of threshold, it measures the overall value at risk by using HKKP to estimate the tail parameters of the POT model and calculates the value at risk of the conditional risk of the inside and outside deception of Chinese commercial banks’ operational risk, and finally employs t-Copula function to connect the marginal distribution of the inside and outside deception in the operational risk. © 2015 Taylor \& Francis Group, London.}}, 
pages = {215--218}, 
number = {NA}, 
volume = {NA}
}
@article{10.1093/ietisy/e91-d.2.277, 
year = {2008}, 
title = {{Fuzzy rule extraction from dynamic data for voltage risk identification}}, 
author = {CHANG, Chen-Sung}, 
journal = {IEICE Transactions on Information and Systems}, 
issn = {09168532}, 
doi = {10.1093/ietisy/e91-d.2.277}, 
abstract = {{This paper presents a methodology for performing on-line voltage risk identification (VRI) in power supply networks using hyperrectangular composite neural networks (HRCNNs) and synchronized phasor measurements. The FHRCNN presented in this study integrates the paradigm of neural networks with the concept of knowledge-based approaches, rendering them both more useful than when applied alone. The fuzzy rules extracted from the dynamic data relating to the power system formalize the knowledge applied by experts when conducting the voltage risk assessment procedure. The efficiency of the proposed technique is demonstrated via its application to the Taiwan Power Provider System (Tai- Power System) under various operating conditions. Overall, the results indicated that the proposed scheme achieves a minimum 97\% success rate in determining the current voltage security level. Copyright © 2008 The Institute of Electronics, Information and Communication Engineers.}}, 
pages = {277--285}, 
number = {2}, 
volume = {E91-D}
}
@article{10.1109/fskd.2009.53, 
year = {2009}, 
title = {{Analysis of different versions of the credibilistic value at risk}}, 
author = {Peng, Jin and Li, Shengguo}, 
journal = {2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery}, 
issn = {NA}, 
doi = {10.1109/fskd.2009.53}, 
abstract = {{Value at risk (VaR) is the most frequently used risk measures in current risk management practice. However, there are different versions of the definitions of VaR. In this paper, we focus on a comparison of different versions of the credibilistic VaR in fuzzy risk analysis. First, we present different versions of the credibilistic VaR based on credibility theory. Then, we examine some properties of the proposed credibilistic VaR. Finally, a kind of fuzzy simulation algorithm is given to show how to calculate the credibilistic VaR. © 2009 IEEE.}}, 
pages = {96--100}, 
number = {NA}, 
volume = {6}
}
@article{10.1016/j.cam.2019.112468, 
year = {2020}, 
title = {{On the uncertainty of VaR of individual risk}}, 
author = {Cheung, K.C. and Yuen, F.L.}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2019.112468}, 
abstract = {{Value at risk (VaR) is a prevalent risk measure used in financial risk management. The calculation of VaR relies on the distribution of the potential loss position which is generally unknown in practice. In this article, we introduce a model of uncertainty for the distribution of a loss variable and investigate the effect on VaR using a worst scenario approach. The proposed model is flexible and can be applied to various types of distributions. The robust VaR and an associated worst scenario measure are identified. It is shown that the choice of the loss model is still important when there is an uncertainty model. © 2019 Elsevier B.V.}}, 
pages = {112468}, 
number = {NA}, 
volume = {367}
}
@article{10.1515/jtse-2017-0016, 
year = {2019}, 
title = {{Dynamic D-Vine Copula Model with Applications to Value-at-Risk (VaR)}}, 
author = {Tófoli, Paula V. and Ziegelmann, Flávio A. and Candido, Osvaldo and Pereira, Pedro L. Valls}, 
journal = {Journal of Time Series Econometrics}, 
issn = {21946507}, 
doi = {10.1515/jtse-2017-0016}, 
abstract = {{Vine copulas are multivariate dependence models constructed from pair-copulas (bivariate copulas). In this paper, we allow the dependence parameters of the pair-copulas in a D-vine decomposition to be potentially time-varying, following a restricted ARMA(1, m) process, in order to obtain a very flexible dependence model for applications to multivariate financial return data. We investigate the dependence among the broad stock market indexes from Germany (DAX), France (CAC 40), Britain (FTSE 100), the United States (S\&P 500) and Brazil (IBOVESPA) both in a crisis and in a non-crisis period. We find evidence of stronger dependence among the indexes in bear markets. Surprisingly, though, the dynamic D-vine copula indicates the occurrence of a sharp decrease in dependence between the indexes FTSE and CAC in the beginning of 2011, and also between CAC and DAX during mid-2011 and in the beginning of 2008, suggesting the absence of contagion in these cases. We evaluate the dynamic D-vine copula with respect to Value-at-Risk (VaR) forecasting accuracy in crisis periods. The dynamic D-vine outperforms the static D-vine in terms of predictive accuracy for our real data sets. We also investigate the dynamic D-vine copula in a simulation study and the overall results of the Monte Carlo experiments are quite favorable to the dynamic D-vine copula in comparison with a static D-vine copula. © 2019 Walter de Gruyter GmbH, Berlin/Boston.}}, 
pages = {20170016}, 
number = {2}, 
volume = {11}
}
@article{10.21314/jrmv.2015.135, 
year = {2015}, 
title = {{Backtesting Solvency II value-at-risk models using a rolling horizon}}, 
author = {Loois, Miriam}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2015.135}, 
abstract = {{Solvency II value-at-risk (VaR) models focus on a one-year horizon and a confidence interval of 0.5\%. To accurately backtest such models, a multiple of 200 years of historic data is necessary. Due to a lack of data, backtests are often performed using a rolling horizon. We investigate the effect of using this rolling horizon. We show that this leads to a significant increase in the probability of finding an extreme event. Not correcting for this effect can lead to false rejections ofVaR models. We illustrate this by analyzing the review of the equity stress parameter for Dutch pension funds. The review report states that the number of historic violations is too high and, therefore, the stress parameter is too low. We find that the number of historic violations can be explained by the use of a rolling window. We propose a step-by-step approach to backtest correctly using a rolling window. To our knowledge, this is the first time the effect of this commonly made error has been quantified. For realistic parameter values, the probability of finding an extreme event can increase by a factor of 7. Therefore, this effect is relevant and should be considered when evaluating VaR models. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {13--31}, 
number = {2}, 
volume = {9}
}
@article{10.1016/j.insmatheco.2005.03.002, 
year = {2005}, 
title = {{Bounds on the value-at-risk for the sum of possibly dependent risks}}, 
author = {Mesfioui, Mhamed and Quessy, Jean-François}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2005.03.002}, 
abstract = {{In this paper, explicit lower and upper bounds on the value-at-risk (VaR) for the sum of possibly dependent risks are derived when only partial information is available about the dependence structure and the individual behaviors. When the marginal distributions are known, a reformulation of a result of Embrechts et al. [Finan. Stoch. 7 (2003) 145-167] makes it possible, under some regularity conditions, to compute explicit bounds for the VaR under various dependence scenarios. In the case where only the means and the variances of the risks are available, explicit bounds are obtained from an optimization over all possible values of the correlation matrix associated with the vector of risks. Analytical and numerical investigations are presented in order to investigate the quality of these bounds. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {135--151}, 
number = {1 SPEC. ISS.}, 
volume = {37}
}
@article{10.1007/s10479-012-1297-9, 
year = {2015}, 
title = {{A parametric family of two ranked objects auctions: equilibria and associated risk}}, 
author = {Alonso, Estrella and Sanchez-Soriano, Joaquin and Tejada, Juan}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-012-1297-9}, 
abstract = {{This paper deals with simultaneous auctions of two commonly ranked objects following the model studied in Menezes and Monteiro (J. Real Estate Finance Econ., 17(3):219–232, 1998). For these problems we introduce a parametric family of auction mechanisms which includes the three classic auctions (discriminatory-price auction, uniform-price auction and Vickrey auction) and we call it the DUV family. We provide the unique Bayesian Nash equilibrium for each auction in DUV and prove a revenue equivalence theorem for the parametric family. Likewise, we study the value at risk of the auctioneer as a reasonable decision criterion to determine which auctions in DUV may be better taking into account the interests of the auctioneer. We show that there are auction mechanisms in DUV which are better than the classic auction mechanisms with respect to this criterion. © 2013, Springer Science+Business Media New York.}}, 
pages = {141--160}, 
number = {1}, 
volume = {225}
}
@article{10.3390/risks7040122, 
year = {2019}, 
title = {{On identifying the systemically important tunisian banks: An empirical approach based on the △covar measures}}, 
author = {Khiari, Wided and Sassi, Salim Ben}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks7040122}, 
abstract = {{The aim of this work is to assess systemic risk of Tunisian listed banks. The goal is to identify the institutions that contribute the most to systemic risk and that are most exposed to it. We use the CoVaR that considered the systemic risk as the value at risk (VaR) of a financial institution conditioned on the VaR of another institution. Thus, if the CoVaR increases with respect to the VaR, the spillover risk also increases among the institutions. The difference between these measurements is termed △CoVaR, and it allows for estimating the exposure and contribution of each bank to systemic risk. Results allow classifying Tunisian banks in terms of systemic risk involvement. They show that public banks occupy the top places, followed by the two largest private banks in Tunisia. These five banks are the main systemic players in the Tunisian banking sector. It seems that they are the least sensitive to the financial difficulties of existing banks and the most important contributors to the distress of the other banks. This work aims to add a broader perspective to the micro prudential application of regulation, including contagion, proposing a macro prudential vision and strengthening of regulatory policy. Supervisors could impose close supervision for institutions considered as potentially systemic banks. Furthermore, regulations should consider the systemic contribution when defining risk requirements to minimize the consequences of possible herd behavior. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {122}, 
number = {4}, 
volume = {7}
}
@article{10.1016/s1042-4431(02)00025-2, 
year = {2003}, 
title = {{Stress testing using VaR approach - A case for Asian currencies}}, 
author = {Tan, Kok-Hui and Chan, Inn-Leng}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/s1042-4431(02)00025-2}, 
abstract = {{This study examines if the assumption of normality is adequate for stress testing under the StressVaR framework. We formulate the StressVaR-x method to account for a fat-tailed distribution. Our results indicate that the StressVaR-x only outperforms the StressVaR marginally at the 99\% confidence level when applying the models to stress test a portfolio comprising of eight Asian currencies that have been under severe stress in the recent years. In fact, at the 95\% level, the StressVaR method performs better than the StressVaR-x method. Despite evidence of fat-tailed return distribution, we conclude that the normality assumption may still be appropriate in stress testing. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {39--55}, 
number = {1}, 
volume = {13}
}
@article{10.1016/j.irfa.2009.04.001, 
year = {2009}, 
title = {{The effectiveness of the VaR-based portfolio insurance strategy: An empirical analysis}}, 
author = {Jiang, Chonghui and Ma, Yongkai and An, Yunbi}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2009.04.001}, 
abstract = {{This paper proposes an approach to constructing the insured portfolios under the VaR-based portfolio insurance strategy (VBPI) and provides a comprehensive analysis of its hedging effectiveness in comparison with the buy-and-hold (B\&H) as well as the constant proportion portfolio insurance (CPPI) strategies in the context of the Chinese market. The results show that both of the insurance strategies are able to limit the downward returns while retaining certain upside returns, and their capabilities of reshaping the return distributions increase as the guarantee or the confidence level rises. In general, the VBPI strategy tends to outperform the CPPI strategy in terms of both the degree of downside protection and the return performance. © 2009 Elsevier Inc. All rights reserved.}}, 
pages = {185--197}, 
number = {4}, 
volume = {18}
}
@article{10.1016/j.mcm.2013.07.001, 
year = {2013}, 
title = {{Measuring the capital charge for operational risk of a bank with the large deviation approach}}, 
author = {Lu, Zhaoyang}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/j.mcm.2013.07.001}, 
abstract = {{In this paper, the large deviation approach for computing the capital charge for operational risk of a bank is explored. Firstly, the negatively-associated structure is utilized to measure the dependence between distinct operational loss cells. Secondly, the lower and upper bounds of the tail distribution function of total aggregated loss processes are determined. In addition, first order approximations using a value-at-risk measure are derived. Finally, an important example calculating the capital charge for operational risk under the class of a heavy-tailed distribution is provided. © 2013 Elsevier Ltd.}}, 
pages = {1634--1647}, 
number = {9-10}, 
volume = {58}
}
@article{10.1049/cp:20062073, 
year = {2006}, 
title = {{Generator allocating strategies in bilateral and day-ahead auction markets under price risk}}, 
author = {Zhang, Xian and Wang, Xifan and Wang, Jianxue and Chen, Haoyong}, 
journal = {7th IET International Conference on Advances in Power System Control, Operation and Management (APSCOM 2006)}, 
issn = {NA}, 
doi = {10.1049/cp:20062073}, 
abstract = {{Generators can sell energy in a bilateral market (BM) and a day-ahead auction market (DAM). Their intermediate-term allocation problem is modeled with utility function based upon both BM and DAM. Generator's utility function is composed of expected benefit and trading risk in the two markets. Value at risk is applied to measure the trading risk. Considering constraints of unit operation, the allocating problem can be formed as a bilevel problem and it is solved by genetic algorithm. Optimal allocating strategies for generators with distinct risk-preferences can be obtained through solving the problem across a rage of market conditions. Such models can be used to analyze the allocating strategies quantitatively and to provide assistance to generators with regard to decision-making. Numerical examples are presented based on market data from New England power market.}}, 
pages = {193--193}, 
number = {523 CP}, 
volume = {NA}
}
@article{10.1515/remav-2020-0019, 
year = {2020}, 
title = {{Protected collateral value: An approach to valuation of commercial properties for loan guarantees}}, 
author = {Florêncio, Lutemberg and Alencar, Claudio Tavares de}, 
journal = {Real Estate Management and Valuation}, 
issn = {23005289}, 
doi = {10.1515/remav-2020-0019}, 
abstract = {{The valuation of commercial properties for the guarantee of loans is extremely relevant for financial institutions, since it directly impacts the calculation of the loan to value ratio (LTV). However, despite the vast literature on the subject, the choice of methodological basis and the definition of the type of value to be employed are not pacified issues among researchers and practitioners. In this sense, the main objective of this paper is to present a methodological approach, as well as a suggestion of the type of value for the valuation of commercial properties bound collateral. The main methods and types of values related to the valuation of bound collateral commercial properties are presented below. Next, we propose a refinement of the income method, based on the concept of the value of the investment opportunity and under the principle of value at risk. Finally, we promote a case study with data from the Brazilian market to illustrate the application of the proposed approach. Based on the case study, it was evidenced that the valuation approach proposed in this paper, anchored in the potential of the income generation of the property, reduces the risk of exposure to banks' credit. © 2020 Sciendo. All rights reserved.}}, 
pages = {1--11}, 
number = {3}, 
volume = {28}
}
@article{10.1109/tpwrs.2010.2070882, 
year = {2011}, 
title = {{Short-term operation model and risk management for wind power penetrated system in electricity market}}, 
author = {Li, Xiaohu and Jiang, Chuanwen}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2010.2070882}, 
abstract = {{In view of the uncertainty and intermittency of wind power, this paper proposes an optimal economical dispatch (ED) model and develops a method to estimate risk and manage hybrid power systems (traditional + wind power systems) for the short-term (24 h) operations. The model and the method have taken into account the large wind power penetration and the wind variability. The particle swarm optimization (PSO) algorithm with constraints is applied to solve the ED problem. Value at risk (VaR) and integrated risk management (IRM) are used separately to assess the risk, so that an optimal tradeoff between the profit and risk is made for the system operations. The model and the method are tested on the standard IEEE 30-bus power system and network in Shanghai. The validity of the model and the method has been approved. © 2010 IEEE.}}, 
pages = {932--939}, 
number = {2}, 
volume = {26}
}
@article{10.1016/j.asoc.2018.01.024, 
year = {2018}, 
title = {{Flexible inverse adaptive fuzzy inference model to identify the evolution of operational value at risk for improving operational risk management}}, 
author = {Peña, Alejandro and Bonet, Isis and Lochmuller, Christian and Chiclana, Francisco and Góngora, Mario}, 
journal = {Applied Soft Computing}, 
issn = {15684946}, 
doi = {10.1016/j.asoc.2018.01.024}, 
abstract = {{Operational risk was one of the most important risks in the 2008 global financial crisis. This is due to limitations of the applied models in explaining and estimating this type of risk from highly qualitative information related to failures in the operations of financial organizations. A review of research literature on this area indicates an increase in the development of models for the estimation of the operational value at risk. However, there is a lack of models that use qualitative information for estimating this type of risk. Motivated by this finding, we propose a Flexible Inverse Adaptive Fuzzy Inference Model that integrates both a novel Montecarlo sampling method for the linguistic input variables of frequency and severity that allow the characterization of a risk event, the impact of risk management matrices to estimate the loss distribution and the associated operational value at risk. The methodology follows a loss distribution approach as defined by Basel II. A benefit of the proposed model is that it works with highly qualitative risk data and it also connects the risk measurement (operational value at risk) with risk management, based on risk management matrices. This way, we mitigate limitations related to a lack of available operational risk event data when assessing operational risk. We evaluate the experimental results obtained through the proposed model by using the Index of Agreement indicator. The results provide a flexible loss distribution under different risk profiles or risk management matrices that explain the evolution of operational risk in real time. © 2018 Elsevier B.V.}}, 
pages = {614--631}, 
number = {NA}, 
volume = {65}
}
@article{10.1016/j.ijepes.2017.01.017, 
year = {2017}, 
title = {{Probabilistic cost prediction for submarine power cable projects}}, 
author = {Schell, Kristen R. and Claro, João and Guikema, Seth D.}, 
journal = {International Journal of Electrical Power \& Energy Systems}, 
issn = {01420615}, 
doi = {10.1016/j.ijepes.2017.01.017}, 
abstract = {{It is estimated that Europe alone will need to add over 250,000 km of transmission capacity by 2050, if it is to meet renewable energy production goals while maintaining security of supply. Estimating the cost of new transmission infrastructure is difficult, but it is crucial to predict these costs as accurately as possible, given their importance to the energy transition. Transmission capacity expansion plans are often founded on optimistic projections of expansion costs. We present probabilistic predictive models of the cost of submarine power cables, which can be used by policymakers, industry, and academia to better approximate the true cost of transmission expansion plans. The models are both generalizable and well-specified for a variety of submarine applications, across a variety of regions. The best performing statistical learning model has slightly more predictive power than a simpler, linear econometric model. The specific decision context will determine whether the extra data gathering effort for the statistical learning model is worth the additional precision. A case study illustrates that incorporating the uncertainty associated with the cost prediction to calculate risk metrics - value-at-risk and conditional-value-at-risk - provides useful information to the decision-maker about cost variability and extremes. © 2017 Elsevier Ltd}}, 
pages = {1--9}, 
number = {NA}, 
volume = {90}
}
@article{10.1063/1.4912382, 
year = {2015}, 
title = {{A decision process with portfolios for value-at-risks}}, 
author = {Yoshida, Yuji}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.4912382}, 
abstract = {{A dynamic financial portfolio model with uncertainty is discussed. A dynamic value-at-risk portfolio method is proposed, and the risk criterion is composed by unexpected short-term risks which occur suddenly in each period. Analytical solutions for the VaR portfolio problem are obtained at each period. By dynamic programming approach, we derive an optimality condition for the optimal value-at-risk portfolio in a decision process. It is shown that a dynamic optimal value-at-risk is a solution of the optimality equation under a reasonable assumption, and an optimal trading strategy is obtained from the equation. © 2015 AIP Publishing LLC.}}, 
pages = {070008}, 
number = {NA}, 
volume = {1648}
}
@article{10.14254/2071-8330.2021/14-1/16, 
year = {2021}, 
title = {{Value at risk estimation of the SET50 index: Comparison between stock exchange of Thailand and Thailand futures exchange}}, 
author = {Jongadsayakul, Woradee}, 
journal = {Journal of International Studies}, 
issn = {20718330}, 
doi = {10.14254/2071-8330.2021/14-1/16}, 
abstract = {{Value at Risk (VaR) is the most widely used measure of risk. This study uses SET50 daily returns from the period from July 3, 2015 to December 27, 2019 to estimate VaR for the assessment of risk exposure at the Stock Exchange of Thailand and Thailand Futures Exchange using the three following methods: Non-parametric method with the historical simulation approach, parametric method with GARCH family models, and semi-parametric method with volatility-weight historical simulation of the GARCH family models. Accuracy of the estimated models is also assessed by performing the VaR backtests of unconditional coverage, independence, and conditional coverage. In forecasting VaR with the confidence level of 95\%, historical simulation and asymmetric GARCH models (TARCH and EGARCH models) give solid results and outrank volatility weight historical simulation. Moreover, a comparison of stock investments with a correlation to the performance of SET50 Index and SET50 Index Futures investment indicates that SET50 Index Futures investment carries higher risk. Therefore, investment decisions on SET50 Index Futures should be taken more carefully since this market is more volatile than the underlying spot market. © Foundation of International Studies, 2021.}}, 
pages = {227--240}, 
number = {1}, 
volume = {14}
}
@article{10.1016/j.eswa.2009.03.007, 
year = {2009}, 
title = {{Pricing fuzzy vulnerable options and risk management}}, 
author = {Liu, Yu-hong}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2009.03.007}, 
abstract = {{The assumption of unrealistic "identical rationality" in classic option pricing theory is released in this article to amend Klein's [Klein, P. (1996). Pricing Black-Scholes options with correlated credit risk. Journal of Banking Finance, 1211-1129] vulnerable option pricing formula. Through this formula, default risk and liquidity risk are both well-explained when the investment behaviors and market expectations of the participants are heterogeneous. The numerical results show that when the investing decisions of each market participant come from their individual rationality and use their own subjective price to trade, the option price becomes a boundary. The upper boundary becomes an absolutely safe line and the lower boundary becomes an absolutely unsafe line for investors who want to invest in some financial securities with default risk. The proposed model suggests a more realistic pricing mechanism for the issuers and traders who want to value options with default risk. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {12188--12199}, 
number = {10}, 
volume = {36}
}
@article{10.1080/03461238.2011.558186, 
year = {2013}, 
title = {{Bounds for sums of random variables when the marginal distributions and the variance of the sum are given}}, 
author = {Cheung, Ka Chun and Vanduffel, Steven}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2011.558186}, 
abstract = {{In this paper, we establish several relations between convex order, variance order, and comonotonicity. In the first part, we extend Cheung (2008b) to show that when the marginal distributions are fixed, a sum with maximal variance is in fact a comonotonic sum. Thus the convex upper bound is achieved if and only if the marginal variables are comonotonic. Next, we study the situation where besides the marginal distributions; the variance of the sum is also fixed. Intuitively one expects that adding this information may lead to a bound that is sharper than the comonotonic upper bound. However, we show that such upper bound does not even exist. Nevertheless, we can still identify a special dependence structure known as upper comonotonicity, in which case the sum behaves like a convex largest sum in the upper tail. Finally, we investigate when the convex order is equivalent to the weaker variance order. Throughout this paper, interpretations and significance of the results in terms of portfolio risks will be emphasized. © 2013 Copyright Taylor and Francis Group, LLC.}}, 
pages = {103--118}, 
number = {2}, 
volume = {NA}
}
@article{10.1007/s11356-020-10320-2, 
year = {2020}, 
title = {{The spillover effects of China’s regional environmental markets to local listed firms: a risk Granger causality approach}}, 
author = {Zhu, Shujin and Tang, Yiding and Qiao, Xingzhi and Peng, Cheng and Li, Dan}, 
journal = {Environmental Science and Pollution Research}, 
issn = {09441344}, 
doi = {10.1007/s11356-020-10320-2}, 
pmid = {32761345}, 
abstract = {{Using the quantile GARCH model estimators to gauge the bidirectional risk magnitude and the Granger causality test in risk distributions to detect the existence of risk spillovers, this paper explores the extreme risk spillovers of China’s regional carbon markets to local listed firm’s stock returns. From the perspectives of macro region level and micro firm level, the findings are outlined as follows. First, among the top three active carbon trading pilots (Hubei, Guangdong, and Shenzhen), Hubei pilot exhibits significant “low risk and high profit” features. Second, the predominant risk spillover effects to local listed firms are heterogeneous across pilots. Specifically, Hubei pilot is dominated by “up-to-down” effect, and Guangdong pilot is dominated by “down-to-down” effect, while Shenzhen pilot has no predominant effect. The heterogeneous risk spillover performance may be caused by the regional divergence in economic development, industry structure, and cap setting concerning each pilot. Third, the risk transmission performance from carbon allowance price to local listed firm’s stock returns depends on the firm’s belonging sector. That is, environment-related firms, either environment-friendly firms or pollution-intensive firms, are more susceptible to carbon markets’ risks compared with environment-unrelated firms. This paper supplies novel information on the risk transmission from carbon markets to local economic entities, which proves valuable not only for firms to improve risk aversion ability but also for policy-makers to perfect carbon markets’ mechanism. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {44123--44136}, 
number = {35}, 
volume = {27}
}
@article{10.1007/978-3-319-55236-1_8, 
year = {2017}, 
title = {{Risk modeling in optimization problems via value at risk, conditional value at risk, and its robustification}}, 
author = {Guloglu, Zeynep Cobandag and Weber, Gerhard Wilhelm}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-55236-1\_8}, 
abstract = {{In this chapter, we explore the portfolio selection problem involving uncertainty, in other words: risk. To deal with this uncertainty, we will utilize Value at Risk (VaR) and Conditional Value at Risk (CVaR). Moreover, we present a Robust Optimization method for specifying the parameter uncertainty while minimizing the Conditional Value at Risk. We investigate optimization problems in order to minimize CVaR. Our approach consists in the use of robust optimization techniques for minimization of CVaR. We research Robust CVaR (RCVaR) optimization models under ellipsoidal uncertainty. Finally, we conclude that one can control the parameteric uncertainty with some robust distribution assumptions and obtain certain optimal solutions. © 2017, Springer International Publishing AG.}}, 
pages = {133--145}, 
number = {NA}, 
volume = {195}
}
@article{10.1504/ijcee.2018.091037, 
year = {2018}, 
title = {{The model confidence set package for R}}, 
author = {Bernardi, Mauro and Catania, Leopoldo}, 
journal = {International Journal of Computational Economics and Econometrics}, 
issn = {17571170}, 
doi = {10.1504/ijcee.2018.091037}, 
abstract = {{This paper presents the R package MCS which implements the model confidence set (MCS) procedure for model comparison. The MCS procedure consists on a sequence of tests which permits to build a set of 'superior' models, where the null hypothesis of equal predictive ability (EPA) is not rejected at a certain confidence level. The EPA statistic test is calculated for an arbitrary loss function, meaning that we could test models on various aspects, such as for example, punctual forecasts and density evaluation. The relevance of the package is shown using an example which aims at illustrating in details the use of the provided functions. The example compares the ability of different models belonging to the generalised autoregressive conditional heteroscedasticity (GARCH) family to predict large financial losses. Codes for reproducibility purposes are also reported. Copyright © 2018 Inderscience Enterprises Ltd.}}, 
pages = {144}, 
number = {2}, 
volume = {8}
}
@article{10.1016/j.frl.2015.12.006, 
year = {2016}, 
title = {{A parsimonious quantile regression model to forecast day-ahead value-at-risk}}, 
author = {Haugom, Erik and Ray, Rina and Ullrich, Carl J. and Veka, Steinar and Westgaard, Sjur}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2015.12.006}, 
abstract = {{This paper proposes a parsimonious quantile regression model for forecasting Value-at-Risk. The model uses only observable measures of daily, weekly, and monthly volatility as input and thus simplifies optimization substantially compared with other methods proposed in the literature. The framework also provides a new way of illustrating the volatility effects of a heterogeneous market. When subjected to formal coverage tests for out-of-sample VaR predictions, model performance is similar to more complicated models. © 2015 Elsevier Ltd.}}, 
pages = {196--207}, 
number = {NA}, 
volume = {16}
}
@article{10.1142/s0217595911003053, 
year = {2011}, 
title = {{Risk management under a factor stochastic volatility model}}, 
author = {ESCOBAR, MARCOS and OLIVARES, PABLO}, 
journal = {Asia-Pacific Journal of Operational Research}, 
issn = {02175959}, 
doi = {10.1142/s0217595911003053}, 
abstract = {{In this paper, we study risk measures and portfolio problems based on a Stochastic Volatility Factor Model (SVFM). We analyze the sensitivity of Value at Risk (VaR) and Expected Shortfall (ES) to the changes in the parameters of the model. We compare the positions of a linear portfolio under assets following a SVFM, a BlackScholes Model and a model with constant dependence structure. We consider an application to a portfolio of three selected Asian funds. © 2011 World Scientific Publishing Co. \& Operational Research Society of Singapore.}}, 
pages = {65--80}, 
number = {1}, 
volume = {28}
}
@article{10.1007/s42519-020-00092-w, 
year = {2020}, 
title = {{Pulled-to-Par Returns for Zero-Coupon Bonds Historical Simulation Value at Risk}}, 
author = {Sousa, J. Beleza and Esquível, Manuel L. and Gaspar, Raquel M.}, 
journal = {Journal of Statistical Theory and Practice}, 
issn = {15598608}, 
doi = {10.1007/s42519-020-00092-w}, 
abstract = {{Due to bond prices pull-to-par, zero-coupon bond historical returns are not stationary, as they tend to zero as time to maturity approaches. Given that the historical simulation method for computing value at risk (VaR) requires a stationary sequence of historical returns, zero-coupon bonds’ historical returns cannot be used to compute VaR by historical simulation. Their use would systematically overestimate VaR, resulting in invalid VaR sequences. In this paper, we propose an adjustment of zero-coupon bonds’ historical returns. We call the adjusted returns “pulled-to-par” returns. We prove that when the zero-coupon bonds’ continuously compounded yields-to-maturity are stationary, the adjusted pulled-to-par returns allow VaR computation by historical simulation. We firstly illustrate the VaR computation in a simulation scenario, and then, we apply it to real data on eurozone STRIPS. © 2020, Grace Scientific Publishing.}}, 
pages = {30}, 
number = {2}, 
volume = {14}
}
@article{10.1002/for.2731, 
year = {2021}, 
title = {{Research constituents, intellectual structure, and collaboration pattern in the Journal of Forecasting: A bibliometric analysis}}, 
author = {Baker, H. Kent and Kumar, Satish and Pattnaik, Debidutta}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2731}, 
abstract = {{This study provides a retrospective of the Journal of Forecasting (JoF) between 1982 and 2019. Evidence shows a considerable increase in JoF's productivity and influence since 1982. For example, a JoF article, on average, receives more than 18 citations, and contributions from 65 nations appear in the journal. All articles have a forecasting implication, but JoF encourages a wide range of contexts recently featuring value-at-risk and financial econometrics. Factors with a positive association with JoF citations include the number of references, article length, article age, special issue, and number of authors. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {577--602}, 
number = {4}, 
volume = {40}
}
@article{10.1016/j.jbankfin.2017.11.017, 
year = {2018}, 
title = {{Option-implied objective measures of market risk}}, 
author = {Leiss, Matthias and Nax, Heinrich H.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2017.11.017}, 
abstract = {{Foster and Hart (2009) introduce an objective measure of the riskiness of an asset that implies a bound on how much of one's wealth is ‘safe’ to invest in the asset while (a.s.) guaranteeing no-bankruptcy. In this study, we translate the Foster–Hart measure from static and abstract gambles to dynamic and applied finance using nonparametric estimation of risk-neutral densities from S\&P 500 call and put option prices covering 2003–2013. The dynamics of the resulting ‘option-implied Foster–Hart bound’ are assessed in light of other well-known option-implied risk measures including value at risk, expected shortfall and risk-neutral volatility, as well as high moments of the densities and several industry measures. Rigorous variable selection reveals that the new measure is a significant predictor of (large) ahead-return downturns. Furthermore, it grasps more characteristics of the risk-neutral probability distributions in terms of moments than other measures and exhibits predictive consistency. The robustness of the risk-neutral density estimation is analyzed via Monte Carlo methods. © 2017 Elsevier B.V.}}, 
pages = {241--249}, 
number = {NA}, 
volume = {88}
}
@article{10.1016/j.orl.2010.05.003, 
year = {2010}, 
title = {{Internal vs. external risk measures: How capital requirements differ in practice}}, 
author = {Eling, Martin and Tibiletti, Luisa}, 
journal = {Operations Research Letters}, 
issn = {01676377}, 
doi = {10.1016/j.orl.2010.05.003}, 
abstract = {{We compare capital requirements derived from tail conditional expectation (TCE) with those derived from the tail conditional median (TCM). In theory, TCE is higher than TCM for most distributions commonly used in finance and at fixed confidence levels; however, we find that in empirical data, there is no clear-cut relationship between the two. Our results highlight the relevance of TCM as a robust alternative to TCE, especially for regulatory control. © 2010 Elsevier B.V. All rights reserved.}}, 
pages = {482--488}, 
number = {5}, 
volume = {38}
}
@article{10.1142/s0219024910005759, 
year = {2010}, 
title = {{Local estimation of dynamic copula models}}, 
author = {MENDES, BEATRIZ V M and MELO, EDUARDO F L DE}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024910005759}, 
abstract = {{It has been empirically verified that the strength of dependence in stock markets usually rises with volatility. In this paper we exploit this stylized fact combined with local maximum likelihood estimation of copula models to analyze the dynamic joint behavior of series of financial log returns. Explanatory variables based on the estimated GARCH volatilities are considered as potential regressors for explaining the dynamics in the copula parameters. The proposed model can assess and discriminate how much of the strength of dependence is due just to the time-varying volatility. The final local-parametric estimates may be used to compute risk measures, to simulate portfolio behavior, and so on. We illustrate our methods using two American indexes. Results indicate that volatility does affect the strength of dependence. The in-sample Value-at-Risk based on the dynamic model outperforms those based on the empirical estimates. © 2010 World Scientific Publishing Company.}}, 
pages = {241--258}, 
number = {2}, 
volume = {13}
}
@article{10.1504/ijmfa.2013.052407, 
year = {2013}, 
title = {{The impact of global financial crisis on the dependence structure of equity markets and on risk management}}, 
author = {Ghorbel, Ahmed and Trabelsi, Abdelwahed}, 
journal = {International Journal of Managerial and Financial Accounting}, 
issn = {17536715}, 
doi = {10.1504/ijmfa.2013.052407}, 
abstract = {{In this work, we use a time varying copula model to investigate the impact of the global financial crisis on dependence between US and each of six major stock markets and on risk management strategies. The model is implemented with an AR-GARCH-t for the marginal distribution and the extreme value copula for the joint distribution, which allow taking into account non-linear dependence, tails behaviour and their development over time. We investigate whether there are significant changes in the time-varying dependence structure of market and in VaR and ES measures especially during global financial crises period. Empirical results show that market dependences between US, European and Brazilian markets tend to increase considerably during crisis period and this increase started around the beginning of 2008. In the other hand, market volatility registered record levels around the end of 2008 due to the increase of the degree of uncertainty in this period. As a consequence, investors will allow more amounts to cover against negative evolution of portfolio value. © 2013 Inderscience Enterprises Ltd.}}, 
pages = {1}, 
number = {1}, 
volume = {5}
}
@article{10.1080/03461238.2017.1350203, 
year = {2018}, 
title = {{Conditional risk measures in a bipartite market structure}}, 
author = {Kley, Oliver and Klüppelberg, Claudia and Reinert, Gesine}, 
journal = {Scandinavian Actuarial Journal}, 
issn = {03461238}, 
doi = {10.1080/03461238.2017.1350203}, 
abstract = {{In this paper, we study the effect of network structure between agents and objects on measures for systemic risk. We model the influence of sharing large exogeneous losses to the financial or (re)insurance market by a bipartite graph. Using Pareto-tailed losses and multivariate regular variation, we obtain asymptotic results for conditional risk measures based on the Value-at-Risk and the Conditional Tail Expectation. These results allow us to assess the influence of an individual institution on the systemic or market risk and vice versa through a collection of conditional risk measures. For large markets, Poisson approximations of the relevant constants are provided. Differences of the conditional risk measures for an underlying homogeneous and inhomogeneous random graph are illustrated by simulations. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--28}, 
number = {4}, 
volume = {2018}
}
@article{10.1109/icmb.2008.11, 
year = {2008}, 
title = {{Mobile service affordability for the needy, addiction, and ICT policy implications}}, 
author = {Pau, L-F}, 
journal = {2008 7th International Conference on Mobile Business}, 
issn = {NA}, 
doi = {10.1109/icmb.2008.11}, 
abstract = {{This paper links communications and media usage to social and household economics boundaries. It highlights that in present day society, communications and media are a necessity, but not always affordable, and that they furthermore open up for addictive behaviours which raise additional financial and social risks. A simple and efficient methodology compatible with state-of-the-art social and communications business statistics is developed, which produces the residual communications and media affordability budget and ultimately the value-at-risk in terms of usage and tariffs. Sensitivity analysis provides precious information on communications and media adoption on the basis of affordability. Case data are surveyed from various countries. ICT policy recommendations are made to support widespread and responsible communications access. © 2008 IEEE.}}, 
pages = {41--48}, 
number = {NA}, 
volume = {NA}
}
@article{10.1080/00036846.2011.566198, 
year = {2012}, 
title = {{Estimating value-at-risk under a Heath-Jarrow-Morton framework with jump}}, 
author = {Ze-To, Samuel Yau Man}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2011.566198}, 
abstract = {{This article proposes a new methodology for measuring Value-at-Risk (hereafter VaR) using a model that incorporates both volatility and jumps. Heath-Jarrow-Morton (HJM) model has been used for the valuation of interest rate derivatives. This study extends the use of HJM model to the estimation VaR. This article specifically uses a two-factor HJM jumpdiffusion model for the computation. The study models the Eurodollar futures prices using its derivatives. In addition, this article uses a new volatility specification of Ze-To (2002) to construct the HJM dynamics. The result indicates that the VaR model using HJM jump-diffusion framework performs well in capturing the nonnormality and in providing accurate VaR forecasts in the in-sample and out-sample tests. © 2012 Taylor \& Francis.}}, 
pages = {2729--2741}, 
number = {21}, 
volume = {44}
}
@article{10.1016/j.najef.2017.06.002, 
year = {2017}, 
title = {{Moments expansion densities for quantifying financial risk}}, 
author = {Ñíguez, Trino-Manuel and Perote, Javier}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2017.06.002}, 
abstract = {{We propose a novel semi-nonparametric distribution that is feasibly parameterized to represent the non-Gaussianities of the asset return distributions. Our Moments Expansion (ME) density presents gains in simplicity attributable to its innovative polynomials, which are defined by the difference between the nth power of the random variable and the nth moment of the density used as the basis. We show that the Gram–Charlier distribution is a particular case of the ME-type of densities. The latter being more tractable and easier to implement when quadratic transformations are used to ensure positiveness. In an empirical application to asset returns, the ME model outperforms both standard and non-Gaussian GARCH models along several risk forecasting dimensions. © 2017 Elsevier Inc.}}, 
pages = {53--69}, 
number = {NA}, 
volume = {42}
}
@article{10.1016/s0261-5606(99)00040-6, 
year = {1999}, 
title = {{Capturing downside risk in financial markets: The case of the Asian crisis}}, 
author = {Pownall, Rachel A.J. and Koedijk, Kees G.}, 
journal = {Journal of International Money and Finance}, 
issn = {02615606}, 
doi = {10.1016/s0261-5606(99)00040-6}, 
abstract = {{Using data on Asian equity markets, we observe that during periods of financial turmoil, deviations from the mean-variance framework become more severe, resulting in periods with additional downside risk to investors. Current risk management techniques failing to take this additional downside risk into account will underestimate the true Value-at-Risk with greater severity during periods of financial turnoil. We provide a conditional approach to the Value-at-Risk methodology, known as conditional VaR-x, which to capture the time variation of non-normalities allows for additional tail fatness in the distribution of expected returns. These conditional VaR-x estimates are then compared to those based on the RiskMetrics™ methodology from J.P. Morgan, where we find that the model provides improved forecasts of the Value-at-Risk. We are therefore able to show that our conditional VaR-x estimates are better able to capture the nature of downside risk, particularly crucial in times of financial crises. © 1999 Elsevier Science Ltd. All rights reserved.}}, 
pages = {853--870}, 
number = {6}, 
volume = {18}
}
@article{10.1080/14697688.2014.942230, 
year = {2015}, 
title = {{Two-step methods in VaR prediction and the importance of fat tails}}, 
author = {Ergen, Ibrahim}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2014.942230}, 
abstract = {{This paper proposes a two-step methodology for Value-at-Risk prediction. The first step involves estimation of a GARCH model using quasi-maximum likelihood estimation and the second step uses model filtered returns with the skewed t distribution of Azzalini and Capitanio [J. R. Stat. Soc. B, 2003, 65, 367–389]. The predictive performance of this method is compared to the single-step joint estimation of the same data generating process, to the well-known GARCH-Evt model and to a comprehensive set of other market risk models. Backtesting results show that the proposed two-step method outperforms most benchmarks including the classical joint estimation method of same data generating process and it performs competitively with respect to the GARCH-Evt model. This paper recommends two robust models to risk managers of emerging market stock portfolios. Both models are estimated in two steps: the GJR-GARCH-Evt model and the two-step GARCH-St model proposed in this study. © 2014 Taylor \& Francis.}}, 
pages = {1013--1030}, 
number = {6}, 
volume = {15}
}
@article{10.1016/j.insmatheco.2008.10.006, 
year = {2009}, 
title = {{Worst VaR scenarios: A remark}}, 
author = {Laeven, Roger J.A.}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2008.10.006}, 
abstract = {{Theorem 15 of Embrechts et al. [Embrechts, Paul, Höing, Andrea, Puccetti, Giovanni, 2005. Worst VaR scenarios. Insurance: Math. Econom. 37, 115-134] proves that comonotonicity gives rise to the on-average-most-adverse Value-at-Risk scenario for a function of dependent risks, when the marginal distributions are known but the dependence structure between the risks is unknown. This note extends this result to the case where, rather than no information, partial information is available on the dependence structure between the risks. A result of Kaas et al. [Kaas, Rob, Dhaene, Jan, Goovaerts, Marc J., 2000. Upper and lower bounds for sums of random variables. Insurance: Math. Econom. 23, 151-168] is also generalized for this purpose. © 2008 Elsevier B.V. All rights reserved.}}, 
pages = {159--163}, 
number = {2}, 
volume = {44}
}
@article{10.1080/07421222.2004.11045799, 
year = {2004}, 
title = {{Information exploitation and interorganizational systems ownership}}, 
author = {HAN, KUNSOO and KAUFFMAN, ROBERT J. and NAULT, RRIE R.}, 
journal = {Journal of Management Information Systems}, 
issn = {07421222}, 
doi = {10.1080/07421222.2004.11045799}, 
abstract = {{We develop a model based on the theory of incomplete contracts for how ownership structure of interorganizational systems (IOS) can affect information exploitation and information technology adoption. Our model yields several propositions that suggest the appropriate strategic actions that a firm may take when there is potential for IOS adopters to question whether adopting the IOS will be value-maximizing. We analyze and illustrate the related strategic thinking in a real-world context involving a financial risk management IOS. We present a case study of the ownership and spin-off of RiskMetrics; developed by New York City-based investment bank, J.P. Morgan, in the late 1980s. The firm first gave RiskMetrics to its correspondent banking, treasury, and investment clients for free, in the context of its clearing account relationship services. Later, the bank spun off the product to an independent company that offered fee-based services. We model the bank's clients in terms of their heterogeneous portfolio risks, and their effects on the value a client can gain from adopting the technology. We also examine the value they may lose if their private portfolio risk information is exploited. A key roadblock to the adoption of the free service may have been the potential for strategic information exploitation by the service provider. When Morgan spun off RiskMetrics with multiparty ownership, wider adoption occurred. Our theory interprets this strategic move as an appropriate means to maximize long-term profits when information exploitation may occur. © 2004 M.E. Sharpe, Inc.}}, 
pages = {109--135}, 
number = {2}, 
volume = {21}
}
@article{10.3923/jas.2013.974.983, 
year = {2013}, 
title = {{Value-at-risk and conditional value-at-risk assessment and accuracy compliance in dynamic of Malaysian industries}}, 
author = {Dargiri, M.N. and Shamsabadi, H.A. and Thim, C.K. and Rasiah, D. and Sayedy, B.}, 
journal = {Journal of Applied Sciences}, 
issn = {18125654}, 
doi = {10.3923/jas.2013.974.983}, 
abstract = {{Risk management becomes increasingly crucial for financial institutions in competitive market today. Value-at-risk (VaR) and Conditional Value-at-risk (CVaR) methods have taken important places in risk management field as recognized by Basel Committee on Banking Supervision (BCBS, 2012). While VaR measures the maximum loss in a given confidence level and period, CVaR gauges the amount of loss exceeding VaR in a given confidence level. This study attempts to describe and compare VaR and CVaR methods within Malaysian industries using both parametric and non-parametric approaches. Moreover, researcher measures the accuracy of predicted VaR and CVaR by applying "Backtesting" technique. To this regards, results revealed that VaR always tends to underestimate the risk, while CVaR models tend to overestimate the risk in most of the cases. The results also indicated Technology industry with the highest risk, while Consumer Product industry had the lowest one. All in all, the choice of picking the right risk model is highly depend on the preference of institutions in Malaysia. © 2013 Asian Network for Scientific Information.}}, 
pages = {974--983}, 
number = {7}, 
volume = {13}
}
@article{10.1016/j.physa.2005.10.017, 
year = {2006}, 
title = {{Long memory in stock index futures markets: A value-at-risk approach}}, 
author = {Tang, Ta-Lun and Shieh, Shwu-Jane}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2005.10.017}, 
abstract = {{In this paper, we investigate the long memory properties for closing prices of three stock index futures markets. The FIGARCH (1, d, 1) and HYGARCH (1, d, 1) models with normal, Student-t, and skewed Student-t distributions for S\&P500, Nasdaq100, and Dow Jones daily prices are estimated first. Then the value-at-risks are calculated by the estimated models. The empirical results show that for the three stock index futures, the HYGARCH (1, d, 1) models with skewed Student-t distribution perform better based on the Kupiec LR tests. In particular, for the S\&P500 and Nasdag 100 futures prices. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {437--448}, 
number = {NA}, 
volume = {366}
}
@article{10.1017/s0515036100013313, 
year = {2003}, 
title = {{Asymptotic Value-at-Risk Estimates for Sums of Dependent Random Variables}}, 
author = {Wüthrich, Mario V.}, 
journal = {ASTIN Bulletin: The Journal of the IAA}, 
issn = {05150361}, 
doi = {10.1017/s0515036100013313}, 
abstract = {{We estimate Value-at-Risk for sums of dependent random variables. We model multivariate dependent random variables using archimedean copulas. This structure allows one to calculate the asymptotic behaviour of extremal events. An important application of such results are Value-at-Risk estimates for sums of dependent random variables. © 2003, ASTIN Bulletin. All rights reserved.}}, 
pages = {75--92}, 
number = {1}, 
volume = {33}
}
@article{10.1007/s00199-007-0238-3, 
year = {2008}, 
title = {{Equilibrium asset pricing with systemic risk}}, 
author = {Daníelsson, Jón and Zigrand, Jean-Pierre}, 
journal = {Economic Theory}, 
issn = {09382259}, 
doi = {10.1007/s00199-007-0238-3}, 
abstract = {{We provide an equilibrium multi-asset pricing model with micro- founded systemic risk and heterogeneous investors. Systemic risk arises due to excessive leverage and risk taking induced by free-riding externalities. Global risk-sensitive financial regulations are introduced with a view of tackling systemic risk, with Value-at-Risk a key component. The model suggests that risk-sensitive regulation can lower systemic risk in equilibrium, at the expense of poor risk-sharing, an increase in risk premia, higher and asymmetric asset volatility, lower liquidity, more comovement in prices, and the chance that markets may not clear. © 2007 Springer-Verlag.}}, 
pages = {293--319}, 
number = {2}, 
volume = {35}
}
@article{10.1111/ajfs.12098, 
year = {2015}, 
title = {{Nonparametric Factor Analytic Risk Measurement in Common Stocks in Financial Firms: Evidence from Korean Firms}}, 
author = {Baek, Seungho and Cursio, Joseph D. and Cha, Seung Y.}, 
journal = {Asia‐Pacific Journal of Financial Studies}, 
issn = {20419945}, 
doi = {10.1111/ajfs.12098}, 
abstract = {{This research examines the efficiency of nonparametric factor analytic approaches in measuring risk in common stocks of Korean financial firms from the risk-management perspective. This paper shows that using only one risk factor extracted from principal component analysis, the parallel shift or market movement factor, has sufficient accuracy for downside risk measures. We assess accuracy by applying Monte Carlo simulation to obtain VaR and ES for the Korean financial sector and industries within the financial sector (banks, insurance companies, and investment andsecurity trading companies), and further estimate the risk contagious effect on financial firms. © 2015 Korean Securities Association}}, 
pages = {497--536}, 
number = {4}, 
volume = {44}
}
@article{10.1109/naps.2005.1560584, 
year = {2005}, 
title = {{Generation expansion under risk using stochastic programming}}, 
author = {Alvarez, L. Juan and Ponnambalam, Kumaraswamy and Quintana, Victor H.}, 
journal = {Proceedings of the 37th Annual North American Power Symposium, 2005}, 
issn = {NA}, 
doi = {10.1109/naps.2005.1560584}, 
abstract = {{In this work, the problem of power plant expansion for electricity generation under risk from demand uncertainty and supply is addressed. We begin with a deterministic model. Then, this model is expanded to a stochastic model by means of considering the various demands for different operation modes as random. After this model is analyzed, a way of quantifying risk using the value at risk methodology (VaR) is proposed. The last model presented is such that randomness in the availability factors is considered. The concepts of Expected Value of Perfect Information (EVPI) and Value of Stochastic Solution (VSS) are also studied. The models presented are based on an extended form of the well known stochastic programming and chance constrained programming.}}, 
pages = {530--537}, 
number = {NA}, 
volume = {2005}
}
@article{10.1007/s00181-016-1146-9, 
year = {2017}, 
title = {{Time-varying copula models in the shipping derivatives market}}, 
author = {Shi, Wenming and Li, Kevin X. and Yang, Zhongzhi and Wang, Ganggang}, 
journal = {Empirical Economics}, 
issn = {03777332}, 
doi = {10.1007/s00181-016-1146-9}, 
abstract = {{In this paper, we provide an alternative hedging method based on a popular risk indicator relating to value at risk (VaR) for shipowners to hedge spot freight rate volatility in the tanker market. To achieve this, we use a univariate generalized autoregressive conditional heteroskedasticity model to capture the volatility characteristics of freight derivative returns and apply time-varying copula models to describe the nonlinear dependence between returns of spot and freight derivatives. Using quotes of spot freight rate and forward freight agreement (FFA) in the tanker market from January 3, 2006 to December 23, 2011, we derive the minimum VaR hedge ratios. Our main findings are as follows: First, we found significant evidence for the presence of volatility persistence in freight rate returns. Second, for dependence, we suggested that a time-varying t-copula performs best in describing how returns of spot freight rates relate to 1-month FFA returns, whereas a time-varying Gumbel copula performs much better for the description of nonlinear dependence between returns of spot freight rates and 2 and 3-month FFA returns. Third, the derived hedge ratios are associated with shipowners’ risk preferences and freight rate dynamics, which have important implications for shipowners in determining the optimal number of FFA contracts. The results provide some insights into the modeling of freight derivatives for risk management. © 2016, Springer-Verlag Berlin Heidelberg.}}, 
pages = {1039--1058}, 
number = {3}, 
volume = {53}
}
@article{10.1007/s11067-009-9109-8, 
year = {2012}, 
title = {{Risk in Transport Investments}}, 
author = {Palma, André de and Picard, Nathalie and Andrieu, Laetitia}, 
journal = {Networks and Spatial Economics}, 
issn = {1566113X}, 
doi = {10.1007/s11067-009-9109-8}, 
abstract = {{We discuss how the standard Cost-Benefit Analysis should be modified in order to take risk (and uncertainty) into account. We propose different approaches used in finance (Value at Risk, Conditional Value at Risk, Downside Risk Measures, and Efficiency Ratio) as useful tools to model the impact of risk in project evaluation. After introducing the concepts, we show how they could be used in CBA and provide some simple examples to illustrate how such concepts can be applied to evaluate the desirability of a new project infrastructure. © 2009 Springer Science+Business Media, LLC.}}, 
pages = {187--204}, 
number = {2}, 
volume = {12}
}
@article{10.1016/j.iref.2017.07.032, 
year = {2018}, 
title = {{Analyzing time–frequency co-movements across gold and oil prices with BRICS stock markets: A VaR based on wavelet approach}}, 
author = {Mensi, Walid and Hkiri, Besma and Al-Yahyaee, Khamis H. and Kang, Sang Hoon}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2017.07.032}, 
abstract = {{This paper examines the co-movements between five of the most important emerging stock markets namely the BRICS (Brazil, Russia, India, China and South Africa) and both the crude oil prices [West Texas Intermediate (WTI) and Europe Brent] and gold prices which are relevant to those commodity exporters and voracious consumers. Our results based on the wavelet approach show that BRICS index returns co-move with the WTI crude oil price at low frequencies (long horizons). Moreover, the strong level of co-movement is particularly captured during the onset of the global financial crisis. On the other hand, we find no evidence of co-movement between the BRICS stock markets and the gold price, indicating that gold can act as a hedge or a safe haven asset for the BRICS against extreme market movements. The implications of these results for the BRICS-commodity portfolios show that the portfolio risk (measured by the Value at Risk) is affected by the co-movements between stock and oil markets. © 2017 Elsevier Inc.}}, 
pages = {74--102}, 
number = {NA}, 
volume = {54}
}
@article{10.1007/978-3-319-09114-3_3, 
year = {2015}, 
title = {{Model risk in incomplete Markets with jumps}}, 
author = {Detering, Nils and Packham, Natalie}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-319-09114-3\_3}, 
abstract = {{We are concerned with determining the model risk of contingent claims when markets are incomplete. Contrary to existing measures of model risk, typically based on price discrepancies between models, we develop value-at-risk and expected shortfall measures based on realized P\&L from model risk, resp. model risk and some residual market risk. This is motivated, e.g., by financial regulators’ plans to introduce extra capital charges for model risk. In an incomplete market setting, we also investigate the question of hedge quality when using hedging strategies from a (deliberately) misspecified model, for example, because the misspecified model is a simplified model where hedges are easily determined. An application to energy markets demonstrates the degree of model error. © The Author(s) 2015.}}, 
pages = {39--56}, 
number = {NA}, 
volume = {99}
}
@article{10.1016/b978-0-12-411549-1.00018-1, 
year = {2014}, 
title = {{Tactical Risk Analysis in Emerging Markets in the Wake of the Credit Crunch and Ensuing Sub-prime Financial Crisis}}, 
author = {Janabi, Mazin A.M. Al}, 
journal = {Part 1: COUNTRY-SPECIFIC EXPERIENCES}, 
issn = {NA}, 
doi = {10.1016/b978-0-12-411549-1.00018-1}, 
abstract = {{The purpose of this chapter is to fill a gap in contemporary risk management literature and especially from the perspective of emerging markets in light of the aftermaths of the most recent sub-prime global financial crisis. This chapter develops a rigorous approach for the assessment of risk exposure under extreme conditions. This approach is based on the renowned concept of Liquidity-Adjusted Value-at-Risk (LVaR) along with the innovation of a systematic optimization risk algorithm. In order to underline the appropriate applications of parametric LVaR method, coherent assessments of risk parameters along with the optimization of maximum LVaR boundary limits are presented for the Gulf Cooperation Council (GCC) financial markets. The modeling techniques can aid financial institutions, regulatory authorities, and policymakers in the establishment of clear-cut simulation algorithms to assess risk exposure particularly in the wake of the latest credit crunch and ensuing sub-prime global financial crisis. © 2014 Elsevier Inc. All rights reserved.}}, 
pages = {413--446}, 
number = {NA}, 
volume = {NA}
}
@article{10.1057/grir.2009.2, 
year = {2010}, 
title = {{Insurance market effects of risk management metrics}}, 
author = {Bernard, Carole and Tian, Weidong}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1057/grir.2009.2}, 
abstract = {{We extend the classical analysis on optimal insurance design to the case when the insurer implements regulatory requirements (Value-at-Risk). Presumably, regulators impose some risk management requirement such as VaR to reduce the insurers insolvency risk, as well as to improve the insurance market stability. We show that VaR requirements may better protect the insured and improve economic efficiency, but have stringent negative effects on the insurance market. Our analysis reveals that the insured are better protected in the event of greater loss irrespective of the optimal design from either the insured or the insurer perspective. However, in the presence of the VaR requirement on the insurer, the insurer's insolvency risk might be increased and there are moral hazard issues in the insurance market because the optimal contract is discontinuous. © 2010 The International Association for the Study of Insurance Economics.}}, 
pages = {47--80}, 
number = {1}, 
volume = {35}
}
@article{10.1080/1351847x.2021.1946414, 
year = {2021}, 
title = {{Liquidity-adjusted value-at-risk: a comprehensive extension with microstructural liquidity components}}, 
author = {Ryu, Doojin and Webb, Robert I and Yu, Jinyoung}, 
journal = {The European Journal of Finance}, 
issn = {1351847X}, 
doi = {10.1080/1351847x.2021.1946414}, 
abstract = {{This study constructs an extended value-at-risk model that incorporates all microstructural liquidity components using a high-quality tick-by-tick index options market dataset. Out-of-sample backtesting and mean-difference analyses suggest that the traditional value-at-risk measure significantly underestimates investors’ potential losses relative to our new liquidity-adjusted measure. Logistic regressions reveal that ex-ante market illiquidity increases violations of liquidity-adjusted value-at-risk and that these violations are often driven by foreign institutional investors. © 2021 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--18}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.omega.2013.04.004, 
year = {2014}, 
title = {{A sequential competitive bidding strategy considering inaccurate cost estimates}}, 
author = {Takano, Yuichi and Ishii, Nobuaki and Muraki, Masaaki}, 
journal = {Omega}, 
issn = {03050483}, 
doi = {10.1016/j.omega.2013.04.004}, 
abstract = {{This paper develops a stochastic dynamic programming model for establishing an optimal sequential bidding strategy in a competitive bidding situation. In competitive bidding, a contractor usually sets the bid price of each contract by putting a markup on the estimated cost, and consequently, the bid price is affected by a cost estimation error. We take a scenario-based approach to determine the optimal markup in consideration of the effect of inaccurate cost estimates. We also introduce a value-at-risk constraint to mitigate the risk of suffering a large loss. Numerical results show that our model increases the average profit and reduces the profit volatility risk. © 2013 Elsevier Ltd.}}, 
pages = {132--140}, 
number = {1}, 
volume = {42}
}
@article{10.21314/jor.2017.363, 
year = {2017}, 
title = {{Risk management for private equity funds}}, 
author = {Buchner, Axel}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2017.363}, 
abstract = {{Although risk management has been explored thoroughly in financial modeling for over three decades, there is still a limited understanding of how to correctly quantify and manage the risks of investing in private equity, which continues to hinder our understanding of the risks associated with other traditional asset classes. This paper develops a comprehensive risk management framework for private equity fund investments, which captures the three main sources of risks that private equity investors face when investing in the asset class: market risk, liquidity risk and cashflow risk. Underlying the framework is a stochastic model for the value and cashflow dynamics of private equity funds, which allows us to derive three dynamic risk measures for private equity fund investments: value-at-risk, liquidity-adjusted value-at-risk and cashflow-at-risk. The model is calibrated with historical data of buyout funds, and the dynamics of the developed risk measures are illustrated for a single fund investment using Monte Carlo simulations. A model extension also studies the dynamics of the risk measures for portfolios of funds and develops a novel dynamic commitment strategy. Finally, sensitivity analyses show the impact of changes in the main model parameters on risk measures. © 2017 Incisive Risk Information (IP) Limited.}}, 
pages = {1--32}, 
number = {6}, 
volume = {19}
}
@article{10.1007/s10951-019-00599-6, 
year = {2019}, 
title = {{Risk-averse single machine scheduling: complexity and approximation}}, 
author = {Kasperski, Adam and Zieliński, Paweł}, 
journal = {Journal of Scheduling}, 
issn = {10946136}, 
doi = {10.1007/s10951-019-00599-6}, 
abstract = {{In this paper, a class of single machine scheduling problems is considered. It is assumed that job processing times and due dates can be uncertain and they are specified in the form of discrete scenario set. A probability distribution in the scenario set is known. In order to choose a schedule, some risk criteria such as the value at risk and conditional value at risk are used. Various positive and negative complexity results are provided for basic single machine scheduling problems. In this paper, new complexity results are shown and some known complexity results are strengthened. © 2019, The Author(s).}}, 
pages = {567--580}, 
number = {5}, 
volume = {22}
}
@article{10.1063/1.3241508, 
year = {2009}, 
title = {{Fast accurate algorithms for tail conditional expectation}}, 
author = {Chen, Bryant and Hsu, William W. Y. and Kao, Ming‐Yang}, 
journal = {AIP Conference Proceedings}, 
issn = {0094243X}, 
doi = {10.1063/1.3241508}, 
abstract = {{This paper proposes novel lattice algorithms to compute tail conditional expectation. To improve the naïve approach, we develop tilting, trinomial, and extrapolation algorithms together with some syntheses of these algorithms. Furthermore, we introduce fractional-step lattices to prevent interpolation error in the extrapolation algorithms. A key finding is that combining the techniques of tilting lattice, extrapolation and fractional steps substantially increases speed and accuracy. We demostrate the efficiency and accuracy of these algorithms with numerical results. © 2009 American Institute of Physics.}}, 
pages = {501--504}, 
number = {NA}, 
volume = {1168}
}
@article{10.1080/03610926.2018.1481970, 
year = {2019}, 
title = {{Value-at-risk estimation with new skew extension of generalized normal distribution}}, 
author = {Altun, Emrah and Tatlidil, Huseyin and Ozel, Gamze}, 
journal = {Communications in Statistics - Theory and Methods}, 
issn = {03610926}, 
doi = {10.1080/03610926.2018.1481970}, 
abstract = {{In this paper, we introduce a new distribution, called the alpha-skew generalized normal (ASGN), for GARCH models in modeling daily Value-at-Risk (VaR). Basic structural properties of the proposed distribution are derived including probability and cumulative density functions, moments and stochastic representation. The real data application based on ISE-100 index is given to show the performance of GARCH model specified under ASGN innovation distribution with respect to normal, Student’s-t, skew normal and generalized normal models in terms of the VaR accuracy. The empirical results show that GARCH model with ASGN innovation distribution generates the most accurate VaR forecasts for all confidence levels. © 2018, © 2018 Taylor \& Francis Group, LLC.}}, 
pages = {1--19}, 
number = {14}, 
volume = {48}
}
@article{10.12988/ams.2015.510676, 
year = {2015}, 
title = {{Optimizing the technical benefits of the insurance company using genetic algorithms}}, 
author = {Attar, Abderrahim El and Elhachloufi, Mostafa and Guennoun, Zine El Abdine}, 
journal = {Applied Mathematical Sciences}, 
issn = {1312885X}, 
doi = {10.12988/ams.2015.510676}, 
abstract = {{In this paper, we present a new method for dynamic optimization to achieve maximum technical benefit to the insurer using genetic algorithms. The objective of this work is to select, from a database containing forms of reinsurance, pricing models and credit ratings parameters (risk measurement methods, such as Value at Risk (VaR), Conditional Value at Risk (CVaR) or ruin probability) a form of reinsurance, a way of pricing and a solvency parameter to maximize technical benefits of the insurance company. © 2015 Abderrahim El Attar et al.}}, 
pages = {7453--7466}, 
number = {149-150}, 
volume = {9}
}
@article{10.1504/ijcee.2020.107372, 
year = {2020}, 
title = {{Bias decomposition in the value-at-risk calculation by a GARCH(1,1)}}, 
author = {Haddad, GholamReza Keshavarz and Hasanzade, Mehrnoosh}, 
journal = {International Journal of Computational Economics and Econometrics}, 
issn = {17571170}, 
doi = {10.1504/ijcee.2020.107372}, 
abstract = {{The recent researches show that value-at-risk (VaR) estimations are biased and is calculated conservatively. Bao and Ullah (2004) proved the bias of an ARCH(1) model for VaR can be decomposed in to two parts: bias due to the returns' misspecification distributional assumption for GARCH(1,1), i.e., (Bias1) and bias due to estimation error, i.e., (Bias2). Using quasi maximum likelihood estimation method this paper intends to find an analytical framework for the two sources of bias. We generate returns from Normal and t-student distributions, then estimate the GARCH(1,1) under Normal and t-student assumptions. Our findings reveal that Bias1 equals to zero for the Normal likelihood function, but Bias2 ≠ 0. Also, Bias1 and Bias2 are not zero for the t-student likelihood function as analytically were expected, however, all the biases become modest, when the number of observations and degree of freedom gets large. Copyright © 2020 Inderscience Enterprises Ltd.}}, 
pages = {183--202}, 
number = {2}, 
volume = {10}
}
@article{10.1007/s00500-020-05010-7, 
year = {2020}, 
title = {{A random-fuzzy portfolio selection DEA model using value-at-risk and conditional value-at-risk}}, 
author = {Shiraz, Rashed Khanjani and Tavana, Madjid and Fukuyama, Hirofumi}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-020-05010-7}, 
abstract = {{The complexity involved in portfolio selection has resulted in the development of a large number of methods to support ambiguous financial decision making. We consider portfolio selection problems where returns from investment securities are random variables with fuzzy information and propose a data envelopment analysis model for portfolio selection with downside risk criteria associated with value-at-risk (V@R) and conditional value-at-risk (CV@R). Both V@R and CV@R criteria are used to define possibility, necessity, and credibility measures, which are formulated as stochastic nonlinear programming programs with random-fuzzy variables. Our constructed stochastic nonlinear programs for analyzing portfolio selection are transformed into deterministic nonlinear programs. Moreover, we show an enumeration algorithm can solve the model without any mathematical programs. Finally, we demonstrate the applicability of the proposed framework and the efficacy of the procedures with a numerical example. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {17167--17186}, 
number = {22}, 
volume = {24}
}
@article{10.1016/s0927-5398(99)00008-0, 
year = {1999}, 
title = {{Computing value at risk with high frequency data}}, 
author = {Beltratti, Andrea and Morana, Claudio}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/s0927-5398(99)00008-0}, 
abstract = {{We compare the computation of value at risk with daily and with high frequency data for the Deutsche mark-US dollar exchange rate. Among the main points considered in the paper are: (a) the comparison of measures of value at risk on the basis of multi-step volatility forecasts; (b) the computation of the degree of fractional differencing for high frequency data in the context of a Fractionally Integrated Generalized Autoregressive Conditional Heteroskedasticity (FIGARCH) model; and (c) the comparison between deterministic and stochastic models for the filtering of high frequency returns.}}, 
pages = {431--455}, 
number = {5}, 
volume = {6}
}
@article{10.1134/s1990478918040063, 
year = {2018}, 
title = {{A Bilevel Stochastic Programming Problem with Random Parameters in the Follower’s Objective Function}}, 
author = {Ivanov, S. V.}, 
journal = {Journal of Applied and Industrial Mathematics}, 
issn = {19904789}, 
doi = {10.1134/s1990478918040063}, 
abstract = {{Under study is a bilevel stochastic linear programming problem with quantile criterion. Bilevel programming problems can be considered as formalization of the process of interaction between two parties. The first party is a Leader making a decision first; the second is a Follower making a decision knowing the Leader’s strategy and the realization of the random parameters. It is assumed that the Follower’s problem is linear if the realization of the random parameters and the Leader’s strategy are given. The aim of the Leader is the minimization of the quantile function of a loss function that depends on his own strategy and the optimal Follower’s strategy. It is shown that the Follower’s problem has a unique solution with probability 1 if the distribution of the random parameters is absolutely continuous. The lower-semicontinuity of the loss function is proved and some conditions are obtained of the solvability of the problem under consideration. Some example shows that the continuity of the quantile function cannot be provided. The sample average approximation of the problem is formulated. The conditions are given to provide that, as the sample size increases, the sample average approximation converges to the original problem with respect to the strategy and the objective value. It is shown that the convergence conditions hold for almost all values of the reliability level. A model example is given of determining the tax rate, and the numerical experiments are executed for this example. © 2018, Pleiades Publishing, Ltd.}}, 
pages = {658--667}, 
number = {4}, 
volume = {12}
}
@article{10.1007/bf03354603, 
year = {2003}, 
title = {{A structure for general and specific market risk}}, 
author = {Platen, Eckhard and Stahl, Gerhard}, 
journal = {Computational Statistics}, 
issn = {09434062}, 
doi = {10.1007/bf03354603}, 
abstract = {{The paper presents a consistent approach to the modeling of general and specific market risk as defined in regulatory documents. It compares the statistically based beta-factor model with a class of benchmark models that use a broadly based index as major building block for modeling. The investigation of log-returns of stock prices that are expressed in units of the market index reveals that these are likely to be Student t distributed. A corresponding discrete time benchmark model is used to calculate Value-at-Risk for equity portfolios.}}, 
pages = {355--373}, 
number = {3}, 
volume = {18}
}
@article{10.1111/saje.12051, 
year = {2015}, 
title = {{Empirical analyses of extreme value models for the South African mining index}}, 
author = {Chinhamu, Knowledge and Huang, Chun‐Kai and Huang, Chun‐Sung and Hammujuddy, Jahvaid}, 
journal = {South African Journal of Economics}, 
issn = {00382280}, 
doi = {10.1111/saje.12051}, 
abstract = {{While the classical normality assumption is simple to implement, it is well known to underestimate the leptokurtic behaviour demonstrated in most financial data. After examining properties of the Johannesburg Stock Exchange Mining Index returns, we propose two extreme value models to fit its negative tail with a higher degree of accuracy. The generalised extreme value distribution (GEVD) is fitted using the block maxima approach, while the generalised Pareto distribution (GPD) is fitted using the peaks-over-threshold method. Numerical assessment of value-at-risk (VaR) estimates indicates that both GEVD and GPD increasingly outperform the normal distribution as we move further into the lower tail. In addition, GEVD produces lower estimates relative to that of the historical VaR, and GPD provides slightly more conservative estimates for adequate capitalisation. © 2014 Economic Society of South Africa.}}, 
pages = {41--55}, 
number = {1}, 
volume = {83}
}
@article{10.4028/www.scientific.net/amm.448-453.4197, 
year = {2014}, 
title = {{Uncertainty analysis of the carbon tax on net profit for an enterprise}}, 
author = {Yu, Tai Yi and Fan, Yung Ching and Lu, Ruei Shan and Peng, Kuo Wei}, 
journal = {Applied Mechanics and Materials}, 
issn = {16609336}, 
doi = {10.4028/www.scientific.net/amm.448-453.4197}, 
abstract = {{A carbon emission trading/carbon tax is an economic incentive to encourage enterprises to reduce their carbon emissions. The information of greenhouse gas emissions at 2005-2009, carbon emissions trading prices in Europe, the euro-NT dollar exchange rate, and the profits and losses of enterprises, manufacturers of panel-related technology, were gathered to estimate VaR (Value at Risk) and CVaR (Conditional Value at Risk) values of implementing carbon tax or carbon emissions trading, using the Monte Carlo simulation method with the Crystal Ball software. This study uses sensitivity analysis, tornado diagrams, and spider graphs to determine the influence that variables have on enterprise net profit. © (2014) Trans Tech Publications, Switzerland.}}, 
pages = {4197--4202}, 
number = {NA}, 
volume = {448-453}
}
@article{10.1016/j.insmatheco.2005.01.006, 
year = {2005}, 
title = {{Worst VaR scenarios}}, 
author = {Embrechts, Paul and Höing, Andrea and Puccetti, Giovanni}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2005.01.006}, 
abstract = {{The worst possible Value-at-Risk for a non-decreasing function ψ of n dependent risks is known when n = 2 or the copula of the portfolio is bounded from below. In this paper we analyze the properties of the dependence structures leading to this solution, in particular their form and the implied functional dependence between the marginals. Furthermore, we criticise the assumption of the worst possible scenario for VaR-based risk management and we provide an alternative approach supporting comonotonicity. © 2005 Elsevier B.V. All rights reserved.}}, 
pages = {115--134}, 
number = {1 SPEC. ISS.}, 
volume = {37}
}
@article{10.1007/s00291-008-0138-3, 
year = {2010}, 
title = {{On the non-existence of conditional value-at-risk under heavy tails and short sales}}, 
author = {Bamberg, Günter and Neuhierl, Andreas}, 
journal = {OR Spectrum}, 
issn = {01716468}, 
doi = {10.1007/s00291-008-0138-3}, 
abstract = {{Value-at-Risk (VaR) and conditional value-at-risk (CVaR) are important risk measures. Especially VaR is very popular and widespread in risk management and banking supervision. However, VaR has some unwelcome properties which are not shared by CVaR. Therefore CVaR is preferable from a theoretical point of view. Both VaR and CVaR are discussed for long and short positions. It is pointed out that short positions and heavy tails are incompatible with a finite CVaR. © 2008 Springer-Verlag.}}, 
pages = {49--60}, 
number = {1}, 
volume = {32}
}
@article{10.1016/s1057-5219(01)00069-2, 
year = {2002}, 
title = {{A benchmark for measuring bias in estimated daily value at risk}}, 
author = {Moosa, Imad A. and Bollen, Bernard}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/s1057-5219(01)00069-2}, 
abstract = {{In this study the notion of "realised volatility," which has been proposed in the literature recently, is employed to obtain a benchmark to test statistically for bias in estimates of daily value at risk (VAR). A number of estimates based on the parametric and historical approaches to VAR are examined. It is found that the bias in the estimates of VAR are sensitive to the sample period and to the methodology chosen to estimate daily volatility. It appears that the estimates based on the exponentially weighted moving average volatility with a high decay factor are unbiased and relatively efficient. © 2002 Elsevier Science Inc. All rights reserved.}}, 
pages = {85--100}, 
number = {1}, 
volume = {11}
}
@article{10.1016/s1042-9573(03)00040-8, 
year = {2003}, 
title = {{A risk-factor model foundation for ratings-based bank capital rules}}, 
author = {Gordy, Michael B.}, 
journal = {Journal of Financial Intermediation}, 
issn = {10429573}, 
doi = {10.1016/s1042-9573(03)00040-8}, 
abstract = {{I demonstrate that ratings-based capital rules, including both the current Basel Accord and its proposed revision, can be reconciled with the general class of credit value-at-risk models. Each exposure's contribution to VaR is portfolio-invariant only if (a) dependence across exposures is driven by a single systematic risk factor, and (b) no exposure accounts for more than an arbitrarily small share of total portfolio exposure. Analysis of rates of convergence to asymptotic VaR leads to a simple and accurate portfolio-level add-on charge for undiversified idiosyncratic risk. There is no similarly simple way to address violation of the single factor assumption. © 2003 Elsevier Inc. All rights reserved.}}, 
pages = {199--232}, 
number = {3}, 
volume = {12}
}
@article{10.1016/j.irfa.2004.06.012, 
year = {2005}, 
title = {{Risk management under extreme events}}, 
author = {Fernandez, Viviana}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2004.06.012}, 
abstract = {{This article presents two applications of extreme value theory (EVT) to financial markets: computation of value at risk (VaR) and cross-section dependence of extreme returns (i.e., tail dependence). We use a sample comprised of the United States, Europe, Asia, and Latin America. Our main findings are the following. First, on average, EVT gives the most accurate estimate of VaR. Second, tail dependence of paired returns decreases substantially when both heteroscedasticity and serial correlation are filtered out by a multivariate GARCH model. Both findings are in agreement with previous research in this area for other financial markets. © 2004 Elsevier Inc. All rights reserved.}}, 
pages = {113--148}, 
number = {2}, 
volume = {14}
}
@article{10.2316/p.2010.702-098, 
year = {2010}, 
title = {{Intermittency model for surface layer wind speed fluctuations: Application to short term forecasting}}, 
author = {Baïle, R and Muzy, J -F and Poggi, P}, 
journal = {Modelling, Identification, and Control}, 
issn = {10258973}, 
doi = {10.2316/p.2010.702-098}, 
abstract = {{This study presents a statistical model of surface layer wind velocity amplitude relying on the notion of continuous cascades. Inspired by recent empirical findings that suggest the existence of some cascading process in the mesoscale range, we consider that wind speed can be described by a seasonal ARMA model where the noise term is "multifrac-tal" i.e., associated with a random cascade. This model can find applications in short-term forecasting: the obtained results show a systematic improvement of the prediction as compared to reference models like persistence. We also address the problem of estimation of wind speed conditional laws. It leads to wind speed distributed according to "transformed" Rice pdf. Such a prediction can be useful to handle specific risk related problems (e.g., the probability that a given event occurs during the next hour or day or the calibration of some "value at risk"). The reliability of the obtained conditional laws are backtested by the means of "p-p plots".}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/s0378-4266(01)00254-0, 
year = {2003}, 
title = {{Bank lending policy, credit scoring and value-at-risk}}, 
author = {Jacobson, Tor and Roszbach, Kasper}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/s0378-4266(01)00254-0}, 
abstract = {{This paper builds on the credit-scoring literature and proposes a method to calculate portfolio credit risk. Individual default risk estimates are used to compose a value-at-risk (VaR) measure of credit risk. In general, credit-scoring models suffer from a sample-selection bias. The starting point is therefore to estimate an unbiased scoring model using the bivariate probit approach. The paper uses a large data set with Swedish consumer credit data that contains extensive financial and personal information on both rejected and approved applicants. We study how marginal changes in a default-risk-based acceptance rule would shift the size of the bank's loan portfolio, its VaR exposure and average credit losses. Finally, we compare the risk in the sample portfolio with that in an efficiently provided portfolio of equal size. The results show that the size of a small consumer loan does not affect associated default risk, implying that the bank provides loans in a way that is not consistent with default-risk minimization. VaR calculations indicate that an efficient selection (by means of a default-riskbased rule) of loan applicants can reduce credit risk by up to 80\%. © 2002 Elsevier Science B.V. All rights reserved.}}, 
pages = {615--633}, 
number = {4}, 
volume = {27}
}
@article{10.1109/cec.2011.47, 
year = {2011}, 
title = {{Long-range evaluation of risk in the migration to cloud storage}}, 
author = {Mastroeni, Loretta and Naldi, Maurizio}, 
journal = {2011 IEEE 13th Conference on Commerce and Enterprise Computing}, 
issn = {NA}, 
doi = {10.1109/cec.2011.47}, 
abstract = {{The economical evaluation of the benefits of cloud migration versus the decision to buy and manage the storage facilities in house is essential for the migration decision. Deterministic models have been proposed in the past, which fail to account for the essentially random nature of any future quantity involved in the decision. The random nature of future events also implies the insurgence of risks when the decision taken turns out to be wrong. Here we adopt a probabilistic model and employ it to assess the risk associated to either decision over an extended time period. We adopt the Value-at-Risk as a risk metric, and compare two decision variables, namely the mean and the median Differential Net Present Value. We show that though the two decision variables bear the same risk for most values of the range of leasing prices, the mean Differential Net Present Value abates the Value-at-Risk even by as much as 60\% in the small region of leasing prices where there is large uncertainty between the two alternatives. © 2011 IEEE.}}, 
pages = {260--266}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/for.2456, 
year = {2017}, 
title = {{Backtesting Value-at-Risk: A Generalized Markov Test}}, 
author = {Pajhede, Thor}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2456}, 
abstract = {{Testing the validity of value-at-risk (VaR) forecasts, or backtesting, is an integral part of modern market risk management and regulation. This is often done by applying independence and coverage tests developed by Christoffersen (International Economic Review, 1998; 39(4), 841–862) to so-called hit-sequences derived from VaR forecasts and realized losses. However, as pointed out in the literature, these aforementioned tests suffer from low rejection frequencies, or (empirical) power when applied to hit-sequences derived from simulations matching empirical stylized characteristics of return data. One key observation of the studies is that higher-order dependence in the hit-sequences may cause the observed lower power performance. We propose to generalize the backtest framework for VaR forecasts, by extending the original first-order dependence of Christoffersen to allow for a higher- or kth-order dependence. We provide closed-form expressions for the tests as well as asymptotic theory. Not only do the generalized tests have power against kth-order dependence by definition, but also included simulations indicate improved power performance when replicating the aforementioned studies. Further, included simulations show much improved size properties of one of the suggested tests. Copyright © 2017 John Wiley \& Sons, Ltd. Copyright © 2017 John Wiley \& Sons, Ltd.}}, 
pages = {597--613}, 
number = {5}, 
volume = {36}
}
@article{10.1007/s10479-014-1584-8, 
year = {2016}, 
title = {{Optimal VaR-based risk management with reinsurance}}, 
author = {Cong, Jianfa and Tan, Ken Seng}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-014-1584-8}, 
abstract = {{It is well-known that reinsurance can be an effective risk management solution for financial institutions such as the insurance companies. The optimal reinsurance solution depends on a number of factors including the criterion of optimization and the premium principle adopted by the reinsurer. In this paper, we analyze the Value-at-Risk based optimal risk management solution using reinsurance under a class of premium principles that is monotonic and piecewise. The monotonic piecewise premium principles include not only those which preserve stop-loss ordering, but also the piecewise premium principles which are monotonic and constructed by concatenating a series of premium principles. By adopting the monotonic piecewise premium principle, our proposed optimal reinsurance model has a number of advantages. In particular, our model has the flexibility of allowing the reinsurer to use different risk loading factors for a given premium principle or use entirely different premium principles depending on the layers of risk. Our proposed model can also analyze the optimal reinsurance strategy in the context of multiple reinsurers that may use different premium principles (as attributed to the difference in risk attitude and/or imperfect information). Furthermore, by artfully imposing certain constraints on the ceded loss functions, the resulting model can be used to capture the reinsurer’s willingness and/or capacity to accept risk or to control counterparty risk from the perspective of the insurer. Under some technical assumptions, we derive explicitly the optimal form of the reinsurance strategies in all the above cases. In particular, we show that a truncated stop-loss reinsurance treaty or a limited stop-loss reinsurance treaty can be optimal depending on the constraint imposed on the retained and/or ceded loss functions. Some numerical examples are provided to further compare and contrast our proposed models to the existing models. © 2014, Springer Science+Business Media New York.}}, 
pages = {177--202}, 
number = {1-2}, 
volume = {237}
}
@article{10.1515/strm-2015-0028, 
year = {2017}, 
title = {{Improved algorithms for computing worst Value-at-Risk}}, 
author = {Hofert, Marius and Memartoluie, Amir and Saunders, David and Wirjanto, Tony}, 
journal = {Statistics \& Risk Modeling}, 
issn = {21931402}, 
doi = {10.1515/strm-2015-0028}, 
abstract = {{Numerical challenges inherent in algorithms for computing worst Value-at-Risk in homogeneous portfolios are identified and solutions as well as words of warning concerning their implementation are provided. Furthermore, both conceptual and computational improvements to the Rearrangement Algorithm for approximating worst Value-at-Risk for portfolios with arbitrary marginal loss distributions are given. In particular, a novel Adaptive Rearrangement Algorithm is introduced and investigated. These algorithms are implemented using the R package qrmtools and may be of interest in any context in which it is required to find columnwise permutations of a matrix such that the minimal (maximal) row sum is maximized (minimized). © 2017 Walter de Gruyter GmbH, Berlin/Boston.}}, 
pages = {13--31}, 
number = {1-2}, 
volume = {34}
}
@article{10.1109/sws.2010.5607440, 
year = {2010}, 
title = {{RETRACTED ARTICLE: Fuzzy value-at-risk and fuzzy conditional value-at-risk: Two risk measures under fuzzy uncertainty}}, 
author = {Shang, Zhaoxia and Liu, Hong and Ma, Xiaoxian and Liu, Yanmin}, 
journal = {2010 IEEE 2nd Symposium on Web Society}, 
issn = {NA}, 
doi = {10.1109/sws.2010.5607440}, 
abstract = {{This paper deals with two risk management tools and their application on Economical crisis management. Next points out some problems in their research and application and the direction of research, introduced some concept and formula, finally puts forward the prospect. © 2010 IEEE.}}, 
pages = {282--290}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/ngi.2011.5985868, 
year = {2011}, 
title = {{Storage Buy-or-Lease decisions in cloud computing under price uncertainty}}, 
author = {Mastroeni, Loretta and Naldi, Maurizio}, 
journal = {2011 7th EURO-NGI Conference on Next Generation Internet Networks}, 
issn = {NA}, 
doi = {10.1109/ngi.2011.5985868}, 
abstract = {{Cloud computing allows to lease computational resources on demand on the network instead of buying them. Storage is a major computational resource that may be leased under the cloud computing approach. An economical model is provided for the analysis of the Buy-or-Lease decision for storage in cloud computing. The model introduces a probabilistic approach to evaluate future prices and the replacement needs for disk arrays (in contrast to the current literature which considers deterministic prices and fixed replacement rates). The median Differential Net Present Value is proposed as the decision variable. A simulation analysis is conducted that shows that the cloud computing approach appears more profitable for medium-sized companies with longer investment plans. The risks associated to wrong decisions is evaluated through the Value-at-Risk tool. © 2011 IEEE.}}, 
pages = {1--8}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/tpwrs.2003.810681, 
year = {2003}, 
title = {{Managing market risk in energy}}, 
author = {Denton, M. and Palmer, A. and Masiello, R. and Skantze, P.}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2003.810681}, 
abstract = {{The market risks encountered by energy asset operators can be categorized as short term/operational, intermediate term/trading, and long term/valuation in nature. This paper describes how the market risks in operations can be measured and managed using real option models and stochastic optimization techniques. It then links these results to intermediate term value at risk and related risk metrics such as cash flow, earnings, and credit risk which can be used to measure trading risks over weeks to months; and how to optimize these portfolios for risk-return relationships. Finally, it then explores the risks in longer term energy portfolio management and how these can be simulated, measured, and optimized.}}, 
pages = {494--502}, 
number = {2}, 
volume = {18}
}
@article{10.3390/risks5030041, 
year = {2017}, 
title = {{Robust estimation of value-at-risk through distribution-free and parametric approaches using the joint severity and frequency model: Applications in financial, actuarial, and natural calamities domains}}, 
author = {Guharay, Sabyasachi and Chang, KC and Xu, Jie}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks5030041}, 
abstract = {{Value-at-Risk (VaR) is a well-accepted risk metric in modern quantitative risk management (QRM). The classical Monte Carlo simulation (MCS) approach, denoted henceforth as the classical approach, assumes the independence of loss severity and loss frequency. In practice, this assumption does not always hold true. Through mathematical analyses, we show that the classical approach is prone to significant biases when the independence assumption is violated. This is also corroborated by studying both simulated and real-world datasets. To overcome the limitations and to more accurately estimate VaR, we develop and implement the following two approaches for VaR estimation: the data-driven partitioning of frequency and severity (DPFS) using clustering analysis, and copula-based parametric modeling of frequency and severity (CPFS). These two approaches are verified using simulation experiments on synthetic data and validated on five publicly available datasets from diverse domains; namely, the financial indices data of Standard \& Poor’s 500 and the Dow Jones industrial average, chemical loss spills as tracked by the US Coast Guard, Australian automobile accidents, and US hurricane losses. The classical approach estimates VaR inaccurately for 80\% of the simulated data sets and for 60\% of the real-world data sets studied in this work. Both the DPFS and the CPFS methodologies attain VaR estimates within 99\% bootstrap confidence interval bounds for both simulated and real-world data. We provide a process flowchart for risk practitioners describing the steps for using the DPFS versus the CPFS methodology for VaR estimation in real-world loss datasets. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {41}, 
number = {3}, 
volume = {5}
}
@article{10.1016/j.ememar.2010.12.004, 
year = {2011}, 
title = {{Relationship between portfolio diversification and value at risk: Empirical evidence}}, 
author = {Kiani, Khurshid M.}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2010.12.004}, 
abstract = {{This research explores the risk associated with the stocks prices in the seventeen selected companies that are listed in Indian BSE (100) National as well as portfolios of investment that are constructed from these seventeen companies employed. Additionally, for considering the possibility of international diversification, construction of portfolios of investment form stock price indexes in various emerging markets and developed countries of the world is considered. Correlations for domestically as well as internationally diversified portfolios are computed to unveil the relationship between stock prices of various firms as well as domestic and internationally diversified portfolios of investments. Further, to understand the effect of diversification on the risk associated with each of the portfolios of investments employed, value at risk analysis (VaR) is undertaken for studying the benefits associated with domestic as well as international diversification (if any).The study results show that domestic diversification lowers the expected losses associated with each of the domestic portfolios of investment employed where the international diversification substantially mitigates the portfolio risks. Results from VaR analysis reveal that diversification lowers the portfolio risks and additional reduction in portfolio risks is realized by international diversification. © 2011 Elsevier B.V.}}, 
pages = {443--459}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.physa.2016.05.002, 
year = {2016}, 
title = {{Regime switching model for financial data: Empirical risk analysis}}, 
author = {Salhi, Khaled and Deaconu, Madalina and Lejay, Antoine and Champagnat, Nicolas and Navet, Nicolas}, 
journal = {Physica A: Statistical Mechanics and its Applications}, 
issn = {03784371}, 
doi = {10.1016/j.physa.2016.05.002}, 
abstract = {{This paper constructs a regime switching model for the univariate Value-at-Risk estimation. Extreme value theory (EVT) and hidden Markov models (HMM) are combined to estimate a hybrid model that takes volatility clustering into account. In the first stage, HMM is used to classify data in crisis and steady periods, while in the second stage, EVT is applied to the previously classified data to rub out the delay between regime switching and their detection. This new model is applied to prices of numerous stocks exchanged on NYSE Euronext Paris over the period 2001-2011. We focus on daily returns for which calibration has to be done on a small dataset. The relative performance of the regime switching model is benchmarked against other well-known modeling techniques, such as stable, power laws and GARCH models. The empirical results show that the regime switching model increases predictive performance of financial forecasting according to the number of violations and tail-loss tests. This suggests that the regime switching model is a robust forecasting variant of power laws model while remaining practical to implement the VaR measurement. © 2016 Elsevier B.V.}}, 
pages = {148--157}, 
number = {NA}, 
volume = {461}
}
@article{10.1016/j.frl.2008.10.002, 
year = {2009}, 
title = {{Time-inconsistency of VaR and time-consistent alternatives}}, 
author = {Cheridito, Patrick and Stadje, Mitja}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2008.10.002}, 
abstract = {{We show that VaR (Value-at-Risk) is not time-consistent and discuss examples where this can lead to dynamically inconsistent behavior. Then we propose two time-consistent alternatives to VaR. The first one is a composition of one-period VaR's. It is time-consistent but not coherent. The second one is a composition of average VaR's. It is a time-consistent coherent risk measure. © 2008 Elsevier Inc. All rights reserved.}}, 
pages = {40--46}, 
number = {1}, 
volume = {6}
}
@article{10.1111/1467-9965.00124, 
year = {2001}, 
title = {{The liquidity discount}}, 
author = {Subramanian, Ajay and Jarrow, Robert A.}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/1467-9965.00124}, 
abstract = {{This paper characterizes the liquidity discount, the difference between the market value of a trader's position and its value when liquidated. This discount occurs whenever traders face downward sloping demand curves for shares and execution lags in selling shares. This characterization enables one to modify the standard value at risk (VaR) computation to include liquidity risk.}}, 
pages = {447--474}, 
number = {4}, 
volume = {11}
}
@article{10.1080/00036846.2016.1208359, 
year = {2017}, 
title = {{Risk assessment of oil price from static and dynamic modelling approaches}}, 
author = {Mi, Zhi-Fu and Wei, Yi-Ming and Tang, Bao-Jun and Cong, Rong-Gang and Yu, Hao and Cao, Hong and Guan, Dabo}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2016.1208359}, 
abstract = {{The price gap between West Texas Intermediate (WTI) and Brent crude oil markets has been completely changed in the past several years. The price of WTI was always a little larger than that of Brent for a long time. However, the price of WTI has been surpassed by that of Brent since 2011. The new market circumstances and volatility of oil price require a comprehensive re-estimation of risk. Therefore, this study aims to explore an integrated approach to assess the price risk in the two crude oil markets through the value at risk (VaR) model. The VaR is estimated by the extreme value theory (EVT) and GARCH model on the basis of generalized error distribution (GED). The results show that EVT is a powerful approach to capture the risk in the oil markets. On the contrary, the traditional variance–covariance (VC) and Monte Carlo (MC) approaches tend to overestimate risk when the confidence level is 95\%, but underestimate risk at the confidence level of 99\%. The VaR of WTI returns is larger than that of Brent returns at identical confidence levels. Moreover, the GED-GARCH model can estimate the downside dynamic VaR accurately for WTI and Brent oil returns. © 2016 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--11}, 
number = {9}, 
volume = {49}
}
@article{10.1137/18m1217322, 
year = {2020}, 
title = {{Nonconcave optimal investment with value-at-risk constraint: An application to life insurance contracts}}, 
author = {Nguyen, Thai and Stadje, Mitja}, 
journal = {SIAM Journal on Control and Optimization}, 
issn = {03630129}, 
doi = {10.1137/18m1217322}, 
abstract = {{This paper studies a value-at-risk (VaR)-regulated optimal portfolio problem of the equity holders of a participating life insurance contract. In a setting with unhedgeable mortality risk and complete financial market, the optimal solution is given explicitly for contracts with mortality risk using a martingale approach for constrained nonconcave optimization problems. We show that regulatory VaR constraints for participating insurance contracts lead to more prudent investment than in the case of no regulation. This result is contrary to the situation where the insurer maximizes the utility of the total wealth of the company (without distinguishing between contributions of equity holders and policyholders), in which case a VaR constraint may induce the insurer to take excessive risks leading to higher losses than in the case of no regulation; see [S. Basak and A. Shapiro, Rev. Financ. Stud., 14 (2001), pp. 371-405]. Compared to the unregulated problem, the VaR-constrained strategy leads to a higher expected utility for the policyholders, highlighting the potential usefulness of a VaR regulation in the context of insurance. The prudent investment behavior is more significant if a VaR-type regulation is replaced by a portfolio insurance (PI)-type regulation. Furthermore, a stricter regulation (a smaller allowed default probability in the VaR problem or a higher minimum guarantee level in the PI problem) enhances the benefit of the policyholder but deteriorates that of the insurer. For both types of regulation, the gains in terms of expected utility are greater for higher participation rates, while being smaller for higher bonus rates. We also extend our analysis to frameworks where dividend and premature death benefit payments are made at an intermediate time date. ©c 2020 Society for Industrial and Applied Mathematics.}}, 
pages = {895--936}, 
number = {2}, 
volume = {58}
}
@article{10.1016/j.ejor.2020.12.051, 
year = {2021}, 
title = {{Bayesian Value-at-Risk backtesting: The case of annuity pricing}}, 
author = {Leung, Melvern and Li, Youwei and Pantelous, Athanasios A. and Vigne, Samuel A.}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2020.12.051}, 
abstract = {{We propose new Unconditional, Independence and Conditional Coverage VaR-forecast backtests for the case of annuity pricing under a Bayesian framework that significantly minimise the direct and indirect effects of p-hacking or other biased outcomes in decision-making, in general. As a consequence of the global financial crisis during 2007–09, regulatory demands arising from Solvency II has required a stricter assessment setting for the internal financial risk models of insurance companies. To put our newly proposed backtesting technique into practice we employ linear and nonlinear Bayesianised variants of two typically used mortality models in the context of annuity pricing. In this regard, we explore whether the stressed longevity scenarios are enough to capture the experienced liability over the forecasted time horizon. Most importantly, we conclude that our Bayesian decision theoretic framework quantitatively produce a strength of evidence favouring one decision over the other. © 2021 Elsevier B.V.}}, 
pages = {786--801}, 
number = {2}, 
volume = {293}
}
@article{10.1016/j.jfi.2005.07.001, 
year = {2006}, 
title = {{An analysis of VaR-based capital requirements}}, 
author = {Cuoco, Domenico and Liu, Hong}, 
journal = {Journal of Financial Intermediation}, 
issn = {10429573}, 
doi = {10.1016/j.jfi.2005.07.001}, 
abstract = {{We study the behavior of a financial institution subject to capital requirements based on self-reported VaR measures, as in the Basel Committee's Internal Models Approach. We view these capital requirements and the associated backtesting procedure as a mechanism designed to induce financial institutions to reveal the risk of their investments and to support this risk with adequate levels of capital. Accordingly, we consider the simultaneous choice of an optimal dynamic reporting and investment strategy. Overall, we find that VaR-based capital requirements can be very effective not only in curbing portfolio risk but also in inducing revelation of this risk. © 2005.}}, 
pages = {362--394}, 
number = {3}, 
volume = {15}
}
@article{10.12988/ams.2015.52144, 
year = {2015}, 
title = {{Time series modeling for risk of stock price with value at risk computation}}, 
author = {Devianto, Dodi and Maiyastri and Fadhilla, Dian Rezki}, 
journal = {Applied Mathematical Sciences}, 
issn = {1312885X}, 
doi = {10.12988/ams.2015.52144}, 
abstract = {{Risk of stock price investment is defined as an unexpected outcome of that asset's value in the future. Value at Risk (VaR) is one of measurement in market risk, it is measured loss that associated with a rare event under normal market conditions or the maximum loss of financial positions during a given time periods for a given probability. The maximum loss in certain time and level of confidence can be computed by forecasting volatility that is represented by standard deviation. The model to be used to represent volatility as risk of stock prices is Generalized Autoregressive Conditional Heteroscedasticity (GARCH). In this study, GARCH(1,1) is the best suited model to forecast volatility of stock price Unilever Indonesia. The refinement of this volatility model can be used to compute VaR as consideration to help investors for their investment. © 2015 Dodi Devianto, Maiyastri and Dian Rezki Fadhilla.}}, 
pages = {2779--2787}, 
number = {53-56}, 
volume = {9}
}
@article{10.1016/j.jempfin.2018.03.004, 
year = {2018}, 
title = {{Measuring long-term tail risk: Evaluating the performance of the square-root-of-time rule}}, 
author = {Wang, Jying-Nan and Du, Jiangze and Hsu, Yuan-Teng}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2018.03.004}, 
abstract = {{This paper focuses on risk over long time horizons and within extreme percentiles, which have attracted considerable recent interest in numerous subfields of finance. Value at risk (VaR) aggregates several components of asset risk into a single quantitative measurement and is commonly used in tail risk management. Due to realistic data limits, many practitioners might use the square-root-of-time rule (SRTR) to compute long-term VaR. However, serial dependence and heavy-tailedness can bias the SRTR. This paper addresses two deficiencies of the study by Wang et al. (2011), who propose the modified-SRTR (MSRTR) to partially correct the serial dependence and use subsampling estimation as the benchmark to verify the performance of MSRTR. First, we investigate the validity of the subsampling approach through numerical simulations. Second, to reduce the heavy-tailedness bias, we propose a new MSRTR approach (MSRTR∗) in light of the Central Limit Theorem (CLT). In the empirical study, 28 country-level exchange-traded funds (ETFs) from 2010 to 2015 are considered to estimate the 30-day VaR. After modifying both serial dependence and heavy-tailedness, our approach reduces the bias from 26.46\% to 5.97\%, on average, compared to the SRTR. We also provide a backtesting analysis to verify the robustness of the MSRTR∗. This new approach should be considered when estimating long-term VaR using short-term VaR. © 2018 Elsevier B.V.}}, 
pages = {120--138}, 
number = {NA}, 
volume = {47}
}
@article{10.1007/978-3-030-43465-6_1, 
year = {2020}, 
title = {{A Tutorial on Quantile Estimation via Monte Carlo}}, 
author = {Dong, Hui and Nakayama, Marvin K.}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {21941009}, 
doi = {10.1007/978-3-030-43465-6\_1}, 
abstract = {{Quantiles are frequently used to assess risk in a wide spectrum of application areas, such as finance, nuclear engineering, and service industries. This tutorial discusses Monte Carlo simulation methods for estimating a quantile, also known as a percentile or value-at-risk, where p of a distribution’s mass lies below its p-quantile. We describe a general approach that is often followed to construct quantile estimators, and show how it applies when employing naive Monte Carlo or variance-reduction techniques. We review some large-sample properties of quantile estimators. We also describe procedures for building a confidence interval for a quantile, which provides a measure of the sampling error. © 2020, Springer Nature Switzerland AG.}}, 
pages = {3--30}, 
number = {NA}, 
volume = {324}
}
@article{10.1146/annurev-financial-073009-104045, 
year = {2010}, 
title = {{Risk management}}, 
author = {Jorion, Philippe}, 
journal = {Annual Review of Financial Economics}, 
issn = {19411367}, 
doi = {10.1146/annurev-financial-073009-104045}, 
abstract = {{Modern risk management systems were developed in the early 1990s to provide centralized risk measures at the top level of financial institutions. These are based on a century of theoretical developments in risk measures. In particular, value at risk (VAR) has become widely used as a statistical measure of market risk based on current positions. This methodology has been extended to credit risk and operational risk. This article reviews the benefits and limitations of these models. In spite of all these advances, risk methods are poorly adapted to measure liquidity risk and systemic risk. Copyright © 2010 by Annual Reviews.}}, 
pages = {347--365}, 
number = {NA}, 
volume = {2}
}
@article{10.1109/icmss.2010.5578209, 
year = {2010}, 
title = {{The researches on the evaluation system of investment value of listed companies on the GEM based on VaR}}, 
author = {Bao-sen, WANG and Juan, Li}, 
journal = {2010 International Conference on Management and Service Science}, 
issn = {NA}, 
doi = {10.1109/icmss.2010.5578209}, 
abstract = {{The global financial crisis hastened the development of the Shenzhen GEM is a venture capital a key link in the chain, force the development of SME financing difficulty in resolving the issue. As a new thing, there is a higher risk of GEM. In view of the characteristics and specific risks of Growth Enterprise Market (GEM), this paper measures the market risk of 28 listed companies on the GEM by use of VaR techniques, and introduces VaR into the company's evaluation system of investment value, so it provides a set of effective evaluation method for investment value analysis of companies on Shenzhen GEM. © 2010 IEEE.}}, 
pages = {1--6}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jeconom.2014.02.007, 
year = {2014}, 
title = {{Extreme-quantile tracking for financial time series}}, 
author = {Chavez-Demoulin, V. and Embrechts, P. and Sardy, S.}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2014.02.007}, 
abstract = {{Time series of financial asset values exhibit well-known statistical features such as heavy tails and volatility clustering. We propose a nonparametric extension of the classical Peaks-Over-Threshold method from extreme value theory to fit the time varying volatility in situations where the stationarity assumption may be violated by erratic changes of regime, say. As a result, we provide a method for estimating conditional risk measures applicable to both stationary and nonstationary series. A backtesting study for the UBS share price over the subprime crisis exemplifies our approach. © 2014 Elsevier B.V. All rights reserved.}}, 
pages = {44--52}, 
number = {1}, 
volume = {181}
}
@article{10.1016/j.jeconom.2019.04.030, 
year = {2019}, 
title = {{Combining statistical intervals and market prices: The worst case state price distribution}}, 
author = {Mykland, Per Aslak}, 
journal = {Journal of Econometrics}, 
issn = {03044076}, 
doi = {10.1016/j.jeconom.2019.04.030}, 
abstract = {{The paper shows how to combine (historical) statistical data and (current) market prices to form conservative trading strategies for options. This gives rise to a “worst case” state price distribution, which provides sharp price bounds for all convex European options. The paper provides for existence and computational algorithms under conditions which can be understood as “no arbitrage”. The worst case distribution converges to a regular state price distribution if the number of traded options increases to span the space of possible option payouts. © 2019 Elsevier B.V.}}, 
pages = {272--285}, 
number = {1}, 
volume = {212}
}
@article{10.1080/1540496x.2018.1509790, 
year = {2020}, 
title = {{The Influences of Book-to-Price Ratio and Stock Capitalization on Value-at-Risk Estimation in Taiwan Stock Market}}, 
author = {Wu, Tsung-Che and Huang, Hung-Hsi and Wang, Ching-Ping and Zhong, Yi-Lin}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2018.1509790}, 
abstract = {{This study examines whether the stock capitalization and book-to-price (B/P) ratio can affect the VaR (value-at-risk) estimation performances in six VaR estimation methodologies. Examining on the daily returns on Taiwan stock market, we find that the market capitalization is not a significant factor in VaR estimation, while the B/P ratio is generally positively related to VaR estimates. Among various VaR estimation models, the historical simulation model performs the best, and the followers are the Student-t and extreme value theory models. Reversely, normal distribution model performs the worst, and the GARCH-family models frequently extremely over-estimate or under-estimate the individual daily VaR. ©, Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {1--18}, 
number = {5}, 
volume = {56}
}
@article{10.3390/risks5040059, 
year = {2017}, 
title = {{A review and some complements on quantile risk measures and their domain}}, 
author = {Fuchs, Sebastian and Schlotter, Ruben and Schmidt, Klaus}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks5040059}, 
abstract = {{In the present paper, we study quantile risk measures and their domain. Our starting point is that, for a probability measure Q on the open unit interval and a wide class LQ of random variables, we define the quantile risk measure ϱQ as the map that integrates the quantile function of a random variable in LQ with respect to Q. The definition of LQ ensures that ϱQ cannot attain the value +∞ and cannot be extended beyond LQ without losing this property. The notion of a quantile risk measure is a natural generalization of that of a spectral risk measure and provides another view of the distortion risk measures generated by a distribution function on the unit interval. In this general setting, we prove several results on quantile or spectral risk measures and their domain with special consideration of the expected shortfall. We also present a particularly short proof of the subadditivity of expected shortfall. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {59}, 
number = {4}, 
volume = {5}
}
@article{10.1016/j.cie.2019.106122, 
year = {2020}, 
title = {{Design optimization for resilience for risk-averse firms}}, 
author = {Giahi, Ramin and MacKenzie, Cameron A. and Hu, Chao}, 
journal = {Computers \& Industrial Engineering}, 
issn = {03608352}, 
doi = {10.1016/j.cie.2019.106122}, 
abstract = {{Designers should try to design systems that are resilient to adverse conditions during a system's lifetime. The resilience of a system under time-dependent adverse conditions can be assessed by modeling the degradation and recovery of the system's components. Decision makers in a firm should attempt to find the optimal design to make the system resilient to the various adverse conditions. A risk-neutral firm maximizes the expected profit gained from fielding the system, but a risk-averse firm may sacrifice some profit in order to avoid failure from these adverse conditions. The uniqueness of this paper lies in its model of a design firm's risk aversion with a utility function or Value-at-Risk (VAR) and its use of that model to identify the optimal resilient design for the risk-averse firm. These risk-averse decision-making methods are applied to a design firm determining the resilience of a new engineered system. This paper significantly advances the engineering design discipline by modeling the firm's appetite for risk within the context of designing a system that can fail due to degradation in the presence of adverse events and can respond to and recover from failure. Since the optimization model requires a complex Monte Carlo simulation to evaluate the objective function, we use a ranking and selection method and Bayesian optimization to find the optimal design. This paper incorporates the design of the wind turbine and the reliability and restoration of the turbine's components for both risk-neutral and risk-averse decision makers. The results show that in order to make the system more resilient, risk-averse firms should pay a larger design cost to prevent catastrophic costs of failure. In this case, the system is less likely to fail due to the high resilience of its physical components. © 2019 Elsevier Ltd}}, 
pages = {106122}, 
number = {NA}, 
volume = {139}
}
@article{10.1142/s0219024920500077, 
year = {2020}, 
title = {{Dynamic Mean-Variance Portfolios with Risk Budget}}, 
author = {LUO, SHENG-FENG}, 
journal = {International Journal of Theoretical and Applied Finance}, 
issn = {02190249}, 
doi = {10.1142/s0219024920500077}, 
abstract = {{We study a dynamic mean-variance portfolio selection problem subject to possible limit of market risk. Three measures of market risk are considered: Value-at-risk, expected shortfall, and median shortfall. They are all calculated in a dynamic consistent sense. After applying the technique of delta-normal approximation, we can explicitly solve for the optimal solution and calculate the economic loss brought by the risk budget constraint. With the analytical results obtained, influential factors of economic loss are then explored by which some guidelines on trading practice are proposed. The guidelines are independent of risk measures, and are valuable to both institutions and regulators, for they suggest that an institutional investor would spontaneously obey good investment discipline to avoid potential impact of risk constraint. This result meets the purpose of external regulation from the perspective of market discipline. © 2020 World Scientific Publishing Company.}}, 
pages = {2050007}, 
number = {1}, 
volume = {23}
}
@article{10.2298/eka1506087c, 
year = {2015}, 
title = {{Extreme value theory in emerging markets: Evidence from the Montenegrin stock exchange}}, 
author = {Cerović, Julija and Karadžić, Vesna}, 
journal = {Economic Annals}, 
issn = {00133264}, 
doi = {10.2298/eka1506087c}, 
abstract = {{The concept of Value at Risk(VaR) estimates the maximum loss of a financial position at a given time for a given probability. This paper considers the adequacy of the methods that are the basis of extreme value theory in the Montenegrin emerging market before and during the global financial crisis. In particular, the purpose of the paper is to investigate whether the peaks-over-threshold method outperforms the block maxima method in evaluation of Value at Risk in emerging stock markets such as the Montenegrin market. The daily return of the Montenegrin stock market index MONEX20 is analysed for the period January 2004 - February 2014. Results of the Kupiec test show that the peaks-over-threshold method is significantly better than the block maxima method, but both methods fail to pass the Christoffersen independence test and joint test due to the lack of accuracy in exception clustering when measuring Value at Risk. Although better, the peaks-over-threshold method still cannot be treated as an accurate VaR model for the Montenegrin frontier stock market.}}, 
pages = {87--116}, 
number = {206}, 
volume = {60}
}
@article{10.1287/ijoc.2020.0965, 
year = {2021}, 
title = {{Selecting the best alternative based on its quantile}}, 
author = {Batur, Demet and Choobineh, F Fred}, 
journal = {INFORMS Journal on Computing}, 
issn = {10919856}, 
doi = {10.1287/ijoc.2020.0965}, 
abstract = {{A value-at-risk, or quantile, is widely used as an appropriate investment selection measure for risk-conscious decision makers. We present two quantile-based sequential procedures—with and without consideration of equivalency between alternatives—for selecting the best alternative from a set of simulated alternatives. These procedures asymptotically guarantee a user-defined target probability of correct selection within a prespecified indifference zone. Experimental results demonstrate the trade-off between the indifference-zone size and the number of simulation iterations needed to render a correct selection while satisfying a desired probability of correct selection. Copyright: © 2020 INFORMS}}, 
number = {2}, 
volume = {33}
}
@article{10.4156/jcit.vol5.issue5.2, 
year = {2010}, 
title = {{The study on the evaluation system of investment value of listed companies on the GEM}}, 
author = {-, Wang Baosen and -, Li Juan and -, Sun Jianmin}, 
journal = {Journal of Convergence Information Technology}, 
issn = {19759320}, 
doi = {10.4156/jcit.vol5.issue5.2}, 
abstract = {{The global financial crisis hastened the development of the Shenzhen GEM is a venture capital a key link in the chain, force the development of SME financing difficulty in resolving the issue. In view of the characteristics and specific risks of Growth Enterprise Market (GEM), this paper measures the market risk of 28 listed companies on the GEM by use of VaR techniques, and introduces VaR into the company's evaluation system of investment value, so it provides a set of effective evaluation method for investment value analysis of companies on Shenzhen GEM.}}, 
pages = {22--29}, 
number = {5}, 
volume = {5}
}
@article{10.1093/jjfinec/nbp003, 
year = {2009}, 
title = {{Measuring event risk}}, 
author = {Nyberg, P and Wilhelmsson, A}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbp003}, 
abstract = {{This paper decomposes the popular risk measure Value-at-Risk (VaR) into one jump- and one continuous component. The continuous component corresponds to general market risk and the jump component is proportional to the event risk as defined in the Basel II accord. We find that event risk, which is currently not incorporated into most banks' VaR models, comprises a substantial part of total VaR. It constitutes 30\% of the risk for a portfolio of small cap stocks but less than 1\% for a portfolio of large cap stocks. The national supervising agency in each membership country is advised by the Basel rules to add an additional capital charge to a bank whose models do not capture event risk. The large variation in event risk, also found across 10 individual stocks, suggests that an approach that varies the capital surcharge, based on the type of asset, should be used by the supervisors. © The Author 2009. Published by Oxford University Press. All rights reserved.}}, 
pages = {265--287}, 
number = {3}, 
volume = {7}
}
@article{10.1016/j.eswa.2010.06.010, 
year = {2011}, 
title = {{Worst-case VaR and robust portfolio optimization with interval random uncertainty set}}, 
author = {Chen, Wei and Tan, Shaohua and Yang, Dongqing}, 
journal = {Expert Systems with Applications}, 
issn = {09574174}, 
doi = {10.1016/j.eswa.2010.06.010}, 
abstract = {{This paper addresses a new uncertainty set - interval random uncertainty set for worst-case value-at-risk and robust portfolio optimization. The form of interval random uncertainty set makes it suitable for capturing the downside and upside deviations of real-world data. These deviation measures capture distributional asymmetry and lead to better optimization results. We also apply our interval random chance-constrained programming to robust worst-case value-at-risk optimization under interval random uncertainty sets in the elements of mean vector and covariance matrix. Numerical experiments with real market data indicate that our approach results in better portfolio performance. © 2010 Elsevier Ltd. All rights reserved.}}, 
pages = {64--70}, 
number = {1}, 
volume = {38}
}
@article{10.1016/j.matcom.2013.08.010, 
year = {2013}, 
title = {{GFC-robust risk management under the Basel Accord using extreme value methodologies}}, 
author = {Jimenez-Martin, Juan-Angel and McAleer, Michael and Pérez-Amaral, Teodosio and Santos, Paulo Araújo}, 
journal = {Mathematics and Computers in Simulation}, 
issn = {03784754}, 
doi = {10.1016/j.matcom.2013.08.010}, 
abstract = {{In this paper we provide further evidence on the suitability of the median of the point VaR forecasts of a set of models as a GFC-robust strategy by using an additional set of new extreme value forecasting models and by extending the sample period for comparison. The median is not affected by extremes, unlike the mean. In periods of contagion, wherein the number and values of extremes are substantially greater, the use of the median would be expected to be even more robust than the mean. These extreme value models include DPOT and Conditional EVT. Such models might be expected to be useful in explaining financial data, especially in the presence of extreme shocks that arise during a GFC. Our empirical results confirm that the median remains GFC-robust even in the presence of these new extreme value models. This is illustrated by using the S\&P500 index before, during and after the 2008-2009 GFC. We investigate the performance of a variety of single and combined VaR forecasts in terms of daily capital requirements and violation penalties under the Basel II Accord, as well as other criteria, including several tests for independence of the violations. The strategy based on the median, or more generally, on combined forecasts of single models, is straightforward to incorporate into existing computer software packages that are used by banks and other financial institutions. © 2013 IMACS.}}, 
pages = {223--237}, 
number = {NA}, 
volume = {94}
}
@article{10.1080/07350015.2014.917979, 
year = {2014}, 
title = {{A Varying-Coefficient Expectile Model for Estimating Value at Risk}}, 
author = {Xie, Shangyu and Zhou, Yong and Wan, Alan T K}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {07350015}, 
doi = {10.1080/07350015.2014.917979}, 
abstract = {{This article develops a nonparametric varying-coefficient approach for modeling the expectile-based value at risk (EVaR). EVaR has an advantage over the conventional quantile-based VaR (QVaR) of being more sensitive to the magnitude of extreme losses. EVaR can also be used for calculating QVaR and expected shortfall (ES) by exploiting the one-to-one mapping from expectiles to quantiles, and the relationship between VaR and ES. Previous studies on conditional EVaR estimation only considered parametric autoregressive model set-ups, which account for the stochastic dynamics of asset returns but ignore other exogenous economic and investment related factors. Our approach overcomes this drawback and allows expectiles to be modeled directly using covariates that may be exogenous or lagged dependent in a flexible way. Risk factors associated with profits and losses can then be identified via the expectile regression at different levels of prudentiality. We develop a local linear smoothing technique for estimating the coefficient functions within an asymmetric least squares minimization set-up, and establish the consistency and asymptotic normality of the resultant estimator. To save computing time, we propose to use a one-step weighted local least squares procedure to compute the estimates. Our simulation results show that the computing advantage afforded by this one-step procedure over full iteration is not compromised by a deterioration in estimation accuracy. Real data examples are used to illustrate our method. Supplementary materials for this article are available online. © 2014, © 2014 American Statistical Association.}}, 
pages = {576--592}, 
number = {4}, 
volume = {32}
}
@article{10.4324/9780203475966, 
year = {2013}, 
title = {{Risk management: Practical risk management}}, 
author = {Alexander, Keith}, 
issn = {NA}, 
doi = {10.4324/9780203475966}, 
abstract = {{Clearly risk can have different meanings but the normal understanding is that the risk could actually happen, and its consequences might not be pleasant. Definitions of risk must always relate to the risk of something happening in a specific time period. A variety of terms are in common usage which are often meaningful to those active in the business within which they are employed, but they can lack precision when it becomes necessary to examine them critically. For example, risk can be used to mean: • a hazard or unsafe practice; • a peril capable of being insured, i.e. fire or storm; • the subject matter of an insurance policy; • a statistical probability; • loss potential as assessed by an insurance surveyor – estimated maximum loss; • the actual value at risk. © 1996 Taylor \& Francis.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1007/s10614-019-09913-y, 
year = {2020}, 
title = {{Risk-Constrained Kelly Portfolios Under Alpha-Stable Laws}}, 
author = {Wesselhöfft, Niels and Härdle, Wolfgang K.}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-019-09913-y}, 
abstract = {{This paper provides a detailed framework for modeling portfolios, achieving the highest growth rate under risk constraints such as value at risk (VaR) and expected shortfall (ES) in the presence of α-stable laws. Although the maximization of the expected logarithm of wealth induces outperforming any other significantly different strategy, the Kelly criterion implies larger bets than a risk-averse investor would accept. Restricting the Kelly optimization by spectral risk measures, the authors provide a generalized mapping for different measures of growth and risk. Analyzing over 30 years of S\&P 500 returns for different sampling frequencies, the authors find evidence for leptokurtic behavior for all respective sampling frequencies. Given that lower sampling frequencies imply a smaller number of data points, this paper argues in favor of α-stable laws and its scaling behavior to model financial market returns for a given horizon in an i.i.d. world. Instead of simulating from the class of elliptically α-stable distributions, a semiparametric scaling approximation, based on hourly NASDAQ data, is proposed. Our paper also uncovers that including long put options into the portfolio optimization, improves portfolio growth for a given level of VaR or ES, leading to a new Kelly portfolio providing the highest geometric mean. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {801--826}, 
number = {3}, 
volume = {55}
}
@article{10.1287/mnsc.1040.0355, 
year = {2005}, 
title = {{Generating volatility forecasts from value at risk estimates}}, 
author = {Taylor, James W}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.1040.0355}, 
abstract = {{Statistical volatility models rely on the assumption that the shape of the conditional distribution is fixed over time and that it is only the volatility that varies. The recently proposed conditional autoregressive value at risk (CAViaR) models require no such assumption, and allow quantiles to be modeled directly in an autoregressive framework. Although useful for risk management, CAViaR models do not provide volatility forecasts. Such forecasts are needed for several other important applications, such as option pricing and portfolio management. It has been found that, for a variety of probability distributions, there is a surprising constancy of the ratio of the standard deviation to the interval between symmetric quantiles in the tails of the distribution, such as the 0.025 and 0.975 quantiles. This result has been used in decision and risk analysis to provide an approximation of the standard deviation in terms of quantile estimates provided by experts. Drawing on the same result, we construct financial volatility forecasts as simple functions of the interval between CAViaR forecasts of symmetric quantiles. Forecast comparison, using five stock indices and 20 individual stocks, shows that the method is able to outperform generalized autoregressive conditional heteroskedasticity (GARCH) models and moving average methods. © 2005 INFORMS.}}, 
pages = {712--725}, 
number = {5}, 
volume = {51}
}
@article{10.1016/j.csda.2006.08.004, 
year = {2007}, 
title = {{Approximating the distributions of estimators of financial risk under an asymmetric Laplace law}}, 
author = {Trindade, A. Alexandre and Zhu, Yun}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2006.08.004}, 
abstract = {{Explicit expressions are derived for parametric and nonparametric estimators (NPEs) of two measures of financial risk, value-at-risk (VaR) and conditional value-at-risk (CVaR), under random sampling from the asymmetric Laplace (AL) distribution. Asymptotic distributions are established under very general conditions. Finite sample distributions are investigated by means of saddlepoint approximations. The latter are highly computationally intensive, requiring novel approaches to approximate moments and special functions that arise in the evaluation of the moment generating functions. Plots of the resulting density functions shed new light on the quality of the estimators. Calculations for CVaR reveal that the NPE enjoys greater asymptotic efficiency relative to the parametric estimator than is the case for VaR. An application of the methodology in modeling currency exchange rates suggests that the AL distribution is successful in capturing the peakedness, leptokurticity, and skewness, inherent in such data. A demonstrated superiority in the resulting parametric-based inferences delivers an important message to the practitioner. © 2006 Elsevier B.V. All rights reserved.}}, 
pages = {3433--3447}, 
number = {7}, 
volume = {51}
}
@article{10.12733/jics20106184, 
year = {2015}, 
title = {{Optimize investment application of data mining based on stochastic simulation and GARCH distribution}}, 
author = {Wang, Binhui}, 
journal = {Journal of Information and Computational Science}, 
issn = {15487741}, 
doi = {10.12733/jics20106184}, 
abstract = {{Data mining aims to find out the law of reality and forecast data. It is a mathematical analysis and forecasting methods. Currently, data mining in the stock market research is increasingly hot. Recent approaches in the use of Bollinger Bands depicts the stock to approximate has certain accuracy, but still insufficient to judge price change. In order to solve this issue, this paper proposes the use of stochastic simulation and GARCH distribution to estimate assets VaR and improve Bollinger rail-lines, thus construct new stock running channel based on VaR. Then, implement this method and apply it on the true Shanghai composite index data. The experimental results show that the use of this method to construct the stock market run channel is effective and can better characterize the running of stock market. Finally, optimize the investment strategies based on this structure optimization. 1548-7741/Copyright © 2015 Binary Information Press}}, 
pages = {3889--3898}, 
number = {10}, 
volume = {12}
}
@article{10.1016/j.ribaf.2021.101435, 
year = {2021}, 
title = {{Multiscale stock-bond correlation: Implications for risk management}}, 
author = {Rababa’a, Abdel Razzaq Al and Alomari, Mohammad and McMillan, David}, 
journal = {Research in International Business and Finance}, 
issn = {02755319}, 
doi = {10.1016/j.ribaf.2021.101435}, 
abstract = {{This paper examines the multiscale return correlation between the stocks and government bonds of different maturities returns in 25 countries. The analysis reveals that developed markets correlations are generally negative at the first time-scale and move in a positive direction at higher scales. This contrasts with emerging markets, where the correlation tends to be positive throughout. Thus, the results support a greater flight-to-safety effect in developed markets. Further evidence highlights the ability of the correlation to produce portfolios with a lower VaR. Results support this at longer time-scales and for both developed and emerging markets. The results here demonstrate the importance of accounting for time-scales in modelling the stock-bond correlation and in constructing portfolios. © 2021 Elsevier B.V.}}, 
pages = {101435}, 
number = {NA}, 
volume = {58}
}
@article{10.1016/j.eneco.2016.04.011, 
year = {2016}, 
title = {{Hedging strategy for ethanol processing with copula distributions}}, 
author = {Awudu, Iddrisu and Wilson, William and Dahl, Bruce}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2016.04.011}, 
abstract = {{It has become important for ethanol producers to hedge input and output price risks. The purpose of this paper is to analyze an ethanol-producing firm's strategy to reduce price risks for inputs and outputs. Corn is the primary input, and the outputs are ethanol, corn oil, distillers' dried grains (DDGs), and renewable identification numbers (RINs). A theoretical model is developed including margins and risk is measured using value at risk (VaR). An empirical model is developed and extended to VaR using copulas to analyze the marginal distribution and dependence structure for input and output prices on margins. Efficient frontier curves analyzing VaR with and without copula are discussed. The results compare varying risk-strategy measures for long corn, short corn, and combining short and long corn. Sensitivity analyses are conducted for functional changes in the margin as a result of ethanol price changes. © 2016 Elsevier B.V.}}, 
pages = {59--65}, 
number = {NA}, 
volume = {57}
}
@article{10.1016/j.jlp.2009.08.003, 
year = {2010}, 
title = {{Risk measures constituting a risk metrics which enables improved decision making: Value-at-Risk}}, 
author = {Prem, Katherine P. and Ng, Dedy and Pasman, Hans J. and Sawyer, Mike and Guo, Yuyan and Mannan, M. Sam}, 
journal = {Journal of Loss Prevention in the Process Industries}, 
issn = {09504230}, 
doi = {10.1016/j.jlp.2009.08.003}, 
abstract = {{In this work, a novel approach is proposed for expressing the risks of process plants consisting of a large number of scenarios, in the form of a risk metrics of leading indicators to prevent potential high profile industry accidents. The methodology includes: 1) risk estimation of a portfolio by CPQRA (or QRA), 2) monetization of the tangible risks with the inclusion of the lost time of production, 3) estimation of the maximum portfolio loss using Value-at-Risk approach, 4) inclusion of intangible risks using FN-curve and, 5) generation of F\$-curve of tangible risks. The proposed methodology can particularly help in understanding the stakes at risk by performing the overall cost-benefit analysis, for identifying the most risky scenarios and identifying critical equipments to enable better risk-informed decision making in order to adopt appropriate risk mitigation measures. This work establishes the groundwork for developing measures for understanding and comparing the large number of risk values derived from QRA studies for large portfolios. It will aid in less subjective decision making as it enables the decision maker to choose the most preferred portfolio option among alternatives. Decisions made with the accurate understanding of the consequences of risks can significantly reduce potential work-related fatalities, property losses and save millions of dollars. © 2009 Elsevier Ltd. All rights reserved.}}, 
pages = {211--219}, 
number = {2}, 
volume = {23}
}
@article{10.1049/cp.2019.0428, 
year = {2019}, 
title = {{Confidence interval based distributionally robust real-time dispatch considering wind power uncertainty}}, 
author = {Li, Peng and Yang, Ming and Yang, Jiajun and Du, Pingjing and Yan, Fangqing and Zhu, Yi}, 
journal = {8th Renewable Power Generation Conference (RPG 2019)}, 
issn = {NA}, 
doi = {10.1049/cp.2019.0428}, 
abstract = {{A confidence interval based distributionally robust real-time dispatch (CI-DRTD) approach considering wind power uncertainty is proposed, which can strike a balance between the operational costs and risk even when the wind power probability distribution cannot be precisely estimated. Based on the imprecise probability theory, the confidence interval based ambiguity set of wind power uncertainty can be constructed. By the identification of the worst distribution, the original distributionally robust optimization problem can be reformulated as a robust economic dispatch problem. By using the algorithm developed in our previous work, the robust economic dispatch model can be reformulated as a mixed integer linear programming problem, for which off-the-shelf solvers are available. Case studies verify the effectiveness and efficiency of the proposed approach. © 2019 Institution of Engineering and Technology. All rights reserved.}}, 
pages = {172 (6 pp.)--172 (6 pp.)}, 
number = {CP764}, 
volume = {2019}
}
@article{10.1016/j.finmar.2020.100562, 
year = {2021}, 
title = {{Measurement of common risks in tails: A panel quantile regression model for financial returns}}, 
author = {Baruník, Jozef and Čech, František}, 
journal = {Journal of Financial Markets}, 
issn = {13864181}, 
doi = {10.1016/j.finmar.2020.100562}, 
abstract = {{We investigate how to measure common risks in the tails of return distributions using the recently proposed panel quantile regression model for financial returns. By exploring how volatility crosses all quantiles of the return distribution and using a fixed effects estimator, we can control for otherwise unobserved heterogeneity among financial assets. Direct benefits are revealed in a portfolio value-at-risk application, where our modeling strategy performs significantly better than several benchmark models. In particular, our results show that the panel quantile regression model for returns consistently outperforms all competitors in the left tail. Sound statistical performance translates directly into economic gains. © 2020 Elsevier B.V.}}, 
pages = {100562}, 
number = {NA}, 
volume = {52}
}
@article{10.1016/j.apm.2011.03.032, 
year = {2011}, 
title = {{A matrix-based VaR model for risk identification in power supply networks}}, 
author = {Chang, Chen-Sung}, 
journal = {Applied Mathematical Modelling}, 
issn = {0307904X}, 
doi = {10.1016/j.apm.2011.03.032}, 
abstract = {{This paper presents a value-at-risk (VaR) model based on the singular value decomposition (SVD) of a sparsity matrix for voltage risk identification in power supply networks. The matrix-based model provides a more computationally efficient risk assessment method than conventional models such as probability analysis and sensitivity analysis, for example, and provides decision makers in the power supply industry with sufficient information to minimize the risk of network collapse or blackouts. The VaR model is incorporated into a risk identification system (RIS) programmed in the MATLAB environment. The feasibility of the proposed approach is confirmed by performing a series of risk assessment simulations using the standard American Electric Power (AEP) test models (i.e. 14-, 30- and 57-node networks) and a real-world power network (Taiwan power network), respectively. In general, the simulated results confirm the ability of the matrix-based model VaR model to efficient identify risk of power supply networks. © 2011 Elsevier Inc.}}, 
pages = {4567--4574}, 
number = {9}, 
volume = {35}
}
@article{10.1080/00207543.2011.564672, 
year = {2011}, 
title = {{A note to 'Enterprise risk management: A DEA VaR approach in vendor selection': A response to Wei and Wang and model extension}}, 
author = {Wu, Desheng Dash and Olson, David}, 
journal = {International Journal of Production Research}, 
issn = {00207543}, 
doi = {10.1080/00207543.2011.564672}, 
abstract = {{Enterprise risk management (ERM) has become an important topic in today's more complex, interrelated global business environment, replete with threats from natural, political, economic and technical sources. Wu and Olson (2010) [Wu, D.S. and Olson, D., 2010. Enterprise risk management: a DEA VaR approach in vendor selection. International Journal of Production Research 48 (16), 4919-4932] present a state-of-the-art overview of Enterprise risk management and discuss the possibility of constructing a value at risk measure at the data envelopment analysis (DEA) framework. Wei and Wang [Wei, G.W. and Wang, J.M., 2011. Value-at-risk and data envelopment analysis: comments on Wu and Olson (2010). International Journal of Production Research, 49 (23), 7189-7193] contend errors in the article. Wei and Wang suggest a model based on Li [Li, S.X., 1998. Stochastic models and variable returns to scales in data envelopment analysis. European Journal of Operational Research, 104, 532-548]. We show that the suggested model in Wei and Wang (2011) does not solve the problem completely. We provide alternative approaches to conduct performance evaluation with good discriminating power. © 2011 Taylor \& Francis.}}, 
pages = {7195--7203}, 
number = {23}, 
volume = {49}
}
@article{10.1016/j.cam.2016.07.015, 
year = {2017}, 
title = {{Comonotonic approximations of risk measures for variable annuity guaranteed benefits with dynamic policyholder behavior}}, 
author = {Feng, Runhuan and Jing, Xiaochen and Dhaene, Jan}, 
journal = {Journal of Computational and Applied Mathematics}, 
issn = {03770427}, 
doi = {10.1016/j.cam.2016.07.015}, 
abstract = {{The computation of various risk metrics is essential to the quantitative risk management of variable annuity guaranteed benefits. The current market practice of Monte Carlo simulation often requires intensive computations, which can be very costly for insurance companies to implement and take so much time that they cannot obtain information and take actions in a timely manner. In an attempt to find low-cost and efficient alternatives, we explore the techniques of comonotonic bounds to produce closed-form approximation of risk measures for variable annuity guaranteed benefits. The techniques are further developed in this paper to address in a systematic way risk measures for death benefits with the consideration of dynamic policyholder behavior, which involves very complex path-dependent structures. In several numerical examples, the method of comonotonic approximation is shown to run several thousand times faster than simulations with only minor compromise of accuracy. © 2016 Elsevier B.V.}}, 
pages = {272--292}, 
number = {NA}, 
volume = {311}
}
@article{10.1109/cse.2008.47, 
year = {2008}, 
title = {{Building Efficient Frontier by CVaR minimization for non-normal asset returns using Copula Theory}}, 
author = {Agrawal, Kapil}, 
journal = {2008 11th IEEE International Conference on Computational Science and Engineering}, 
issn = {NA}, 
doi = {10.1109/cse.2008.47}, 
abstract = {{In the realm of Computational Finance, the performance of the optimal portfolio largely depends upon its composition and its ability to accurately predict the market movements. Recent empirical studies have shown that the underlying assumption of normality of asset returns for risk modeling is seriously flawed, in view of their asymmetric and fat-tailed behavior. This problem is further aggravated when we delve into the functioning of the Financial market and realize that the market parameters have highly nonlinear kind of inter-dependence amongst themselves. Any investment portfolio that does not account for these factors and their mutual relationship, will tend to under-perform. This work is a novel attempt, which aims at developing a framework which solves all of these problems in an integrated fashion, without overlooking any of them or pre-assigning lesser importance to any of these issues. The contemporary techniques often neglect one of them, resulting in an incomplete and sometimes even a misleading picture of the market scenario. In this work, copula theory effectively captures the non-linear inter-dependence. The scenarios are generated from a non-elliptical multivariate distribution constructed by a Students t-copula assuming marginal distributions as Gaussian in the center and EVT distributed in the tail. For gauging the market risk we have used CVaR (Conditional Value-at-Risk) as the risk measure. The efficient frontier thus resulted by minimizing the CVaR and maximizing the returns, gives a clear insight into how does the composition of the optimal portfolio changes with respect to change in CVaR of the portfolio. Our aim is to prove that much more reliable conclusions will certainly be drawn if a more realistic representation of data can be done using the concept of Copulas. © 2008 IEEE.}}, 
pages = {319--326}, 
number = {NA}, 
volume = {NA}
}
@article{10.11591/ijeecs.v21.i1.pp406-411, 
year = {2021}, 
title = {{Evaluation of options portfolios for exchange rate hedges}}, 
author = {Jiménez-Gómez, Miguel and Acevedo-Prins, Natalia and Rojas-López, Miguel}, 
journal = {Indonesian Journal of Electrical Engineering and Computer Science}, 
issn = {25024752}, 
doi = {10.11591/ijeecs.v21.i1.pp406-411}, 
abstract = {{In this paper evaluate six exchange rate hedging strategies with financial options from the OTC market in Colombia. Three hedging strategies for importers and three for exporters were raised. The coverage for importers was carried out with the traditional strategy of Long Call, Bull Call Spread and Bull Put Spread, the last two correspond to options portfolios. The coverage for importers was carried out with the traditional strategy of Long Put, Bear Call Spread and Bear Put Spread, the last two correspond to options portfolios. To determine the best hedging strategy, the currency price was modeled with a Wiener process and the VaR for the six covered scenarios was calculated and compared with the VaR of the uncovered scenario. The results shown by the six hedging strategies manage to mitigate the exchange risk, but the most efficient strategies are the traditional ones for both importers and exporters. © 2021 Institute of Advanced Engineering and Science. All rights reserved.}}, 
pages = {406--411}, 
number = {1}, 
volume = {21}
}
@article{10.1007/s11156-006-0010-y, 
year = {2007}, 
title = {{A robust VaR model under different time periods and weighting schemes}}, 
author = {Angelidis, Timotheos and Benos, Alexandros and Degiannakis, Stavros}, 
journal = {Review of Quantitative Finance and Accounting}, 
issn = {0924865X}, 
doi = {10.1007/s11156-006-0010-y}, 
abstract = {{This paper analyses several volatility models by examining their ability to forecast Value-at-Risk (VaR) for two different time periods and two capitalization weighting schemes. Specifically, VaR is calculated for large and small capitalization stocks, based on Dow Jones (DJ) Euro Stoxx indices and is modeled for long and short trading positions by using non parametric, semi parametric and parametric methods. In order to choose one model among the various forecasting methods, a two-stage backtesting procedure is implemented. In the first stage the unconditional coverage test is used to examine the statistical accuracy of the models. In the second stage a loss function is applied to investigate whether the differences between the models, that calculated accurately the VaR, are statistically significant. Under this framework, the combination of a parametric model with the historical simulation produced robust results across the sample periods, market capitalization schemes, trading positions and confidence levels and therefore there is a risk measure that is reliable. © Springer Science+Business Media, LLC 2007.}}, 
pages = {187--201}, 
number = {2}, 
volume = {28}
}
@article{10.1023/b:rast.0000028190.48665.d0, 
year = {2004}, 
title = {{How banks' value-at-risk disclosures predict their total and priced risk: Effects of bank technical sophistication and learning over time}}, 
author = {Liu, Chi-chun and Ryan, Stephen G. and Tan, Hung}, 
journal = {Review of Accounting Studies}, 
issn = {13806653}, 
doi = {10.1023/b:rast.0000028190.48665.d0}, 
abstract = {{Using a sample of eight large commercial banks from 1994 to 2000, Jorion (2002) finds that banks' VaR disclosures for their trading portfolios predict trading income variability. We extend Jorion's findings using a larger sample of 17 banks from 1997 to 2002 reporting trading VaRs under FRR No. 48 (1997). We find that banks' trading VaRs have predictive power for trading income variability that increases with bank technical sophistication and over time. We find that banks' trading VaRs have predictive power for a bank-wide measure of total risk, return variability, and for two bank-wide measures of priced risk, beta and realized returns.}}, 
pages = {265--294}, 
number = {2-3}, 
volume = {9}
}
@article{10.1016/j.pacfin.2016.01.004, 
year = {2017}, 
title = {{Islamic or conventional mutual funds: Who has the upper hand? Evidence from Malaysia}}, 
author = {Boo, Yee Ling and Ee, Mong Shan and Li, Bob and Rashid, Mamunur}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2016.01.004}, 
abstract = {{Contradictory results are documented in the literature regarding which type of mutual fund has superior performance; an Islamic or conventional mutual fund. Due to the relative short history of the Islamic mutual funds’ industry, prior literature has inevitably relied on a small sample size with a short sample period. With the longest applicable sample period, this study represents one of the most recent attempts to address this conflicting evidence. We find there is no clear cut over performance by Islamic mutual funds against their conventional peers across the three financial crises in our sample period, with the exception of the most recent global financial crisis, where Islamic mutual funds generally outperformed their conventional counterparts. We further find that Islamic funds significantly outperformed conventional funds in the riskiest asset class, equity, one year before and during the global financial crisis. We further reveal that the modified value at risk for Islamic mutual funds was significantly lower than their conventional peers during the global financial crisis. This seems to indicate that Islamic mutual funds have better risk management compared to conventional peers. © 2016 Elsevier B.V.}}, 
pages = {183--192}, 
number = {NA}, 
volume = {42}
}
@article{10.21314/jop.2019.225, 
year = {2019}, 
title = {{Quantification of operational risk: Statistical insights on coherent risk measures}}, 
author = {Vee, Dany Ng Cheong and Gonpot, Preethee and Ramanathan, T V}, 
journal = {Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2019.225}, 
abstract = {{Operational risk is becoming a major part of corporate governance in companies, especially in the financial services industry. In this paper, we review some of the existing methods used to quantify operational risks in the banking and insurance industries. These methods use recent statistical concepts such as extreme value theory and copula modeling.We explore the possibility of using a coherent risk measure – expected shortfall (ES) – to quantify operational risk. The suitability of the suggested risk measures has been investigated with the help of simulated data sets for two business lines. The generalized Pareto distribution is used for modeling the tails, and three distributions – lognormal, Weibull and Gamma – are used for the body data. Our results show that ES under all three distributions tends to be significantly larger than value-at-risk, which may lead to overestimating the operational loss and consequently overestimating the capital charge. However, the modified ES seems to provide a better way of mitigating any overestimation. © 2019 Infopro Digital Risk (IP) Limited. All rights reserved.}}, 
pages = {39--50}, 
number = {2}, 
volume = {14}
}
@article{10.3969/j.issn.1001-0505.2012.06.036, 
year = {2012}, 
title = {{Discrete traffic network design model under supply and demand uncertainty}}, 
author = {}, 
issn = {10010505}, 
doi = {10.3969/j.issn.1001-0505.2012.06.036}, 
abstract = {{Regarding the uncertainty of traffic supply, demand and decision principles in traffic design, a design model for uncertain discrete traffic network based on uncertainty theory is proposed. Firstly, supply and demand elements are assumed to be stochastic variables. The semi-standard error and value at risk are used to measure uncertainty and a multi-objective bi-level optimized model considering decision risk attitude is built. Then, SPEA2 (strength pareto evolutionary algorithm 2) based on the conception of Pareto is applied to solve the multi-objective bi-level optimized model. The numerical results of Nguyen-Dupuis network show that the risk value can reflect decision maker's attitude towards risk in measuring traffic design. In various targets there are alternative relations in multi-object design. The increase of the construction funds can effectively improve the network performance. However, such improvement is not obvious when the investment is beyond a certain limit.}}, 
number = {6}, 
volume = {42}
}
@article{10.1061/9780784478479.009, 
year = {2014}, 
title = {{An optimization model for setting subway station foundation pit monitoring item}}, 
author = {Liu, Xin and Hong, BaoNing and Zhang, Peng and Sheng, Ke}, 
journal = {Analysis, Design, and Construction of Tunnels and Underground Structures}, 
issn = {08950563}, 
doi = {10.1061/9780784478479.009}, 
abstract = {{This paper takes probability of status judgment, value-at-risk of foundation pit, and cost of monitoring items into consideration, which related to the influence of foundation pit risk event to establishing a loss function of monitoring items and constructing a model for setting optimizing monitoring items to maximize their effect at the lowest cost, based on Bayes theorem. The safety status of the foundation pit was divided into nine levels. Also divided was the security coefficients obtained by numerical calculating into discrete data based the reliability theory to establish an optimization model as the corresponding calculation method. By calculating and dividing the security coefficient of an engineering example, the foundation pit monitoring items were ascertained through the optimization model, and the results of optimization were analyzed. © ASCE 2014.}}, 
pages = {70--77}, 
number = {247 GSP}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2018.05.012, 
year = {2018}, 
title = {{Downside risk and stock returns in the G7 countries: An empirical analysis of their long-run and short-run dynamics}}, 
author = {Chen, Cathy Yi-Hsuan and Chiang, Thomas C. and Härdle, Wolfgang Karl}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2018.05.012}, 
abstract = {{Any risk-return tradeoff analysis in aggregate equity markets relies on appropriate measures of risk, in most studies based on (co-)variance relations. Consequently, in integrated global markets, country-specific expected return is priced with a world price of covariance risk. This study relates domestic excess stock returns to the world downside risk. Evidence shows that downside tail risk (as a multiplier of volatility) has long memory cointegration properties; hence, the underlying risk aversion behavior in an integrated market is associated with the conditional quantile ratio, the correlation of stock returns, and the cointegrating coefficient of downside risk. Our empirical results based on G7 countries indicate that investors are averse to downside risk, which via Cornish–Fisher expansions is related to higher moment risk and interpretable in a utility-based decision framework. © 2018 Elsevier B.V.}}, 
pages = {21--32}, 
number = {NA}, 
volume = {93}
}
@article{10.3390/su10030747, 
year = {2018}, 
title = {{A probabilistic alternative approach to optimal project profitability based on the value-at-risk}}, 
author = {Kim, Yonggu and Lee, Eul-Bum}, 
journal = {Sustainability}, 
issn = {20711050}, 
doi = {10.3390/su10030747}, 
abstract = {{This paper focuses on an investment decision-making process for sustainable development based on the profitability impact factors for overseas projects. Investors prefer to use the discounted cash-flow method. Although this method is simple and straightforward, its critical weakness is its inability to reflect the factor volatility associated with the project evaluation. To overcome this weakness, the Value-at-Risk method is used to apply the volatility of the profitability impact factors, thereby reflecting the risks and establishing decision-making criteria for risk-averse investors. Risk-averse investors can lose relatively acceptable investment opportunities to risk-neutral or risk-amenable investors due to strict investment decision-making criteria. To overcome this problem, critical factors are selected through a Monte Carlo simulation and a sensitivity analysis, and solutions to the critical-factor problems are then found by using the Theory of Inventive Problem Solving and a business version of the Project Definition Rating Index. This study examines the process of recovering investment opportunities with projects that are investment feasible and that have been rejected when applying the criterion of the Value-at-Risk method. To do this, a probabilistic alternative approach is taken. To validate this methodology, the proposed framework for an improved decision-making process is demonstrated using two actual overseas projects of a Korean steel-making company. © 2018 by the authors.}}, 
pages = {747}, 
number = {3}, 
volume = {10}
}
@article{10.1080/14697688.2019.1579923, 
year = {2021}, 
title = {{Forecasting robust value-at-risk estimates: evidence from UK banks}}, 
author = {Sampid, Marius Galabe and Hasim, Haslifah M.}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697688.2019.1579923}, 
abstract = {{In this paper, we present a novel approach for forecasting Value-at-Risk (VaR) by combining a Bayesian GARCH(1,1) model with Student's-t distribution for the underlying volatility models, vine copula functions to model dependence, and the peaks-over-threshold (POT) method of extreme value theory (EVT) to model the tail behaviour of asset returns. We further propose a new approach for threshold selection in extreme value analysis, which we call a hybrid method. The empirical results and back-testing analysis show that the model captures VaR quite well through periods of calmness and crisis; therefore, it is suitable for use as a measure of risk. Our results also suggest that with a correct implementation of the VaR model, Basel III is not needed. © 2019 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--21}, 
number = {11}, 
volume = {21}
}
@article{10.21314/jrmv.2017.179, 
year = {2017}, 
title = {{The use of the triangular approximation for some complicated risk measurement calculations}}, 
author = {Georgiopoulos, Nick}, 
journal = {The Journal of Risk Model Validation}, 
issn = {17539579}, 
doi = {10.21314/jrmv.2017.179}, 
abstract = {{We introduce the triangular approximation to the normal distribution in order to extract closed- and semi-closed-form solutions that are useful in risk measurement calculations. In risk measurement models there is usually a normal distribution together with some other distributions in a portfolio of risks. Exceedance probability or value-atrisk (VaR) calculations for these portfolios require simulations. However, with the use of the triangular approximation to the normal density we can have closed-form solutions for risk measurements using actuarial models that include not only insurance risk, such as gamma- and Pareto-distributed losses, but also financial risk. We also approximate the collective risk model under lognormally distributed severities and estimate its VaR. We evaluate the accuracy of the analytic solutions versus the Monte Carlo estimates and, according to our results, the analytic solutions requiring much less computational time are quite good. © 2017 Infopro Digital Risk (IP) Limited.}}, 
pages = {69--98}, 
number = {3}, 
volume = {11}
}
@article{10.1016/j.jmva.2019.03.004, 
year = {2019}, 
title = {{Model selection in sparse high-dimensional vine copula models with an application to portfolio risk}}, 
author = {Nagler, T. and Bumann, C. and Czado, C.}, 
journal = {Journal of Multivariate Analysis}, 
issn = {0047259X}, 
doi = {10.1016/j.jmva.2019.03.004}, 
abstract = {{Vine copulas allow the construction of flexible dependence models for an arbitrary number of variables using only bivariate building blocks. The number of parameters in a vine copula model increases quadratically with the dimension, which poses challenges in high-dimensional applications. To alleviate the computational burden and risk of overfitting, we propose a modified Bayesian information criterion (BIC) tailored to sparse vine copula models. We argue that this criterion can consistently distinguish between the true and alternative models under less stringent conditions than the classical BIC. The criterion suggested here can further be used to select the hyper-parameters of sparse model classes, such as truncated and thresholded vine copulas. We present a computationally efficient implementation and illustrate the benefits of the proposed concepts with a case study where we model the dependence in a large portfolio. © 2019 Elsevier Inc.}}, 
pages = {180--192}, 
number = {NA}, 
volume = {172}
}
@article{10.1109/ifsa-scis.2017.8023285, 
year = {2017}, 
title = {{Interval-valued risk measure models and empirical analysis}}, 
author = {Li, Zihe and Zhang, Jinping and Wang, Xiaoying}, 
journal = {2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS)}, 
issn = {NA}, 
doi = {10.1109/ifsa-scis.2017.8023285}, 
abstract = {{In financial market, we take returns of risk assets as random intervals due to uncertainty. For this case, models of value at risk (VaR) and conditional value at risk (CVaR) are proposed. Some empirical examples are given. © 2017 IEEE.}}, 
pages = {1--4}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/tpwrs.2009.2016521, 
year = {2009}, 
title = {{An improved OPA model and blackout risk assessment}}, 
author = {Mei, Shengwei and He, Fei and Zhang, Xuemin and Wu, Shengyu and Wang, Gang}, 
journal = {IEEE Transactions on Power Systems}, 
issn = {08858950}, 
doi = {10.1109/tpwrs.2009.2016521}, 
abstract = {{The ORNL-PSerc-Alaska (OPA) model is a blackout model proposed by researchers at Oak Ridge National Laboratory (ORNL), Power System Engineering Research Center of Wisconsin University (PSerc), and Alaska University (Alaska). Although the OPA model is a landmark study, it has two limitations. First, there is a significant difference between simulation and practice in transmission line outage and update; and second, the simulation of cascading failure and the probability distribution of blackout size are in general not accurate enough. Hence, an improved OPA model is proposed in this paper to address these limitations. The proposed model contains two layers of iteration. The inner iteration describes the fast dynamics of the system and considers the influence of power flow, dispatching, automation, relay protection, and so on. The outer iteration describes the slow overall system evolution and is concerned with the update of the power grid, operation modes and planning. Such a model can be applied to practical large-scale systems. Furthermore, based on the Value at Risk (VaR) and Conditional Value at Risk (CVaR), two new complementary blackout risk indices are defined, which reveal critical characteristics of blackouts and are used to evaluate security levels of power systems. The effectiveness of the improved OPA model is verified by the simulations concerning the Northeast Power Grid of China. © 2009 IEEE.}}, 
pages = {814--823}, 
number = {2}, 
volume = {24}
}
@article{10.3390/math7030274, 
year = {2019}, 
title = {{Risk measurement of stock markets in BRICS, G7, and G20: Vine copulas versus factor copulas}}, 
author = {Song, Quanrui and Liu, Jianxu and Sriboonchitta, Songsak}, 
journal = {Mathematics}, 
issn = {22277390}, 
doi = {10.3390/math7030274}, 
abstract = {{Multivariate copulas have been widely used to handle risk in the financial market. This paper aimed to adopt two novel multivariate copulas, Vine copulas and Factor copulas, to measure and compare the financial risks of the emerging economy, developed economy, and global economy. In this paper, we used data from three groups (BRICS, which stands for emerging markets, specifically, those of Brazil, Russia, India, China, and South Africa; G7, which refers to developed countries; and G20, which represents the global market), separated into three periods (pre-crisis, crisis, and post-crisis) and weighed Value at Risk (VaR) and Expected Shortfall (ES) (based on their market capitalization) to compare among three copulas, C-Vine, D-Vine, and Factor copulas. Also, real financial data demonstrated that Factor copulas have stronger stability and perform better than the other two copulas in high-dimensional data. Moreover, we showed that BRICS has the highest risk and G20 has the lowest risk of the three groups. © 2019 by the authors.}}, 
pages = {274}, 
number = {3}, 
volume = {7}
}
@article{10.1016/j.gfj.2018.07.001, 
year = {2018}, 
title = {{Spectral measures of risk for international futures markets: A comparison of extreme value and Lévy models}}, 
author = {Mozumder, Sharif and Choudhry, Taufiq and Dempsey, Michael}, 
journal = {Global Finance Journal}, 
issn = {10440283}, 
doi = {10.1016/j.gfj.2018.07.001}, 
abstract = {{This paper investigates Lévy spectral risk measures (SRM) as a coherent alternative to generalized Pareto spectral risk measures. Specifically, using futures data from major indexes, we consider using SRM for conditional distributions belonging to the generalized hyperbolic family of Lévy processes, and compare and contrast the results with those obtained from the traditional unconditional extreme value approach. Compared with Lévy models, the extreme value model provides poor estimates of quantiles outside the fixed tails, which in turn yield poor estimates of the spectral risk measure itself. The superiority of the Lévy models is increasingly apparent as investors become increasingly risk averse. © 2018 Elsevier Inc.}}, 
pages = {248--261}, 
number = {NA}, 
volume = {37}
}
@article{10.1108/jrf-09-2014-0132, 
year = {2015}, 
title = {{Computing value-at-risk using genetic algorithm}}, 
author = {Sharma, Bhanu and Thulasiram, Ruppa K. and Thulasiraman, Parimala}, 
journal = {The Journal of Risk Finance}, 
issn = {15265943}, 
doi = {10.1108/jrf-09-2014-0132}, 
abstract = {{Purpose – Value-at-risk (VaR) is a risk measure of potential loss on a specific portfolio. The main uses of VaR are in risk management and financial reporting. Researchers are continuously looking for new and efficient ways to evaluate VaR, and the 2008 financial crisis has given further impetus to finding new and reliable ways of evaluating and using VaR. In this study, the authors use genetic algorithm (GA) to evaluate VaR and compare the results with conventional VaR techniques. Design/methodology/approach – In essence, the authors propose two modifications to the standard GA: normalized population selection and strict population selection. For a typical set of simulation, eight chromosomes were used each with eight stored values, and the authors get eight values for VaR. Findings – The experiments using data from four different market indices show that by adjusting the volatility, the VaR computed using GA is more conservative as compared to those computed using Monte Carlo simulation. Research limitations/implications – The proposed methodology is designed for VaR computation only. This could be generalized for other applications. Practical implications – This is achieved with much less cost of computation, and hence, the proposed methodology could be a viable practical approach for computing VaR. Originality/value – The proposed methodology is simple and, at the same time, novel that could have far-reaching impact on practitioners. © 2015, © Emerald Group Publishing Limited.}}, 
pages = {170--189}, 
number = {2}, 
volume = {16}
}
@article{10.7500/aeps201206172, 
year = {2013}, 
title = {{Evaluation of value-at-risk in electricity markets based on multifractal theory}}, 
author = {}, 
issn = {10001026}, 
doi = {10.7500/aeps201206172}, 
abstract = {{Although some empirical analysis has already been done on the application of the fractal and multifractal theories in electricity price modeling, appropriate risk indices and associate calculation methods have not yet been systematically addressed. Given that, the well-established multifractal-detrended fluctuation analysis (MF-DFA) is employed to analyze and validate the multifractal characteristics of electricity price series. Then, a return interval approach (RIA) based method is presented to calculate the value-at-risk (VaR) caused by the electricity price fluctuations in electricity markets. Based on the proposed method, a risk optimization model for a short-term purchasing portfolio of the transmission company is developed. Finally, practical real-time electricity price data from the PJM electricity market in USA is served as a case study for analyzing the financial risk of a transmission company acting as a single buyer. Moreover, the risk optimization model is employed to determine the purchasing portfolio of typical trading days. It is demonstrated by simulation results that the developed RIA based VaR calculation method can accurately reflect the financial risk of the transmission company in a single-buyer electricity market, but also adapt to different fluctuation characteristics of electricity prices in different time periods. It provides a new method of risk optimization of purchasing portfolio for transmission companies. ©2013 State Grid Electric Power Research Institute Press.}}, 
number = {7}, 
volume = {37}
}
@article{10.1016/j.intfin.2016.08.008, 
year = {2017}, 
title = {{Value-at-Risk under Lévy GARCH models: Evidence from global stock markets}}, 
author = {Slim, Skander and Koubaa, Yosra and BenSaïda, Ahmed}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2016.08.008}, 
abstract = {{The aim of this paper is to reconsider the evidence on the forecasting ability of GARCH-type models in estimating the Value-at-Risk (VaR) of global stock market indices with improved return distribution. The performance of twenty-one VaR models that are generated by a combination of three conditional volatility specifications including GARCH, GJR and FIGARCH and seven distributional assumptions for return innovations is investigated. We implement stringent backtesting during crisis and post-crisis periods for developed, emerging and frontier markets. Results show that the skewed-t along with heavy-tailed Lévy distributions considerably improve the forecasts of one-day-ahead VaR for long and short trading positions during crisis period, regardless of the volatility model. However, we find no evidence that a given volatility specification outperforms the others across markets. The relevant models show evidence of long memory in developed markets and conditional asymmetry in frontier markets; whereas the standard GARCH is found to be the best suited specification for estimating VaR forecasts in emerging markets. The inclusion of high volatility period in the estimation sample highlights the predictability of VaR during post-crisis period, where even the normal distribution rivals the more sophisticated ones in terms of statistical accuracy and regulatory capital allocation. © 2016 Elsevier B.V.}}, 
pages = {30--53}, 
number = {NA}, 
volume = {46}
}
@article{10.3390/en9110931, 
year = {2016}, 
title = {{Forecasting electricity market risk using empirical mode decomposition (EMD)-based multiscale methodology}}, 
author = {He, Kaijian and Wang, Hongqian and Du, Jiangze and Zou, Yingchao}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en9110931}, 
abstract = {{The electricity market has experienced an increasing level of deregulation and reform over the years. There is an increasing level of electricity price fluctuation, uncertainty, and risk exposure in the marketplace. Traditional risk measurement models based on the homogeneous and efficient market assumption no longer suffice, facing the increasing level of accuracy and reliability requirements. In this paper, we propose a new Empirical Mode Decomposition (EMD)-based Value at Risk (VaR) model to estimate the downside risk measure in the electricity market. The proposed model investigates and models the inherent multiscale market risk structure. The EMD model is introduced to decompose the electricity time series into several Intrinsic Mode Functions (IMF) with distinct multiscale characteristics. The Exponential Weighted Moving Average (EWMA) model is used to model the individual risk factors across different scales. Experimental results using different models in the Australian electricity markets show that EMD-EWMA models based on Student's t distribution achieves the best performance, and outperforms the benchmark EWMA model significantly in terms of model reliability and predictive accuracy. © 2016 by the authors.}}, 
pages = {931}, 
number = {11}, 
volume = {9}
}
@article{10.1177/0972262918803172, 
year = {2018}, 
title = {{Corporate Risk Management, Firms’ Characteristics and Capital Structure: Evidence from Bombay Stock Exchange (BSE) Sensex Companies}}, 
author = {Joshi, Himanshu}, 
journal = {Vision: The Journal of Business Perspective}, 
issn = {09722629}, 
doi = {10.1177/0972262918803172}, 
abstract = {{The current article takes account of the existing status of risk management practices of the Indian publicly listed companies and establishes the relationship of their risk management programme with the firms’ financial characteristics such as capital structure, assets’ size, asset tangibility, profitability and valuation multiples. To establish the relationship, a risk management score is constructed using publicly disclosed information for Bombay Stock Exchange (BSE) Sensex 30 companies. Results suggest that companies with more comprehensive risk management programmes are likely to enjoy lower costs of debt and have a higher propensity to invest in intangible assets. These firms with more comprehensive risk management programmes also demonstrate more stable cash flows, sales and net operating profit. It is also evident that firms that are deeply indulged in risk management activities are likely to have higher financial leverage as higher leverage increases a firm’s total risk, and their risk management activities act to balance that risk. Consequently, firms with extensive risk management activities can endure higher debt in their capital structure; hence, a risk management programme works as a substitute of equity capital. © 2018 Management Development Institute.}}, 
pages = {395--404}, 
number = {4}, 
volume = {22}
}
@article{10.1016/j.qref.2008.01.001, 
year = {2009}, 
title = {{Value at risk: Is a theoretically consistent axiomatic formulation possible?}}, 
author = {Joaquin, Domingo Castelo}, 
journal = {The Quarterly Review of Economics and Finance}, 
issn = {10629769}, 
doi = {10.1016/j.qref.2008.01.001}, 
abstract = {{This note identifies three properties of a risk measure, the acceptance of all of which implies the acceptance of the VaR risk measure; and the rejection of any one of which implies the rejection of the VaR risk measure. First, a risk measure should reflect weak aversion to losses. Second, only sufficiently likely threats matter. Finally, the risk measurement should be unaffected by how promising the upside may look like. These properties, by themselves, constitute a consistent set of axioms that are necessary and sufficient for the acceptance of the VaR risk measure on a given probability space. The axiomatization highlights a peculiar characteristic of VaR: it ignores the upside, while at the same time neglecting the worse of the downside. © 2008 The Board of Trustees of the University of Illinois.}}, 
pages = {725--729}, 
number = {2}, 
volume = {49}
}
@article{10.1007/s00500-021-05623-6, 
year = {2021}, 
title = {{Portfolio selection of uncertain random returns based on value at risk}}, 
author = {Liu, Yajuan and Ahmadzade, Hamed and Farahikia, Mehran}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-021-05623-6}, 
abstract = {{Value at risk is a device to measure the maximum possible loss when the right tail distribution is ignored. Since uncertain random variables provide a tool to deal with phenomena in which uncertainty and randomness simultaneously exist, this paper proposes a computational approach for value at risk of uncertain random variables. In fact, a chance distribution can be expressed based on expectation with respect to probability distribution functions. Thus, chance distributions are computed via Monte Carlo simulation. And consequently, value at risk is obtained via statistical quintile index. As an application in finance, portfolio selection problems of uncertain random returns are optimized via mean–value at risk models. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.}}, 
pages = {6339--6346}, 
number = {8}, 
volume = {25}
}
@article{10.1016/j.insmatheco.2020.03.003, 
year = {2020}, 
title = {{Distributionally robust inference for extreme Value-at-Risk}}, 
author = {Yuen, Robert and Stoev, Stilian and Cooley, Daniel}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2020.03.003}, 
abstract = {{Under general multivariate regular variation conditions, the extreme Value-at-Risk of a portfolio can be expressed as an integral of a known kernel with respect to a generally unknown spectral measure supported on the unit simplex. The estimation of the spectral measure is challenging in practice and virtually impossible in high dimensions. This motivates the problem studied in this work, which is to find universal lower and upper bounds of the extreme Value-at-Risk under practically estimable constraints. That is, we study the infimum and supremum of the extreme Value-at-Risk functional, over the infinite dimensional space of all possible spectral measures that meet a finite set of constraints. We focus on extremal coefficient constraints, which are popular and easy to interpret in practice. Our contributions are twofold. First, we show that optimization problems over an infinite dimensional space of spectral measures are in fact dual problems to linear semi-infinite programs (LSIPs) – linear optimization problems in Euclidean space with an uncountable set of linear constraints. This allows us to prove that the optimal solutions are in fact attained by discrete spectral measures supported on finitely many atoms. Second, in the case of balanced portfolia, we establish further structural results for the lower bounds as well as closed form solutions for both the lower- and upper-bounds of extreme Value-at-Risk in the special case of a single extremal coefficient constraint. The solutions unveil important connections to the Tawn–Molchanov max-stable models. The results are illustrated with two applications: a real data example and closed-form formulae in a market plus sectors framework. © 2020 Elsevier B.V.}}, 
pages = {70--89}, 
number = {NA}, 
volume = {92}
}
@article{10.1007/s11009-007-9067-x, 
year = {2009}, 
title = {{Bayesian copulae distributions, with application to operational risk management}}, 
author = {Valle, Luciana Dalla}, 
journal = {Methodology and Computing in Applied Probability}, 
issn = {13875841}, 
doi = {10.1007/s11009-007-9067-x}, 
abstract = {{The aim of this paper is to introduce a new methodology for operational risk management, based on Bayesian copulae. One of the main problems related to operational risk management is understanding the complex dependence structure of the associated variables. In order to model this structure in a flexible way, we construct a method based on copulae. This allows us to split the joint multivariate probability distribution of a random vector of losses into individual components characterized by univariate marginals. Thus, copula functions embody all the information about the correlation between variables and provide a useful technique for modelling the dependency of a high number of marginals. Another important problem in operational risk modelling is the lack of loss data. This suggests the use of Bayesian models, computed via simulation methods and, in particular, Markov chain Monte Carlo. We propose a new methodology for modelling operational risk and for estimating the required capital. This methodology combines the use of copulae and Bayesian models. © Springer Science+Business Media, LLC 2007.}}, 
pages = {95--115}, 
number = {1}, 
volume = {11}
}
@article{10.1111/poms.12405, 
year = {2016}, 
title = {{Technical Note - Pricing below Cost under Exchange-Rate Risk}}, 
author = {Park, John and Kazaz, Burak and Webster, Scott}, 
journal = {Production and Operations Management}, 
issn = {10591478}, 
doi = {10.1111/poms.12405}, 
abstract = {{Pricing below cost is often classified as "dumping" in international trade and as "predatory pricing" in local markets. It is legally prohibited from practice because of earlier findings that it leads to predatory behavior by either eliminating competition or stealing market share. This study shows that a stochastic exchange rate can create incentives for a profit-minded monopoly firm to set price below marginal cost. Our result departs from earlier findings because the optimal pricing decision is based on a rational behavior that does not exhibit any malicious intent against the competition to be considered as violating anti-trust laws. The finding is a robust result, because our analysis demonstrates that this behavior occurs under various settings such as when the firm (i) is risk-averse, (ii) can postpone prices until after exchange rates are realized, (iii) is capable of manufacturing in multiple countries, and (iv) operates under demand uncertainty in addition to the random exchange rate. © 2015 Production and Operations Management Society.}}, 
pages = {153--159}, 
number = {1}, 
volume = {25}
}
@article{10.29020/nybg.ejpam.v14i3.3951, 
year = {2021}, 
title = {{Extremal copulas and tail dependence in modeling stochastic financial risk}}, 
author = {Mallam, Hassane Abba and Moutari, Natatou Dodo and Diakarya, Barro and Bisso, Saley}, 
journal = {European Journal of Pure and Applied Mathematics}, 
issn = {13075543}, 
doi = {10.29020/nybg.ejpam.v14i3.3951}, 
abstract = {{UFR-SEG, Universite Thomas SANKARA, Burkina Faso Abstract. These last years the stochastic modeling became essential in financial risk management related to the ownership and valuation of financial products such as assets, options and bonds. This paper presents a contribution to the modeling of stochastic risks in finance by using both extensions of tail dependence coefficients and extremal dependance structures based on copulas. In particular, we show that when the stochastic behavior of a set of risks can be modeled by a multivariate extremal process a corresponding form of the underlying copula describing their dependence is determined. Moreover a new tail dependence measure is proposed and properties of this measure are established. © 2021 EJPAM All rights reserved.}}, 
pages = {1057--1081}, 
number = {3}, 
volume = {14}
}
@article{10.3390/w12061805, 
year = {2020}, 
title = {{Spatial dependence modeling of flood risk using max-stable processes: The example of Austria}}, 
author = {Albrecher, Hansjörg and Kortschak, Dominik and Prettenthaler, Franz}, 
journal = {Water}, 
issn = {20734441}, 
doi = {10.3390/w12061805}, 
abstract = {{We propose a new approach to model the dependence structure for aggregating the risk of flood damages from a local level to larger areas, which is based on the structure of the river network of a country and can be calibrated with publicly available data of river discharges. Building upon a suitable adaptation of max-stable processes for a flood-relevant geometry as recently introduced in the literature, this enables the assessment of flood risk without the need for a hydrological model, and can easily be adapted for different countries. We illustrate its use for the particular case of Austria. We first develop marginal flood models for individual municipalities by intertwining available HORA risk maps with the actual location of buildings. As a second alternative for the marginal modeling, we advocate an approach based on suitably normalized historical damage data of municipalities together with techniques from extreme value statistics. We implement and compare the two alternatives and apply the calibrated dependence structure to each of them, leading to estimates for average flood damage as well as its extreme quantiles on the municipality, state, and country level. This also allows us to quantify the diversification potential for flood risk on each of these levels, a topic of considerable importance in view of the natural and strong spatial dependence of this particular natural peril. © 2020 by the authors.}}, 
pages = {1805}, 
number = {6}, 
volume = {12}
}
@article{10.1093/jjfinec/nbm022, 
year = {2008}, 
title = {{Kernel conditional quantile estimation for stationary processes with application to conditional value-at-risk}}, 
author = {Wu, W B and Yu, K and Mitra, G}, 
journal = {Journal of Financial Econometrics}, 
issn = {14798409}, 
doi = {10.1093/jjfinec/nbm022}, 
abstract = {{The paper considers kernel estimation of conditional quantiles for both short-range and long-range-dependent processes. Under mild regularity conditions, we obtain Bahadur representations and central limit theorems for kernel quantile estimates of those processes. Our theory is applicable to many price processes of assets in finance. In particular, we present an asymptotic theory for kernel estimates of the value-at-risk (VaR) of the market value of an asset conditional on the historical information or a state process. The results are assessed based on a small simulation and are applied to AT\&T monthly returns. © The Author 2007. Published by Oxford University Press. All rights reserved.}}, 
pages = {253--270}, 
number = {2}, 
volume = {6}
}
@article{10.1007/s00780-019-00401-7, 
year = {2019}, 
title = {{Financial risk measures for a network of individual agents holding portfolios of light-tailed objects}}, 
author = {Klüppelberg, Claudia and Seifert, Miriam Isabel}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-019-00401-7}, 
abstract = {{We investigate a financial network of agents holding portfolios of independent light-tailed risky objects whose losses are asymptotically exponentially distributed with distinct tail parameters. We show that the asymptotic distributions of portfolio losses belong to the class of functional exponential mixtures which we introduce in this paper. We also provide results for value-at-risk and expected shortfall risk measures, as well as for their conditional counterparts. Compared to heavy-tail settings, we establish important qualitative differences in the asymptotic behaviour of portfolio risks under a light-tail assumption which have to be accounted for in practical risk management. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.}}, 
pages = {795--826}, 
number = {4}, 
volume = {23}
}
@article{10.1109/ifsa-nafips.2013.6608410, 
year = {2013}, 
title = {{Optimization of value-at-risk portfolios in uncertain lognormal models}}, 
author = {Yoshida, Yuji}, 
journal = {2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)}, 
issn = {NA}, 
doi = {10.1109/ifsa-nafips.2013.6608410}, 
abstract = {{A value-at-risk portfolio selection model to maximize not only the expected daily geometric return but also value-at-risk is discussed. The analytical solutions of the value-at-risk portfolio problem are derived. From the analytical results, this paper gives formulae to show the explicit relations among the following important parameters in portfolio: Value-at-risk, the expected daily geometric return, the risk probability of falling and bankruptcy and the falling rate of the asset prices. A numerical example is given to explain how to obtain the optimal portfolio and these parameters from the asset prices in the stock market. © 2013 IEEE.}}, 
pages = {263--268}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.ejor.2015.09.007, 
year = {2016}, 
title = {{Accuracy of mortgage portfolio risk forecasts during financial crises}}, 
author = {Lee, Yongwoong and Rösch, Daniel and Scheule, Harald}, 
journal = {European Journal of Operational Research}, 
issn = {03772217}, 
doi = {10.1016/j.ejor.2015.09.007}, 
abstract = {{This paper explores whether factor based credit portfolio risk models are able to predict losses in severe economic downturns such as the recent Global Financial Crisis (GFC) within standard confidence levels. The paper analyzes (i) the accuracy of default rate forecasts, and (ii) whether forecast downturn percentiles (Value-at-Risk, VaR) are sufficient to cover default rate outcomes over a quarterly and an annual forecast horizon. Uninformative maximum likelihood and informative Bayesian techniques are compared as they imply different degrees of uncertainty. We find that quarterly VaR estimates are generally sufficient but annual VaR estimates may be insufficient during economic downturns. In addition, the paper develops and analyzes models based on auto-regressive adjustments of scores, which provide a higher forecast accuracy. The consideration of parameter uncertainty and auto-regressive error terms mitigates the shortfall. © 2015 Elsevier B.V.}}, 
pages = {440--456}, 
number = {2}, 
volume = {249}
}
@article{10.1016/j.ijforecast.2017.04.004, 
year = {2017}, 
title = {{Volatility measures and Value-at-Risk}}, 
author = {Bams, Dennis and Blanchard, Gildas and Lehnert, Thorsten}, 
journal = {International Journal of Forecasting}, 
issn = {01692070}, 
doi = {10.1016/j.ijforecast.2017.04.004}, 
abstract = {{We evaluate and compare the abilities of the implied volatility and historical volatility models to provide accurate Value-at-Risk forecasts. Our empirical tests on the S\&P 500, Dow Jones Industrial Average and Nasdaq 100 indices over long time series of more than 20 years of daily data indicate that an implied volatility based Value-at-Risk cannot beat, and tends to be outperformed by, a simple GJR-GARCH based Value-at-Risk. This finding is robust to the use of the likelihood ratio, the dynamic quantile test or a statistical loss function for evaluating the Value-at-Risk performance. The poor performance of the option based Value-at-Risk is due to the volatility risk premium embedded in implied volatilities. We apply both non-parametric and parametric adjustments to correct for the negative price of the volatility risk. However, although this adjustment is effective in reducing the bias, it still does not allow the implied volatility to outperform the historical volatility models. These results are in contrast to the volatility forecasting literature, which favors implied volatilities over the historical volatility model. We show that forecasting the volatility and forecasting a quantile of the return distribution are two different objectives. While the implied volatility is useful for the earlier objective function, it is not for the latter, due to the non-linear and regime changing dynamics of the volatility risk premium. © 2017 International Institute of Forecasters}}, 
pages = {848--863}, 
number = {4}, 
volume = {33}
}
@article{10.1016/j.ijpe.2012.06.021, 
year = {2012}, 
title = {{Electronic price-testing scheme for fashion retailing with information updating}}, 
author = {Choi, Tsan-Ming and Chow, Pui-Sze and Xiao, Tiaojun}, 
journal = {International Journal of Production Economics}, 
issn = {09255273}, 
doi = {10.1016/j.ijpe.2012.06.021}, 
abstract = {{Pricing is a crucial decision for electronic fashion retailers. Motivated by various observed industrial practices in electronic retailing, we study in this paper the optimal Internet pricing schemes which employ price testing with Bayesian information updating following the Bernoulli process. This paper contributes to the literature and advancement of knowledge in a number of ways: (i) we propose an analytical model to study the Internet pricing problem with price-testing and Bayesian information updating for fashion retailers. (ii) We derive the closed-form expressions of the expected value of sampling information (EVSI) and the expected value of perfect information (EVPI) under the price testing scheme. (iii) We conduct the pre-posterior analysis and construct the optimal sampling plan with three different rules. (iv) We develop the optimal posterior pricing policies, with respect to the mean-risk and Value-at-Risk (VaR) objectives. Numerical analyses, which include the studies on EVPI and the efficient frontiers, are presented to generate more insights. © 2012 Elsevier B.V. All rights reserved.}}, 
pages = {396--406}, 
number = {1}, 
volume = {140}
}
@article{10.1007/s00291-016-0462-y, 
year = {2017}, 
title = {{Omega-CVaR portfolio optimization and its worst case analysis}}, 
author = {Sharma, Amita and Utz, Sebastian and Mehra, Aparna}, 
journal = {OR Spectrum}, 
issn = {01716468}, 
doi = {10.1007/s00291-016-0462-y}, 
abstract = {{This paper presents a novel framework for optimizing portfolios using distribution dependent thresholds in Omega ratio to control the downside risk. Portfolios resulting from the maximization of the classical Omega ratio simultaneously maximize the probability of superior performance compared to a threshold point set by an investor and minimize the probability of a worse performance compared to the same threshold. However, there is no mandatory rule or mechanism to choose this threshold point in the Omega ratio optimization model yet. In this paper, we redefine the Omega ratio for a loss averse investor by taking the distribution dependent threshold point as the conditional value-at-risk at an α confidence level (CVaR α) of the benchmark market. The α-value reflects the attitude of an investor towards losses. We then embed this new Omega-CVaR α model in a robust portfolio optimization framework and present its worst case analysis under three uncertainty sets. The robustness is introduced both in the Omega measure and the CVaR α measure. We show that the worst case Omega-CVaR α robust optimization models are linear programs for mixed and box uncertainty sets and a second order cone program under ellipsoidal sets, and hence tractable in all three cases. We conduct a comprehensive empirical investigation of the classical CVaR α model, the STARRα model, the Omega-CVaR α model, and robust Omega-CVaR α model under a mixed uncertainty set for listed stocks of the S\&amp;P 500. The optimal portfolios resulting from the Omega-CVaR α model exhibit a superior performance compared to the classical CVaR α model in the sense of higher expected returns, Sharpe ratios, modified Sharpe ratios, and lesser losses in terms of VaR α and CVaR α values. The robust Omega-CVaR α model under mixed uncertainty set is shown to dominate the Omega-CVaR α model in terms of all performance measures. Furthermore, both the Omega-CVaR α and robust Omega-CVaR α model under a mixed uncertainty set yield significantly lower risk compared to STARRα model in terms of CVaR α and variance values. © 2016, Springer-Verlag Berlin Heidelberg.}}, 
pages = {505--539}, 
number = {2}, 
volume = {39}
}
@article{10.21314/jop.2017.195, 
year = {2017}, 
title = {{Fast, accurate and straightforward extreme quantiles of compound loss distributions}}, 
author = {Opdyke, John Douglas}, 
journal = {The Journal of Operational Risk}, 
issn = {17446740}, 
doi = {10.21314/jop.2017.195}, 
abstract = {{In this paper, we present an easy-to-implement, fast and accurate method for approximating extreme quantiles of compound loss distributions (frequency C severity), which are commonly used in insurance and operational risk capital models. The interpolated single-loss approximation (ISLA) of J. D. Opdyke is based on the widely used single-loss approximation (SLA) of M. Degen. It maintains two important advantages over its competitors. First, ISLA correctly accounts for a discontinuity in SLA that can otherwise systematically and notably bias the quantile (capital) approximation under conditions of both finite and infinite mean. Second, because it is based on a closed-form approximation, ISLA maintains the notable speed advantages of SLA over other methods requiring algorithmic looping (eg, fast Fourier transform or Panjer recursion). Speed is important when simulating many quantile (capital) estimates, as is so often required in practice, and essential when simulations of simulations are needed (eg, in some power studies). The modified ISLA (MISLA) presented herein increases the range of application across the severity distributions most commonly used in these settings; it is tested against extensive Monte Carlo simulation (one billion years’ worth of losses) and the best competing method (the perturbative expansion (PE2) of L. Hernández, J. Tejero, Al. Suárez and S. Carrillo-Menéndez) using twelve heavy-tailed severity distributions, some of which are truncated. MISLA is shown to be comparable to PE2 in terms of both speed and accuracy, and it is arguably more straightforward to implement for the majority of advanced measurement approach banks that are already using SLA (and failing to take into account its biasing discontinuity). © 2017 Infopro Digital Risk (IP) Limited.}}, 
number = {4}, 
volume = {12}
}
@article{10.1016/j.pacfin.2020.101326, 
year = {2020}, 
title = {{Do Islamic indices provide diversification to bitcoin? A time-varying copulas and value at risk application}}, 
author = {Rehman, Mobeen Ur and Asghar, Nadia and Kang, Sang Hoon}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2020.101326}, 
abstract = {{The emergence of new asset classes offers avenues to international investment community however understanding relationship between any two assets in a single portfolio is important. We investigate the risk dependence between daily Bitcoin and major Islamic equity markets spanning over from July 2010 to March 2018. We start by examining long memory properties of Bitcoin and sampled Islamic indices and report significant results. The residuals from fractionally integrated models are then used in bivariate time invariant and time varying copulas to investigate dependence structure. Among all Islamic indices, DJIUK, DJIJP and DJICA exhibit time varying dependence with Bitcoin. In addition, we apply VaR, CoVaR and ΔCoVaR as risk measure to examine spillover between Bitcoin and Islamic equity markets. VaR of Bitcoin exceeds from VaR of Islamic indices and CoVaR of both Islamic and Bitcoin exceeds their respective VaR, suggesting presence of risk spillover between each other. Our results also report asymmetry between downside and upside ΔCoVaR suggesting implications for investors with different risk preferences. Finally, the diversification benefits indicate that Islamic equity market serves as an effective hedge in a portfolio along with Bitcoin. © 2020}}, 
pages = {101326}, 
number = {NA}, 
volume = {61}
}
@article{10.1016/j.econmod.2014.02.036, 
year = {2014}, 
title = {{Fuzzy value-at-risk and expected shortfall for portfolios with heavy-tailed returns}}, 
author = {Moussa, A. Mbairadjim and Kamdem, J. Sadefo and Terraza, M.}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2014.02.036}, 
abstract = {{This paper is concerned with linear portfolio value-at-risk (VaR) and expected shortfall (ES) computation when the portfolio risk factors are leptokurtic, imprecise and/or vague. Following Yoshida (2009), the risk factors are modeled as fuzzy random variables in order to handle both their random variability and their vagueness. We discuss and extend the Yoshida model to some non-Gaussian distributions and provide associated ES. Secondly, assuming that the risk factors' degree of imprecision changes over time, original fuzzy portfolio VaR and ES models are introduced. For a given subjectivity level fixed by the investor, these models allow the computation of a pessimistic and an optimistic estimation of the value-at-risk and of the expected shortfall. Finally, some empirical examples carried out on three portfolios constituted by some chosen French stocks, show the effectiveness of the proposed methods. © 2014 Elsevier B.V.}}, 
pages = {247--256}, 
number = {NA}, 
volume = {39}
}
@article{10.1287/mnsc.2015.2342, 
year = {2017}, 
title = {{Backtesting expected shortfall: Accounting for tail risk}}, 
author = {Du, Zaichao and Escanciano, Juan Carlos}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.2015.2342}, 
abstract = {{The Basel Committee on Banking Supervision (BIS) has recently sanctioned expected shortfall (ES) as the market risk measure to be used for banking regulatory purposes, replacing the well-known value at risk (VaR). This change is motivated by the appealing theoretical properties of ES as a measure of risk and the poor properties of VaR. In particular, VaR fails to control for "tail risk." In this transition, the major challenge faced by financial institutions is the unavailability of simple tools for evaluation of ES forecasts (i.e., backtesting ES). The main purpose of this paper is to propose such tools. Specifically, we propose backtests for ES based on cumulative violations, which are the natural analogue of the commonly used backtests for VaR. We establish the asymptotic properties of the tests, and investigate their finite sample performance through some Monte Carlo simulations. An empirical application to three major stock indexes shows that VaR is generally unresponsive to extreme events such as those experienced during the recent financial crisis, whereas ES provides a more accurate description of the risk involved. © 2016 INFORMS.}}, 
pages = {940--958}, 
number = {4}, 
volume = {63}
}
@article{10.1007/s00780-013-0220-9, 
year = {2014}, 
title = {{Beyond cash-additive risk measures: When changing the numéraire fails}}, 
author = {Farkas, Walter and Koch-Medina, Pablo and Munari, Cosimo}, 
journal = {Finance and Stochastics}, 
issn = {09492984}, 
doi = {10.1007/s00780-013-0220-9}, 
eprint = {1206.0478}, 
abstract = {{We discuss risk measures representing the minimum amount of capital a financial institution needs to raise and invest in a pre-specified eligible asset to ensure it is adequately capitalized. Most of the literature has focused on cash-additive risk measures, for which the eligible asset is a risk-free bond, on the grounds that the general case can be reduced to the cash-additive case by a change of numéraire. However, discounting does not work in all financially relevant situations, especially when the eligible asset is a defaultable bond. In this paper, we fill this gap by allowing general eligible assets. We provide a variety of finiteness and continuity results for the corresponding risk measures and apply them to risk measures based on value-at-risk and tail value-at-risk on L p spaces, as well as to shortfall risk measures on Orlicz spaces. We pay special attention to the property of cash subadditivity, which has been recently proposed as an alternative to cash additivity to deal with defaultable bonds. For important examples, we provide characterizations of cash subadditivity and show that when the eligible asset is a defaultable bond, cash subadditivity is the exception rather than the rule. Finally, we consider the situation where the eligible asset is not liquidly traded and the pricing rule is no longer linear. We establish when the resulting risk measures are quasiconvex and show that cash subadditivity is only compatible with continuous pricing rules. © 2013 Springer-Verlag Berlin Heidelberg.}}, 
pages = {145--173}, 
number = {1}, 
volume = {18}
}
@article{10.1007/978-3-642-23854-3_23, 
year = {2011}, 
title = {{A new MOPSO to solve a multi-objective portfolio selection model with fuzzy value-at-risk}}, 
author = {Wang, Bo and Li, You and Watada, Junzo}, 
journal = {Lecture Notes in Computer Science}, 
issn = {03029743}, 
doi = {10.1007/978-3-642-23854-3\_23}, 
abstract = {{This study proposes an novel fuzzy multi-objective model that can evaluate the invest risk properly and increase the probability of obtaining the expected return. In building the model, fuzzy Value-at-Risk is used to evaluate the exact future risk, in term of loss. And, variance is utilized to make the selection more stable. This model can provide investors with more significant information in decision-making. To solve this model, a new Pareto-optimal set based multi-objective particle swarm optimization algorithm is designed to obtain better solutions among the Pareto-front. At the end of this study, the proposed model and algorithm are exemplified by one numerical example. Experiment results show that the model and algorithm are effective in solving the multi-objective portfolio selection problem. © 2011 Springer-Verlag.}}, 
pages = {217--226}, 
number = {PART 3}, 
volume = {6883 LNAI}
}
@article{10.1080/1540496x.2018.1520088, 
year = {2020}, 
title = {{Comparing Hedging Effectiveness of Portfolios in the Greater Chinese Stock Exchanges: Evidence from a Modified Value-at-Risk Model}}, 
author = {Chuang, Chung-Chu and Wang, Yi-Hsien and Yeh, Tsai-Jung}, 
journal = {Emerging Markets Finance and Trade}, 
issn = {1540496X}, 
doi = {10.1080/1540496x.2018.1520088}, 
abstract = {{The higher moments of hedged portfolio returns often influence the calculation of value-at-risk (VaR). To establish future short and long hedged portfolios, this study proposes a new modified VaR model, an expected utility maximization (EUM) subject to the modified VaR of higher moments (EUM-MVaR) of stock index futures in markets in greater China. EUM-MVaR has the greatest hedging effectiveness in determining hedged portfolios, while the minimum variance (MV) model had the least hedging effectiveness; the consideration of higher moments of a hedged portfolio return is more effective than non-consideration in determining the hedging effectiveness. ©, Copyright © Taylor \& Francis Group, LLC.}}, 
pages = {1--19}, 
number = {3}, 
volume = {56}
}
@article{10.1186/s40854-020-00178-1, 
year = {2020}, 
title = {{On the volatility of daily stock returns of Total Nigeria Plc: evidence from GARCH models, value-at-risk and backtesting}}, 
author = {Emenogu, Ngozi G. and Adenomon, Monday Osagie and Nweze, Nwaze Obini}, 
journal = {Financial Innovation}, 
issn = {21994730}, 
doi = {10.1186/s40854-020-00178-1}, 
abstract = {{This study investigates the volatility in daily stock returns for Total Nigeria Plc using nine variants of GARCH models: sGARCH, girGARCH, eGARCH, iGARCH, aGARCH, TGARCH, NGARCH, NAGARCH, and AVGARCH along with value at risk estimation and backtesting. We use daily data for Total Nigeria Plc returns for the period January 2, 2001 to May 8, 2017, and conclude that eGARCH and sGARCH perform better for normal innovations while NGARCH performs better for student t innovations. This investigation of the volatility, VaR, and backtesting of the daily stock price of Total Nigeria Plc is important as most previous studies covering the Nigerian stock market have not paid much attention to the application of backtesting as a primary approach. We found from the results of the estimations that the persistence of the GARCH models are stable except for few cases for which iGARCH and eGARCH were unstable. Additionally, for student t innovation, the sGARCH and girGARCH models failed to converge; the mean reverting number of days for returns differed from model to model. From the analysis of VaR and its backtesting, this study recommends shareholders and investors continue their business with Total Nigeria Plc because possible losses may be overcome in the future by improvements in stock prices. Furthermore, risk was reflected by significant up and down movement in the stock price at a 99\% confidence level, suggesting that high risk brings a high return. © 2020, The Author(s).}}, 
pages = {18}, 
number = {1}, 
volume = {6}
}
@article{10.18576/amis/100535, 
year = {2016}, 
title = {{Portfolio optimization by mean-value at risk framework}}, 
author = {Banihashemi, Shokufeh and Azarpour, Ali Moayedi and Navvabpour, Hamidreza}, 
journal = {Applied Mathematics \& Information Sciences}, 
issn = {19350090}, 
doi = {10.18576/amis/100535}, 
abstract = {{The purpose of this study is to evaluate various tools used for improving performance of portfolios and assets selection using mean-value at risk models. The study is mainly based on a non-parametric efficiency analysis tool, namely Data Envelopment Analysis (DEA). Conventional DEA models assume non-negative values for inputs and outputs, but variance is the only variable in models that takes non-negative values. At the beginning variance was considered as a risk measure. However, both theories and practices indicate that variance is not a good measure of risk and has some disadvantages. This paper focuses on the evaluation process of the portfolios and replaces variance by value at risk (VaR) and tries to decrease it in a mean-value at risk framework with negative data by using mean-value at risk efficiency (MVE) model and multi objective mean-value at risk (MOMV) model. Finally, a numerical example with historical and Monte Carlo simulations is conducted to calculate value at risk and determine extreme efficiencies that can be obtained by mean-value at risk framework. © 2016 NSP.}}, 
pages = {1935--1948}, 
number = {5}, 
volume = {10}
}
@article{10.1080/00036846.2017.1284996, 
year = {2017}, 
title = {{Event-study analysis by using dynamic conditional score models}}, 
author = {Blazsek, Szabolcs and Monteros, Luis Antonio}, 
journal = {Applied Economics}, 
issn = {00036846}, 
doi = {10.1080/00036846.2017.1284996}, 
abstract = {{This article considers the most important information technology (IT) products in order to perform an event-study analysis of the out-of-sample predictability of IT stock returns. We define two subperiods, the estimation and forecast windows, for each IT product that are separated by the product release date. We investigate whether post-release-date returns can be predicted by using data on pre-release-date returns. We use static one-step-ahead density forecasting. We compare the forecast performance of autoregressive moving average (ARMA) plus generalized autoregressive conditional heteroscedasticity (GARCH) and quasi-ARMA (QARMA) plus Beta-t-EGARCH (exponential-GARCH). QARMA plus Beta-t-EGARCH belongs to the family of dynamic conditional score (DCS) models. We find that the in-sample statistical performance of DCS is superior to that of ARMA plus GARCH for most of the IT stocks. We also find that the out-of-sample density predictive performance of ARMA plus GARCH is never significantly superior to that of DCS. However, the predictive performance of DCS significantly dominates that of ARMA plus GARCH for several IT products. We undertake a Monte Carlo value-at-risk (VaR) application of our results to Windows 95 of Microsoft. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.}}, 
pages = {1--12}, 
number = {45}, 
volume = {49}
}
@article{10.21314/jor.2019.418, 
year = {2019}, 
title = {{Backtesting expected shortfall: A simple recipe?}}, 
author = {Moldenhauer, Felix and Pitera, Marcin}, 
journal = {Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2019.418}, 
abstract = {{We propose a new backtesting framework for expected shortfall (ES) that can be used by regulators. Instead of looking at estimated capital reserve and realized cashflow separately, one can bind them into a secured position, for which risk measurement is much easier. Using this simple concept combined with monotonicity of ES with respect to its target confidence level, we introduce a natural and efficient backtesting framework. Our test statistics is given by the biggest number of worst realizations for the secured position that adds up to a negative total. Surprisingly, this simple quantity can be used to construct an efficient backtesting framework for unconditional coverage of ES in a natural extension of the regulatory traffic-light approach for value-at-risk. While being easy to calculate, the test statistic is based on the underlying duality between coherent risk measures and scale-invariant performance measures. © 2019 Infopro Digital Risk (IP) Limited.}}, 
pages = {17--42}, 
number = {1}, 
volume = {22}
}
@article{10.1016/j.insmatheco.2014.08.002, 
year = {2014}, 
title = {{On the distribution of sums of random variables with copula-induced dependence}}, 
author = {Gijbels, Irène and Herrmann, Klaus}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2014.08.002}, 
abstract = {{We investigate distributional properties of the sum of d possibly unbounded random variables. The joint distribution of the random vector is formulated by means of an absolutely continuous copula, allowing for a variety of different dependence structures between the summands. The obtained expression for the distribution of the sum features a separation property into marginal and dependence structure contributions typical for copula approaches. Along the same lines we obtain the formulation of a conditional expectation closely related to the expected shortfall common in actuarial and financial literature. We further exploit the separation to introduce new numerical algorithms to compute the distribution and quantile function, as well as this conditional expectation. A comparison with the most common competitors shows that the discussed Path Integration algorithm is the most suitable method for computing these quantities. In our example, we apply the theory to compute Value-at-Risk forecasts for a trivariate portfolio of index returns. © 2014 Elsevier B.V.}}, 
pages = {27--44}, 
number = {NA}, 
volume = {59}
}
@article{10.1002/for.2697, 
year = {2020}, 
title = {{Volatility specifications versus probability distributions in VaR forecasting}}, 
author = {Garcia‐Jorcano, Laura and Novales, Alfonso}, 
journal = {Journal of Forecasting}, 
issn = {02776693}, 
doi = {10.1002/for.2697}, 
abstract = {{We provide evidence suggesting that the assumption on the probability distribution for return innovations is more influential for value-at-risk (VaR) performance than the conditional volatility specification. We also show that some recently proposed asymmetric probability distributions and the APARCH and FGARCH volatility specifications beat more standard alternatives for VaR forecasting, and they should be preferred when estimating tail risk. The flexibility of the free power parameter in conditional volatility in the APARCH and FGARCH models explains their better performance. Indeed, our estimates suggest that for a number of financial assets the dynamics of volatility should be specified in terms of the conditional standard deviation. We draw our results on VaR forecasting performance from (i) a variety of backtesting approaches, (ii) the model confidence set approach, as well as (iii) establishing a ranking among alternative VaR models using a precedence criterion that we introduce in this paper. © 2020 John Wiley \& Sons, Ltd.}}, 
pages = {189--212}, 
number = {NA}, 
volume = {NA}
}
@article{10.3390/en11061423, 
year = {2018}, 
title = {{An optimal scheduling dispatch of a microgrid under risk assessment}}, 
author = {Lin, Whei-Min and Yang, Chung-Yuen and Tu, Chia-Sheng and Tsai, Ming-Tang}, 
journal = {Energies}, 
issn = {19961073}, 
doi = {10.3390/en11061423}, 
abstract = {{This paper presents the scheduling dispatch of a microgrid (MG), while considering renewable energy, battery storage systems, and time-of-use price. For the risk evaluation of an MG, the Value-at-Risk (VAR) is calculated by using the Historical Simulation Method (HSM). By considering the various confidence levels of the VAR, a scheduling dispatch model of the MG is formulated to achieve a reasonable trade-off between the risk and cost. An Improved Bee Swarm Optimization (IBSO) is proposed to solve the scheduling dispatch model of the MG. In the IBSO procedure, the Sin-wave Weight Factor (SWF) and Forward-Backward Control Factor (FBCF) are embedded in the bee swarm of the BSO to improve the movement behaviors of each bee, specifically, its search efficiency and accuracy. The effectiveness of the IBSO is demonstrated via a real MG case and the results are compared with other methods. In either a grid-connected scenario or a stand-alone scenario, an optimal scheduling dispatch of MGs is carried out, herein, at various confidence levels of risk. The simulation results provide more information for handling uncertain environments when analyzing the VAR of MGs. © 2018 by the authors.}}, 
pages = {1423}, 
number = {6}, 
volume = {11}
}
@article{10.1142/s0219091512500154, 
year = {2012}, 
title = {{Capturing tail risks beyond VaR}}, 
author = {Wong, Woon Kong and Fan, Guobin and Zeng, Yong}, 
journal = {Review of Pacific Basin Financial Markets and Policies}, 
issn = {02190915}, 
doi = {10.1142/s0219091512500154}, 
abstract = {{Since Value-at-Risk (VaR) disregards tail losses beyond the VaR boundary, the expected shortfall (ES), which measures the average loss when a VaR is exceeded, and the tail-risk-of-VaR (TR), which sums the sizes of tail losses, are used to investigate risks at the tails of distributions for major stock markets. As VaR exceptions are rare, we employ the saddlepoint or small sample asymptotic technique to backtest ES and TR. Because the two risk measures are complementary to each other and hence provide more powerful backtests, we are able to show that (a) the correct specification of distribution tail, rather than heteroscedastic process, plays a key role to accurate risk forecasts; and (b) it is best to model the tails separately from the central part of distribution using the Generalized Pareto Distribution (GPD). To sum up, we provide empirical evidence that financial markets behave differently during crises, and extreme risks cannot be modeled effectively under normal market conditions or based on a short data history. © 2012 World Scientific Publishing Co. and Center for Pacific Basin Business, Economics and Finance Research.}}, 
pages = {1250015}, 
number = {3}, 
volume = {15}
}
@article{10.1080/03610918.2020.1869985, 
year = {2020}, 
title = {{On approximations of value at risk and expected shortfall involving kurtosis}}, 
author = {Barczy, Mátyás and Dudás, Ádám and Gáll, József}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {03610918}, 
doi = {10.1080/03610918.2020.1869985}, 
abstract = {{We derive new approximations for the Value at Risk and the Expected Shortfall at high levels of loss distributions with positive skewness and excess kurtosis, and we describe their precisions for notable ones such as for exponential, Pareto type I, lognormal and compound (Poisson) distributions. Our approximations are motivated by that kind of extensions of the so-called Normal Power Approximation, used for approximating the cumulative distribution function of a random variable, which incorporate not only the skewness but the kurtosis of the random variable in question as well. We show the performance of our approximations in numerical examples and we also give comparisons with some known ones in the literature. © 2021 Taylor \& Francis Group, LLC.}}, 
pages = {1--25}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.csda.2015.07.011, 
year = {2016}, 
title = {{Nonlinear expectile regression with application to Value-at-Risk and expected shortfall estimation}}, 
author = {Kim, Minjo and Lee, Sangyeol}, 
journal = {Computational Statistics \& Data Analysis}, 
issn = {01679473}, 
doi = {10.1016/j.csda.2015.07.011}, 
abstract = {{This paper considers nonlinear expectile regression models to estimate conditional expected shortfall (ES) and Value-at-Risk (VaR). In the literature, the asymmetric least squares (ALS) regression method has been widely used to estimate expectile regression models. However, no literatures rigorously investigated the asymptotic properties of the ALS estimates in nonlinear models with heteroscedasticity. Motivated by this aspect, this paper studies the consistency and asymptotic normality of the ALS estimates and conditional VaR and ES in those models. To illustrate, a simulation study and real data analysis are conducted. © 2015 Elsevier B.V. All rights reserved.}}, 
pages = {1--19}, 
number = {NA}, 
volume = {94}
}
@article{10.1016/j.insmatheco.2021.03.006, 
year = {2021}, 
title = {{Joint generalized quantile and conditional tail expectation regression for insurance risk analysis}}, 
author = {Guillen, Montserrat and Bermúdez, Lluís and Pitarque, Albert}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2021.03.006}, 
abstract = {{Based on recent developments in joint regression models for quantile and expected shortfall, this paper seeks to develop models to analyse the risk in the right tail of the distribution of non-negative dependent random variables. We propose an algorithm to estimate conditional tail expectation regressions, introducing generalized risk regression models with link functions that are similar to those in generalized linear models. To preserve the natural ordering of risk measures conditional on a set of covariates, we add extra non-negative terms to the quantile regression. A case using telematics data in motor insurance illustrates the practical implementation of predictive risk models and their potential usefulness in actuarial analysis. © 2021 The Author(s)}}, 
pages = {1--8}, 
number = {NA}, 
volume = {99}
}
@article{10.1057/grir.2009.3, 
year = {2009}, 
title = {{Optimal insurance under the insurer's VaR constraint}}, 
author = {Zhou, Chunyang and Wu, Chongfeng}, 
journal = {The Geneva Risk and Insurance Review}, 
issn = {1554964X}, 
doi = {10.1057/grir.2009.3}, 
abstract = {{In this paper, we impose the insurer's Value at Risk (VaR) constraint on Arrow's optimal insurance model. The insured aims to maximize his expected utility of terminal wealth, under the constraint that the insurer wishes to control the VaR of his terminal wealth to be maintained below a prespecified level. It is shown that when the insurer's VaR constraint is binding, the solution to the problem is not linear, but piecewise linear deductible, and the insured's optimal expected utility will increase as the insurer becomes more risk-tolerant. Basak and Shapiro (2001) showed that VaR risk managers often choose larger risk exposures to risky assets. We draw a similar conclusion in this paper. It is shown that when the insured has an exponential utility function, optimal insurance based on VaR constraint causes the insurer to suffer larger losses than optimal insurance without insurer's risk constraint. © 2009 The International Association for the Study of Insurance Economics.}}, 
pages = {140--154}, 
number = {2}, 
volume = {34}
}
@article{10.1016/j.econlet.2015.05.023, 
year = {2015}, 
title = {{Copula-MGARCH with continuous covariance decomposition}}, 
author = {Herwartz, Helmut and Raters, Fabian H.C.}, 
journal = {Economics Letters}, 
issn = {01651765}, 
doi = {10.1016/j.econlet.2015.05.023}, 
abstract = {{The Copula-MGARCH (C-MGARCH) model by Lee and Long (2009) incorporates standardized copula distributed innovations in MGARCH models. We motivate an extension of the C-MGARCH model by means of a continuous decomposition of the innovations' covariance matrix. An extended BEKK(1, 1) model with rotated standardized innovations is outlined for the bivariate case. The model parameters and the rotation angle are jointly estimated by means of Maximum Likelihood. We conduct an application to the log-differences of Euro/US-Dollar and Japanese Yen/US-Dollar daily exchange rates. In-sample information criteria and ex-ante portfolio Value-at-Risk coverage tests show that the enhanced flexibility of the rotated C-MGARCH is supported by the data. © 2015 Elsevier B.V.}}, 
pages = {73--76}, 
number = {NA}, 
volume = {133}
}
@article{10.25103/jestr.081.11, 
year = {2015}, 
title = {{Tail risk assessment using support vector machine}}, 
author = {Serbia, University of Niš , Faculty of Economics, and Radović, O and Stanković, J}, 
journal = {Journal of Engineering Science and Technology Review}, 
issn = {17912377}, 
doi = {10.25103/jestr.081.11}, 
abstract = {{In this paper, authors apply Support Vector Regression (SVR) tool Oracle DM in forecasting volatility of Belex 15 index and estimation of Value-at-Risk (VaR). VaR is calculated using SVR model and compared to the results achieved implementing Markov Regime Switching model VaR and Feed Forward Neural Network VaR (ANN FFNN VaR). The results show that the SVR tools give better estimates of VaR comparing to other methods. © 20135Kavala Institute of Technology.}}, 
pages = {61--64}, 
number = {1}, 
volume = {8}
}
@article{10.1080/09603107.2011.595677, 
year = {2011}, 
title = {{GJR-GARCH model in value-at-risk of financial holdings}}, 
author = {Su, Y. C. and Huang, H. C. and Lin, Y. J.}, 
journal = {Applied Financial Economics}, 
issn = {09603107}, 
doi = {10.1080/09603107.2011.595677}, 
abstract = {{In this study, we introduce an asymmetric Generalized Autoregressive Conditional Heteroscedastic (GARCH) model, Glosten, Jagannathan and Runkle-GARCH (GJR-GARCH), in Value-at-Risk (VaR) to examine whether or not GJR-GARCH is a good method to evaluate the market risk of financial holdings. Because of lacking the actual daily Profit and Loss (P\&L) data, portfolios A and B, representing FuBon and Cathay financial holdings are simulated. We take 400 observations as sample group to do the backward test and use the rest of the observations to forecast the change of VaR. We find GJR-GARCH works very well in VaR forecasting. Nonetheless, it also performs very well under the symmetric GARCH-in-Mean (GARCH-M) model, suggesting no leverage effect exists. Further, a 5-day moving window is opened to update parameter estimates. Comparing the results under different models, we find that the model is more accurate by updating parameter estimates. It is a trade-off between violations and capital charges. © 2011 Taylor amp;\& Francis.}}, 
pages = {1819--1829}, 
number = {24}, 
volume = {21}
}
@article{10.1080/14697680600739120, 
year = {2006}, 
title = {{On a subjective approach to risk measurement}}, 
author = {Jaworski, Piotr}, 
journal = {Quantitative Finance}, 
issn = {14697688}, 
doi = {10.1080/14697680600739120}, 
abstract = {{This study is based on the analogy between hedging a risky asset and keeping reserves to meet an unknown demand. The optimal hedging level, which depends on individual preferences, is regarded as a measure of risk. We determine the set of optimal levels and investigate the properties of the associated risk measures. This approach provides a new insight into Value at Risk (VaR). We consider it as a solution of a certain optimal inventory problem with linear cost and loss functions. We show that these functions determine the confidence level of VaR. In this way we obtain a simple model that helps us to choose a proper confidence level α and explains why supervisory institutions (such as the Basle Committee) choose a higher α than financial institutions themselves.}}, 
pages = {495--511}, 
number = {6}, 
volume = {6}
}
@article{10.1016/j.frl.2014.02.001, 
year = {2014}, 
title = {{Bankruptcy risk induced by career concerns of regulators}}, 
author = {Cole, John A. and Cadogan, Godfrey}, 
journal = {Finance Research Letters}, 
issn = {15446123}, 
doi = {10.1016/j.frl.2014.02.001}, 
abstract = {{We introduce a model in which a regulator employs mechanism design to embed her human capital beta signal(s) in a firm's capital structure. This can enhance her compensation at the firm, and the value of her contract with the firm in the form of an executive stock option. We prove that the agency cost of this revolving door behavior increases the firm's financial leverage, bankruptcy risk, and affects estimation of firm value at risk (VaR). © 2014 Elsevier Inc.}}, 
pages = {259--271}, 
number = {3}, 
volume = {11}
}
@article{10.3850/978-981-14-8593-0_5255-cd, 
year = {2020}, 
title = {{Allocation of defense resources against cyber attacks to cyber-physical systems}}, 
author = {Wang, Wei and Maio, Francesco Di and Zio, Enrico}, 
journal = {Proceedings of the 30th European Safety and Reliability Conference and 15th Probabilistic Safety Assessment and Management Conference}, 
issn = {NA}, 
doi = {10.3850/978-981-14-8593-0\_5255-cd}, 
abstract = {{Protecting Cyber-Physical Systems (CPSs) from cyber attacks requires properly allocating defense resources. These can be selected by defend-attack and Adversarial Risk Analysis (ARA) models, which search for the optimal allocation based on specific assumptions. In particular, the defend-attack model assumes that each player is fully aware of the preferences of the opponent, considering complete information, whereas the ARA model assumes incomplete information and subjective probability distributions of the defender utilities, improving the realism of the modelling but still lacking a proper management of the uncertainties of the results it provides. In this work, we complement the ARA model with a multi-criteria decision model based on Value-at-Risk (VaR) measures to support the defender in identifying the optimal defense portfolio among alternatives, considering budget constraints and accounting for the uncertainties which the ARA model is subjected to. For demonstration purposes, an application is carried out concerning the digital control system of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED). © ESREL2020-PSAM15 Organizers.Published by Research Publishing, Singapore.}}, 
pages = {1331--1337}, 
number = {NA}, 
volume = {NA}
}
@article{10.1109/isgt-mideast.2011.6220792, 
year = {2011}, 
title = {{Risk-based security assessment in smart power grids}}, 
author = {Shaaban, Mohamed}, 
journal = {2011 IEEE PES Conference on Innovative Smart Grid Technologies - Middle East}, 
issn = {NA}, 
doi = {10.1109/isgt-mideast.2011.6220792}, 
abstract = {{Future power grids are increasingly complex as they encompass elements such as renewables, storage, consumer options, and smart appliances. While these are likely to exacerbate uncertainties in both supply and demand, the overture of smart grid sensor and communication technologies promises, on the other hand, enhanced visibility and refined controls. Nonetheless, an evolutionary change in the tools used in system analysis is necessary to realize the full potential of the smart grid. This paper outlines a framework for assessing security in the operation planning horizon based on a quantitative risk measure. Value at Risk (VaR) is proposed as a measure of system security and is applied to an illustrative six-bus test system. © 2011 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {NA}
}
@article{10.1524/strm.2013.2002, 
year = {2013}, 
title = {{Risk management with high-dimensional vine copulas: An analysis of the Euro Stoxx 50}}, 
author = {Brechmann, Eike Christain and Czado, Claudia}, 
journal = {Statistics \& Risk Modeling}, 
issn = {21931402}, 
doi = {10.1524/strm.2013.2002}, 
abstract = {{The demand for an accurate financial risk management involving larger numbers of assets is strong not only in view of the financial crisis of 2007-2009. Especially dependencies among assets have not been captured adequately. While standard multivariate copulas have added some flexibility, this flexibility is insufficient in higher dimensional applications. Vine copulas can fill this gap by benefiting from the rich class of existing bivariate parametric copula families. Exploiting this in combination with GARCH models for the margins, we develop a regular vine copula based factor model for asset returns, the Regular Vine Market Sector model, which is motivated by the classical CAPM and shown to be superior to the CAVA model proposed by Heinen and Valdesogo (2009). The model can also be used to separate the systematic and idiosyncratic risk of specific stocks, and we explicitly discuss how vine copula models can be employed for active and passive portfolio management. In particular, Value-at-Risk forecasting and asset allocation are treated in detail. All developed models and methods are used to analyze the Euro Stoxx 50 index, a major market indicator for the Eurozone. Relevant benchmark models such as the popular DCC model and the common Student's t copula are taken into account. © 2013, by Walter de Gruyter Berlin Boston. All Rights Reserved.}}, 
pages = {307--342}, 
number = {4}, 
volume = {30}
}
@article{10.1515/strm-2019-0011, 
year = {2020}, 
title = {{Fair estimation of capital risk allocation}}, 
author = {Bielecki, Tomasz R. and Cialenco, Igor and Pitera, Marcin and Schmidt, Thorsten}, 
journal = {Statistics \& Risk Modeling}, 
issn = {21931402}, 
doi = {10.1515/strm-2019-0011}, 
abstract = {{In this paper, we develop a novel methodology for estimation of risk capital allocation. The methodology is rooted in the theory of risk measures. We work within a general, but tractable class of law-invariant coherent risk measures, with a particular focus on expected shortfall. We introduce the concept of fair capital allocations and provide explicit formulae for fair capital allocations in case when the constituents of the risky portfolio are jointly normally distributed. The main focus of the paper is on the problem of approximating fair portfolio allocations in the case of not fully known law of the portfolio constituents. We define and study the concepts of fair allocation estimators and asymptotically fair allocation estimators. A substantial part of our study is devoted to the problem of estimating fair risk allocations for expected shortfall. We study this problem under normality as well as in a nonparametric setup. We derive several estimators, and prove their fairness and/or asymptotic fairness. Last, but not least, we propose two backtesting methodologies that are oriented at assessing the performance of the allocation estimation procedure. The paper closes with a substantial numerical study of the subject and an application to market data. © 2020 Walter de Gruyter GmbH, Berlin/Boston 2020.}}, 
pages = {1--24}, 
number = {NA}, 
volume = {NA}
}
@article{10.1002/asmb.2425, 
year = {2019}, 
title = {{V@R representation theorems in ambiguous frameworks}}, 
author = {Balbás, Alejandro and Charron, Jean‐Philippe}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2425}, 
abstract = {{The value at risk (V@R) is a very important risk measure with significant applications in finance (risk management, pricing, hedging, portfolio theory, etc), insurance (premium principles, optimal reinsurance, etc), production, marketing (newsvendor problem), etc. It also plays a critical role in regulation about risk (Basel, Solvency, etc), it is very appreciated by practitioners due to its intuitive interpretation, and it is the unique popular risk measure remaining finite for heavy tailed risks with unbounded expectation. Besides, ambiguous frameworks are becoming more and more usual in applications of risk analysis. Lack of data or committed errors may provoke discrepancies between real probabilities and estimated ones. This paper combines both V@R and ambiguous settings, and a new representation theorem for V@R is given. Consequently, inspired by previous studies dealing with coherent risk measures and their representation, we will give new methods to compute and optimize V@R under ambiguity. This seems to be a relevant finding because the analytical properties of V@R are very weak if one compares with a coherent risk measure. Indeed, V@R is neither continuous nor convex, which makes it very complicated to deal with it in mathematical approaches. Nevertheless, the results of this paper will allow us to transform computation and optimization problems involving V@R into continuous and differentiable problems. © 2019 John Wiley \& Sons, Ltd.}}, 
pages = {414--430}, 
number = {3}, 
volume = {35}
}
@article{10.1007/s00199-007-0272-1, 
year = {2008}, 
title = {{Expected utility inequalities: Theory and applications}}, 
author = {Zambrano, Eduardo}, 
journal = {Economic Theory}, 
issn = {09382259}, 
doi = {10.1007/s00199-007-0272-1}, 
abstract = {{Suppose we know the utility function of a risk averse decision maker who values a risky prospect X at a price CE. Based on this information alone I develop upper bounds for the tails of the probabilistic belief about X of the decision maker. In the paper I also illustrate how to use these expected utility bounds in a variety of applications, which include the estimation of risk measures from observed data, option valuation, and the study of credit risk. © 2007 Springer-Verlag.}}, 
pages = {147--158}, 
number = {1}, 
volume = {36}
}
@article{10.1016/j.iref.2013.01.006, 
year = {2013}, 
title = {{Estimating hedged portfolio value-at-risk using the conditional copula: An illustration of model risk}}, 
author = {Chen, Yi-Hsuan and Tu, Anthony H.}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2013.01.006}, 
abstract = {{The conventional portfolio value-at-risk model with the assumption of normal joint distribution, which is commonly practiced, exhibits considerable biases due to model specification errors. This paper utilizes the estimation of hedged portfolio value-at-risk (HPVaR) to illustrate the potential model risk due to inappropriate use of the correlation coefficient and normal joint distribution between index spot and futures returns. The results show that HPVaR estimation can be improved by using the conditional copulas and their mixture models to form joint distributions to calculate the optimal hedge ratio. Backtesting diagnostics indicate that the copula-based HPVaR outperforms the conventional HPVaR estimator at both the 99\% and the 95\% coverage rates. The conventional models obviously underestimate the HPVaR, especially under a 99\% coverage rate. We then employ a bootstrap resampling technique to quantify and compare the magnitude of model risk by constructing confidence intervals around HPVaR point estimates. The results suggest that the risk management models should apply a smaller nominal coverage rate (95\% instead of 99\%) to avoid the model risk mentioned above. © 2013 Elsevier Inc.}}, 
pages = {514--528}, 
number = {NA}, 
volume = {27}
}
@article{10.1108/s1569-375920140000096018, 
year = {2014}, 
title = {{Challenges in the application of extreme value theory in emerging markets: A case study of Pakistan}}, 
author = {Uppal, Jamshed Y. and Mudakkar, Syeda Rabab}, 
journal = {Contemporary Studies in Economic and Financial Analysis}, 
issn = {15693759}, 
doi = {10.1108/s1569-375920140000096018}, 
abstract = {{Application of financial risk models in the emerging markets poses special challenges. A fundamental challenge is to accurately model the return distributions which are particularly fat tailed and skewed. Value-at-Risk (VaR) measures based on the Extreme Value Theory (EVT) have been suggested, but typically data histories are limited, making it hard to test and apply EVT. The chapter addresses issues in (i) modeling the VaR measure in the presence of structural breaks in an economy, (ii) the choice of stable innovation distribution with volatility clustering effects, (iii) modeling the tails of the empirical distribution, and (iv) fixing the cut-off point for isolating extreme observations. Pakistan offers an instructive case since its equity market exhibits high volatility and incidence of extreme returns. The recent Global Financial Crisis has been another source of extreme returns. The confluence of the two sources of volatility provides us with a rich data set to test the VaR/EVT model rigorously and examine practical challenges in its application in an emerging market. Copyright © 2014 by Emerald Group Publishing Limited All rights of reproduction in any form reserved.}}, 
pages = {417--437}, 
number = {NA}, 
volume = {96}
}
@article{10.1007/s10614-020-10034-0, 
year = {2021}, 
title = {{On a Bivariate Hysteretic AR-GARCH Model with Conditional Asymmetry in Correlations}}, 
author = {Chen, Cathy W. S. and Than-Thi, Hong and Asai, Manabu}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-020-10034-0}, 
abstract = {{This paper investigates a conditionally dynamic asymmetric structure in correlations when multivariate time series follow a hysteretic autoregressive GARCH process that exhibits nonlinear switching in mean, volatility, and correlation. The hysteresis variable in the proposed model controls regime switching and time-varying delay. This new model allows for distinct responses to negative return shocks, as it can flexibly explore the possibility of asymmetry in the conditional correlation of two target variables. We employ an adaptive Bayesian MCMC method for the parameter estimation and quantile forecasting, which includes value-at-risk and volatility. We conduct backtesting to measure the effectiveness of value-at-risk forecasting, illustrate the proposed methods by using both simulated and real examples, and jointly measure for industry downside tail risk. Lastly, we evaluate the accuracy of the volatility forecast and determine whether there is persistence of conditional asymmetry in conditional correlation in the target time series. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {413--433}, 
number = {2}, 
volume = {58}
}
@article{10.1002/tee.22133, 
year = {2015}, 
title = {{Cost allocation of spinning reserve based on risk contribution}}, 
author = {Liu, Yangyang and Jiang, Chuanwen and Shen, Jingshuang and Hu, Jiakai}, 
journal = {IEEJ Transactions on Electrical and Electronic Engineering}, 
issn = {19314973}, 
doi = {10.1002/tee.22133}, 
abstract = {{Current cost allocation methods require generating companies (GENCOs) to afford spinning reserve (SR) costs according to their energy production rather than the impact on grid stabilization. The differences in generator reliability and forecast accuracy of renewables cause difficulty in quantifying the contribution of individual factors on the SR requirements (SRRs). First, this paper employs a reliability-constrained unit commitment (RCUC) model to determine the SRR and SR costs according to the grid reliability. Then, a cost allocation method is proposed to allocate these SR costs based on risk contribution theories. The risk contribution theories, marginal contribution, and stand-alone contribution are employed to measure the effect of individual risk factors on grid safety. The cost allocation method is demonstrated and discussed in the IEEE-RTS. The proposed risk contribution method can quantify the impacts of risk factors on grid safety and allocate SR costs into them according to their contributions. Additionally, this risk-based cost allocation method can encourage GENCOs to enhance the reliability level of generators and continuously improve the forecast accuracy of renewables to lower SR costs. © 2015 Institute of Electrical Engineers of Japan. Published by John Wiley \& Sons, Inc.}}, 
pages = {664--673}, 
number = {6}, 
volume = {10}
}
@article{10.1016/s0165-1889(03)00116-7, 
year = {2004}, 
title = {{Optimal portfolios under a value-at-risk constraint}}, 
author = {Yiu, K.F.C.}, 
journal = {Journal of Economic Dynamics and Control}, 
issn = {01651889}, 
doi = {10.1016/s0165-1889(03)00116-7}, 
abstract = {{This paper looks at the optimal portfolio problem when a value-at-risk constraint is imposed. This provides a way to control risks in the optimal portfolio and to fulfil the requirement of regulators on market risks. The value-at-risk constraint is derived for n risky assets plus a risk-free asset and is imposed continuously over time. The problem is formulated as a constrained utility maximization problem over a period of time. The dynamic programming technique is applied to derive the Hamilton-Jacobi-Bellman equation and the method of Lagrange multiplier is used to tackle the constraint. A numerical method is proposed to solve the HJB-equation and hence the optimal constrained portfolio allocation. Under this formulation, we find that investments in risky assets are optimally reduced by the imposed value-at-risk constraint. © 2003 Elsevier B.V. All rights reserved.}}, 
pages = {1317--1334}, 
number = {7}, 
volume = {28}
}
@article{10.1016/j.iref.2018.08.014, 
year = {2019}, 
title = {{Modeling the joint dynamic value at risk of the volatility index, oil price, and exchange rate}}, 
author = {Peng, Wei and Hu, Shichao and Chen, Wang and Zeng, Yu-feng and Yang, Lu}, 
journal = {International Review of Economics \& Finance}, 
issn = {10590560}, 
doi = {10.1016/j.iref.2018.08.014}, 
abstract = {{In this study, we investigate how the volatility index (VIX) and oil price influence the foreign exchange rate based on a conditional autoregressive value at risk model. We find that the oil price affects the value at risk (VaR) of exchange rates of oil-importing and oil-exporting countries differently. Further, the VIX as a volatility measure can only influence the tail risk of these currencies when the US financial market fluctuates significantly. In addition, we find that there is a significant increase in the volatility of the VaR of these currencies after the financial crisis. Our empirical results would provide useful information for investors. © 2018 Elsevier Inc.}}, 
pages = {137--149}, 
number = {NA}, 
volume = {59}
}
@article{10.1007/s10436-013-0223-8, 
year = {2013}, 
title = {{Risk classes for structured products: Mathematical aspects and their implications on behavioral investors}}, 
author = {Cao, Ji and Rieger, Marc Oliver}, 
journal = {Annals of Finance}, 
issn = {16142446}, 
doi = {10.1007/s10436-013-0223-8}, 
abstract = {{The new regulation of the EU for financial products (UCITS IV) prescribes Value at Risk (VaR) as the benchmark for assessing the risk of structured products. We discuss the limitations of this approach and show that, in theory, the expected return of structured products is unbounded while the VaR requirement for the lowest risk class can still be satisfied. Real-life examples of large returns within the lowest risk class are then provided. The results demonstrate that the new regulation could lead to new seemingly safe products that hide large risks. Behavioral investors that choose products only based on their official risk classes and their expected returns will, therefore, invest into suboptimal products. To overcome these limitations, we suggest a new risk-return measure for financial products based on the martingale measure that could erase such loopholes. © 2013 Springer-Verlag Berlin Heidelberg.}}, 
pages = {167--183}, 
number = {2}, 
volume = {9}
}
@article{10.1007/s40070-014-0024-y, 
year = {2014}, 
title = {{Pricing conspicuous consumption products in recession periods with uncertain strength}}, 
author = {Huschto, Tony and Sager, Sebastian}, 
journal = {EURO Journal on Decision Processes}, 
issn = {21939438}, 
doi = {10.1007/s40070-014-0024-y}, 
abstract = {{We compare different approaches of optimization under uncertainty in the context of pricing strategies for conspicuous consumption products in recession periods of uncertain duration and strength. We consider robust worst-case ideas and how the concepts of Value at Risk (VaR) and Conditional Value at Risk (CVaR) can be incorporated efficiently. The approaches are generic in the sense that they can be applied to other economic decision-making problems with uncertainty. We discuss the strengths and weaknesses of these approaches in general. We quantify runtimes and differences when applied to the special case of pricing decision making. We notice that VaR results in reliable strategies, although it is not a coherent measure of risk. The CVaR idea that has become the method of choice in financial mathematics is a very risk-averse version of safeguarding and thus a bit too conservative for pricing decisions. Also the resulting optimal control problem is the most expensive one from a computational point of view. From an economic point of view we observe different safeguarding strategies with respect to when and how prices are adapted. Qualitatively, no surprises arise: The more conservative a strategy is, the sooner prices are reduced to avoid bankruptcy. Yet, the discussion of the advantages and disadvantages is generic and can be transferred to other economic problems. The underlying mathematical model simplifies, as is often the case in economics. In the long run, there is a necessity to consider stochastic processes for the evolvement in time to reduce model-plant mismatch. Thus, our work to understand the behavior of the deterministic model and the impact of different robustification techniques should be seen as a step in this direction and as a blueprint for similar decision-making tasks with quantitatively verified mathematical models. © 2014, Springer-Verlag Berlin Heidelberg and EURO - The Association of European Operational Research Societies.}}, 
pages = {3--30}, 
number = {1-2}, 
volume = {2}
}
@article{10.1142/s021962200600209x, 
year = {2006}, 
title = {{Model risk in VaR estimation: An empirical study}}, 
author = {YAO, JING and LI, ZHONG-FEI and NG, KAI W}, 
journal = {International Journal of Information Technology \& Decision Making}, 
issn = {02196220}, 
doi = {10.1142/s021962200600209x}, 
abstract = {{This paper studies the model risk; the risk of selecting a model for estimating the Value-at-Risk (VaR). By considering four GARCH-type volatility processes exponentially weighted moving average (EWMA), generalized autoregressive conditional heteroskedasticity (GARCH), exponential GARCH (EGARCH), and fractionally integrated GARCH (FIGARCH), we evaluate the performance of the estimated VaRs using statistical tests including the Kupiec's likelihood ratio (LR) test, the Christoffersen's LR test, the CHI (Christoffersen, Hahn, and Inoue) specification test, and the CHI nonnested test. The empirical study based on Shanghai Stock Exchange A Share Index indicates that both EGARCH and FIGARCH models perform much better than the other two in VaR computation and that the two CHI tests are more suitable for analyzing model risk. © World Scientific Publishing Company.}}, 
pages = {503--512}, 
number = {3}, 
volume = {5}
}
@article{10.1016/j.insmatheco.2007.06.006, 
year = {2008}, 
title = {{Portfolio diversification under local and moderate deviations from power laws}}, 
author = {Ibragimov, Rustam and Walden, Johan}, 
journal = {Insurance: Mathematics and Economics}, 
issn = {01676687}, 
doi = {10.1016/j.insmatheco.2007.06.006}, 
abstract = {{This paper analyzes portfolio diversification for nonlinear transformations of heavy-tailed risks. It is shown that diversification of a portfolio of convex functions of heavy-tailed risks increases the portfolio's riskiness if expectations of these risks are infinite. In contrast, for concave functions of heavy-tailed risks with finite expectations, the stylized fact that diversification is preferable continues to hold. The framework of transformations of heavy-tailed risks includes many models with Pareto-type distributions that exhibit local or moderate deviations from power tails in the form of additional slowly varying or exponential factors. The class of distributions under study is therefore extended beyond the stable class. © 2007 Elsevier Ltd. All rights reserved.}}, 
pages = {594--599}, 
number = {2}, 
volume = {42}
}
@article{10.1007/s10479-011-0900-9, 
year = {2014}, 
title = {{Portfolio value-at-risk estimation in energy futures markets with time-varying copula-GARCH model}}, 
author = {Lu, Xun Fa and Lai, Kin Keung and Liang, Liang}, 
journal = {Annals of Operations Research}, 
issn = {02545330}, 
doi = {10.1007/s10479-011-0900-9}, 
abstract = {{This paper combines copula functions with GARCH-type models to construct the conditional joint distribution, which is used to estimate Value-at-Risk (VaR) of an equally weighted portfolio comprising crude oil futures and natural gas futures in energy market. Both constant and time-varying copulas are applied to fit the dependence structure of the two assets returns. The findings show that the constant Student t copula is a good compromise for effectively fitting the dependence structure between crude oil futures and natural gas futures. Moreover, the skewed Student t distribution has a better fit than Normal and Student t distribution to the marginal distribution of each asset. Asymmetries and excess kurtosis are found in marginal distributions as well as in dependence. We estimate VaR of the underlying portfolio to be 95\% and 99\%, by using the Monte Carlo simulation. Then using backtesting, we compare the out-of-sample forecasting performances of VaR estimated by different models. © 2011 Springer Science+Business Media, LLC.}}, 
pages = {333--357}, 
number = {1}, 
volume = {219}
}
@article{10.1016/j.irfa.2004.06.011, 
year = {2005}, 
title = {{Value at risk methodology of international index portfolio under soft conditions (fuzzy-stochastic approach)}}, 
author = {Zmeškal, Zdeněk}, 
journal = {International Review of Financial Analysis}, 
issn = {10575219}, 
doi = {10.1016/j.irfa.2004.06.011}, 
abstract = {{The approach to modelling uncertainty of the international index portfolio by the value at risk (VAR) methodology under soft conditions by fuzzy-stochastic methodology is described in the paper. The generalised term uncertainty is understood to have two aspects: risk modelled by probability (stochastic methodology) and vagueness sometimes called impreciseness, ambiguity, softness is modelled by fuzzy methodology. Thus, hybrid model is called fuzzy-stochastic model. Input data for a stochastic model are unique distribution functions and crisp (real) data. Input data for fuzzy model are fuzzy numbers and crisp (real) data. Input data for hybrid model are fuzzy probability distribution functions, unique distribution functions, and crisp (real) data. Softly defined VAR model is constructed as hybrid model because it is supposed that the input data are difficult to determine as crisp numbers or as some unique distribution functions. Risk is modelled by stochastic methodology on the VAR basis and vagueness is modelled through the fuzzy numbers. The analytical delta normal VAR methodology for international index portfolio under soft conditions is described including illustrative example. It is shown, that methodology described could be considered to be generalised sensitivity analysis. © 2004 Elsevier Inc. All rights reserved.}}, 
pages = {263--275}, 
number = {2}, 
volume = {14}
}
@article{10.1002/rfe.1139, 
year = {2021}, 
title = {{Estimating value-at-risk models for non-conventional equity market index}}, 
author = {Baig, Ahmed S. and Butt, Hassan A. and Khalid, Rizwan}, 
journal = {Review of Financial Economics}, 
issn = {10583300}, 
doi = {10.1002/rfe.1139}, 
abstract = {{In this study, we evaluate Value-at-Risk (VaR) forecasts for non-conventional (i.e., Islamic) equity markets using various time-varying volatility models. Recent evidence suggests that volatility shifts in returns cause non-normality by significantly increasing kurtosis. Consequently, we endogenously detect significant shifts in the volatility of our Islamic equity market index returns and incorporate this information into selected models to estimate the downside risk. Our results show that the best forecast is generated by the dynamic quantile regression (DQR) model that does not make any specific assumptions about the underlying returns distribution. We also show the economic implications of our findings by calculating daily capital charges under Basel II Accord. © 2021 University of New Orleans.}}, 
number = {NA}, 
volume = {NA}
}
@article{10.1108/00214980380001139, 
year = {2003}, 
title = {{Systemic risk in U.S. crop reinsurance programs}}, 
author = {Mason, Chuck and Hayes, Dermot J. and Lence, Sergio H.}, 
journal = {Agricultural Finance Review}, 
issn = {00021466}, 
doi = {10.1108/00214980380001139}, 
abstract = {{This study develops a method to estimate the probability density function of the Federal Risk Management Agency’s (RMA’s) net income from reinsuring crop insurance for corn, wheat, and soybeans. When calibrated using 1997 data, results from the advocated method show that in 1997 there was a 5\% probability RMA would have had to reimburse at least \$1 billion to insurance companies, and the fair value of RMA’s insurance services to insurance firms in 1997 was \$78.7 million. © 2001, Emerald Group Publishing Ltd. All rights reserved.}}, 
pages = {23--39}, 
number = {1}, 
volume = {63}
}
@article{10.24818/18423264/54.3.20.03, 
year = {2020}, 
title = {{Bitcoin cash: Stochastic models of fat-tail returns and risk modeling}}, 
author = {MUHAMMAD, SHERAZ and SILVIA, DEDU}, 
journal = {ECONOMIC COMPUTATION AND ECONOMIC CYBERNETICS STUDIES AND RESEARCH}, 
issn = {0424267X}, 
doi = {10.24818/18423264/54.3.20.03}, 
abstract = {{Bitcoin (BTC) is a digital currency that has gained significant attention from researchers. The aim of this paper consists in analyzing some stochastic models of fat-tail returns and risk models. The evidence of fat-tailed returns distribution for the BCH data is investigated, by performing a statistical analysis of Bitcoin Cash (BCH) in the U.S. dollar. By using daily Close, Open, Low, and High returns of BCH data series, the monthly-divided daily returns study describes further properties such as skewness, kurtosis, and correlation analysis. The results obtained prove that variance gamma distribution best fit the close, open and low returns, where high returns follow the generalized hyperbolic distribution. In addition, for the best-fitted fat-tailed returns distributions, several risk measures such as volatility, Value-at-Risk and Expected Shortfall measures are computed, analyzed and compared. © 2020, Bucharest University of Economic Studies. All rights reserved.}}, 
pages = {43--58}, 
number = {3}, 
volume = {54}
}
@article{10.1016/s0895-7177(01)00129-7, 
year = {2001}, 
title = {{Stable modeling of value at risk}}, 
author = {Khindanova, I. and Rachev, S. and Schwartz, E.}, 
journal = {Mathematical and Computer Modelling}, 
issn = {08957177}, 
doi = {10.1016/s0895-7177(01)00129-7}, 
abstract = {{The value-at-risk (VAR) measurements are widely applied to estimate exposure to market risks. The traditional approaches to VAR computations - the variance-covariance method, historical simulation, Monte Carlo simulation, and stress-testing - do not provide satisfactory evaluation of possible losses. In this paper, we analyze the use of stable Paretian distributions in VAR modeling. © 2001 Elsevier Science Ltd. All rights reserved.}}, 
pages = {1223--1259}, 
number = {9-11}, 
volume = {34}
}
@article{10.1142/s0219091519500255, 
year = {2019}, 
title = {{Impact of Expected Shortfall Approach on Capital Requirement under Basel}}, 
author = {Siu, Yam Wing}, 
journal = {Review of Pacific Basin Financial Markets and Policies}, 
issn = {02190915}, 
doi = {10.1142/s0219091519500255}, 
abstract = {{This paper proposes a method that uses volatility index of US and six other markets of Pacific Basin, namely Hong Kong, Australia, India, Japan, Korea, and China, to provide value-at-risk (VaR) and expected shortfall (ES) forecasts. Empirical constants that are used to multiply the levels of volatility indexes for estimating VaR and ES of various significance levels for 1-22 days ahead, one by one, for seven market indexes have been statistically determined using daily data spanning from 4.75 to 16 years. It is because it would be inappropriate to simply scale up the one-day volatility by multiplying the square root of time (or the number of days) ahead to determine the risk over a longer horizon of i days. Results show that the shift to ES approach generally increases the regulatory capital requirements from 2.09\% of India market to 8.56\% of Korea market except for the China market where ES approach yields an unexpected decrease of 3.21\% of capital requirement. © 2019 World Scientific Publishing Co.}}, 
pages = {1950025}, 
number = {4}, 
volume = {22}
}
@article{10.3390/risks8040118, 
year = {2020}, 
title = {{Managing meteorological risk through expected shortfall}}, 
author = {Stefani, Silvana and Kutrolli, Gleda and Moretto, Enrico and Kulakov, Sergei}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks8040118}, 
abstract = {{This paper focuses on weather derivatives as efficient risk management instruments and proposes a more advanced approach for their pricing. An “hybrid” contract is introduced, combining insurance properties, specifically tailored for the region under study and introducing Value-at-Risk (VaR) and Expected Shortfall (ES) as appropriate measures for the strike price. The numerical results show that VaR and ES are both efficient ways for managing the so-called Tail Risk; further, being ES more conservative than VaR and due to its subadditivity property, it can be seen that seasonal contracts are generally better off than monthly contracts in reducing global risk. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.}}, 
pages = {118}, 
number = {4}, 
volume = {8}
}
@article{10.2298/fil1803815b, 
year = {2018}, 
title = {{Portfolio optimization by using meanSharp-βVaR and multi objective MeanSharp-βVaR models}}, 
author = {Banihashemi, Shokoofeh and Navidi, Sarah}, 
journal = {Filomat}, 
issn = {03545180}, 
doi = {10.2298/fil1803815b}, 
abstract = {{The purpose of this study is to develop portfolio optimization and assets allocation using our proposed models. For this, three steps are considered. In the first step, the stock companies screen by their financial data. For second step, we need some inputs and outputs for solving Data Envelopment Analysis (DEA) models. Conventional DEA models assume non-negative data for inputs and outputs. However, many of these data take the negative value, therefore we propose the MeanSharp-beta;VaR (MShbeta;V) model and the Multi Objective MeanSharp-beta;VaR (MOMShbeta;V) model based on Range Directional Measure (RDM) that can take positive and negative values. Also, we consider one of downside risk measures named Value at Risk (VaR) and try to decrease it. After using our proposed models, the efficient stock companies will be selected for making the portfolio. In the third step, Multi Objective Decision Making (MODM) model was used to specify the capital allocation to the stock companies that was selected for the portfolio. Finally, a numerical example of the purposed method in Iranian stock companies is presented. © 2018, University of Nis. All rights reserved.}}, 
pages = {815--823}, 
number = {3}, 
volume = {32}
}
@article{10.1016/j.econmod.2014.05.037, 
year = {2014}, 
title = {{Backtesting VaR in consideration of the higher moments of the distribution for minimum-variance hedging portfolios}}, 
author = {Chuang, Chung-Chu and Wang, Yi-Hsien and Yeh, Tsai-Jung and Chuang, Shuo-Li}, 
journal = {Economic Modelling}, 
issn = {02649993}, 
doi = {10.1016/j.econmod.2014.05.037}, 
abstract = {{The higher moments of a distribution often lead to estimated value-at-risk (VaR) biases. This study's objective is to examine the backtesting of VaR models that consider the higher moments of the distribution for minimum-variance hedging portfolios (MVHPs) of the stock indices and futures in the Greater China Region for both short and long hedgers. The results reveal that the best backtesting VaR for the MVHP considered both the higher moments of the MVHP distribution and the asymmetry in volatility, cross-market asymmetry in volatility, and level effects in the covariance matrix of assets in the MVHP. These empirical results provide references for investors in risk management. © 2014.}}, 
pages = {15--19}, 
number = {NA}, 
volume = {42}
}
@article{10.1007/978-3-030-72792-5_10, 
year = {2021}, 
title = {{Debt Risk Research on PPP Model Based on VAR (Value at Risk) Model}}, 
author = {Yang, GuangLi and Wang, Chao and Kuang, Wenmin}, 
journal = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering}, 
issn = {18678211}, 
doi = {10.1007/978-3-030-72792-5\_10}, 
abstract = {{The report of the 19th national congress points out that from now on to 2020 is the decisive period for building a moderately prosperous society in an all-round way, while PPP project investment involves 19 industries, such as transportation, comprehensive development of cities and towns, education, health care, pension, etc., providing more convenience and services for people’s life. At present, the amount of investment in PPP projects in China is relatively large, and the cumulative investment is more than 13 trillion yuan. China’s PPP projects have formed the largest PPP market in the world. The promotion of PPP model is conducive to solving the problem of funds shortage of local governments, but the poor application of PPP model will lead to debt risk, even financial risk. This paper first analyzes the current situation of PPP model in China, then uses VAR model to quantify the debt risk loss caused by PPP projects, and local governments should prepare corresponding reserve funds to prevent the loss. Finally, in view of the debt risk, it puts forward some policy suggestions, such as the establishment of PPP project feasibility analysis, implementation process supervision, performance evaluation, and risk prevention mechanism. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2021.}}, 
pages = {105--116}, 
number = {NA}, 
volume = {369}
}
@article{10.1016/j.jbankfin.2010.12.007, 
year = {2011}, 
title = {{Time series analysis for financial market meltdowns}}, 
author = {Kim, Young Shin and Rachev, Svetlozar T. and Bianchi, Michele Leonardo and Mitov, Ivan and Fabozzi, Frank J.}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2010.12.007}, 
abstract = {{There appears to be a consensus that the recent instability in global financial markets may be attributable in part to the failure of financial modeling. More specifically, it is alleged that current risk models have failed to properly assess the risks associated with large adverse stock price behavior. In this paper, we first discuss the limitations of classical time series models for forecasting financial market meltdowns. Then we set forth a framework capable of forecasting both extreme events and highly volatile markets. Based on the empirical evidence presented in this paper, our framework offers an improvement over prevailing models for evaluating stock market risk exposure during distressed market periods. © 2011 Elsevier B.V.}}, 
pages = {1879--1891}, 
number = {8}, 
volume = {35}
}
@article{10.1002/asmb.2479, 
year = {2019}, 
title = {{Quantile forecasting based on a bivariate hysteretic autoregressive model with GARCH errors and time -varying correlations}}, 
author = {Chen, Cathy W.S. and Than‐Thi, Hong and So, Mike K.P. and Sriboonchitta, Songsak}, 
journal = {Applied Stochastic Models in Business and Industry}, 
issn = {15241904}, 
doi = {10.1002/asmb.2479}, 
abstract = {{To understand and predict chronological dependence in the second-order moments of asset returns, this paper considers a multivariate hysteretic autoregressive (HAR) model with generalized autoregressive conditional heteroskedasticity (GARCH) specification and time-varying correlations, by providing a new method to describe a nonlinear dynamic structure of the target time series. The hysteresis variable governs the nonlinear dynamics of the proposed model in which the regime switch can be delayed if the hysteresis variable lies in a hysteresis zone. The proposed setup combines three useful model components for modeling economic and financial data: (1) the multivariate HAR model, (2) the multivariate hysteretic volatility models, and (3) a dynamic conditional correlation structure. This research further incorporates an adapted multivariate Student t innovation based on a scale mixture normal presentation in the HAR model to tolerate for dependence and different shaped innovation components. This study carries out bivariate volatilities, Value at Risk, and marginal expected shortfall based on a Bayesian sampling scheme through adaptive Markov chain Monte Carlo (MCMC) methods, thus allowing to statistically estimate all unknown model parameters and forecasts simultaneously. Lastly, the proposed methods herein employ both simulated and real examples that help to jointly measure for industry downside tail risk. © 2019 John Wiley \& Sons, Ltd.}}, 
pages = {1301--1321}, 
number = {6}, 
volume = {35}
}
@article{10.1093/imaman/dpq001, 
year = {2011}, 
title = {{A model of equilibrium in markets of cash balances}}, 
author = {Mierzejewski, F}, 
journal = {IMA Journal of Management Mathematics}, 
issn = {1471678X}, 
doi = {10.1093/imaman/dpq001}, 
abstract = {{A model is presented to characterize the equilibrium in markets of short-term loans. First, the cash demand is characterized as the optimal balance maintained to avoid the losses produced by some portfolio with random payoffs. This problem is formulated in actuarial terms in such a way that the optimal balance is expressed as the quantile function of the probability distribution describing the underlying risk (i.e. the value-at-risk). An expression is then obtained for the semi-elasticity of the demand for balances with respect to the interest rate. The effects of credit and investment flows over the equilibrium can be precisely described on these grounds. In the particular case, when the series of price returns of the underlying portfolio is described by a Gaussian probability distribution, episodes of liquidity crises can be corresponded to specific combinations of the risk parameters and the level of the interest rate. Theoretical evidence is thus given of these phenomena. © 2010 The authors Published by Oxford University Press on behalf of the Institute of Mathematics and its Applications. All rights reserved.}}, 
pages = {253--270}, 
number = {3}, 
volume = {22}
}
@article{10.4018/jgim.2018100102, 
year = {2018}, 
title = {{News sentiment incorporation in real-time trading: Alpha testing the event trading strategy in HFT}}, 
author = {Karn, Arodh Lal and Qiang, YE and Karna, Rakshha Kumari and Wang, Xiaolin}, 
journal = {Journal of Global Information Management}, 
issn = {10627375}, 
doi = {10.4018/jgim.2018100102}, 
abstract = {{This article describes how machines are the new breed of traders as news sentiment arrivals drive the stock price change. Strategies are the technical approach to search for profit from event-based speculations. This paper revisits these topics in a novel way and first uncovers distinctive characteristics of high frequency trading in Helsinki stock exchange insinuating the impression on positive recovers of event trading. Here is a better prediction by the incorporation of news on returns that proposed event trading strategy has significant effects on Finnish stock. This article contributes to the con temporarily embarked, upgrading form of practical paperwork on the take of news events in high economic science. Copyright © 2018, IGI Global.}}, 
pages = {18--35}, 
number = {4}, 
volume = {26}
}
@article{10.1016/j.pacfin.2015.09.001, 
year = {2015}, 
title = {{Governance mechanisms and downside risk}}, 
author = {Wang, Li-Hsun and Lin, Chu-Hsiung and Fung, Hung-Gay and Chen, Hsien-Ming}, 
journal = {Pacific-Basin Finance Journal}, 
issn = {0927538X}, 
doi = {10.1016/j.pacfin.2015.09.001}, 
abstract = {{This study uses data for Taiwanese firms from 2002 to 2012 to investigate the relation between corporate governance and downside risk. Our results show that good corporate governance reduces downside risk while increasing firm value. That is, firms with high managerial ownership, market power, and independent boards appear to have lower downside risk, likely because their decision-making is more transparent than that of firms without these characteristics. © 2015 Elsevier B.V.}}, 
pages = {485--498}, 
number = {NA}, 
volume = {35}
}
@article{10.1007/s10614-020-09996-y, 
year = {2021}, 
title = {{A Statistical Analysis of Global Economies Using Time Varying Copulas}}, 
author = {Afuecheta, Emmanuel and Nadarajah, Saralees and Chan, Stephen}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-020-09996-y}, 
abstract = {{The application of time varying copulas has become popular in recent years. Here, we illustrate an application involving stock indices of ten major economies covering all of the six continents. The dependence among them and its variation with respect to time are modeled using ten different copulas. The Gaussian copula is found to give the best fit. Predictions are given in terms of correlations and value at risk. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1167--1194}, 
number = {4}, 
volume = {58}
}
@article{10.5705/ss.2014.228t, 
year = {2016}, 
title = {{Value at risk for integrated returns and its applications to equity portfolios}}, 
author = {Ho, Hwai-Chung and Chen, Hungyin and Tsai, Henghsiu}, 
journal = {Statistica Sinica}, 
issn = {10170405}, 
doi = {10.5705/ss.2014.228t}, 
abstract = {{The present paper investigates the distribution quantile for integrated portfolio returns that follow a general class of multivariate stochastic volatility model. We propose a non-parametric quantile estimate that incorporates the rate with which the true quantile diverges as the integration horizon expands. The asymptotic normality established for the estimate enables us to construct the confidence interval for the true quantile. Monte Carlo experiments are conducted to demonstrate both the consistency and the advantages of our approach. Results on quantile estimates for the return distribution of the S\&P 500 index are also presented.}}, 
number = {4}, 
volume = {26}
}
@article{10.1017/asb.2016.4, 
year = {2016}, 
title = {{Using weighted distributions to model operational risk}}, 
author = {Afonso, Lourdes B. and Real, Pedro Corte}, 
journal = {ASTIN Bulletin}, 
issn = {05150361}, 
doi = {10.1017/asb.2016.4}, 
abstract = {{The quantification of operational risk has to deal with various concerns regarding data, much more than other types of risk which banks and insurers are obliged to manage. One of the main questions that worries both researchers and practitioners is the bias in the data on the operational losses amounts recorded. We support the assertions made by several authors and defend that this concern is serious when modeling operational losses data and, typically, is presented in all the databases. We show that it's possible, based on mild assumptions on the internal procedures put in place to manage operational losses, to make parametric inference using loss data statistics, that is, to estimate the parameters for the losses amounts, taking in consideration the bias that, not being considered, generates a two fold error in the estimators for the mean loss amount and the total loss amount, the former being overvalued and the last undervalued. In this paper, we do not consider the existence of a threshold for which, all losses above, are reported and available for analysis and estimation procedures. In this sense, we follow a different approach to the parametric inference. Here, we consider that the probability that a loss is reported and ends up recorded for analysis, increases with the size of the loss, what causes the bias in the database but, at the same time, we do not consider the existence of a threshold, above which, all losses are recorded. Hence, no loss has probability one of being recorded, in what we defend is a realist framework. We deduce the general formulae, present simulations for common theoretical distributions used to model (operational reported) losses amounts, estimate the impact for not considering the bias factor when estimating the value at risk and estimate the true total operational losses the bank incurred. Copyright © Astin Bulletin 2016.}}, 
pages = {469--485}, 
number = {2}, 
volume = {46}
}
@article{10.1007/s10614-018-9806-9, 
year = {2019}, 
title = {{Risk: An R Package for Financial Risk Measures}}, 
author = {Chan, Stephen and Nadarajah, Saralees}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-018-9806-9}, 
abstract = {{A new R contributed package written by the authors is introduced. The package is believed to be the most comprehensive one to date for financial risk measures. It computes twenty six financial risk measures for any continuous distribution. The use of the package is illustrated. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.}}, 
pages = {1337--1351}, 
number = {4}, 
volume = {53}
}
@article{10.1111/j.1539-6924.2007.00943.x, 
year = {2007}, 
title = {{Extreme risk analysis of interdependent economic and infrastructure sectors}}, 
author = {Lian, Chenyang and Santos, Joost R. and Haimes, Yacov Y.}, 
journal = {Risk Analysis}, 
issn = {02724332}, 
doi = {10.1111/j.1539-6924.2007.00943.x}, 
pmid = {17958511}, 
abstract = {{Willful attacks or natural disasters pose extreme risks to sectors of the economy. An extreme-event analysis extension is proposed for the Inoperability Input-Output Model (IIM) and the Dynamic IIM (DIIM), which are analytical methodologies for assessing the propagated consequences of initial disruptions to a set of sectors. The article discusses two major risk categories that the economy typically experiences following extreme events: (i) significant changes in consumption patterns due to lingering public fear and (ii) adjustments to the production outputs of the interdependent economic sectors that are necessary to match prevailing consumption levels during the recovery period. Probability distributions associated with changes in the consumption of directly affected sectors are generated based on trends, forecasts, and expert evidence to assess the expected losses of the economy. Analytical formulations are derived to quantify the extreme risks associated with a set of initially affected sectors. In addition, Monte Carlo simulation is used to handle the more complex calculations required for a larger set of sectors and general types of probability distributions. A two-sector example is provided at the end of the article to illustrate the proposed extreme risk model formulations. © 2007 Society for Risk Analysis.}}, 
pages = {1053--1064}, 
number = {4}, 
volume = {27}
}
@article{10.1007/978-3-642-30433-0_5, 
year = {2012}, 
title = {{Solvency capital estimation and risk measures}}, 
author = {Ferri, Antoni and Guillén, Montserrat and Bermúdez, Lluís}, 
journal = {Lecture Notes in Business Information Processing}, 
issn = {18651348}, 
doi = {10.1007/978-3-642-30433-0\_5}, 
abstract = {{This paper examines why a financial entity's solvency capital estimation might be underestimated if the total amount required is obtained directly from a risk measurement. Using Monte Carlo simulation we show that, in some instances, a common risk measure such as Value-at-Risk is not subadditive when certain dependence structures are considered. Higher risk evaluations are obtained for independence between random variables than those obtained in the case of comonotonicity. The paper stresses, therefore, the relationship between dependence structures and capital estimation. © 2012 Springer-Verlag.}}, 
pages = {34--43}, 
number = {NA}, 
volume = {115 LNBIP}
}
@article{10.1007/s10100-009-0084-4, 
year = {2009}, 
title = {{The credit risk+ model with general sector correlations}}, 
author = {Deshpande, Amogh and Iyer, Srikanth K.}, 
journal = {Central European Journal of Operations Research}, 
issn = {1435246X}, 
doi = {10.1007/s10100-009-0084-4}, 
abstract = {{We consider an enhancement of the credit risk+ model to incorporate correlations between sectors. We model the sector default rates as linear combinations of a common set of independent variables that represent macro-economic variables or risk factors. We also derive the formula for exact VaR contributions at the obligor level. © 2009 Springer-Verlag.}}, 
pages = {219--228}, 
number = {2}, 
volume = {17}
}
@article{10.3311/pp.ee.2011-3-4.01, 
year = {2011}, 
title = {{Decision tree combined with neural networks for financial forecast}}, 
author = {Bozsik, József}, 
journal = {Periodica Polytechnica Electrical Engineering}, 
issn = {03246000}, 
doi = {10.3311/pp.ee.2011-3-4.01}, 
abstract = {{In this article I would like to introduce a hybrid adaptive method. There is wide range of financial forecasts. This method is focusing on the economical default forecast, but the method can be used generally for other financial forecasts as well, for example for calculating the Value at Risk. This hybrid method is combined by two classical adaptive methods: the decision trees and the artificial neural networks. In this article I will show the structure of the hybrid method, the problems which occurred during the construction of the model and the solutions for the problems. I will show the results of the model and compare them with the results of another financial default forecast model. I will analyse the results and the reliability of the method and I will show how the parameters can influence the reliability of the results © Periodica Polytechnica 2011.}}, 
pages = {95}, 
number = {3-4}, 
volume = {55}
}
@article{10.7148/2014-0752, 
year = {2014}, 
title = {{The definition of stress situations and their prediction using liquidity in the framework of the EMIR regulation}}, 
author = {Doemoetoer, Barbara and Varadi, Kata}, 
journal = {ECMS 2014 Proceedings edited by: Flaminio Squazzoni, Fabio Baronio, Claudia Archetti, Marco Castellani}, 
issn = {NA}, 
doi = {10.7148/2014-0752}, 
abstract = {{The role of the central counterparties (CCP) in the financial sector is very important, since they bear the counterparty risk during the trading on stock exchanges. Because of the notable risk central counterparties have to face, the attention of the regulators has turned towards them lately, by defining several processes how the CCPs should measure and manage their risk. The definition of stress has a crucial role, however it is not specified clearly. Based on the regulation, we investigate a possible definition of stress, its consequences on the Hungarian stock market, and its relationship to and predictability from market liquidity. Proceedings 28th European Conference on Modelling and Simulation © ECMS Flaminio Squazzoni, Fabio Baronio, Claudia Archetti, Marco Castellani (Editors).}}, 
pages = {752--757}, 
number = {NA}, 
volume = {NA}
}
@article{10.1016/j.jbankfin.2014.05.033, 
year = {2014}, 
title = {{Semiparametric estimation of multi-asset portfolio tail risk}}, 
author = {Dias, Alexandra}, 
journal = {Journal of Banking \& Finance}, 
issn = {03784266}, 
doi = {10.1016/j.jbankfin.2014.05.033}, 
abstract = {{When correlations between assets turn positive, multi-asset portfolios can become riskier than single assets. This article presents the estimation of tail risk at very high quantiles using a semiparametric estimator which is particularly suitable for portfolios with a large number of assets. The estimator captures simultaneously the information contained in each individual asset return that composes the portfolio, and the interrelation between assets. Noticeably, the accuracy of the estimates does not deteriorate when the number of assets in the portfolio increases. The implementation is as easy for a large number of assets as it is for a small number. We estimate the probability distribution of large losses for the American stock market considering portfolios with ten, fifty and one hundred assets of stocks with different market capitalization. In either case, the approximation for the portfolio tail risk is very accurate. We compare our results with well known benchmark models. © 2014 Elsevier B.V.}}, 
pages = {398--408}, 
number = {NA}, 
volume = {49}
}
@article{10.1016/j.intfin.2017.02.002, 
year = {2017}, 
title = {{Measuring skill in the Islamic mutual fund industry: Evidence from GCC countries}}, 
author = {Hammami, Yacine and Oueslati, Abdelmonem}, 
journal = {Journal of International Financial Markets, Institutions and Money}, 
issn = {10424431}, 
doi = {10.1016/j.intfin.2017.02.002}, 
abstract = {{We examine the performance of Islamic mutual funds in GCC countries using the Berk and Van Binsbergen (2015) value-added measure. We find compelling evidence that skilled managers exist in the Islamic mutual fund industry. The average mutual fund has used this skill to generate approximately \$198,000 per month. The bootstrap methodology highlights that this performance is not obtained by chance. Finally, we document that in bad economic times, Islamic mutual funds have lower value-at-risk and higher Sharpe ratios than conventional benchmarks, supporting the claim that they are a means of hedging against international financial crises. © 2017 Elsevier B.V.}}, 
pages = {15--31}, 
number = {NA}, 
volume = {49}
}
@article{10.1109/melcon.2010.5476316, 
year = {2010}, 
title = {{Risk sensitivity of failure rate and maintenance expenditure: Application of VaR metrics in risk management}}, 
author = {Schreiner, Andrej and Balzer, Gerd and Precht, Armin}, 
journal = {Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference}, 
issn = {NA}, 
doi = {10.1109/melcon.2010.5476316}, 
abstract = {{Risk analysis is becoming actually an important part of asset management of power systems. The paper presents the simple but reliable method for risk management of power system carried out for an urban 10 kV network located in Germany. The basis of the method is the indication of risks by Value at Risk index according to Loss Distribution Approach developed by the insurance and financial industry. The Loss Distribution Approach is based on estimation of the frequency and severity distribution of failure events over a one year time horizon. Especially the proposed approach combines the probability and severity of risk contingencies based on reliability indices of the system assets. The indices are finally applied for sensitivity analysis of two particular risk factors, failure rate and maintenance expenditure. © 2010 IEEE.}}, 
pages = {1624--1629}, 
number = {NA}, 
volume = {NA}
}
@article{10.3860/ber.v20i2.1910, 
year = {2011}, 
title = {{Empirical comparison of extreme value theory vis-À-vis other methods of VaR estimation using ASEAN+3 exchange rates}}, 
author = {Rufino, Cesar C and Guia, Emmanuel G De}, 
journal = {DLSU Business \& Economics Review}, 
issn = {01167111}, 
doi = {10.3860/ber.v20i2.1910}, 
abstract = {{This study applies Extreme Value Theory in calculating Value-at-Risk (VaR) of portfolios consisting of foreign exchange exposures of ASEAN+3 countries. This paper addresses the issue that traditional VaR models assume normality of the return distribution. Empirical evidence confirms the stylized facts that financial asset returns are typically negatively skewed and fat-tailed. Moreover, risk management concerns itself with the distribution of the tails, or events in the extremes of the distribution. Estimation of magnitude and the likelihood of extreme events should be given greater attention than central tendency characteristics. Thus, this paper proposes the application of Extreme Value Theory in computing an "Extreme VaR" to directly focus on the behavior of the tail of return distribution. The modeling is done on daily exchange rates returns of ASEAN+3 countries from January 24, 2004 to January 31, 2010. © 2011 De La Salle University, Philippines.}}, 
number = {2}, 
volume = {20}
}
@article{10.1016/j.procs.2018.10.213, 
year = {2018}, 
title = {{Forecasting Exchange Rate Value at Risk using Deep Belief Network Ensemble based Approach}}, 
author = {He, Kaijian and Ji, Lei and Tso, Geoffrey K.F. and Zhu, Bangzhu and Zou, Yingchao}, 
journal = {Procedia Computer Science}, 
issn = {18770509}, 
doi = {10.1016/j.procs.2018.10.213}, 
abstract = {{In this paper, we propose a new Value at Risk estimate based on the Deep Belief Network ensemble model with Empirical Mode Decomposition (EMD) technique. It attempts to capture the multi-scale data features with the EMD-DBN ensemble model and predict the risk movement more accurately. Individual data components are extracted using EMD model while individual forecasts can be calculated at different scales using ARMA-GARCH model. The DBN model is introduced to search for the optimal nonlinear ensemble weights to combine the individual forecasts at different scales into the ensembled exchange rate VaR forecasts. Empirical studies using major exchange rates confirm that the proposed model demonstrates the superior performance compared to the benchmark models. © 2018 The Authors. Published by Elsevier B.V.}}, 
pages = {25--32}, 
number = {NA}, 
volume = {139}
}
@article{10.3390/risks5010011, 
year = {2017}, 
title = {{Optimal reinsurance policies under the var risk measure when the interests of both the cedent and the reinsurer are taken into account}}, 
author = {Jiang, Wenjun and Ren, Jiandong and Zitikis, Ričardas}, 
journal = {Risks}, 
issn = {22279091}, 
doi = {10.3390/risks5010011}, 
abstract = {{Optimal forms of reinsurance policies have been studied for a long time in the actuarial literature. Most existing results are from the insurer’s point of view, aiming at maximizing the expected utility or minimizing the risk of the insurer. However, as pointed out by Borch (1969), it is understandable that a reinsurance arrangement that might be very attractive to one party (e.g., the insurer) can be quite unacceptable to the other party (e.g., the reinsurer). In this paper, we follow this point of view and study forms of Pareto-optimal reinsurance policies whereby one party’s risk, measured by its value-at-risk (VaR), cannot be reduced without increasing the VaR of the counter-party in the reinsurance transaction. We show that the Pareto-optimal policies can be determined by minimizing linear combinations of the VaRs of the two parties in the reinsurance transaction. Consequently, we succeed in deriving user-friendly, closed-form, optimal reinsurance policies and their parameter values. © 2017 by the authors; licensee MDPI, Basel, Switzerland.}}, 
pages = {11}, 
number = {1}, 
volume = {5}
}
@article{10.1287/mnsc.2019.3378, 
year = {2020}, 
title = {{Risk-based loan pricing: Portfolio optimization approach with marginal risk contribution}}, 
author = {Chun, So Yeon and Lejeune, Miguel A}, 
journal = {Management Science}, 
issn = {00251909}, 
doi = {10.1287/mnsc.2019.3378}, 
abstract = {{We consider a lender (bank) that determines the optimal loan price (interest rate) to offer to prospective borrowers under uncertain borrower response and default risk. A borrower may or may not accept the loan at the price offered, and both the principal loaned and the interest income become uncertain because of the risk of default. We present a risk-based loan pricing optimization framework that explicitly takes into account the marginal risk contribution, the portfolio risk, and a borrower's acceptance probability. Marginal risk assesses the incremental risk contribution of a prospective loan to the bank's overall portfolio risk by capturing the dependencies between the prospective loan and the existing portfolio and is evaluated with respect to the value-at-risk and conditional value-at-risk measures. We examine the properties and computational challenges of the formulations. We design a reformulation method based on the concavifiability concept to transform the nonlinear objective functions and to derive equivalent mixed-integer nonlinear reformulations with convex continuous relaxations. We also extend the approach to multiloan pricing problems, which feature explicit loan selection decisions in addition to pricing decisions. We derive formulations with multiple loans that take the form of mixed-integer nonlinear problems with nonconvex continuous relaxations and develop a computationally efficient algorithmic method. We provide numerical evidence demonstrating the value of the proposed framework, test the computational tractability, and discuss managerial implications. © 2020 INFORMS}}, 
pages = {3735--3753}, 
number = {8}, 
volume = {66}
}
@article{10.1007/978-3-030-14730-3_9, 
year = {2019}, 
title = {{Analyzing and classifying risks: A case-study in the furniture industry}}, 
author = {Leão, Celina P. and Rodrigues, Matilde A. and Brito, Irene}, 
journal = {Studies in Systems, Decision and Control}, 
issn = {21984182}, 
doi = {10.1007/978-3-030-14730-3\_9}, 
abstract = {{In this work we propose a methodology which permits the risk analysis and classification of occupational accidents in industrial settings. Data used in this study corresponds to accidents that occurred in the furniture industry in Portugal in 2010. A loss random variable is constructed in order to model the number of lost days implied by different contact modes of injuries in industry. The corresponding risk measures, such as Value-at-Risk, expected loss, loss variance and exceedance probabilities are determined in order to analyze and classify the contact modes according to their risk level, allowing the identification of the most problematic and the less problematic accident category in this industry. Contact with cutting, sharp, rough material was the most problematic whereas contact with electrical current, temperature, hazardous substance was the one with lower risk level. © Springer Nature Switzerland AG 2019.}}, 
pages = {81--87}, 
number = {NA}, 
volume = {202}
}
@article{10.1177/0973801015583739, 
year = {2015}, 
title = {{Backtesting of Value at Risk Methodology: Analysis of Banking Shares in India}}, 
author = {Patra, Biswajit and Padhi, Puja}, 
journal = {Margin: The Journal of Applied Economic Research}, 
issn = {09738010}, 
doi = {10.1177/0973801015583739}, 
abstract = {{Value at risk (VaR) is used by financial experts to calculate and predict the risk of financial exposure. In the presence of volatility and long memory, it is a model useful for the prediction of loss in the equity index return series. Checking the accuracy of this model is necessary from the practitioners’ point of view. This article initially checks the presence of autoregressive conditional heteroscedastic (ARCH) and long-memory effects in the daily closing price of the Bombay Stock Exchange (BSE)-BANKEX return series. After confirming the ARCH and long-memory presence, it analyses the different methods of VaR calculation such as asymmetric power ARCH (APARCH), fractionally integrated exponential generalised ARCH (FIEGARCH), hyperbolic generalised GARCH (HYGARCH) and risk metrics. Then, it empirically tests the forecasting capacity of these VaR methods through techniques such as the Kupiec likelihood ratio (LR test) and dynamic quantile test. Furthermore, it checks the root-mean-squared error (RMSE) and mean absolute error (MAE) to determine the model with the least error. From the set of VaR models used here, by and large it concludes that the BANKEX return series has both long-memory and asymmetry effects. By comparing these models, it is implied that the HYGARCH model gives a better result, although the other models have their significance in the estimation and forecasting of the BANKEX return series. © 2015, © 2015 National Council of Applied Economic Research.}}, 
pages = {254--277}, 
number = {3}, 
volume = {9}
}
@article{10.1016/j.jempfin.2003.04.003, 
year = {2004}, 
title = {{Modelling daily Value-at-Risk using realized volatility and ARCH type models}}, 
author = {Giot, Pierre and Laurent, Sébastien}, 
journal = {Journal of Empirical Finance}, 
issn = {09275398}, 
doi = {10.1016/j.jempfin.2003.04.003}, 
abstract = {{In this paper, we compare the performance of a daily ARCH type model (which uses daily returns) with the performance of a model based on the daily realized volatility (which uses intraday returns) when the 1-day ahead value-at-risk (VaR) is to be computed. While the VaR specification based on a long memory skewed Student model for the daily realized volatility provides adequate 1-day-ahead VaR forecasts for two stock indexes (the CAC40 and SP500) and two exchange rate returns (the YEN-USD and DEM-USD), it does not really improve on the performance of a VaR model based on the skewed Student APARCH model and estimated using daily data only. Thus both methods seem to be equivalent. This paper also shows that daily returns standardized by the square root of the 1-day-ahead forecast of the daily realized volatility are not normally distributed. © 2004 Elsevier B.V. All rights reserved.}}, 
pages = {379--398}, 
number = {3}, 
volume = {11}
}
@article{10.1080/01605682.2019.1686958, 
year = {2021}, 
title = {{Allocation of risk capital in a cost cooperative game induced by a modified expected shortfall}}, 
author = {Bernardi, Mauro and Cerqueti, Roy and Palestini, Arsen}, 
journal = {Journal of the Operational Research Society}, 
issn = {01605682}, 
doi = {10.1080/01605682.2019.1686958}, 
abstract = {{The standard theory of coherent risk measures fails to consider individual institutions as part of a system which might itself experience instability and spread new sources of risk to the market participants. This paper fills this gap and proposes a cooperative market game where agents and institutions play the same role. We take into account a multiple institutions framework where some institutions jointly experience distress, and evaluate their individual and collective impact on the remaining institutions in the market. To carry out the analysis, we define a new risk measure (SCoES) which is a generalization of the Expected Shortfall of and we characterize the riskiness profile as the outcome of a cost cooperative game played by institutions in distress. Each institution’s marginal contribution to the spread of riskiness towards the safe institutions in then evaluated by calculating suitable solution concepts of the game such as the Banzhaf–Coleman and the Shapley–Shubik values. © 2019 Operational Research Society.}}, 
pages = {1--14}, 
number = {3}, 
volume = {72}
}
@article{10.1111/j.2041-6156.2009.00002.x, 
year = {2010}, 
title = {{Risk management lessons from 'knock-in knock-out' option disaster}}, 
author = {Khil, Jaeuk and Suh, Sangwon}, 
journal = {Asia‐Pacific Journal of Financial Studies}, 
issn = {12261165}, 
doi = {10.1111/j.2041-6156.2009.00002.x}, 
abstract = {{Currency knock-in knock-out (KIKO) options had been widely used for hedging exchange rate risks in Korean financial markets. However, as the Korean won moved in an unexpected direction during the global financial crisis period of 2007 and 2008, the hedging instruments incurred huge losses to the option holders. In this paper, we analyze the event from the viewpoint of risk assessment and management. We find that, first, if the option holders had assessed the risk levels with and without the KIKO options by using standard risk measures like value-at-risk or conditional value-at-risk, then many KIKO option contracts would not have been justifiable from the beginning. Second, having a proper view on the exchange rate dynamics turned out to be crucial for risk assessment and management. If the companies had a proper view instead of a myopic view on the exchange rate movement, then the KIKO options might not have been chosen. Finally, 'hedge-and-forget' behavior proved to be very costly and reckless. If the companies had continuously assessed and managed their risks, then the losses from the KIKO options could have been significantly mitigated. Some relevant pricing issues are also investigated. We find that most KIKO option contracts under study might not be significantly overpriced. However, potential impacts of the possible mispricing could be considerable in some cases. Nonetheless, the risk management failure proved to be more important for the KIKO option losses than the possible mispricing problem. © 2010 Korean Securities Association.}}, 
pages = {28--52}, 
number = {1}, 
volume = {39}
}
@article{10.1016/j.najef.2018.06.010, 
year = {2019}, 
title = {{Quantile range-based volatility measure for modelling and forecasting volatility using high frequency data}}, 
author = {Tan, Shay-Kee and Ng, Kok-Haur and Chan, Jennifer So-Kuen and Mohamed, Ibrahim}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2018.06.010}, 
abstract = {{Volatility of asset prices in financial market is not directly observable. Return-based models have been proposed to estimate the volatility using daily closing prices. Recently, many new range-based volatility measures were proposed to estimate the financial volatility. This paper proposes quantile Parkinson (QPK) measure to estimate daily volatility and to show how it can robustify the Parkinson (PK) measure in the presence of intraday extreme returns. Results from extensive simulation studies show that the QPK measure is more efficient than intraday (open-to-close) squared returns and PK measures in the presence of intraday extreme returns. To demonstrate the applicability of QPK measure, we analyse the daily Standard and Poor 500 index by fitting the QPK measure to the conditional autoregressive range (CARR) models. Results show that choosing a suitable interquantile level width for the QPK measure will reduce its variance and hence improve its efficiency. In addition, the QPK measure using asymmetric CARR model gives the best in-sample model fit based on Akaike information criterion and provides the best out-of-sample forecast based on root mean squared forecast error and other measures. Mincer Zarnowitz test is carried out to confirm the unbiasedness of the forecasted volatility. Different levels of value-at-risk and conditional value-at-risk forecasts are also provided and tested. © 2018 Elsevier Inc.}}, 
pages = {537--551}, 
number = {NA}, 
volume = {47}
}
@article{10.4310/sii.2017.v10.n3.a9, 
year = {2017}, 
title = {{Bayesian forecasting of Value-at-Risk based on variant smooth transition heteroskedastic models}}, 
author = {Chen, Cathy W S and Weng, Monica M C and Watanabe, Toshiaki}, 
journal = {Statistics and Its Interface}, 
issn = {19387989}, 
doi = {10.4310/sii.2017.v10.n3.a9}, 
abstract = {{To allow for a higher degree of flexibility in model parameters, we propose a general and time-varying nonlinear smooth transition (ST) heteroskedastic model with a second-order logistic function of varying speed in the mean and variance. This paper evaluates the performance of Value-at-Risk (VaR) measures in a class of risk models, specifically focusing on three distinct ST functions with GARCH structures: first- and second-order logistic functions, and the exponential function. The likelihood function is non-differentiable in terms of the threshold values and delay parameter. We employ Bayesian Markov chain Monte Carlo sampling methods to update the estimates and quantile forecasts. The proposed methods are illustrated using simulated data and an empirical study. We estimate VaR forecasts for the proposed models alongside some competing asymmetric models with skew and fat-tailed error probability distributions, including realized volatility models. To evaluate the accuracy of VaR estimates, we implement two loss functions and three backtests. The results show that at the 1\% level the ST model with a second-order logistic function and skew Student's t error is a worthy choice, when compared to a range of existing alternatives.}}, 
pages = {451--470}, 
number = {3}, 
volume = {10}
}
@article{10.1016/j.najef.2014.10.002, 
year = {2014}, 
title = {{Non-linear volatility dynamics and risk management of precious metals}}, 
author = {Demiralay, Sercan and Ulusoy, Veysel}, 
journal = {The North American Journal of Economics and Finance}, 
issn = {10629408}, 
doi = {10.1016/j.najef.2014.10.002}, 
abstract = {{In this paper, we investigate the value-at-risk predictions of four major precious metals (gold, silver, platinum, and palladium) with non-linear long memory volatility models, namely FIGARCH, FIAPARCH and HYGARCH, under normal and Student-t innovations' distributions. For these analyses, we consider both long and short trading positions. Overall, our results reveal that long memory volatility models under Student-t distribution perform well in forecasting a one-day-ahead VaR for both long and short positions. In addition, we find that FIAPARCH model with Student-t distribution, which jointly captures long memory and asymmetry, as well as fat-tails, outperforms other models in VaR forecasting. Our results have potential implications for portfolio managers, producers, and policy makers. © 2014 Elsevier Inc.}}, 
pages = {183--202}, 
number = {NA}, 
volume = {30}
}
@article{10.1109/ctte.2017.8260991, 
year = {2017}, 
title = {{Methodology for the introduction of var estimation in telecommunications' diffusion forecasting}}, 
author = {Kanellos, Nikolaos and Varoutas, Dimitrios}, 
journal = {2017 Internet of Things Business Models, Users, and Networks}, 
issn = {NA}, 
doi = {10.1109/ctte.2017.8260991}, 
abstract = {{Forecasting diffusion of novel telecommunication technologies is usually based on S-shaped models, mainly due to their accurate long-term predictions. Yet, the use of these models does not allow the introduction of risk in the forecast. In this paper, a methodology for the introduction of volatility, the most popular and traditional measure of risk, in the underlying calculations is presented. Risk is estimated with the use of Monte Carlo Simulation and the Value at Risk statistical technique is used for the evaluation of forecast risk. This model can find applications in all high-technology markets, where a diffusion model is usually applied for obtaining future forecasts. © 2017 IEEE.}}, 
pages = {1--5}, 
number = {NA}, 
volume = {2018-January}
}
@article{10.1080/1350486x.2011.620396, 
year = {2012}, 
title = {{Viterbi-based estimation for Markov switching GARCH model}}, 
author = {Elliott, Robert J. and Lau, John W. and Miao, Hong and Siu, Tak Kuen}, 
journal = {Applied Mathematical Finance}, 
issn = {1350486X}, 
doi = {10.1080/1350486x.2011.620396}, 
abstract = {{We outline a two-stage estimation method for a Markov-switching Generalized Autoregressive Conditional Heteroscedastic (GARCH) model modulated by a hidden Markov chain. The first stage involves the estimation of a hidden Markov chain using the Vitberi algorithm given the model parameters. The second stage uses the maximum likelihood method to estimate the model parameters given the estimated hidden Markov chain. Applications to financial risk management are discussed through simulated data. © 2012 Taylor \& Francis.}}, 
pages = {219--231}, 
number = {3}, 
volume = {19}
}
@article{10.1007/s11009-013-9345-8, 
year = {2014}, 
title = {{Comparing the Value at Risk Performance of the CreditRisk+ and its Enhancement: A Large Deviations Approach}}, 
author = {Deshpande, Amogh}, 
journal = {Methodology and Computing in Applied Probability}, 
issn = {13875841}, 
doi = {10.1007/s11009-013-9345-8}, 
abstract = {{The standard CreditRisk+ (CR+) is a well-known default-mode credit risk model. An extension to the CR+ that introduces correlation through a two-stage hierarchy of randomness has been discussed by Deshpande and Iyer (Central Eur J Oper Res 17(2):219–228, 2009) and more recently by Sowers (2010). It is termed the 2-stage CreditRisk+ (2-CR+) in the former. Unlike the standard CR+, the 2-CR+ model is formulated to allow correlation between sectoral default rates through dependence on a common set of macroeconomic variables. Furthermore the default rates for a 2-CR+ are distributed according to a general univariate distribution which is in stark contrast to the uniformly Gamma distributed sectoral default rates in the CR+. We would then like to understand the behaviour of these two models with regards to their computed Value at Risk (VaR) as the number of sectors and macroeconomic variables approaches infinity. In particular we would like to ask whether the 2-CR+ produces higher VaR than the CR+ and if so then for which type of credit portfolio. Utilizing the theory of Large deviations, we provide a methodology for comparing the Value at risk performance of these two competing models by computing certain associated rare event probabilities. In particular we show that the 2-Stage CR+ definitely produces higher VaR than the CR+ for a particular class of a credit portfolio which we term as a “balanced” credit portfolio. We support this statistical risk analysis through numerical examples. © 2013, Springer Science+Business Media New York.}}, 
pages = {1009--1023}, 
number = {4}, 
volume = {16}
}
@article{10.1108/jes-06-2012-0082, 
year = {2014}, 
title = {{Multivariate modelling of 10-day-ahead VaR and dynamic correlation for worldwide real estate and stock indices}}, 
author = {Degiannakis, Stavros and Kiohos, Apostolos}, 
journal = {Journal of Economic Studies}, 
issn = {01443585}, 
doi = {10.1108/jes-06-2012-0082}, 
abstract = {{Purpose: The Basel Committee regulations require the estimation of value-at-risk (VaR) at 99 percent confidence level for a ten-trading-day-ahead forecasting horizon. The paper provides a multivariate modelling framework for multi-period VaR estimates for leptokurtic and asymmetrically distributed real estate portfolio returns. The purpose of the paper is to estimate accurate ten-day-ahead 99\%VaR forecasts for real estate markets along with stock markets for seven countries across the world (the USA, the UK, Germany, Japan, Australia, Hong Kong and Singapore) following the Basel Committee requirements for financial regulation. Design/methodology/approach: A 14-dimensional multivariate Diag-VECH model for seven equity indices and their relative real estate indices is estimated. The authors evaluate the VaR forecasts over a period of two weeks in calendar time, or ten-trading-days, and at 99 percent confidence level based on the Basle Committee on Banking Supervision requirements. Findings: The Basel regulations require ten-day-ahead 99\%VaR forecasts. This is the first study that provides successful evidence for ten-day-ahead 99\%VaR estimations for real estate markets. Additionally, the authors provide evidence that there is a statistically significant relationship between the magnitude of the ten-day-ahead 99\%VaR and the level of dynamic correlation for real estate and stock market indices; a valuable recommendation for risk managers who forecast risk across markets. Practical implications: Risk managers, investors and financial institutions require dynamic multi-period VaR forecasts that will take into account properties of financial time series. Such accurate dynamic forecasts lead to successful decisions for controlling market risks. Originality/value: This paper is the first approach which models simultaneously the volatility and VaR estimates for real estate and stock markets from the USA, Europe and Asia-Pacific over a period of more than 20 years. Additionally, the local correlation between stock and real estate indices has statistically significant explanatory power in estimating the ten-day-ahead 99\%VaR. © Emerald Group Publishing Limited.}}, 
pages = {216--232}, 
number = {2}, 
volume = {41}
}
@article{10.1007/s12197-007-9016-0, 
year = {2008}, 
title = {{Incorporating correlation regimes in an integrated stressed risk modeling process}}, 
author = {Aragonés, José R. and Blanco, Carlos}, 
journal = {Journal of Economics and Finance}, 
issn = {10550925}, 
doi = {10.1007/s12197-007-9016-0}, 
abstract = {{The practice of using stress tests to complement Value at Risk (VaR) estimates suffers from some limitations such as the lack of coherence between a statistical risk measure and a subjective one. On the other hand there is a wide consensus that using the same correlation matrix to design various stress tests is not likely to provide an accurate representation of relationship amongst risk factors in periods of market stress. In this paper we introduce a solution to these problems by explicitly considering different correlation regimes and incorporating the result of the stress test to the traditional market risk measurement models. © Springer Science + Business Media, LLC 2007.}}, 
pages = {148--157}, 
number = {2}, 
volume = {32}
}
@article{10.1007/s10614-006-9071-1, 
year = {2007}, 
title = {{Portfolio optimization when risk factors are conditionally varying and heavy tailed}}, 
author = {Doganoglu, Toker and Hartz, Christoph and Mittnik, Stefan}, 
journal = {Computational Economics}, 
issn = {09277099}, 
doi = {10.1007/s10614-006-9071-1}, 
abstract = {{Assumptions about the dynamic and distributional behavior of risk factors are crucial for the construction of optimal portfolios and for risk assessment. Although asset returns are generally characterized by conditionally varying volatilities and fat tails, the normal distribution with constant variance continues to be the standard framework in portfolio management. Here we propose a practical approach to portfolio selection. It takes both the conditionally varying volatility and the fat-tailedness of risk factors explicitly into account, while retaining analytical tractability and ease of implementation. An application to a portfolio of nine German DAX stocks illustrates that the model is strongly favored by the data and that it is practically implementable. © Springer Science+Business Media, LLC 2006.}}, 
pages = {333--354}, 
number = {3-4}, 
volume = {29}
}
@article{10.1016/s2077-1886(13)70032-5, 
year = {2013}, 
title = {{Optimal investment paths during the life cycle of a multi-funds system [Trayectorias óptimas de inversión durante el ciclo de vida en un sistema de multifondos]}}, 
author = {Bedoya, Alejandra Arboleda and Quintero, Carlos Alberto Soto and Betancur, Juan Carlos Gutiérrez}, 
journal = {Journal of Economics Finance and Administrative Science}, 
issn = {20771886}, 
doi = {10.1016/s2077-1886(13)70032-5}, 
abstract = {{Taking into account the new Colombian multi-funds scheme of the Individual Benefits Plan, an analysis was made through the application of stochastic and actuarial tools with the purpose of determining the moment in which agents (according to their social characteristics: sex, income expectation, moment they begin to pay contributions, contribution probability) must transfer their retirement account into the different offered portfolios. Considering the profitability and the risk the agents want to assume, as key elements, it is generally observable that they must remain a great percentage of their working lives in the high-risk investment fund to achieve a greater financial capital accumulation and a greater probability to attain a reasonable replacement rate. © 2013 Universidad ESAN.}}, 
pages = {72--88}, 
number = {35}, 
volume = {18}
}
@article{10.1080/0740817x.2014.991476, 
year = {2015}, 
title = {{Loss-constrained minimum cost flow under arc failure uncertainty with applications in risk-aware kidney exchange}}, 
author = {Zheng, Qipeng P. and Shen, Siqian and Shi, Yuhui}, 
journal = {IIE Transactions}, 
issn = {0740817X}, 
doi = {10.1080/0740817x.2014.991476}, 
abstract = {{In this article, we study a Stochastic Minimum Cost Flow (SMCF) problem under arc failure uncertainty, where an arc flow solution may correspond to multiple path flow representations. We assume that the failure of an arc will cause flow losses on all paths using that arc, and for any path carrying positive flows, the failure of any arc on the path will lose all flows carried by the path. We formulate two SMCF variants to minimize the cost of arc flows, while respectively restricting the Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) of random path flow losses due to uncertain arc failure (reflected as network topological changes). We formulate a linear program to compute possible losses, yielding a mixed-integer programming formulation of SMCF-VaR and a linear programming formulation of SMCF-CVaR. We present a kidney exchange problem under uncertain match failure as an application and use the two SMCF models to maximize the utility/social welfare of pairing kidneys subject to constrained risk of utility losses. Our results show the efficacy of our approaches, the conservatism of using CVaR, and optimal flow patterns given by VaR and CVaR models on diverse instances. Copyright © "IIE" 2015.}}, 
pages = {961--977}, 
number = {9}, 
volume = {47}
}
@article{10.1016/j.ememar.2014.11.001, 
year = {2015}, 
title = {{Are the KOSPI 200 implied volatilities useful in value-at-risk models?}}, 
author = {Kim, Jun Sik and Ryu, Doojin}, 
journal = {Emerging Markets Review}, 
issn = {15660141}, 
doi = {10.1016/j.ememar.2014.11.001}, 
abstract = {{In terms of quantifying market risk, this study examines the information and indication embedded in implied volatilities extracted from the KOSPI 200 options and proposes a modified value-at-risk (VaR) framework utilizing the implied volatilities. Our empirical results indicate that the model-free implied volatility index of the KOSPI 200 (VKOSPI) does not greatly enhance the performance of suggested VaR models, compared with other volatility forecasting models, especially during and after the recent financial crisis. Furthermore, under the VaR framework, the VKOSPI does not perform better than Black-Scholes (BS) implied volatilities in measuring market risk. We also find that before the financial crisis, the BS implied volatility of out-of-the-money (OTM) options yields a better performance of the VaR models than the BS implied volatility of at-the-money (ATM) options. However, during and after the crisis, the VaR models incorporating the BS ATM implied volatility outperform the VaR models incorporating the BS OTM implied volatility. Our additional analyses show that combining with an extended GJR-GARCH model, which captures the asymmetric volatility effect, improves the overall performance of VaR models. © 2014 Elsevier B.V.}}, 
pages = {43--64}, 
number = {NA}, 
volume = {22}
}
@article{10.1111/1467-9965.00121, 
year = {2001}, 
title = {{Optimal portfolios with bounded capital at risk}}, 
author = {Emmer, Susanne and Klüppelberg, Claudia and Korn, Ralf}, 
journal = {Mathematical Finance}, 
issn = {09601627}, 
doi = {10.1111/1467-9965.00121}, 
abstract = {{We consider some continuous-time Markowitz type portfolio problems that consist of maximizing expected terminal wealth under the constraint of an upper bound for the capital at risk. In a Black-Scholes setting we obtain closed-form explicit solutions and compare their form and implications to those of the classical continuous-time mean-variance problem. We also consider more general price processes that allow for larger fluctuations in the returns.}}, 
pages = {365--384}, 
number = {4}, 
volume = {11}
}
@article{10.1007/s00500-019-04376-7, 
year = {2020}, 
title = {{Duration gap analysis revisited method in order to improve risk management: the case of Chinese commercial bank interest rate risks after interest rate liberalization}}, 
author = {Ausloos, Marcel and Ma, Qianhui and Kaur, Parmjit and Syed, Babar and Dhesi, Gurjeet}, 
journal = {Soft Computing}, 
issn = {14327643}, 
doi = {10.1007/s00500-019-04376-7}, 
abstract = {{Modern theories attach much attention to interest rate-related problems. We discuss the impacts of the interest rate liberalization, in China, for ten commercial banks of three markedly different ownership types. The methodology is based on revisited interest rate sensitivity analysis, duration analysis and value-at-risk analysis. The situation is examined within both vertical (composition of operating income and interest rate sensitivity gap for the ten banks in the same year) and horizontal (one bank over a 7-year period) aspects. Thereafter, we discuss the present management of interest rate risks by such banks. We conclude with several suggestions on how such commercial banks risk management can be refocused and on how their cases can be used for comforting other banking cases. © 2019, The Author(s).}}, 
pages = {13609--13627}, 
number = {18}, 
volume = {24}
}
@article{10.21314/jor.2015.319, 
year = {2015}, 
title = {{A simple normal inverse Gaussian-type approach to calculate value-at-risk based on realized moments}}, 
author = {Lau, Christian}, 
journal = {The Journal of Risk}, 
issn = {14651211}, 
doi = {10.21314/jor.2015.319}, 
abstract = {{Expanding the realized variance concept through realized skewness and kurtosis is a straightforward process. We calculate one-day forecasts for these moments with a simple exponentially weighted moving average approach. Once the forecasts are computed, we apply them in a method of moments to fit a sophisticated distribution. The normal inverse Gaussian distribution is appropriate for this purpose because it exhibits higher moments, and a simple analytical solution for the method of moments exists.We then calculate the value-at-risk for the Deutsche Aktienindex (DAX) using this technique. Although the model is comparatively simple, the empirical analysis shows good results in terms of backtesting. © 2015 Incisive Risk Information (IP) Limited.}}, 
pages = {1--18}, 
number = {4}, 
volume = {17}
}
@article{10.35940/ijeat.f9568.088619, 
year = {2019}, 
title = {{Financial risk quantification of indian agro-commodities using value at risk}}, 
journal = {International Journal of Engineering and Advanced Technology}, 
issn = {22498958}, 
doi = {10.35940/ijeat.f9568.088619}, 
abstract = {{Indian commodity traders are exposed to various risks like price risk, market risk, financial risk, credit risk, etc. To understand the risk resulting in the financial impact, this paper attempts to assess the historical trends of commodity prices and probability of loss occurrence in the commodity invested. The present study analyses five Indian agro commodities namely, Almond, Cardamom, Cotton, Guar Seed and Wheat using the data collected from secondary sources like Multi Commodity Exchange (MCX), Securities Exchange Board of India (SEBI) etc. This paper uses the Historical Simulation method for the calculation of Value at Risk (VaR) by considering spot prices of the commodities on MCX for a five year period (2013-2018). It is established that Value at Risk (VaR) is a relevant measure to arrive at risk which is useful for the commodity traders to estimate the financial risk and thus control the risk exposure. ©BEIESP.}}, 
pages = {5138--5144}, 
number = {6}, 
volume = {8}
}